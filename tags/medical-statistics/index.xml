<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Medical Statistics | Be Ambitious</title>
    <link>https://wangcc.me/tags/medical-statistics/</link>
      <atom:link href="https://wangcc.me/tags/medical-statistics/index.xml" rel="self" type="application/rss+xml" />
    <description>Medical Statistics</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2017-2020 Chaochen Wang | 王超辰</copyright><lastBuildDate>Fri, 16 Aug 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://wangcc.me/img/icon-192.png</url>
      <title>Medical Statistics</title>
      <link>https://wangcc.me/tags/medical-statistics/</link>
    </image>
    
    <item>
      <title>等級線性回歸模型的 Rstan 貝葉斯實現</title>
      <link>https://wangcc.me/post/multilevel-model-rstan/</link>
      <pubDate>Fri, 16 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/multilevel-model-rstan/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#多層等級線性回歸模型混合效應模型-multilevelmixed-effect-regression-model&#34;&gt;多層(等級)線性回歸模型/混合效應模型 multilevel/mixed effect regression model&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#適用於等級線性回歸模型的數據&#34;&gt;適用於等級線性回歸模型的數據&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#確認數據分佈&#34;&gt;確認數據分佈&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#如果不考慮組間公司間差異&#34;&gt;如果不考慮組間(公司間)差異&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#如果要考慮組間差異&#34;&gt;如果要考慮組間差異&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#等級線性回歸的貝葉斯實現&#34;&gt;等級線性回歸的貝葉斯實現&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#模型機制-mechanism&#34;&gt;模型機制 mechanism&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;多層等級線性回歸模型混合效應模型-multilevelmixed-effect-regression-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;多層(等級)線性回歸模型/混合效應模型 multilevel/mixed effect regression model&lt;/h2&gt;
&lt;p&gt;關於等級線性回歸的基本知識和概念，請參考&lt;a href=&#34;https://wangcc.me/LSHTMlearningnote/Hierarchical.html&#34;&gt;讀書筆記58-60章節&lt;/a&gt;。簡單來說，等級線性回歸通過給數據內部可能存在或者已知存在的結構或者層級增加隨機截距或者隨機斜率的方式來輔助解釋組間差異和組內的差異。&lt;/p&gt;
&lt;div id=&#34;適用於等級線性回歸模型的數據&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;適用於等級線性回歸模型的數據&lt;/h3&gt;
&lt;p&gt;本章節使用的數據是四家大公司40名社員的年齡和年收入數據：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d &amp;lt;- read.csv(file=&amp;#39;../../static/files/data-salary-2.txt&amp;#39;)
d&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     X   Y KID
## 1   7 457   1
## 2  10 482   1
## 3  16 518   1
## 4  25 535   1
## 5   5 427   1
## 6  25 603   1
## 7  26 610   1
## 8  18 484   1
## 9  17 508   1
## 10  1 380   1
## 11  5 453   1
## 12  4 391   1
## 13 19 559   1
## 14 10 453   1
## 15 21 517   1
## 16 12 553   2
## 17 17 653   2
## 18 22 763   2
## 19  9 538   2
## 20 18 708   2
## 21 21 740   2
## 22  6 437   2
## 23 15 646   2
## 24  4 422   2
## 25  7 444   2
## 26 10 504   2
## 27  2 376   2
## 28 15 522   3
## 29 27 623   3
## 30 14 515   3
## 31 18 542   3
## 32 20 529   3
## 33 18 540   3
## 34 11 411   3
## 35 26 666   3
## 36 22 641   3
## 37 25 592   3
## 38 28 722   4
## 39 24 726   4
## 40 22 728   4&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;X&lt;/code&gt;: 社員年齡減去23獲得的數據（23歲是大部分人大學畢業入職時的年齡）&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Y&lt;/code&gt;: 年收入（萬日元）&lt;/li&gt;
&lt;li&gt;&lt;code&gt;KID&lt;/code&gt;: 公司編號&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我們認爲，年收入 &lt;code&gt;Y&lt;/code&gt;，是基本平均年收入和隨機誤差（服從均值爲零，方差是 &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; 的正態分佈）之和。且基本平均年收入和年齡成正比（年功序列型企業）。但是呢，因爲不同的公司入職時的基本收入可能不同，且可能隨着年齡增加而增長薪水的速度可能也不一樣。那麼由於不同公司所造成的差異，可以被認爲是組間差異。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;確認數據分佈&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;確認數據分佈&lt;/h3&gt;
&lt;p&gt;這次分析的目的是要瞭解「每個公司&lt;code&gt;KID&lt;/code&gt;內隨着年齡的增加而增長的薪水幅度是多少」，那麼我們要在結果報告中體現的就是每家公司的基本年收入，新入職時的年收入，以及隨着年齡增長而上升的薪水的事後分佈。&lt;/p&gt;
&lt;p&gt;我們先來看把四家公司職員放在一起時的整體圖形：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)

d$KID &amp;lt;- as.factor(d$KID)
res_lm &amp;lt;- lm(Y ~ X, data=d)
coef &amp;lt;- as.numeric(res_lm$coefficients)

p &amp;lt;- ggplot(d, aes(X, Y, shape=KID))
p &amp;lt;- p + theme_bw(base_size=18)
p &amp;lt;- p + geom_abline(intercept=coef[1], slope=coef[2], size=2, alpha=0.3)
p &amp;lt;- p + geom_point(size=2)
p &amp;lt;- p + scale_shape_manual(values=c(16, 2, 4, 9))
p &amp;lt;- p + labs(x=&amp;#39;X (age-23)&amp;#39;, y=&amp;#39;Y (10,000 Yen/year)&amp;#39;)
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:fig8-1&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-08-16-multilevel-model-rstan_files/figure-html/fig8-1-1.png&#34; alt=&#34;年齡和年收入的散點圖，不同點的形狀代表不同的公司編號。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: 年齡和年收入的散點圖，不同點的形狀代表不同的公司編號。
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;從總體的散點圖 &lt;a href=&#34;#fig:fig8-1&#34;&gt;1&lt;/a&gt; 來看，似乎年收入確實是隨着年齡增長而呈現直線增加的趨勢。但是公司編號 &lt;code&gt;KID = 4&lt;/code&gt; 的三名社員薪水似乎是在同一水平的並無明顯變化。這一點可以把四家公司社員的數據分開來看更加清晰:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- ggplot(d, aes(X, Y, shape=KID))
p &amp;lt;- p + theme_bw(base_size=20)
p &amp;lt;- p + geom_abline(intercept=coef[1], slope=coef[2], size=2, alpha=0.3)
p &amp;lt;- p + facet_wrap(~KID)
p &amp;lt;- p + geom_line(stat=&amp;#39;smooth&amp;#39;, method=&amp;#39;lm&amp;#39;, se=FALSE, size=1, color=&amp;#39;black&amp;#39;, linetype=&amp;#39;31&amp;#39;, alpha=0.8)
p &amp;lt;- p + geom_point(size=3)
p &amp;lt;- p + scale_shape_manual(values=c(16, 2, 4, 9))
p &amp;lt;- p + labs(x=&amp;#39;X (age-23)&amp;#39;, y=&amp;#39;Y (10,000 Yen/year)&amp;#39;)
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:fig8-2&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-08-16-multilevel-model-rstan_files/figure-html/fig8-2-1.png&#34; alt=&#34;年齡和年收入的散點圖，不同的公司在四個平面中展示,黑色點線是每家公司數據單獨使用線性回歸時獲得的直線。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: 年齡和年收入的散點圖，不同的公司在四個平面中展示,黑色點線是每家公司數據單獨使用線性回歸時獲得的直線。
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;如果不考慮組間公司間差異&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;如果不考慮組間(公司間)差異&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;模型的數學描述&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
              Y[n] &amp;amp; = y_{\text{base}}[n] + \varepsilon[n] &amp;amp; n = 1, \dots, N \\
y_{\text{base}}[n] &amp;amp; = a + bX[n]                           &amp;amp; n = 1, \dots, N \\
    \varepsilon[n] &amp;amp; \sim \text{Normal}(0, \sigma_Y^2)     &amp;amp; n = 1, \dots, N \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;當然，如果你想，模型可以直接簡化成：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y[n] \sim \text{Normal}(a + bX[n], \sigma^2_Y) \;\;\;\;\;\; n = 1, \dots, N \\
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;上述簡化版的模型，翻譯成Stan語言如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data {
  int N;
  real X[N];
  real Y[N];
}

parameters{
  real a;
  real b;
  real&amp;lt;lower = 0&amp;gt; s_Y;
}

model {
  for (n in 1 : N)
  Y[n] = normal(a + b * X[n], s_Y);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;下面的 R 代碼用來實現對上面 Stan 模型的擬合:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rstan)
d &amp;lt;- read.csv(file=&amp;#39;../../static/files/data-salary-2.txt&amp;#39;)
d$KID &amp;lt;- as.factor(d$KID)

data &amp;lt;- list(N=nrow(d), X=d$X, Y=d$Y)
fit &amp;lt;- stan(file=&amp;#39;stanfiles/model8-1.stan&amp;#39;, data=data, seed=1234)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &amp;#39;model8-1&amp;#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 9e-06 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.051415 seconds (Warm-up)
## Chain 1:                0.036278 seconds (Sampling)
## Chain 1:                0.087693 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &amp;#39;model8-1&amp;#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 4e-06 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.054178 seconds (Warm-up)
## Chain 2:                0.03123 seconds (Sampling)
## Chain 2:                0.085408 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &amp;#39;model8-1&amp;#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 4e-06 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.055162 seconds (Warm-up)
## Chain 3:                0.033103 seconds (Sampling)
## Chain 3:                0.088265 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &amp;#39;model8-1&amp;#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 4e-06 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.056718 seconds (Warm-up)
## Chain 4:                0.03356 seconds (Sampling)
## Chain 4:                0.090278 seconds (Total)
## Chain 4:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Stan model: model8-1.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##         mean se_mean    sd    2.5%     25%     50%     75%   97.5% n_eff Rhat
## a     376.97    0.58 24.54  329.21  360.86  377.27  393.00  425.66  1811    1
## b      10.99    0.03  1.41    8.25   10.07   10.98   11.94   13.78  1865    1
## s_Y    68.85    0.19  8.21   54.92   63.07   68.08   73.79   86.74  1938    1
## lp__ -184.12    0.03  1.29 -187.39 -184.69 -183.79 -183.17 -182.66  1450    1
## 
## Samples were drawn using NUTS(diag_e) at Tue Jan  7 14:55:40 2020.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;現在有更加方便的 &lt;code&gt;rstanarm&lt;/code&gt; 包可以幫助我們省去寫 Stan 模型的過程：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rstanarm)

rstanarm_results = stan_glm(Y ~ X, data=d, iter=2000, warmup=1000, cores=4)
summary(rstanarm_results, probs=c(.025, .975), digits=3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Model Info:
##  function:     stan_glm
##  family:       gaussian [identity]
##  formula:      Y ~ X
##  algorithm:    sampling
##  sample:       4000 (posterior sample size)
##  priors:       see help(&amp;#39;prior_summary&amp;#39;)
##  observations: 40
##  predictors:   2
## 
## Estimates:
##               mean    sd      2.5%    97.5%
## (Intercept) 376.287  23.966 329.948 423.392
## X            11.028   1.413   8.243  13.842
## sigma        67.884   8.214  54.036  86.018
## 
## Fit Diagnostics:
##            mean    sd      2.5%    97.5%
## mean_PPD 547.732  14.914 517.835 576.914
## 
## The mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help(&amp;#39;summary.stanreg&amp;#39;)).
## 
## MCMC diagnostics
##               mcse  Rhat  n_eff
## (Intercept)   0.416 0.999 3319 
## X             0.024 1.000 3490 
## sigma         0.145 1.000 3217 
## mean_PPD      0.234 1.000 4047 
## log-posterior 0.029 1.000 1758 
## 
## For each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;可以看到強制不同公司社員的年收入來自同一個正態分佈時，方差顯得非常的大。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;如果要考慮組間差異&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;如果要考慮組間差異&lt;/h3&gt;
&lt;p&gt;我們認爲每家公司社員新入職時的起點薪水不同(截距不同-隨機截距)，進入公司之後隨年齡增加的薪水幅度也不同(斜率不同-隨機斜率)。因此，用 &lt;span class=&#34;math inline&#34;&gt;\(a[1]\sim a[K], K = 1, 2, 3, 4\)&lt;/span&gt; 表示每家公司的截距，用 &lt;span class=&#34;math inline&#34;&gt;\(b[1] \sim b[K], K = 1, 2, 3, 4\)&lt;/span&gt; 表示每家公司薪水上升的斜率。那麼每家公司的薪水年齡線性回歸模型可以寫作是 &lt;span class=&#34;math inline&#34;&gt;\(a[K] + b[K] X, K = 1, 2, 3, 4\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;模型數學描述&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y[n] \sim \text{Normal}(a[\text{KID[n]}] + b[\text{KID}[n]] X[n], \sigma^2_Y) \\ n = 1, \dots, N
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;上述模型的 Stan 譯文如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data {
  int N;
  int K;
  real X[N];
  real Y[N];
  int&amp;lt;lower = 1, upper = K&amp;gt; KID[N];
}

parameters {
  real a[K];
  real b[K];
  real&amp;lt;lower = 0&amp;gt; s_Y; 
}

model {
  for (n in 1:N)
  Y[n] ~ normal(a[KID[n]] + b[KID[n]] * X[n], s_Y);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;下面的 R 代碼用來實現上面貝葉斯多組不同截距不同斜率線性回歸模型的擬合:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rstan)
d &amp;lt;- read.csv(file=&amp;#39;../../static/files/data-salary-2.txt&amp;#39;)

data &amp;lt;- list(N=nrow(d), X=d$X, Y=d$Y, KID = d$KID, K = 4)
fit &amp;lt;- stan(file=&amp;#39;stanfiles/model8-2.stan&amp;#39;, data=data, seed=1234)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &amp;#39;model8-2&amp;#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 1.2e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.291379 seconds (Warm-up)
## Chain 1:                0.247714 seconds (Sampling)
## Chain 1:                0.539093 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &amp;#39;model8-2&amp;#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 4e-06 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.307839 seconds (Warm-up)
## Chain 2:                0.19135 seconds (Sampling)
## Chain 2:                0.499189 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &amp;#39;model8-2&amp;#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 5e-06 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.345166 seconds (Warm-up)
## Chain 3:                0.177124 seconds (Sampling)
## Chain 3:                0.52229 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &amp;#39;model8-2&amp;#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 4e-06 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.325194 seconds (Warm-up)
## Chain 4:                0.206306 seconds (Sampling)
## Chain 4:                0.5315 seconds (Total)
## Chain 4:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Stan model: model8-2.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##         mean se_mean     sd    2.5%     25%     50%     75%   97.5% n_eff Rhat
## a[1]  387.11    0.27  14.17  359.31  377.78  387.20  396.47  415.13  2828    1
## a[2]  329.03    0.33  16.43  296.89  317.97  328.89  340.33  360.46  2517    1
## a[3]  314.11    0.75  34.43  246.03  291.69  313.70  335.87  382.48  2129    1
## a[4]  748.39    2.88 153.50  446.07  643.82  746.53  853.26 1054.33  2834    1
## b[1]    7.52    0.02   0.87    5.82    6.92    7.52    8.10    9.26  2604    1
## b[2]   19.82    0.02   1.21   17.47   19.01   19.82   20.63   22.20  2438    1
## b[3]   12.44    0.03   1.70    9.08   11.34   12.44   13.57   15.82  2595    1
## b[4]   -0.93    0.12   6.20  -13.37   -5.14   -0.91    3.22   11.19  2827    1
## s_Y    27.17    0.07   3.58   21.23   24.68   26.82   29.30   35.16  2695    1
## lp__ -148.01    0.06   2.40 -153.78 -149.35 -147.61 -146.26 -144.55  1414    1
## 
## Samples were drawn using NUTS(diag_e) at Tue Jan  7 14:56:32 2020.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;等級線性回歸的貝葉斯實現&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;等級線性回歸的貝葉斯實現&lt;/h2&gt;
&lt;div id=&#34;模型機制-mechanism&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;模型機制 mechanism&lt;/h3&gt;
&lt;p&gt;如果我們認爲每家公司的起點薪水 &lt;span class=&#34;math inline&#34;&gt;\(a[k]\)&lt;/span&gt; 服從正態分佈，且該正態分佈的平均值是全體公司的起點薪水的均值 &lt;span class=&#34;math inline&#34;&gt;\(a_\mu\)&lt;/span&gt;，方差是 &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_a\)&lt;/span&gt;。類似地，假設每家公司內隨着年齡增長而增加薪水的幅度 &lt;span class=&#34;math inline&#34;&gt;\(b[k]\)&lt;/span&gt; 也服從某個正態分佈，均值和方差分別是 &lt;span class=&#34;math inline&#34;&gt;\(b_\mu, \sigma^2_b\)&lt;/span&gt;。這樣我們就不僅僅是允許了各家公司的薪水年齡回歸直線擁有不同的斜率和截距，還對這些隨機斜率和截距的前概率分佈進行了設定。&lt;/p&gt;
&lt;p&gt;此時，隨機效應模型的數學表達式就可以寫成下面這樣:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
Y[n] &amp;amp;\sim \text{Normal}(a[\text{KID[n]}] + b[\text{KID}[n]] X[n], \sigma^2_Y) &amp;amp; n = 1, \dots, N \\
a[k] &amp;amp;= a_\mu + a_\varepsilon[k]   &amp;amp; k = 1, \dots, K \\
a_\varepsilon[k] &amp;amp; \sim \text{Normal}(0, \sigma^2_a) &amp;amp; k = 1, \dots, K \\
b[k] &amp;amp; = b_\mu + b_\varepsilon[k]  &amp;amp; k = 1, \dots, K \\
b_\varepsilon[k] &amp;amp;\sim \text{Normal}(0, \sigma^2_b) &amp;amp; k = 1, \dots, K
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;使用 &lt;code&gt;rstanarm&lt;/code&gt; 包可以使用下面的代碼實現&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rstanarm)
M1_stanlmer &amp;lt;- stan_lmer(formula = Y ~ X  + (X | KID), 
                            data = d,
                            seed = 1234)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &amp;#39;continuous&amp;#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 8.1e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.81 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 5.02763 seconds (Warm-up)
## Chain 1:                1.63677 seconds (Sampling)
## Chain 1:                6.6644 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &amp;#39;continuous&amp;#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 1.6e-05 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 6.86217 seconds (Warm-up)
## Chain 2:                2.63666 seconds (Sampling)
## Chain 2:                9.49883 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &amp;#39;continuous&amp;#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 1.7e-05 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.17 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 4.64937 seconds (Warm-up)
## Chain 3:                2.72015 seconds (Sampling)
## Chain 3:                7.36952 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &amp;#39;continuous&amp;#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 2e-05 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.2 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 5.35582 seconds (Warm-up)
## Chain 4:                2.13219 seconds (Sampling)
## Chain 4:                7.488 seconds (Total)
## Chain 4:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(M1_stanlmer, digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## stan_lmer
##  family:       gaussian [identity]
##  formula:      Y ~ X + (X | KID)
##  observations: 40
## ------
##             Median MAD_SD
## (Intercept) 359.02  15.02
## X            12.80   2.98
## 
## Auxiliary parameter(s):
##       Median MAD_SD
## sigma 30.00   3.58 
## 
## Error terms:
##  Groups   Name        Std.Dev. Corr 
##  KID      (Intercept) 23.94         
##           X            8.76    -0.26
##  Residual             30.28         
## Num. levels: KID 4 
## 
## ------
## * For help interpreting the printed output see ?print.stanreg
## * For info on the priors used see ?prior_summary.stanreg&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(M1_stanlmer, 
        pars = c(&amp;quot;(Intercept)&amp;quot;, &amp;quot;X&amp;quot;,&amp;quot;sigma&amp;quot;, 
                 &amp;quot;Sigma[KID:(Intercept),(Intercept)]&amp;quot;,
                 &amp;quot;Sigma[KID:X,(Intercept)]&amp;quot;, &amp;quot;Sigma[KID:X,X]&amp;quot;),
        probs = c(0.025, 0.975),
        digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Model Info:
##  function:     stan_lmer
##  family:       gaussian [identity]
##  formula:      Y ~ X + (X | KID)
##  algorithm:    sampling
##  sample:       4000 (posterior sample size)
##  priors:       see help(&amp;#39;prior_summary&amp;#39;)
##  observations: 40
##  groups:       KID (4)
## 
## Estimates:
##                                      mean    sd      2.5%    97.5%
## (Intercept)                         358.72   18.63  322.31  394.35
## X                                    12.59    3.97    3.57   20.91
## sigma                                30.28    3.82   23.76   38.95
## Sigma[KID:(Intercept),(Intercept)]  572.98 1381.86    1.70 3803.32
## Sigma[KID:X,(Intercept)]            -54.05  190.09 -444.45  144.95
## Sigma[KID:X,X]                       76.74  130.06    6.91  412.48
## 
## MCMC diagnostics
##                                    mcse  Rhat  n_eff
## (Intercept)                         0.38  1.00 2464 
## X                                   0.14  1.00  771 
## sigma                               0.08  1.00 2032 
## Sigma[KID:(Intercept),(Intercept)] 33.93  1.01 1659 
## Sigma[KID:X,(Intercept)]            4.80  1.00 1569 
## Sigma[KID:X,X]                      4.34  1.01  898 
## 
## For each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;和非貝葉斯版本的概率論隨機效應線性回歸模型的結果相對比一下：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lme4)
M1 &amp;lt;- lmer(formula = Y ~ X  + (X | KID), 
           data = d, 
           REML = TRUE)
summary(M1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by REML [&amp;#39;lmerMod&amp;#39;]
## Formula: Y ~ X + (X | KID)
##    Data: d
## 
## REML criterion at convergence: 387.2
## 
## Scaled residuals: 
##      Min       1Q   Median       3Q      Max 
## -2.36969 -0.51837 -0.03545  0.76358  1.87881 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev. Corr 
##  KID      (Intercept) 503.78   22.445        
##           X            28.53    5.341   -1.00
##  Residual             833.95   28.878        
## Number of obs: 40, groups:  KID, 4
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)  358.207     15.383  23.286
## X             13.067      2.741   4.767
## 
## Correlation of Fixed Effects:
##   (Intr)
## X -0.848
## convergence code: 0
## boundary (singular) fit: see ?isSingular&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>泊松回歸模型的貝葉斯Stan實現</title>
      <link>https://wangcc.me/post/poisson-stan/</link>
      <pubDate>Thu, 11 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/poisson-stan/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#分析目的數據和選擇-poisson-回歸模型的原因&#34;&gt;分析目的，數據，和選擇 Poisson 回歸模型的原因&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#想象模型機制&#34;&gt;想象模型機制&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#寫下數學模型表達式&#34;&gt;寫下數學模型表達式&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#把數學模型翻譯成-stan-模型代碼&#34;&gt;把數學模型翻譯成 Stan 模型代碼&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#運行結果的解釋&#34;&gt;運行結果的解釋&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;分析目的數據和選擇-poisson-回歸模型的原因&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;分析目的，數據，和選擇 Poisson 回歸模型的原因&lt;/h1&gt;
&lt;p&gt;我們這裏使用&lt;a href=&#34;https://wangcc.me/post/logistic-rstan/&#34;&gt;之前擬合貝葉斯邏輯回歸模型&lt;/a&gt;時使用的相同的數據來展示如何跑貝葉斯泊松回歸模型。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d &amp;lt;- read.table(&amp;quot;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap05/input/data-attendance-2.txt&amp;quot;, sep = &amp;quot;,&amp;quot;, header = T)
head(d)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   PersonID A Score  M  Y
## 1        1 0    69 43 38
## 2        2 1   145 56 40
## 3        3 0   125 32 24
## 4        4 1    86 45 33
## 5        5 1   158 33 23
## 6        6 0   133 61 60&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;PersonID&lt;/code&gt;: 是學生的編號；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;A&lt;/code&gt;, &lt;code&gt;Score&lt;/code&gt;: 用來預測出勤率的兩個預測變量，分別是表示是否喜歡打工的 &lt;code&gt;A&lt;/code&gt;，和表示對學習本身是否喜歡的評分 (滿分200)；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;M&lt;/code&gt;: 過去三個月內，該名學生一共需要上課的總課時數；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Y&lt;/code&gt;: 過去三個月內，該名學生實際上出勤的課時數。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;這一次我們希望通過分析泊松回歸來回答「&lt;code&gt;A&lt;/code&gt; 和 &lt;code&gt;Score&lt;/code&gt; 對總課時數 &lt;code&gt;M&lt;/code&gt; 具體有多大的影響？」這個問題。&lt;a href=&#34;https://wangcc.me/post/logistic-rstan/&#34;&gt;之前擬合貝葉斯邏輯回歸模型&lt;/a&gt;時，使用的結果變量是 &lt;code&gt;Y&lt;/code&gt;，也就是實際出勤課時數。但是本小節我們用 &lt;code&gt;M&lt;/code&gt; 作爲結果變量。因爲總課時數是學生自己選課時的結果，也就是說學生本身的態度（是否喜歡打工，是否熱愛學習），可能本身左右了他/她到底會選多少課。背景知識假設是：喜歡多去打工的學生，選課可能態度消極，總課時數從開始可能就選的少。那麼像總選課時數這樣的非負（計數型）離散變量作爲結果變量的時候，&lt;strong&gt;泊松回歸模型是我們的第一選擇。&lt;/strong&gt;&lt;/p&gt;
&lt;div id=&#34;想象模型機制&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;想象模型機制&lt;/h2&gt;
&lt;p&gt;如果使用&lt;a href=&#34;https://wangcc.me/post/rstan-wonderful-r3/&#34;&gt;上上節介紹的多重線性回歸模型&lt;/a&gt;，那麼模型的預測變量的分佈便可能取到負數，這樣就不符合實際情況下“總選課時數”是非負（計數型）離散變量這一事實。這就需要把預測變量 &lt;code&gt;A&lt;/code&gt; 和 &lt;code&gt;Score&lt;/code&gt; 相加的線性模型 &lt;span class=&#34;math inline&#34;&gt;\((b_1 + b_2A + b_3Score)\)&lt;/span&gt;，通過數學轉換限制在非負數範圍。假設平均總課時數是 &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;，我們認爲它服從均值是 &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; 的泊松分佈。關於泊松分佈的詳細知識，期望值和方差的推導可以參考&lt;a href=&#34;https://wangcc.me/LSHTMlearningnote/poisson.html&#34;&gt;學習筆記&lt;/a&gt;。另外，非貝葉斯版本的一般性傳統泊松回歸模型可以參照學習筆記的&lt;a href=&#34;https://wangcc.me/LSHTMlearningnote/poisson-regression.html&#34;&gt;廣義線性回歸的泊松回歸模型章節&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;對泊松回歸模型略有瞭解的話應該很自然地想到，把結果變量限制在非負數範圍的標準鏈接方程是 &lt;span class=&#34;math inline&#34;&gt;\(\log(\lambda)\)&lt;/span&gt;，或者在 Stan 模型中，我們更自然地把線性模型部分寫在指數模型中: &lt;span class=&#34;math inline&#34;&gt;\(\exp(b_1 + b_2A + b_3Score)\)&lt;/span&gt;。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;寫下數學模型表達式&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;寫下數學模型表達式&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\lambda[n] &amp;amp; = \exp(b_1 + b_2A[n] + b_3Score[n]) &amp;amp; n = 1, \dots, N \\
M[n]       &amp;amp; \sim \text{Poisson}(\lambda[n])     &amp;amp; n = 1, \dots, N
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;，是該數據中學生的人數；&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;，是每名學生的標籤/編號（下標）；&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(b_1, b_2, b_3\)&lt;/span&gt; 是我們感興趣的參數。&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;把數學模型翻譯成-stan-模型代碼&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;把數學模型翻譯成 Stan 模型代碼&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;data {
  int N; 
  int&amp;lt;lower=0, upper=1&amp;gt; A[N]; 
  real&amp;lt;lower=0, upper=1&amp;gt; Score[N]; 
  int&amp;lt;lower=0&amp;gt; M[N];
}

parameters {
  real b[3]; 
}

transformed parameters {
  real lambda[N];
  for (n in 1:N) {
    lambda[n] = exp(b[1] + b[2]*A[n] + b[3]*Score[n]);
  }
}

model {
  for (n in 1:N) {
    M[n] ~ poisson(lambda[n]); 
  }
}

generated quantities {
  int m_pred[N]; 
  for (n in 1:N) {
    m_pred[n] = poisson_rng(M[n], q[n]);
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;值得一提的是，在 Stan 中，提供了 &lt;code&gt;poisson_log(x)&lt;/code&gt; 分佈函數，其實它等價於使用 &lt;code&gt;poisson(exp(x))&lt;/code&gt;。除了更加接近我們熟悉的泊松回歸模型的數學表達式，避免了 &lt;code&gt;exp&lt;/code&gt; 指數運算，計算結果穩定。於是我們還可以把上面的模型修改成：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data {
  int N; 
  int&amp;lt;lower=0, upper=1&amp;gt; A[N]; 
  real&amp;lt;lower=0, upper=1&amp;gt; Score[N]; 
  int&amp;lt;lower=0&amp;gt; M[N];
}

parameters {
  real b[3]; 
}

transformed parameters {
  real lambda[N];
  for (n in 1:N) {
    lambda[n] = b[1] + b[2]*A[n] + b[3]*Score[n]；
  }
}

model {
  for (n in 1:N) {
    M[n] ~ poisson_log(lambda[n]); 
  }
}

generated quantities {
  int m_pred[N]; 
  for (n in 1:N) {
    m_pred[n] = poisson_log_rng(M[n], q[n]);
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;運行它的代碼如下：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rstan)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: StanHeaders&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggplot2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## rstan (Version 2.19.2, GitRev: 2e1f913d3ca3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## For execution on a local, multicore CPU with excess RAM we recommend calling
## options(mc.cores = parallel::detectCores()).
## To avoid recompilation of unchanged Stan programs, we recommend calling
## rstan_options(auto_write = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data &amp;lt;- list(N=nrow(d), A=d$A, Score=d$Score/200, M=d$M)
# fit &amp;lt;- stan(file=&amp;#39;model/model5-6.stan&amp;#39;, data=data, seed=1234)
fit &amp;lt;- stan(file=&amp;#39;stanfiles/model5-6b.stan&amp;#39;, data=data, seed=1234, pars = c(&amp;quot;b&amp;quot;, &amp;quot;lambda&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &amp;#39;model5-6b&amp;#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 1.3e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.101417 seconds (Warm-up)
## Chain 1:                0.11273 seconds (Sampling)
## Chain 1:                0.214147 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &amp;#39;model5-6b&amp;#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 9e-06 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.103552 seconds (Warm-up)
## Chain 2:                0.113478 seconds (Sampling)
## Chain 2:                0.21703 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &amp;#39;model5-6b&amp;#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 6e-06 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.106149 seconds (Warm-up)
## Chain 3:                0.103742 seconds (Sampling)
## Chain 3:                0.209891 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &amp;#39;model5-6b&amp;#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 7e-06 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.10037 seconds (Warm-up)
## Chain 4:                0.11621 seconds (Sampling)
## Chain 4:                0.21658 seconds (Total)
## Chain 4:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Stan model: model5-6b.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##               mean se_mean   sd    2.5%     25%     50%     75%   97.5% n_eff
## b[1]          3.58    0.00 0.10    3.38    3.51    3.58    3.64    3.76  1187
## b[2]          0.26    0.00 0.04    0.18    0.24    0.26    0.29    0.35  1831
## b[3]          0.29    0.00 0.15    0.01    0.19    0.29    0.39    0.58  1227
## lambda[1]     3.68    0.00 0.05    3.58    3.65    3.68    3.71    3.77  1276
## lambda[2]     4.05    0.00 0.03    3.98    4.03    4.05    4.07    4.12  2189
## lambda[3]     3.76    0.00 0.03    3.70    3.74    3.76    3.78    3.81  2467
## lambda[4]     3.97    0.00 0.04    3.88    3.94    3.97    3.99    4.05  1735
## lambda[5]     4.07    0.00 0.04    3.99    4.04    4.07    4.10    4.15  1793
## lambda[6]     3.77    0.00 0.03    3.71    3.75    3.77    3.79    3.83  2509
## lambda[7]     3.74    0.00 0.03    3.68    3.72    3.74    3.76    3.79  1991
## lambda[8]     4.05    0.00 0.03    3.98    4.03    4.06    4.08    4.12  2102
## lambda[9]     3.79    0.00 0.03    3.72    3.77    3.79    3.81    3.85  2282
## lambda[10]    3.79    0.00 0.03    3.72    3.77    3.79    3.81    3.85  2305
## lambda[11]    4.05    0.00 0.03    3.98    4.02    4.05    4.07    4.11  2347
## lambda[12]    3.78    0.00 0.03    3.72    3.76    3.78    3.80    3.83  2465
## lambda[13]    4.01    0.00 0.03    3.95    3.99    4.01    4.03    4.07  2499
## lambda[14]    3.74    0.00 0.03    3.68    3.72    3.74    3.76    3.79  1991
## lambda[15]    3.73    0.00 0.03    3.68    3.72    3.74    3.76    3.79  1910
## lambda[16]    3.98    0.00 0.04    3.91    3.96    3.98    4.01    4.05  2088
## lambda[17]    3.74    0.00 0.03    3.68    3.72    3.74    3.76    3.80  2122
## lambda[18]    3.70    0.00 0.04    3.61    3.67    3.70    3.72    3.78  1360
## lambda[19]    3.85    0.00 0.05    3.74    3.81    3.85    3.88    3.95  1653
## lambda[20]    4.07    0.00 0.04    3.99    4.04    4.07    4.09    4.14  1835
## lambda[21]    3.97    0.00 0.04    3.88    3.94    3.97    3.99    4.05  1735
## lambda[22]    4.00    0.00 0.03    3.93    3.98    4.00    4.02    4.06  2298
## lambda[23]    3.99    0.00 0.03    3.93    3.97    3.99    4.02    4.06  2252
## lambda[24]    4.05    0.00 0.03    3.98    4.03    4.05    4.07    4.11  2283
## lambda[25]    4.01    0.00 0.03    3.95    3.99    4.01    4.03    4.07  2483
## lambda[26]    4.03    0.00 0.03    3.97    4.01    4.03    4.05    4.09  2548
## lambda[27]    3.75    0.00 0.03    3.69    3.73    3.75    3.77    3.80  2321
## lambda[28]    3.75    0.00 0.03    3.69    3.73    3.75    3.77    3.80  2321
## lambda[29]    3.81    0.00 0.04    3.73    3.78    3.81    3.84    3.89  1971
## lambda[30]    3.74    0.00 0.03    3.68    3.72    3.74    3.76    3.80  2077
## lambda[31]    4.08    0.00 0.04    4.00    4.05    4.08    4.11    4.17  1672
## lambda[32]    3.95    0.00 0.05    3.85    3.92    3.95    3.98    4.04  1571
## lambda[33]    4.04    0.00 0.03    3.98    4.02    4.04    4.06    4.10  2409
## lambda[34]    4.02    0.00 0.03    3.96    4.00    4.02    4.04    4.08  2554
## lambda[35]    3.76    0.00 0.03    3.70    3.74    3.76    3.78    3.81  2482
## lambda[36]    3.77    0.00 0.03    3.71    3.75    3.77    3.79    3.82  2516
## lambda[37]    3.99    0.00 0.03    3.93    3.97    3.99    4.02    4.06  2229
## lambda[38]    3.74    0.00 0.03    3.68    3.72    3.74    3.76    3.79  1950
## lambda[39]    3.71    0.00 0.04    3.63    3.68    3.71    3.73    3.78  1437
## lambda[40]    3.70    0.00 0.04    3.63    3.68    3.70    3.73    3.78  1424
## lambda[41]    3.76    0.00 0.03    3.71    3.75    3.76    3.78    3.82  2512
## lambda[42]    3.77    0.00 0.03    3.71    3.75    3.77    3.79    3.83  2509
## lambda[43]    3.75    0.00 0.03    3.70    3.74    3.75    3.77    3.81  2404
## lambda[44]    3.79    0.00 0.03    3.72    3.77    3.79    3.82    3.86  2236
## lambda[45]    3.84    0.00 0.05    3.74    3.81    3.84    3.88    3.95  1683
## lambda[46]    3.73    0.00 0.03    3.67    3.71    3.73    3.75    3.79  1772
## lambda[47]    3.65    0.00 0.06    3.53    3.61    3.65    3.69    3.77  1222
## lambda[48]    3.79    0.00 0.04    3.73    3.77    3.79    3.82    3.86  2191
## lambda[49]    3.72    0.00 0.03    3.65    3.70    3.72    3.74    3.79  1613
## lambda[50]    3.98    0.00 0.04    3.91    3.96    3.98    4.01    4.05  2088
## lp__       6896.53    0.03 1.28 6893.17 6895.97 6896.87 6897.45 6897.95  1452
##            Rhat
## b[1]          1
## b[2]          1
## b[3]          1
## lambda[1]     1
## lambda[2]     1
## lambda[3]     1
## lambda[4]     1
## lambda[5]     1
## lambda[6]     1
## lambda[7]     1
## lambda[8]     1
## lambda[9]     1
## lambda[10]    1
## lambda[11]    1
## lambda[12]    1
## lambda[13]    1
## lambda[14]    1
## lambda[15]    1
## lambda[16]    1
## lambda[17]    1
## lambda[18]    1
## lambda[19]    1
## lambda[20]    1
## lambda[21]    1
## lambda[22]    1
## lambda[23]    1
## lambda[24]    1
## lambda[25]    1
## lambda[26]    1
## lambda[27]    1
## lambda[28]    1
## lambda[29]    1
## lambda[30]    1
## lambda[31]    1
## lambda[32]    1
## lambda[33]    1
## lambda[34]    1
## lambda[35]    1
## lambda[36]    1
## lambda[37]    1
## lambda[38]    1
## lambda[39]    1
## lambda[40]    1
## lambda[41]    1
## lambda[42]    1
## lambda[43]    1
## lambda[44]    1
## lambda[45]    1
## lambda[46]    1
## lambda[47]    1
## lambda[48]    1
## lambda[49]    1
## lambda[50]    1
## lp__          1
## 
## Samples were drawn using NUTS(diag_e) at Tue Jan  7 14:46:49 2020.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;運行結果的解釋&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;運行結果的解釋&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;...{省略}...
              mean se_mean   sd    2.5%     25%     50%     75%   97.5% n_eff Rhat
b[1]          3.58    0.00 0.09    3.38    3.51    3.58    3.64    3.76  1373    1
b[2]          0.26    0.00 0.04    0.18    0.24    0.26    0.29    0.35  1797    1
b[3]          0.29    0.00 0.15    0.00    0.20    0.29    0.39    0.59  1422    1
lambda[1]     3.68    0.00 0.05    3.58    3.65    3.68    3.71    3.77  1510    1
...{省略}...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;我們把計算獲得的事後概率分佈均值放入前面寫下的數學表達式:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\lambda[n] &amp;amp; = \exp(3.58 + 0.26A[n] + 0.29Score[n]/200) &amp;amp; n = 1, \dots, N \\
M[n]       &amp;amp; \sim \text{Poisson}(\lambda[n])     &amp;amp; n = 1, \dots, N
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;例如說，&lt;code&gt;Score = 150&lt;/code&gt; 和 &lt;code&gt;Score = 50&lt;/code&gt; 的兩名學生，如果對打工喜好態度相同的話，他們之間選課的總課時數之比爲：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\frac{M_\text{Score = 150}}{M_\text{Score = 50}} &amp;amp; = \frac{\exp(3.58 + 0.26A + 0.29\times\frac{150}{200})}{\exp(3.58 + 0.26A + 0.29\times\frac{50}{200})} \\ 
&amp;amp; = \exp(0.29\times\frac{150-50}{200}) \approx 1.16
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;也就是熱愛學習分數 &lt;code&gt;Score&lt;/code&gt; 達到150的人和只有50的人相比，選課總課時數平均多 16%。相似地，喜歡打工 &lt;code&gt;A = 1&lt;/code&gt; 的學生和不喜歡打工 &lt;code&gt;A = 0&lt;/code&gt; 的學生選課總課時數之比爲 &lt;span class=&#34;math inline&#34;&gt;\(\exp(0.26)\approx1.30\)&lt;/span&gt;。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Rstan Wonderful R-(5)</title>
      <link>https://wangcc.me/post/logistic-rstan2/</link>
      <pubDate>Sun, 03 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/logistic-rstan2/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#另一種形式的貝葉斯邏輯回歸&#34;&gt;另一種形式的貝葉斯邏輯回歸&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#分析的目的&#34;&gt;分析的目的&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#確認數據分佈&#34;&gt;確認數據分佈&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#思考數據模型&#34;&gt;思考數據模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#寫下-stan-模型代碼&#34;&gt;寫下 Stan 模型代碼&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#檢查模型參數的收斂情況&#34;&gt;檢查模型參數的收斂情況&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#檢查模型的擬合情況&#34;&gt;檢查模型的擬合情況&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;另一種形式的貝葉斯邏輯回歸&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;另一種形式的貝葉斯邏輯回歸&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://wangcc.me/post/logistic-rstan/&#34;&gt;前面一節&lt;/a&gt;使用的數據是以學生爲單位，將每名學生的實際課時數和實際出勤數進行了彙總之後的總結性數據，本章我們來看看相同數據的另一種形式。由於分析中有人建議說，天氣狀況對出勤率也是有較大的影響的，所以希望在&lt;a href=&#34;https://wangcc.me/post/logistic-rstan/&#34;&gt;前一節&lt;/a&gt;已有的邏輯回歸模型中增加對天氣狀況的調整。那麼這時候需要使用的就是彙總之前的數據，也就是要是用實際記錄了每名學生每一次課時的出勤與否的原始數據。值得注意的是，這時候&lt;a href=&#34;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap05/input/data-attendance-3.txt&#34;&gt;原始數據&lt;/a&gt;中每名學生的記錄有許多行，因爲每行記錄的是該名學生每次上課時的天氣狀況和他/她是否出勤(0,1)的結果。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d &amp;lt;- read.table(&amp;quot;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap05/input/data-attendance-3.txt&amp;quot;, 
                     sep = &amp;quot;,&amp;quot;, header = T)
head(d, 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    PersonID A Score Weather Y
## 1         1 0    69       B 1
## 2         1 0    69       A 1
## 3         1 0    69       C 1
## 4         1 0    69       A 1
## 5         1 0    69       B 1
## 6         1 0    69       B 1
## 7         1 0    69       C 0
## 8         1 0    69       B 1
## 9         1 0    69       A 1
## 10        1 0    69       A 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Weather\)&lt;/span&gt;，天氣數據 (&lt;code&gt;A&lt;/code&gt; = 晴天，&lt;code&gt;B&lt;/code&gt; = 多雲，&lt;code&gt;C&lt;/code&gt; = 下雨)；&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;，該次課時學生是否出勤 (&lt;code&gt;0&lt;/code&gt; = 缺勤，&lt;code&gt;1&lt;/code&gt; = 出勤)。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其他的數據和前一節中使用的數據相同。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;分析的目的&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;分析的目的&lt;/h1&gt;
&lt;p&gt;本次數據分析的目的依然是瞭解幾個預測變量，天氣，是否喜歡打工，是否熱愛學習，對學生出勤率的影響。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;確認數據分佈&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;確認數據分佈&lt;/h1&gt;
&lt;p&gt;你可以用先進的 &lt;code&gt;tidyverse&lt;/code&gt; 進行簡單的數據彙總，看看天氣狀況不同時實際出勤率是否有差別:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
d %&amp;gt;% 
  group_by(Weather, Y) %&amp;gt;% 
  summarise (n= n()) %&amp;gt;%
  mutate(rel.freq = paste0(round(100 * n/sum(n), 2), &amp;quot;%&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 4
## # Groups:   Weather [3]
##   Weather     Y     n rel.freq
##   &amp;lt;fct&amp;gt;   &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;   
## 1 A           0   306 24.31%  
## 2 A           1   953 75.69%  
## 3 B           0   230 31.51%  
## 4 B           1   500 68.49%  
## 5 C           0   138 33.91%  
## 6 C           1   269 66.09%&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果你不想學習 &lt;code&gt;tidyverse&lt;/code&gt;，也可以用下面的方法獲得類似的效果，&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;aggregate(Y ~ Weather, data = d, FUN = table)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Weather Y.0 Y.1
## 1       A 306 953
## 2       B 230 500
## 3       C 138 269&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;無論是哪種方法，我們都能大概猜出，天氣是晴天的時候 (&lt;code&gt;Weather = A&lt;/code&gt;)，出勤率相對較高。&lt;/p&gt;
&lt;p&gt;在作者的原著中，&lt;a href=&#34;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap05/run-model5-5.R&#34;&gt;使用的是給分類型變量強制賦予連續值的方法&lt;/a&gt;，這點確實有點噁心，爲了正常的模型，我們需要把天氣轉換成爲更加常見的啞變量 (dummy variable) 如下:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d &amp;lt;- fastDummies::dummy_cols(d, select_columns = &amp;quot;Weather&amp;quot;)
head(d)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   PersonID A Score Weather Y Weather_A Weather_B Weather_C
## 1        1 0    69       B 1         0         1         0
## 2        1 0    69       A 1         1         0         0
## 3        1 0    69       C 1         0         0         1
## 4        1 0    69       A 1         1         0         0
## 5        1 0    69       B 1         0         1         0
## 6        1 0    69       B 1         0         1         0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;思考數據模型&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;思考數據模型&lt;/h1&gt;
&lt;p&gt;我們設想的數學模型應該是這樣子的:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\text{logit}(q[i]) &amp;amp; = b_{1} + b_{2}A_{i} + b_{3}\text{Score}_{i} + b_{4}\text{WeatherB} + b_{5}\text{WeatherC} \\ 
\text{where} &amp;amp; \\ 
&amp;amp; \text{ WeatherB} = 0, \text{ WeatherC} = 0 \text{ indicates weather = A} \\ 
&amp;amp; \text{ WeatherB} = 1, \text{ WeatherC} = 0 \text{ indicates weather = B} \\ 
&amp;amp; \text{ WeatherB} = 0, \text{ WeatherC} = 1 \text{ indicates weather = C} \\
Y[i] &amp;amp;\sim \text{Bernulli}(q[i])
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;寫下-stan-模型代碼&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;寫下 Stan 模型代碼&lt;/h1&gt;
&lt;p&gt;下面是相應的 Stan 模型:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data {
  int I;
  int&amp;lt;lower=0, upper=1&amp;gt; A[I];
  real&amp;lt;lower=0, upper=1&amp;gt; Score[I];
  int&amp;lt;lower=0, upper=1&amp;gt; W_B[I];
  int&amp;lt;lower=0, upper=1&amp;gt; W_C[I];
  int&amp;lt;lower=0, upper=1&amp;gt; Y[I];
}

// The parameters accepted by the model. 
parameters {
  real b[5];
}

// The model to be estimated. 
model {
   for (i in 1:I)
    Y[i] ~ bernoulli_logit(b[1] + b[2]*A[i] + b[3]*Score[i] + b[4]*W_B[i] + b[5]*W_C[i]);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;和跑它們的 R 代碼&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rstan)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: StanHeaders&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## rstan (Version 2.19.2, GitRev: 2e1f913d3ca3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## For execution on a local, multicore CPU with excess RAM we recommend calling
## options(mc.cores = parallel::detectCores()).
## To avoid recompilation of unchanged Stan programs, we recommend calling
## rstan_options(auto_write = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;rstan&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:tidyr&amp;#39;:
## 
##     extract&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data &amp;lt;- list(I=nrow(d), A=d$A, Score=d$Score/200, 
             W_A=d$Weather_A, W_B = d$Weather_B, W_C = d$Weather_C, 
             Y=d$Y)
fit1 &amp;lt;- stan(file=&amp;#39;stanfiles/myex4.stan&amp;#39;, data=data, pars=c(&amp;#39;b&amp;#39;, &amp;quot;OR1&amp;quot;, &amp;quot;OR2&amp;quot;, &amp;quot;OR3&amp;quot;, &amp;quot;OR4&amp;quot;, &amp;quot;q&amp;quot;), seed=1234)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &amp;#39;myex4&amp;#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0.000882 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 8.82 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 10.175 seconds (Warm-up)
## Chain 1:                9.91225 seconds (Sampling)
## Chain 1:                20.0872 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &amp;#39;myex4&amp;#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 0.000481 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 4.81 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 9.50031 seconds (Warm-up)
## Chain 2:                10.1304 seconds (Sampling)
## Chain 2:                19.6307 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &amp;#39;myex4&amp;#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 0.000467 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 4.67 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 9.76009 seconds (Warm-up)
## Chain 3:                10.4794 seconds (Sampling)
## Chain 3:                20.2395 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &amp;#39;myex4&amp;#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 0.000714 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 7.14 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 9.27456 seconds (Warm-up)
## Chain 4:                11.3889 seconds (Sampling)
## Chain 4:                20.6635 seconds (Total)
## Chain 4:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Stan model: myex4.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##             mean se_mean   sd     2.5%      25%      50%      75%    97.5%
## b[1]        0.27    0.00 0.23    -0.18     0.12     0.28     0.43     0.72
## b[2]       -0.63    0.00 0.09    -0.81    -0.69    -0.63    -0.57    -0.44
## b[3]        1.95    0.01 0.37     1.22     1.71     1.96     2.20     2.69
## b[4]       -0.38    0.00 0.11    -0.59    -0.45    -0.38    -0.31    -0.18
## b[5]       -0.49    0.00 0.12    -0.74    -0.58    -0.49    -0.41    -0.25
## OR1         0.54    0.00 0.05     0.44     0.50     0.53     0.57     0.64
## OR2         7.56    0.06 2.92     3.40     5.52     7.08     9.05    14.79
## OR3         0.69    0.00 0.07     0.55     0.64     0.68     0.74     0.84
## OR4         0.61    0.00 0.08     0.48     0.56     0.61     0.66     0.78
## q[1]        0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.70
## q[2]        0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[3]        0.61    0.00 0.04     0.54     0.59     0.61     0.64     0.68
## q[4]        0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[5]        0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.70
## q[6]        0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.70
## q[7]        0.61    0.00 0.04     0.54     0.59     0.61     0.64     0.68
## q[8]        0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.70
## q[9]        0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[10]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[11]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[12]       0.61    0.00 0.04     0.54     0.59     0.61     0.64     0.68
## q[13]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[14]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[15]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[16]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[17]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[18]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.70
## q[19]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[20]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[21]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.70
## q[22]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.70
## q[23]       0.61    0.00 0.04     0.54     0.59     0.61     0.64     0.68
## q[24]       0.61    0.00 0.04     0.54     0.59     0.61     0.64     0.68
## q[25]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[26]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[27]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.70
## q[28]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[29]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.70
## q[30]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[31]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.70
## q[32]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[33]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.70
## q[34]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[35]       0.61    0.00 0.04     0.54     0.59     0.61     0.64     0.68
## q[36]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[37]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[38]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[39]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.70
## q[40]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[41]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[42]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.70
## q[43]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.70
## q[44]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[45]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[46]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[47]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[48]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[49]       0.66    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[50]       0.66    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[51]       0.66    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[52]       0.66    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[53]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[54]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[55]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[56]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[57]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[58]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[59]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[60]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[61]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[62]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[63]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[64]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[65]       0.66    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[66]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[67]       0.66    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[68]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[69]       0.66    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[70]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[71]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[72]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[73]       0.66    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[74]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[75]       0.66    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[76]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[77]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[78]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[79]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[80]       0.66    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[81]       0.66    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[82]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[83]       0.66    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[84]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[85]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[86]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[87]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[88]       0.66    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[89]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[90]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[91]       0.66    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[92]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[93]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[94]       0.66    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[95]       0.66    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[96]       0.66    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[97]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[98]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[99]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[100]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[101]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[102]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[103]      0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[104]      0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[105]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[106]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[107]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[108]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[109]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[110]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[111]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[112]      0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[113]      0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[114]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[115]      0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[116]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[117]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[118]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[119]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[120]      0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[121]      0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[122]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[123]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[124]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[125]      0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[126]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[127]      0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[128]      0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[129]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[130]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[131]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[132]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[133]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[134]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[135]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[136]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[137]      0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[138]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[139]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[140]      0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[141]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[142]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[143]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[144]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[145]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[146]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[147]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[148]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[149]      0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[150]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[151]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[152]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[153]      0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[154]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[155]      0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[156]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[157]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[158]      0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[159]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[160]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[161]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[162]      0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[163]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[164]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[165]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[166]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[167]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[168]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[169]      0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[170]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[171]      0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[172]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[173]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[174]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[175]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[176]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[177]      0.67    0.00 0.03     0.61     0.65     0.67     0.69     0.72
## q[178]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[179]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[180]      0.67    0.00 0.03     0.61     0.65     0.67     0.69     0.72
## q[181]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[182]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[183]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[184]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[185]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[186]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[187]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[188]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[189]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[190]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[191]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[192]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[193]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[194]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[195]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[196]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[197]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[198]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[199]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[200]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[201]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[202]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[203]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[204]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[205]      0.67    0.00 0.03     0.61     0.65     0.67     0.69     0.72
## q[206]      0.67    0.00 0.03     0.61     0.65     0.67     0.69     0.72
## q[207]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[208]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[209]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[210]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[211]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[212]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[213]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[214]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[215]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[216]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[217]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[218]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[219]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[220]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[221]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[222]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[223]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[224]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[225]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[226]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[227]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[228]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[229]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[230]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[231]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[232]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[233]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[234]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[235]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[236]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[237]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[238]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[239]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[240]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[241]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[242]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[243]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[244]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[245]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[246]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[247]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[248]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[249]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[250]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[251]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[252]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[253]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[254]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[255]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[256]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[257]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[258]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[259]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[260]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[261]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[262]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[263]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[264]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[265]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[266]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[267]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[268]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[269]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[270]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[271]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[272]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[273]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[274]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[275]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[276]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[277]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[278]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[279]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[280]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[281]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[282]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[283]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[284]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[285]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[286]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[287]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[288]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[289]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[290]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[291]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[292]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[293]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[294]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[295]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[296]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[297]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[298]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[299]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[300]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[301]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[302]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[303]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[304]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[305]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[306]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[307]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[308]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[309]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[310]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[311]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[312]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[313]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[314]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[315]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[316]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[317]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[318]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[319]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[320]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[321]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[322]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[323]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[324]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[325]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[326]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[327]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[328]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[329]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[330]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[331]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[332]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[333]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[334]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[335]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[336]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[337]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[338]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[339]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[340]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[341]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[342]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[343]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[344]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[345]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[346]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[347]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[348]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[349]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[350]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[351]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[352]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[353]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[354]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[355]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[356]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[357]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[358]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[359]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[360]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[361]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[362]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[363]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[364]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[365]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[366]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[367]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[368]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[369]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[370]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[371]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[372]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[373]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[374]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[375]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[376]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[377]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[378]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[379]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[380]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[381]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[382]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[383]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[384]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[385]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[386]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[387]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[388]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[389]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[390]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[391]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[392]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[393]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[394]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[395]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[396]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[397]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[398]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[399]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.81
## q[400]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[401]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[402]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[403]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[404]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[405]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[406]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[407]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.81
## q[408]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[409]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.81
## q[410]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[411]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[412]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[413]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[414]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[415]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[416]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[417]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.81
## q[418]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[419]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[420]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[421]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[422]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[423]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[424]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[425]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[426]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[427]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[428]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[429]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[430]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[431]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[432]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[433]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[434]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[435]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.81
## q[436]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[437]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[438]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[439]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[440]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[441]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[442]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[443]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[444]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[445]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[446]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[447]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[448]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[449]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[450]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[451]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[452]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[453]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[454]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[455]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[456]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[457]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[458]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[459]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.81
## q[460]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[461]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.81
## q[462]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[463]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[464]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[465]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.81
## q[466]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[467]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[468]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[469]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[470]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[471]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[472]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.81
## q[473]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[474]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[475]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[476]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[477]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[478]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[479]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[480]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[481]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[482]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[483]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[484]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[485]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[486]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[487]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[488]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[489]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[490]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[491]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[492]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[493]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[494]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[495]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[496]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[497]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[498]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[499]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[500]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[501]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[502]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[503]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[504]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[505]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[506]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[507]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[508]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[509]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[510]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[511]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[512]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[513]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[514]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[515]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[516]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[517]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[518]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[519]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[520]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[521]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[522]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[523]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[524]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[525]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[526]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[527]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[528]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[529]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[530]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[531]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[532]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[533]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[534]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[535]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[536]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[537]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[538]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[539]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[540]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[541]      0.66    0.00 0.02     0.61     0.64     0.66     0.67     0.70
## q[542]      0.66    0.00 0.02     0.61     0.64     0.66     0.67     0.70
## q[543]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[544]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[545]      0.63    0.00 0.03     0.57     0.61     0.63     0.65     0.68
## q[546]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[547]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[548]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[549]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[550]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[551]      0.66    0.00 0.02     0.61     0.64     0.66     0.67     0.70
## q[552]      0.63    0.00 0.03     0.57     0.61     0.63     0.65     0.68
## q[553]      0.66    0.00 0.02     0.61     0.64     0.66     0.67     0.70
## q[554]      0.66    0.00 0.02     0.61     0.64     0.66     0.67     0.70
## q[555]      0.63    0.00 0.03     0.57     0.61     0.63     0.65     0.68
## q[556]      0.63    0.00 0.03     0.57     0.61     0.63     0.65     0.68
## q[557]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[558]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[559]      0.63    0.00 0.03     0.57     0.61     0.63     0.65     0.68
## q[560]      0.66    0.00 0.02     0.61     0.64     0.66     0.67     0.70
## q[561]      0.63    0.00 0.03     0.57     0.61     0.63     0.65     0.68
## q[562]      0.66    0.00 0.02     0.61     0.64     0.66     0.67     0.70
## q[563]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[564]      0.66    0.00 0.02     0.61     0.64     0.66     0.67     0.70
## q[565]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[566]      0.63    0.00 0.03     0.57     0.61     0.63     0.65     0.68
## q[567]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[568]      0.66    0.00 0.02     0.61     0.64     0.66     0.67     0.70
## q[569]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[570]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[571]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[572]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[573]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[574]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[575]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[576]      0.75    0.00 0.02     0.71     0.74     0.75     0.77     0.80
## q[577]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[578]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[579]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[580]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[581]      0.75    0.00 0.02     0.71     0.74     0.75     0.77     0.80
## q[582]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[583]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[584]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[585]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[586]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[587]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[588]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[589]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[590]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[591]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[592]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[593]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[594]      0.75    0.00 0.02     0.71     0.74     0.75     0.77     0.80
## q[595]      0.75    0.00 0.02     0.71     0.74     0.75     0.77     0.80
## q[596]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[597]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[598]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[599]      0.75    0.00 0.02     0.71     0.74     0.75     0.77     0.80
## q[600]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[601]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[602]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[603]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[604]      0.75    0.00 0.02     0.71     0.74     0.75     0.77     0.80
## q[605]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[606]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[607]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[608]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[609]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[610]      0.75    0.00 0.02     0.71     0.74     0.75     0.77     0.80
## q[611]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[612]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[613]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[614]      0.75    0.00 0.02     0.71     0.74     0.75     0.77     0.80
## q[615]      0.75    0.00 0.02     0.71     0.74     0.75     0.77     0.80
## q[616]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[617]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[618]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[619]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[620]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[621]      0.58    0.00 0.03     0.52     0.56     0.58     0.59     0.63
## q[622]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[623]      0.58    0.00 0.03     0.52     0.56     0.58     0.59     0.63
## q[624]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[625]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[626]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[627]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[628]      0.58    0.00 0.03     0.52     0.56     0.58     0.59     0.63
## q[629]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[630]      0.58    0.00 0.03     0.52     0.56     0.58     0.59     0.63
## q[631]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[632]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[633]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[634]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[635]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[636]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[637]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[638]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[639]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[640]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[641]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[642]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[643]      0.58    0.00 0.03     0.52     0.56     0.58     0.59     0.63
## q[644]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[645]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[646]      0.58    0.00 0.03     0.52     0.56     0.58     0.59     0.63
## q[647]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[648]      0.58    0.00 0.03     0.52     0.56     0.58     0.59     0.63
## q[649]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[650]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[651]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[652]      0.58    0.00 0.03     0.52     0.56     0.58     0.59     0.63
## q[653]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[654]      0.58    0.00 0.03     0.52     0.56     0.58     0.59     0.63
## q[655]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[656]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[657]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[658]      0.58    0.00 0.03     0.52     0.56     0.58     0.59     0.63
## q[659]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[660]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[661]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[662]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[663]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[664]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[665]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[666]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[667]      0.58    0.00 0.03     0.52     0.56     0.58     0.59     0.63
## q[668]      0.58    0.00 0.03     0.52     0.56     0.58     0.59     0.63
## q[669]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[670]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[671]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[672]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[673]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[674]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[675]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[676]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[677]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[678]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[679]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[680]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[681]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[682]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[683]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[684]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[685]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[686]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[687]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[688]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[689]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[690]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[691]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[692]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[693]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[694]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[695]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[696]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[697]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[698]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[699]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[700]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[701]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[702]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[703]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[704]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[705]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[706]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[707]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[708]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[709]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[710]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[711]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[712]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[713]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[714]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[715]      0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.75
## q[716]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[717]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[718]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[719]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[720]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[721]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[722]      0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.75
## q[723]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[724]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[725]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[726]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[727]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[728]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[729]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[730]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[731]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[732]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[733]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[734]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[735]      0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.75
## q[736]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[737]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[738]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[739]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[740]      0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.75
## q[741]      0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.75
## q[742]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[743]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[744]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[745]      0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.75
## q[746]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[747]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[748]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[749]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[750]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[751]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[752]      0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.75
## q[753]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[754]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[755]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[756]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[757]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[758]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[759]      0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.75
## q[760]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[761]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[762]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[763]      0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.75
## q[764]      0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.75
## q[765]      0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[766]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[767]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[768]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[769]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[770]      0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[771]      0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[772]      0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[773]      0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[774]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[775]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[776]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[777]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[778]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[779]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[780]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[781]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[782]      0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[783]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[784]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[785]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[786]      0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[787]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[788]      0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[789]      0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[790]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[791]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[792]      0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[793]      0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[794]      0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[795]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[796]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[797]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[798]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[799]      0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[800]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[801]      0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[802]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[803]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[804]      0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[805]      0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[806]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[807]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[808]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[809]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[810]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[811]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[812]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[813]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[814]      0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[815]      0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[816]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[817]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[818]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[819]      0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[820]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[821]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[822]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[823]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[824]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[825]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[826]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[827]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[828]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[829]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[830]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[831]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[832]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[833]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[834]      0.71    0.00 0.02     0.66     0.69     0.71     0.73     0.75
## q[835]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[836]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[837]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[838]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[839]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[840]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[841]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[842]      0.71    0.00 0.02     0.66     0.69     0.71     0.73     0.75
## q[843]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[844]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[845]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[846]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[847]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[848]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[849]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[850]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[851]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[852]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[853]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[854]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[855]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[856]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[857]      0.71    0.00 0.02     0.66     0.69     0.71     0.73     0.75
## q[858]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[859]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[860]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[861]      0.71    0.00 0.02     0.66     0.69     0.71     0.73     0.75
## q[862]      0.67    0.00 0.03     0.61     0.65     0.67     0.69     0.72
## q[863]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[864]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[865]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[866]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[867]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[868]      0.67    0.00 0.03     0.61     0.65     0.67     0.69     0.72
## q[869]      0.67    0.00 0.03     0.61     0.65     0.67     0.69     0.72
## q[870]      0.67    0.00 0.03     0.61     0.65     0.67     0.69     0.72
## q[871]      0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.70
## q[872]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[873]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[874]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[875]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[876]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[877]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[878]      0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.70
## q[879]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[880]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[881]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[882]      0.67    0.00 0.03     0.61     0.65     0.67     0.69     0.72
## q[883]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[884]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[885]      0.67    0.00 0.03     0.61     0.65     0.67     0.69     0.72
## q[886]      0.67    0.00 0.03     0.61     0.65     0.67     0.69     0.72
## q[887]      0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.70
## q[888]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[889]      0.67    0.00 0.03     0.61     0.65     0.67     0.69     0.72
## q[890]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[891]      0.67    0.00 0.03     0.61     0.65     0.67     0.69     0.72
## q[892]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[893]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[894]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[895]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[896]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[897]      0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.70
## q[898]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[899]      0.67    0.00 0.03     0.61     0.65     0.67     0.69     0.72
## q[900]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[901]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[902]      0.67    0.00 0.03     0.61     0.65     0.67     0.69     0.72
## q[903]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[904]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[905]      0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.70
## q[906]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[907]      0.89    0.00 0.02     0.86     0.88     0.89     0.90     0.92
## q[908]      0.83    0.00 0.02     0.78     0.81     0.83     0.85     0.87
## q[909]      0.84    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[910]      0.84    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[911]      0.83    0.00 0.02     0.78     0.81     0.83     0.85     0.87
## q[912]      0.89    0.00 0.02     0.86     0.88     0.89     0.90     0.92
## q[913]      0.84    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[914]      0.89    0.00 0.02     0.86     0.88     0.89     0.90     0.92
## q[915]      0.89    0.00 0.02     0.86     0.88     0.89     0.90     0.92
## q[916]      0.84    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[917]      0.89    0.00 0.02     0.86     0.88     0.89     0.90     0.92
## q[918]      0.84    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[919]      0.89    0.00 0.02     0.86     0.88     0.89     0.90     0.92
## q[920]      0.89    0.00 0.02     0.86     0.88     0.89     0.90     0.92
## q[921]      0.84    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[922]      0.84    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[923]      0.83    0.00 0.02     0.78     0.81     0.83     0.85     0.87
## q[924]      0.84    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[925]      0.89    0.00 0.02     0.86     0.88     0.89     0.90     0.92
## q[926]      0.84    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[927]      0.84    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[928]      0.89    0.00 0.02     0.86     0.88     0.89     0.90     0.92
## q[929]      0.84    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[930]      0.83    0.00 0.02     0.78     0.81     0.83     0.85     0.87
## q[931]      0.89    0.00 0.02     0.86     0.88     0.89     0.90     0.92
## q[932]      0.83    0.00 0.02     0.78     0.81     0.83     0.85     0.87
## q[933]      0.84    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[934]      0.89    0.00 0.02     0.86     0.88     0.89     0.90     0.92
## q[935]      0.89    0.00 0.02     0.86     0.88     0.89     0.90     0.92
## q[936]      0.84    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[937]      0.84    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[938]      0.89    0.00 0.02     0.86     0.88     0.89     0.90     0.92
## q[939]      0.89    0.00 0.02     0.86     0.88     0.89     0.90     0.92
## q[940]      0.89    0.00 0.02     0.86     0.88     0.89     0.90     0.92
## q[941]      0.89    0.00 0.02     0.86     0.88     0.89     0.90     0.92
## q[942]      0.89    0.00 0.02     0.86     0.88     0.89     0.90     0.92
## q[943]      0.84    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[944]      0.89    0.00 0.02     0.86     0.88     0.89     0.90     0.92
## q[945]      0.89    0.00 0.02     0.86     0.88     0.89     0.90     0.92
## q[946]      0.83    0.00 0.02     0.78     0.81     0.83     0.85     0.87
## q[947]      0.83    0.00 0.02     0.78     0.81     0.83     0.85     0.87
## q[948]      0.89    0.00 0.02     0.86     0.88     0.89     0.90     0.92
## q[949]      0.84    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[950]      0.89    0.00 0.02     0.86     0.88     0.89     0.90     0.92
## q[951]      0.89    0.00 0.02     0.86     0.88     0.89     0.90     0.92
## q[952]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[953]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[954]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[955]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[956]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[957]      0.69    0.00 0.02     0.64     0.67     0.69     0.70     0.73
## q[958]      0.69    0.00 0.02     0.64     0.67     0.69     0.70     0.73
## q[959]      0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[960]      0.69    0.00 0.02     0.64     0.67     0.69     0.70     0.73
## q[961]      0.69    0.00 0.02     0.64     0.67     0.69     0.70     0.73
## q[962]      0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[963]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[964]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[965]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[966]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[967]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[968]      0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[969]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[970]      0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[971]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[972]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[973]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[974]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[975]      0.69    0.00 0.02     0.64     0.67     0.69     0.70     0.73
## q[976]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[977]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[978]      0.69    0.00 0.02     0.64     0.67     0.69     0.70     0.73
## q[979]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[980]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[981]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[982]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[983]      0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[984]      0.69    0.00 0.02     0.64     0.67     0.69     0.70     0.73
## q[985]      0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[986]      0.69    0.00 0.02     0.64     0.67     0.69     0.70     0.73
## q[987]      0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[988]      0.69    0.00 0.02     0.64     0.67     0.69     0.70     0.73
## q[989]      0.69    0.00 0.02     0.64     0.67     0.69     0.70     0.73
## q[990]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[991]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[992]      0.69    0.00 0.02     0.64     0.67     0.69     0.70     0.73
## q[993]      0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[994]      0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[995]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[996]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[997]      0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[998]      0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[999]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1000]     0.69    0.00 0.02     0.64     0.67     0.69     0.70     0.73
## q[1001]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1002]     0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[1003]     0.69    0.00 0.02     0.64     0.67     0.69     0.70     0.73
## q[1004]     0.69    0.00 0.02     0.64     0.67     0.69     0.70     0.73
## q[1005]     0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[1006]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1007]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1008]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1009]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1010]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1011]     0.69    0.00 0.02     0.64     0.67     0.69     0.70     0.73
## q[1012]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1013]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1014]     0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[1015]     0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[1016]     0.69    0.00 0.02     0.64     0.67     0.69     0.70     0.73
## q[1017]     0.69    0.00 0.02     0.64     0.67     0.69     0.70     0.73
## q[1018]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1019]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1020]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1021]     0.69    0.00 0.02     0.64     0.67     0.69     0.70     0.73
## q[1022]     0.69    0.00 0.02     0.64     0.67     0.69     0.70     0.73
## q[1023]     0.69    0.00 0.02     0.64     0.67     0.69     0.70     0.73
## q[1024]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1025]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1026]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1027]     0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[1028]     0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[1029]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1030]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1031]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1032]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1033]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1034]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1035]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1036]     0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[1037]     0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[1038]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1039]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1040]     0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[1041]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1042]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1043]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1044]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1045]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1046]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1047]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1048]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1049]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1050]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1051]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1052]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1053]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1054]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1055]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1056]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1057]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1058]     0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[1059]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1060]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1061]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1062]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1063]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1064]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1065]     0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[1066]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1067]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1068]     0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[1069]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1070]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1071]     0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[1072]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1073]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1074]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1075]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1076]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1077]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1078]     0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[1079]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1080]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1081]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1082]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1083]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1084]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1085]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1086]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1087]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1088]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1089]     0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[1090]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1091]     0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[1092]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1093]     0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[1094]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1095]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1096]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1097]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1098]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1099]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1100]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1101]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1102]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1103]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1104]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1105]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1106]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1107]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1108]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1109]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1110]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1111]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1112]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1113]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1114]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1115]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1116]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1117]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1118]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1119]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1120]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1121]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1122]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1123]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1124]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1125]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1126]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1127]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1128]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1129]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1130]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1131]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1132]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1133]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1134]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1135]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1136]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1137]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1138]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1139]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1140]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1141]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1142]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1143]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1144]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1145]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1146]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1147]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1148]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1149]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1150]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1151]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1152]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1153]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1154]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1155]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1156]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1157]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1158]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1159]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1160]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1161]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1162]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1163]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1164]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1165]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1166]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1167]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1168]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1169]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1170]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1171]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1172]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1173]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1174]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1175]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1176]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1177]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1178]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1179]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.60
## q[1180]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1181]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.60
## q[1182]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1183]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.60
## q[1184]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1185]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1186]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1187]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1188]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1189]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1190]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1191]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1192]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1193]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1194]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1195]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1196]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1197]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1198]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1199]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1200]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1201]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1202]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1203]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1204]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.60
## q[1205]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1206]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1207]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1208]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1209]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1210]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1211]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1212]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.60
## q[1213]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.60
## q[1214]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1215]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.60
## q[1216]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1217]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.60
## q[1218]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1219]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1220]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1221]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1222]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1223]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1224]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.60
## q[1225]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1226]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1227]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1228]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1229]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1230]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.60
## q[1231]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1232]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1233]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1234]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.60
## q[1235]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1236]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1237]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1238]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.60
## q[1239]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1240]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1241]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1242]     0.63    0.00 0.03     0.58     0.61     0.63     0.65     0.69
## q[1243]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1244]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1245]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1246]     0.66    0.00 0.02     0.61     0.64     0.66     0.68     0.70
## q[1247]     0.66    0.00 0.02     0.61     0.64     0.66     0.68     0.70
## q[1248]     0.66    0.00 0.02     0.61     0.64     0.66     0.68     0.70
## q[1249]     0.66    0.00 0.02     0.61     0.64     0.66     0.68     0.70
## q[1250]     0.66    0.00 0.02     0.61     0.64     0.66     0.68     0.70
## q[1251]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1252]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1253]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1254]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1255]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1256]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1257]     0.66    0.00 0.02     0.61     0.64     0.66     0.68     0.70
## q[1258]     0.63    0.00 0.03     0.58     0.61     0.63     0.65     0.69
## q[1259]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1260]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1261]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1262]     0.66    0.00 0.02     0.61     0.64     0.66     0.68     0.70
## q[1263]     0.66    0.00 0.02     0.61     0.64     0.66     0.68     0.70
## q[1264]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1265]     0.66    0.00 0.02     0.61     0.64     0.66     0.68     0.70
## q[1266]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1267]     0.66    0.00 0.02     0.61     0.64     0.66     0.68     0.70
## q[1268]     0.63    0.00 0.03     0.58     0.61     0.63     0.65     0.69
## q[1269]     0.66    0.00 0.02     0.61     0.64     0.66     0.68     0.70
## q[1270]     0.66    0.00 0.02     0.61     0.64     0.66     0.68     0.70
## q[1271]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1272]     0.66    0.00 0.02     0.61     0.64     0.66     0.68     0.70
## q[1273]     0.63    0.00 0.03     0.58     0.61     0.63     0.65     0.69
## q[1274]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1275]     0.66    0.00 0.02     0.61     0.64     0.66     0.68     0.70
## q[1276]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1277]     0.66    0.00 0.02     0.61     0.64     0.66     0.68     0.70
## q[1278]     0.63    0.00 0.03     0.58     0.61     0.63     0.65     0.69
## q[1279]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1280]     0.63    0.00 0.03     0.58     0.61     0.63     0.65     0.69
## q[1281]     0.66    0.00 0.02     0.61     0.64     0.66     0.68     0.70
## q[1282]     0.66    0.00 0.02     0.61     0.64     0.66     0.68     0.70
## q[1283]     0.66    0.00 0.02     0.61     0.64     0.66     0.68     0.70
## q[1284]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1285]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1286]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1287]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1288]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1289]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1290]     0.66    0.00 0.02     0.61     0.64     0.66     0.68     0.70
## q[1291]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1292]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1293]     0.63    0.00 0.03     0.58     0.61     0.63     0.65     0.69
## q[1294]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1295]     0.66    0.00 0.02     0.61     0.64     0.66     0.68     0.70
## q[1296]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1297]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1298]     0.63    0.00 0.03     0.58     0.61     0.63     0.65     0.69
## q[1299]     0.66    0.00 0.02     0.61     0.64     0.66     0.68     0.70
## q[1300]     0.66    0.00 0.02     0.61     0.64     0.66     0.68     0.70
## q[1301]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1302]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1303]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1304]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1305]     0.63    0.00 0.03     0.58     0.61     0.63     0.65     0.69
## q[1306]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1307]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1308]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1309]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1310]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1311]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1312]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1313]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1314]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1315]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1316]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1317]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1318]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1319]     0.57    0.00 0.03     0.52     0.55     0.57     0.59     0.63
## q[1320]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1321]     0.57    0.00 0.03     0.52     0.55     0.57     0.59     0.63
## q[1322]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1323]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1324]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1325]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1326]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1327]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1328]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1329]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1330]     0.57    0.00 0.03     0.52     0.55     0.57     0.59     0.63
## q[1331]     0.57    0.00 0.03     0.52     0.55     0.57     0.59     0.63
## q[1332]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1333]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1334]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1335]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1336]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1337]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1338]     0.57    0.00 0.03     0.52     0.55     0.57     0.59     0.63
## q[1339]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1340]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1341]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1342]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1343]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1344]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1345]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1346]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1347]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1348]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1349]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1350]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1351]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1352]     0.57    0.00 0.03     0.52     0.55     0.57     0.59     0.63
## q[1353]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1354]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1355]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1356]     0.57    0.00 0.03     0.52     0.55     0.57     0.59     0.63
## q[1357]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1358]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1359]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1360]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1361]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1362]     0.63    0.00 0.02     0.58     0.61     0.63     0.64     0.67
## q[1363]     0.60    0.00 0.03     0.55     0.58     0.60     0.62     0.65
## q[1364]     0.63    0.00 0.02     0.58     0.61     0.63     0.64     0.67
## q[1365]     0.63    0.00 0.02     0.58     0.61     0.63     0.64     0.67
## q[1366]     0.60    0.00 0.03     0.55     0.58     0.60     0.62     0.65
## q[1367]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1368]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1369]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1370]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1371]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1372]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1373]     0.63    0.00 0.02     0.58     0.61     0.63     0.64     0.67
## q[1374]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1375]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1376]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1377]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1378]     0.63    0.00 0.02     0.58     0.61     0.63     0.64     0.67
## q[1379]     0.60    0.00 0.03     0.55     0.58     0.60     0.62     0.65
## q[1380]     0.63    0.00 0.02     0.58     0.61     0.63     0.64     0.67
## q[1381]     0.63    0.00 0.02     0.58     0.61     0.63     0.64     0.67
## q[1382]     0.63    0.00 0.02     0.58     0.61     0.63     0.64     0.67
## q[1383]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1384]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1385]     0.63    0.00 0.02     0.58     0.61     0.63     0.64     0.67
## q[1386]     0.60    0.00 0.03     0.55     0.58     0.60     0.62     0.65
## q[1387]     0.60    0.00 0.03     0.55     0.58     0.60     0.62     0.65
## q[1388]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1389]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1390]     0.63    0.00 0.02     0.58     0.61     0.63     0.64     0.67
## q[1391]     0.60    0.00 0.03     0.55     0.58     0.60     0.62     0.65
## q[1392]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1393]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1394]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1395]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1396]     0.60    0.00 0.03     0.55     0.58     0.60     0.62     0.65
## q[1397]     0.60    0.00 0.03     0.55     0.58     0.60     0.62     0.65
## q[1398]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1399]     0.60    0.00 0.03     0.55     0.58     0.60     0.62     0.65
## q[1400]     0.63    0.00 0.02     0.58     0.61     0.63     0.64     0.67
## q[1401]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1402]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1403]     0.63    0.00 0.02     0.58     0.61     0.63     0.64     0.67
## q[1404]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1405]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1406]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1407]     0.60    0.00 0.03     0.55     0.58     0.60     0.62     0.65
## q[1408]     0.72    0.00 0.02     0.67     0.70     0.72     0.73     0.76
## q[1409]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1410]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1411]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.78
## q[1412]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.78
## q[1413]     0.72    0.00 0.02     0.67     0.70     0.72     0.73     0.76
## q[1414]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1415]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1416]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1417]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1418]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1419]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.78
## q[1420]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.78
## q[1421]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.78
## q[1422]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1423]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.78
## q[1424]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.78
## q[1425]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1426]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1427]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1428]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1429]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1430]     0.72    0.00 0.02     0.67     0.70     0.72     0.73     0.76
## q[1431]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1432]     0.72    0.00 0.02     0.67     0.70     0.72     0.73     0.76
## q[1433]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1434]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.78
## q[1435]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1436]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1437]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1438]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.78
## q[1439]     0.72    0.00 0.02     0.67     0.70     0.72     0.73     0.76
## q[1440]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1441]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.78
## q[1442]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1443]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.78
## q[1444]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1445]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.78
## q[1446]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1447]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1448]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.78
## q[1449]     0.72    0.00 0.02     0.67     0.70     0.72     0.73     0.76
## q[1450]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.78
## q[1451]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1452]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.78
## q[1453]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.78
## q[1454]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.78
## q[1455]     0.72    0.00 0.02     0.67     0.70     0.72     0.73     0.76
## q[1456]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1457]     0.72    0.00 0.02     0.67     0.70     0.72     0.73     0.76
## q[1458]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1459]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1460]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.78
## q[1461]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.78
## q[1462]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1463]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1464]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1465]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1466]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1467]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.78
## q[1468]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1469]     0.72    0.00 0.02     0.67     0.70     0.72     0.73     0.76
## q[1470]     0.72    0.00 0.02     0.67     0.70     0.72     0.73     0.76
## q[1471]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1472]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1473]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1474]     0.86    0.00 0.01     0.84     0.85     0.86     0.87     0.89
## q[1475]     0.81    0.00 0.02     0.78     0.80     0.81     0.83     0.85
## q[1476]     0.79    0.00 0.02     0.75     0.78     0.79     0.81     0.84
## q[1477]     0.86    0.00 0.01     0.84     0.85     0.86     0.87     0.89
## q[1478]     0.86    0.00 0.01     0.84     0.85     0.86     0.87     0.89
## q[1479]     0.81    0.00 0.02     0.78     0.80     0.81     0.83     0.85
## q[1480]     0.86    0.00 0.01     0.84     0.85     0.86     0.87     0.89
## q[1481]     0.86    0.00 0.01     0.84     0.85     0.86     0.87     0.89
## q[1482]     0.86    0.00 0.01     0.84     0.85     0.86     0.87     0.89
## q[1483]     0.79    0.00 0.02     0.75     0.78     0.79     0.81     0.84
## q[1484]     0.79    0.00 0.02     0.75     0.78     0.79     0.81     0.84
## q[1485]     0.81    0.00 0.02     0.78     0.80     0.81     0.83     0.85
## q[1486]     0.79    0.00 0.02     0.75     0.78     0.79     0.81     0.84
## q[1487]     0.79    0.00 0.02     0.75     0.78     0.79     0.81     0.84
## q[1488]     0.81    0.00 0.02     0.78     0.80     0.81     0.83     0.85
## q[1489]     0.86    0.00 0.01     0.84     0.85     0.86     0.87     0.89
## q[1490]     0.81    0.00 0.02     0.78     0.80     0.81     0.83     0.85
## q[1491]     0.79    0.00 0.02     0.75     0.78     0.79     0.81     0.84
## q[1492]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1493]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1494]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1495]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1496]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1497]     0.71    0.00 0.02     0.66     0.69     0.71     0.72     0.75
## q[1498]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1499]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1500]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1501]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1502]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1503]     0.71    0.00 0.02     0.66     0.69     0.71     0.72     0.75
## q[1504]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1505]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1506]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1507]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1508]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1509]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1510]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1511]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1512]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1513]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1514]     0.71    0.00 0.02     0.66     0.69     0.71     0.72     0.75
## q[1515]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1516]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1517]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1518]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1519]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1520]     0.71    0.00 0.02     0.66     0.69     0.71     0.72     0.75
## q[1521]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1522]     0.71    0.00 0.02     0.66     0.69     0.71     0.72     0.75
## q[1523]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1524]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1525]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1526]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1527]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1528]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1529]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1530]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1531]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1532]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1533]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1534]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1535]     0.71    0.00 0.02     0.66     0.69     0.71     0.72     0.75
## q[1536]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1537]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1538]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1539]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1540]     0.71    0.00 0.02     0.66     0.69     0.71     0.72     0.75
## q[1541]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1542]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1543]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1544]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1545]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.75
## q[1546]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.75
## q[1547]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1548]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.75
## q[1549]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.75
## q[1550]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1551]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1552]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1553]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1554]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.75
## q[1555]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1556]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1557]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1558]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1559]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1560]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.75
## q[1561]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1562]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.75
## q[1563]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1564]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.75
## q[1565]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1566]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1567]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.75
## q[1568]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1569]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.75
## q[1570]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1571]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1572]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.75
## q[1573]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1574]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.75
## q[1575]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.75
## q[1576]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1577]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.75
## q[1578]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1579]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1580]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1581]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1582]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1583]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.75
## q[1584]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1585]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1586]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1587]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.75
## q[1588]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.75
## q[1589]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.75
## q[1590]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1591]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1592]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1593]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1594]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1595]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1596]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1597]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.75
## q[1598]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1599]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1600]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1601]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1602]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.75
## q[1603]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.75
## q[1604]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1605]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1606]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.75
## q[1607]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1608]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.75
## q[1609]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1610]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.75
## q[1611]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1612]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1613]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1614]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1615]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1616]     0.59    0.00 0.03     0.54     0.57     0.59     0.61     0.64
## q[1617]     0.59    0.00 0.03     0.54     0.57     0.59     0.61     0.64
## q[1618]     0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[1619]     0.59    0.00 0.03     0.54     0.57     0.59     0.61     0.64
## q[1620]     0.59    0.00 0.03     0.54     0.57     0.59     0.61     0.64
## q[1621]     0.47    0.00 0.04     0.40     0.44     0.47     0.49     0.54
## q[1622]     0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[1623]     0.59    0.00 0.03     0.54     0.57     0.59     0.61     0.64
## q[1624]     0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[1625]     0.59    0.00 0.03     0.54     0.57     0.59     0.61     0.64
## q[1626]     0.59    0.00 0.03     0.54     0.57     0.59     0.61     0.64
## q[1627]     0.59    0.00 0.03     0.54     0.57     0.59     0.61     0.64
## q[1628]     0.65    0.00 0.02     0.60     0.63     0.65     0.66     0.69
## q[1629]     0.65    0.00 0.02     0.60     0.63     0.65     0.66     0.69
## q[1630]     0.62    0.00 0.03     0.57     0.60     0.62     0.64     0.68
## q[1631]     0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.76
## q[1632]     0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.76
## q[1633]     0.62    0.00 0.03     0.57     0.60     0.62     0.64     0.68
## q[1634]     0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.76
## q[1635]     0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.76
## q[1636]     0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.76
## q[1637]     0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.76
## q[1638]     0.65    0.00 0.02     0.60     0.63     0.65     0.66     0.69
## q[1639]     0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.76
## q[1640]     0.65    0.00 0.02     0.60     0.63     0.65     0.66     0.69
## q[1641]     0.62    0.00 0.03     0.57     0.60     0.62     0.64     0.68
## q[1642]     0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.76
## q[1643]     0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.76
## q[1644]     0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.76
## q[1645]     0.65    0.00 0.02     0.60     0.63     0.65     0.66     0.69
## q[1646]     0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.76
## q[1647]     0.62    0.00 0.03     0.57     0.60     0.62     0.64     0.68
## q[1648]     0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.76
## q[1649]     0.65    0.00 0.02     0.60     0.63     0.65     0.66     0.69
## q[1650]     0.65    0.00 0.02     0.60     0.63     0.65     0.66     0.69
## q[1651]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1652]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1653]     0.59    0.00 0.03     0.54     0.57     0.59     0.61     0.64
## q[1654]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1655]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1656]     0.59    0.00 0.03     0.54     0.57     0.59     0.61     0.64
## q[1657]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1658]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1659]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1660]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1661]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1662]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1663]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1664]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1665]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1666]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1667]     0.59    0.00 0.03     0.54     0.57     0.59     0.61     0.64
## q[1668]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1669]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1670]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1671]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1672]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1673]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1674]     0.59    0.00 0.03     0.54     0.57     0.59     0.61     0.64
## q[1675]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1676]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1677]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1678]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1679]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1680]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1681]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1682]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1683]     0.59    0.00 0.03     0.54     0.57     0.59     0.61     0.64
## q[1684]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1685]     0.59    0.00 0.03     0.54     0.57     0.59     0.61     0.64
## q[1686]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1687]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1688]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1689]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1690]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1691]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1692]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1693]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1694]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1695]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1696]     0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.78
## q[1697]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1698]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1699]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1700]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1701]     0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.78
## q[1702]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1703]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1704]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1705]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1706]     0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.78
## q[1707]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1708]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1709]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1710]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1711]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1712]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1713]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1714]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1715]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1716]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1717]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1718]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1719]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1720]     0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.78
## q[1721]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1722]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1723]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1724]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1725]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1726]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1727]     0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.78
## q[1728]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1729]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1730]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1731]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1732]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1733]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1734]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1735]     0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.78
## q[1736]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1737]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1738]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1739]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1740]     0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.78
## q[1741]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1742]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1743]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1744]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1745]     0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.78
## q[1746]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1747]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1748]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1749]     0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.79
## q[1750]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1751]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1752]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1753]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1754]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1755]     0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.79
## q[1756]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1757]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1758]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1759]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1760]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1761]     0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.79
## q[1762]     0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.79
## q[1763]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1764]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1765]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1766]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1767]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1768]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1769]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1770]     0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.79
## q[1771]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1772]     0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.79
## q[1773]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1774]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1775]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1776]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1777]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1778]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1779]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1780]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1781]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1782]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1783]     0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.79
## q[1784]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1785]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1786]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1787]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1788]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1789]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1790]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1791]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1792]     0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.79
## q[1793]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1794]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1795]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1796]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1797]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1798]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1799]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1800]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1801]     0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.79
## q[1802]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1803]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1804]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1805]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1806]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1807]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1808]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1809]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1810]     0.54    0.00 0.03     0.49     0.52     0.54     0.56     0.60
## q[1811]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1812]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1813]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1814]     0.54    0.00 0.03     0.49     0.52     0.54     0.56     0.60
## q[1815]     0.54    0.00 0.03     0.49     0.52     0.54     0.56     0.60
## q[1816]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1817]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1818]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1819]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1820]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1821]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1822]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1823]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1824]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1825]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1826]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1827]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1828]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1829]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1830]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1831]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1832]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1833]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1834]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1835]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1836]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1837]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1838]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1839]     0.54    0.00 0.03     0.49     0.52     0.54     0.56     0.60
## q[1840]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1841]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1842]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1843]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1844]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1845]     0.54    0.00 0.03     0.49     0.52     0.54     0.56     0.60
## q[1846]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1847]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1848]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1849]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1850]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1851]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1852]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1853]     0.54    0.00 0.03     0.49     0.52     0.54     0.56     0.60
## q[1854]     0.54    0.00 0.03     0.49     0.52     0.54     0.56     0.60
## q[1855]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1856]     0.54    0.00 0.03     0.49     0.52     0.54     0.56     0.60
## q[1857]     0.54    0.00 0.03     0.49     0.52     0.54     0.56     0.60
## q[1858]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1859]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1860]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1861]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1862]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1863]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1864]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1865]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1866]     0.54    0.00 0.03     0.49     0.52     0.54     0.56     0.60
## q[1867]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1868]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1869]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1870]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1871]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1872]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1873]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1874]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1875]     0.54    0.00 0.03     0.49     0.52     0.54     0.56     0.60
## q[1876]     0.54    0.00 0.03     0.49     0.52     0.54     0.56     0.60
## q[1877]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1878]     0.54    0.00 0.03     0.49     0.52     0.54     0.56     0.60
## q[1879]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1880]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1881]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1882]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1883]     0.54    0.00 0.03     0.49     0.52     0.54     0.56     0.60
## q[1884]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1885]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1886]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1887]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1888]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1889]     0.54    0.00 0.03     0.49     0.52     0.54     0.56     0.60
## q[1890]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1891]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1892]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1893]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1894]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1895]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1896]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1897]     0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.75
## q[1898]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1899]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1900]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1901]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1902]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1903]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1904]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1905]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1906]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1907]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1908]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1909]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1910]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1911]     0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.75
## q[1912]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1913]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1914]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1915]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1916]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1917]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1918]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1919]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1920]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1921]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1922]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1923]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1924]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1925]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1926]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1927]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1928]     0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.75
## q[1929]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1930]     0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.75
## q[1931]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1932]     0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.75
## q[1933]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1934]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1935]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1936]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1937]     0.68    0.00 0.02     0.63     0.67     0.68     0.70     0.73
## q[1938]     0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.71
## q[1939]     0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.71
## q[1940]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1941]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1942]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1943]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1944]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1945]     0.68    0.00 0.02     0.63     0.67     0.68     0.70     0.73
## q[1946]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1947]     0.68    0.00 0.02     0.63     0.67     0.68     0.70     0.73
## q[1948]     0.68    0.00 0.02     0.63     0.67     0.68     0.70     0.73
## q[1949]     0.68    0.00 0.02     0.63     0.67     0.68     0.70     0.73
## q[1950]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1951]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1952]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1953]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1954]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1955]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1956]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1957]     0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.71
## q[1958]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1959]     0.68    0.00 0.02     0.63     0.66     0.68     0.70     0.72
## q[1960]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1961]     0.65    0.00 0.03     0.60     0.63     0.65     0.67     0.71
## q[1962]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1963]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1964]     0.68    0.00 0.02     0.63     0.66     0.68     0.70     0.72
## q[1965]     0.65    0.00 0.03     0.60     0.63     0.65     0.67     0.71
## q[1966]     0.68    0.00 0.02     0.63     0.66     0.68     0.70     0.72
## q[1967]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1968]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1969]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1970]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1971]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1972]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1973]     0.68    0.00 0.02     0.63     0.66     0.68     0.70     0.72
## q[1974]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1975]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1976]     0.68    0.00 0.02     0.63     0.66     0.68     0.70     0.72
## q[1977]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1978]     0.68    0.00 0.02     0.63     0.66     0.68     0.70     0.72
## q[1979]     0.65    0.00 0.03     0.60     0.63     0.65     0.67     0.71
## q[1980]     0.68    0.00 0.02     0.63     0.66     0.68     0.70     0.72
## q[1981]     0.68    0.00 0.02     0.63     0.66     0.68     0.70     0.72
## q[1982]     0.68    0.00 0.02     0.63     0.66     0.68     0.70     0.72
## q[1983]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1984]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1985]     0.65    0.00 0.03     0.60     0.63     0.65     0.67     0.71
## q[1986]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1987]     0.65    0.00 0.03     0.60     0.63     0.65     0.67     0.71
## q[1988]     0.68    0.00 0.02     0.63     0.66     0.68     0.70     0.72
## q[1989]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1990]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1991]     0.68    0.00 0.02     0.63     0.66     0.68     0.70     0.72
## q[1992]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1993]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1994]     0.65    0.00 0.03     0.60     0.63     0.65     0.67     0.71
## q[1995]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1996]     0.68    0.00 0.02     0.63     0.66     0.68     0.70     0.72
## q[1997]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1998]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1999]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[2000]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2001]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2002]     0.74    0.00 0.02     0.69     0.72     0.74     0.75     0.78
## q[2003]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2004]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2005]     0.74    0.00 0.02     0.69     0.72     0.74     0.75     0.78
## q[2006]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2007]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2008]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2009]     0.74    0.00 0.02     0.69     0.72     0.74     0.75     0.78
## q[2010]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2011]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2012]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2013]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2014]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2015]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2016]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2017]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2018]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2019]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2020]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2021]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2022]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2023]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2024]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2025]     0.74    0.00 0.02     0.69     0.72     0.74     0.75     0.78
## q[2026]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2027]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2028]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2029]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2030]     0.74    0.00 0.02     0.69     0.72     0.74     0.75     0.78
## q[2031]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2032]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2033]     0.74    0.00 0.02     0.69     0.72     0.74     0.75     0.78
## q[2034]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2035]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2036]     0.74    0.00 0.02     0.69     0.72     0.74     0.75     0.78
## q[2037]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2038]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2039]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2040]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2041]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2042]     0.74    0.00 0.02     0.69     0.72     0.74     0.75     0.78
## q[2043]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2044]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2045]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2046]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2047]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2048]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2049]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2050]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2051]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2052]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2053]     0.74    0.00 0.02     0.69     0.72     0.74     0.75     0.78
## q[2054]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2055]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2056]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2057]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2058]     0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[2059]     0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[2060]     0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[2061]     0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[2062]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2063]     0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[2064]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2065]     0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[2066]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2067]     0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[2068]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2069]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2070]     0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[2071]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2072]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2073]     0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[2074]     0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[2075]     0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[2076]     0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[2077]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2078]     0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[2079]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2080]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2081]     0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[2082]     0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[2083]     0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[2084]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2085]     0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[2086]     0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[2087]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2088]     0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[2089]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2090]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2091]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2092]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2093]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2094]     0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.77
## q[2095]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2096]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2097]     0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.77
## q[2098]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2099]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2100]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2101]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2102]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2103]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2104]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2105]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2106]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2107]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2108]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2109]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2110]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2111]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2112]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2113]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2114]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2115]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2116]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2117]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2118]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2119]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2120]     0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.77
## q[2121]     0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.77
## q[2122]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2123]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2124]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2125]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2126]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2127]     0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.77
## q[2128]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2129]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2130]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2131]     0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.77
## q[2132]     0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.77
## q[2133]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2134]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2135]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2136]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2137]     0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.77
## q[2138]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2139]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2140]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2141]     0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.77
## q[2142]     0.79    0.00 0.02     0.76     0.78     0.79     0.80     0.82
## q[2143]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2144]     0.77    0.00 0.02     0.73     0.76     0.77     0.79     0.81
## q[2145]     0.77    0.00 0.02     0.73     0.76     0.77     0.79     0.81
## q[2146]     0.79    0.00 0.02     0.76     0.78     0.79     0.80     0.82
## q[2147]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2148]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2149]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2150]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2151]     0.79    0.00 0.02     0.76     0.78     0.79     0.80     0.82
## q[2152]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2153]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2154]     0.79    0.00 0.02     0.76     0.78     0.79     0.80     0.82
## q[2155]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2156]     0.79    0.00 0.02     0.76     0.78     0.79     0.80     0.82
## q[2157]     0.77    0.00 0.02     0.73     0.76     0.77     0.79     0.81
## q[2158]     0.79    0.00 0.02     0.76     0.78     0.79     0.80     0.82
## q[2159]     0.79    0.00 0.02     0.76     0.78     0.79     0.80     0.82
## q[2160]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2161]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2162]     0.77    0.00 0.02     0.73     0.76     0.77     0.79     0.81
## q[2163]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2164]     0.77    0.00 0.02     0.73     0.76     0.77     0.79     0.81
## q[2165]     0.79    0.00 0.02     0.76     0.78     0.79     0.80     0.82
## q[2166]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2167]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2168]     0.79    0.00 0.02     0.76     0.78     0.79     0.80     0.82
## q[2169]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2170]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2171]     0.79    0.00 0.02     0.76     0.78     0.79     0.80     0.82
## q[2172]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2173]     0.89    0.00 0.01     0.85     0.88     0.89     0.90     0.91
## q[2174]     0.89    0.00 0.01     0.85     0.88     0.89     0.90     0.91
## q[2175]     0.84    0.00 0.02     0.80     0.83     0.84     0.86     0.88
## q[2176]     0.82    0.00 0.02     0.78     0.81     0.83     0.84     0.87
## q[2177]     0.89    0.00 0.01     0.85     0.88     0.89     0.90     0.91
## q[2178]     0.89    0.00 0.01     0.85     0.88     0.89     0.90     0.91
## q[2179]     0.89    0.00 0.01     0.85     0.88     0.89     0.90     0.91
## q[2180]     0.89    0.00 0.01     0.85     0.88     0.89     0.90     0.91
## q[2181]     0.82    0.00 0.02     0.78     0.81     0.83     0.84     0.87
## q[2182]     0.84    0.00 0.02     0.80     0.83     0.84     0.86     0.88
## q[2183]     0.84    0.00 0.02     0.80     0.83     0.84     0.86     0.88
## q[2184]     0.84    0.00 0.02     0.80     0.83     0.84     0.86     0.88
## q[2185]     0.89    0.00 0.01     0.85     0.88     0.89     0.90     0.91
## q[2186]     0.84    0.00 0.02     0.80     0.83     0.84     0.86     0.88
## q[2187]     0.89    0.00 0.01     0.85     0.88     0.89     0.90     0.91
## q[2188]     0.89    0.00 0.01     0.85     0.88     0.89     0.90     0.91
## q[2189]     0.89    0.00 0.01     0.85     0.88     0.89     0.90     0.91
## q[2190]     0.89    0.00 0.01     0.85     0.88     0.89     0.90     0.91
## q[2191]     0.89    0.00 0.01     0.85     0.88     0.89     0.90     0.91
## q[2192]     0.89    0.00 0.01     0.85     0.88     0.89     0.90     0.91
## q[2193]     0.82    0.00 0.02     0.78     0.81     0.83     0.84     0.87
## q[2194]     0.89    0.00 0.01     0.85     0.88     0.89     0.90     0.91
## q[2195]     0.89    0.00 0.01     0.85     0.88     0.89     0.90     0.91
## q[2196]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2197]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2198]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2199]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2200]     0.71    0.00 0.02     0.67     0.70     0.72     0.73     0.75
## q[2201]     0.69    0.00 0.03     0.64     0.67     0.69     0.71     0.74
## q[2202]     0.71    0.00 0.02     0.67     0.70     0.72     0.73     0.75
## q[2203]     0.69    0.00 0.03     0.64     0.67     0.69     0.71     0.74
## q[2204]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2205]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2206]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2207]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2208]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2209]     0.71    0.00 0.02     0.67     0.70     0.72     0.73     0.75
## q[2210]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2211]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2212]     0.71    0.00 0.02     0.67     0.70     0.72     0.73     0.75
## q[2213]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2214]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2215]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2216]     0.69    0.00 0.03     0.64     0.67     0.69     0.71     0.74
## q[2217]     0.71    0.00 0.02     0.67     0.70     0.72     0.73     0.75
## q[2218]     0.71    0.00 0.02     0.67     0.70     0.72     0.73     0.75
## q[2219]     0.71    0.00 0.02     0.67     0.70     0.72     0.73     0.75
## q[2220]     0.71    0.00 0.02     0.67     0.70     0.72     0.73     0.75
## q[2221]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2222]     0.69    0.00 0.03     0.64     0.67     0.69     0.71     0.74
## q[2223]     0.69    0.00 0.03     0.64     0.67     0.69     0.71     0.74
## q[2224]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2225]     0.69    0.00 0.03     0.64     0.67     0.69     0.71     0.74
## q[2226]     0.69    0.00 0.03     0.64     0.67     0.69     0.71     0.74
## q[2227]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2228]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2229]     0.71    0.00 0.02     0.67     0.70     0.72     0.73     0.75
## q[2230]     0.71    0.00 0.02     0.67     0.70     0.72     0.73     0.75
## q[2231]     0.69    0.00 0.03     0.64     0.67     0.69     0.71     0.74
## q[2232]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2233]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2234]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2235]     0.69    0.00 0.03     0.64     0.67     0.69     0.71     0.74
## q[2236]     0.69    0.00 0.03     0.64     0.67     0.69     0.71     0.74
## q[2237]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2238]     0.69    0.00 0.03     0.64     0.67     0.69     0.71     0.74
## q[2239]     0.71    0.00 0.02     0.67     0.70     0.72     0.73     0.75
## q[2240]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2241]     0.71    0.00 0.02     0.67     0.70     0.72     0.73     0.75
## q[2242]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2243]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2244]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2245]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2246]     0.69    0.00 0.03     0.64     0.67     0.69     0.71     0.74
## q[2247]     0.57    0.00 0.04     0.49     0.54     0.57     0.60     0.65
## q[2248]     0.60    0.00 0.04     0.52     0.57     0.60     0.62     0.67
## q[2249]     0.60    0.00 0.04     0.52     0.57     0.60     0.62     0.67
## q[2250]     0.60    0.00 0.04     0.52     0.57     0.60     0.62     0.67
## q[2251]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[2252]     0.60    0.00 0.04     0.52     0.57     0.60     0.62     0.67
## q[2253]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[2254]     0.60    0.00 0.04     0.52     0.57     0.60     0.62     0.67
## q[2255]     0.60    0.00 0.04     0.52     0.57     0.60     0.62     0.67
## q[2256]     0.60    0.00 0.04     0.52     0.57     0.60     0.62     0.67
## q[2257]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[2258]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[2259]     0.60    0.00 0.04     0.52     0.57     0.60     0.62     0.67
## q[2260]     0.60    0.00 0.04     0.52     0.57     0.60     0.62     0.67
## q[2261]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[2262]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[2263]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[2264]     0.60    0.00 0.04     0.52     0.57     0.60     0.62     0.67
## q[2265]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[2266]     0.57    0.00 0.04     0.49     0.54     0.57     0.60     0.65
## q[2267]     0.60    0.00 0.04     0.52     0.57     0.60     0.62     0.67
## q[2268]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[2269]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2270]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2271]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2272]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2273]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2274]     0.78    0.00 0.02     0.73     0.76     0.78     0.79     0.82
## q[2275]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2276]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2277]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2278]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2279]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2280]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2281]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2282]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2283]     0.78    0.00 0.02     0.73     0.76     0.78     0.79     0.82
## q[2284]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2285]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2286]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2287]     0.78    0.00 0.02     0.73     0.76     0.78     0.79     0.82
## q[2288]     0.78    0.00 0.02     0.73     0.76     0.78     0.79     0.82
## q[2289]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2290]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2291]     0.78    0.00 0.02     0.73     0.76     0.78     0.79     0.82
## q[2292]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2293]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2294]     0.78    0.00 0.02     0.73     0.76     0.78     0.79     0.82
## q[2295]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2296]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2297]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2298]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2299]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2300]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2301]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2302]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2303]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2304]     0.78    0.00 0.02     0.73     0.76     0.78     0.79     0.82
## q[2305]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2306]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2307]     0.70    0.00 0.02     0.66     0.69     0.70     0.72     0.74
## q[2308]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.73
## q[2309]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.81
## q[2310]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.81
## q[2311]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.73
## q[2312]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.81
## q[2313]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.81
## q[2314]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.81
## q[2315]     0.70    0.00 0.02     0.66     0.69     0.70     0.72     0.74
## q[2316]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.73
## q[2317]     0.70    0.00 0.02     0.66     0.69     0.70     0.72     0.74
## q[2318]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.81
## q[2319]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.81
## q[2320]     0.70    0.00 0.02     0.66     0.69     0.70     0.72     0.74
## q[2321]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.81
## q[2322]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.73
## q[2323]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.81
## q[2324]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.81
## q[2325]     0.70    0.00 0.02     0.66     0.69     0.70     0.72     0.74
## q[2326]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.81
## q[2327]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.73
## q[2328]     0.70    0.00 0.02     0.66     0.69     0.70     0.72     0.74
## q[2329]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.81
## q[2330]     0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[2331]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2332]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2333]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2334]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2335]     0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[2336]     0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[2337]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[2338]     0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[2339]     0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[2340]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[2341]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2342]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2343]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2344]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2345]     0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[2346]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[2347]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2348]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2349]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2350]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2351]     0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[2352]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2353]     0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[2354]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2355]     0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[2356]     0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[2357]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2358]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2359]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2360]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2361]     0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[2362]     0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[2363]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2364]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[2365]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2366]     0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[2367]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2368]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2369]     0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[2370]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[2371]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[2372]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2373]     0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[2374]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2375]     0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[2376]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2377]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[2378]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2379]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2380]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2381]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2382]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2383]     0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[2384]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[2385]     0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[2386]     0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[2387]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2388]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2389]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2390]     0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[2391]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[2392]     0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[2393]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2394]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2395]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[2396]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## lp__    -1379.64    0.04 1.55 -1383.58 -1380.50 -1379.29 -1378.49 -1377.56
##         n_eff Rhat
## b[1]     2165    1
## b[2]     2912    1
## b[3]     2184    1
## b[4]     3323    1
## b[5]     2891    1
## OR1      2931    1
## OR2      2256    1
## OR3      3316    1
## OR4      2857    1
## q[1]     2525    1
## q[2]     2329    1
## q[3]     2636    1
## q[4]     2329    1
## q[5]     2525    1
## q[6]     2525    1
## q[7]     2636    1
## q[8]     2525    1
## q[9]     2329    1
## q[10]    2329    1
## q[11]    2329    1
## q[12]    2636    1
## q[13]    2329    1
## q[14]    2329    1
## q[15]    2329    1
## q[16]    2329    1
## q[17]    2329    1
## q[18]    2525    1
## q[19]    2329    1
## q[20]    2329    1
## q[21]    2525    1
## q[22]    2525    1
## q[23]    2636    1
## q[24]    2636    1
## q[25]    2329    1
## q[26]    2329    1
## q[27]    2525    1
## q[28]    2329    1
## q[29]    2525    1
## q[30]    2329    1
## q[31]    2525    1
## q[32]    2329    1
## q[33]    2525    1
## q[34]    2329    1
## q[35]    2636    1
## q[36]    2329    1
## q[37]    2329    1
## q[38]    2329    1
## q[39]    2525    1
## q[40]    2329    1
## q[41]    2329    1
## q[42]    2525    1
## q[43]    2525    1
## q[44]    2894    1
## q[45]    2894    1
## q[46]    2894    1
## q[47]    3001    1
## q[48]    2894    1
## q[49]    3490    1
## q[50]    3490    1
## q[51]    3490    1
## q[52]    3490    1
## q[53]    2894    1
## q[54]    2894    1
## q[55]    2894    1
## q[56]    2894    1
## q[57]    2894    1
## q[58]    3001    1
## q[59]    2894    1
## q[60]    3001    1
## q[61]    2894    1
## q[62]    2894    1
## q[63]    2894    1
## q[64]    2894    1
## q[65]    3490    1
## q[66]    2894    1
## q[67]    3490    1
## q[68]    3001    1
## q[69]    3490    1
## q[70]    3001    1
## q[71]    3001    1
## q[72]    2894    1
## q[73]    3490    1
## q[74]    2894    1
## q[75]    3490    1
## q[76]    3001    1
## q[77]    2894    1
## q[78]    2894    1
## q[79]    3001    1
## q[80]    3490    1
## q[81]    3490    1
## q[82]    3001    1
## q[83]    3490    1
## q[84]    2894    1
## q[85]    2894    1
## q[86]    2894    1
## q[87]    2894    1
## q[88]    3490    1
## q[89]    2894    1
## q[90]    2894    1
## q[91]    3490    1
## q[92]    2894    1
## q[93]    2894    1
## q[94]    3490    1
## q[95]    3490    1
## q[96]    3490    1
## q[97]    2894    1
## q[98]    2894    1
## q[99]    3001    1
## q[100]   3050    1
## q[101]   3050    1
## q[102]   3327    1
## q[103]   3946    1
## q[104]   3946    1
## q[105]   3050    1
## q[106]   3050    1
## q[107]   3050    1
## q[108]   3327    1
## q[109]   3050    1
## q[110]   3050    1
## q[111]   3050    1
## q[112]   3946    1
## q[113]   3946    1
## q[114]   3327    1
## q[115]   3946    1
## q[116]   3050    1
## q[117]   3327    1
## q[118]   3050    1
## q[119]   3327    1
## q[120]   3946    1
## q[121]   3946    1
## q[122]   3050    1
## q[123]   3050    1
## q[124]   3050    1
## q[125]   3946    1
## q[126]   3050    1
## q[127]   3946    1
## q[128]   3946    1
## q[129]   3050    1
## q[130]   3050    1
## q[131]   3327    1
## q[132]   2890    1
## q[133]   2890    1
## q[134]   2890    1
## q[135]   3152    1
## q[136]   3152    1
## q[137]   2945    1
## q[138]   2890    1
## q[139]   2890    1
## q[140]   2945    1
## q[141]   2890    1
## q[142]   2890    1
## q[143]   2890    1
## q[144]   2890    1
## q[145]   2890    1
## q[146]   3152    1
## q[147]   2890    1
## q[148]   2890    1
## q[149]   2945    1
## q[150]   3152    1
## q[151]   3152    1
## q[152]   3152    1
## q[153]   2945    1
## q[154]   3152    1
## q[155]   2945    1
## q[156]   2890    1
## q[157]   2890    1
## q[158]   2945    1
## q[159]   2890    1
## q[160]   3152    1
## q[161]   2890    1
## q[162]   2945    1
## q[163]   3152    1
## q[164]   2890    1
## q[165]   2890    1
## q[166]   3152    1
## q[167]   2890    1
## q[168]   2890    1
## q[169]   2945    1
## q[170]   2890    1
## q[171]   2945    1
## q[172]   3152    1
## q[173]   2890    1
## q[174]   2890    1
## q[175]   2890    1
## q[176]   2890    1
## q[177]   2823    1
## q[178]   3152    1
## q[179]   3152    1
## q[180]   2823    1
## q[181]   3152    1
## q[182]   2650    1
## q[183]   2650    1
## q[184]   3152    1
## q[185]   2650    1
## q[186]   2650    1
## q[187]   3152    1
## q[188]   3152    1
## q[189]   3152    1
## q[190]   2650    1
## q[191]   3152    1
## q[192]   2650    1
## q[193]   3152    1
## q[194]   3152    1
## q[195]   2650    1
## q[196]   2650    1
## q[197]   3152    1
## q[198]   2650    1
## q[199]   2650    1
## q[200]   2650    1
## q[201]   2650    1
## q[202]   2650    1
## q[203]   3152    1
## q[204]   2650    1
## q[205]   2823    1
## q[206]   2823    1
## q[207]   2650    1
## q[208]   3152    1
## q[209]   2650    1
## q[210]   2998    1
## q[211]   2998    1
## q[212]   2998    1
## q[213]   3292    1
## q[214]   2998    1
## q[215]   4018    1
## q[216]   4018    1
## q[217]   3292    1
## q[218]   4018    1
## q[219]   4018    1
## q[220]   3292    1
## q[221]   2998    1
## q[222]   2998    1
## q[223]   2998    1
## q[224]   3292    1
## q[225]   2998    1
## q[226]   2998    1
## q[227]   2998    1
## q[228]   2998    1
## q[229]   2998    1
## q[230]   4018    1
## q[231]   2998    1
## q[232]   2998    1
## q[233]   2998    1
## q[234]   4018    1
## q[235]   2998    1
## q[236]   3292    1
## q[237]   4018    1
## q[238]   2998    1
## q[239]   4018    1
## q[240]   4018    1
## q[241]   2998    1
## q[242]   2998    1
## q[243]   4018    1
## q[244]   3292    1
## q[245]   4018    1
## q[246]   3292    1
## q[247]   2998    1
## q[248]   2998    1
## q[249]   4018    1
## q[250]   3292    1
## q[251]   2998    1
## q[252]   2998    1
## q[253]   2998    1
## q[254]   2998    1
## q[255]   2998    1
## q[256]   2998    1
## q[257]   3292    1
## q[258]   3292    1
## q[259]   4018    1
## q[260]   4018    1
## q[261]   2998    1
## q[262]   2998    1
## q[263]   2998    1
## q[264]   4018    1
## q[265]   4018    1
## q[266]   2998    1
## q[267]   4018    1
## q[268]   2998    1
## q[269]   3292    1
## q[270]   3292    1
## q[271]   2933    1
## q[272]   3465    1
## q[273]   3465    1
## q[274]   3257    1
## q[275]   2933    1
## q[276]   2933    1
## q[277]   3257    1
## q[278]   2933    1
## q[279]   3465    1
## q[280]   2933    1
## q[281]   2933    1
## q[282]   3465    1
## q[283]   2933    1
## q[284]   3465    1
## q[285]   2933    1
## q[286]   2933    1
## q[287]   3465    1
## q[288]   3257    1
## q[289]   3465    1
## q[290]   2933    1
## q[291]   3465    1
## q[292]   3465    1
## q[293]   3257    1
## q[294]   3465    1
## q[295]   3257    1
## q[296]   2933    1
## q[297]   2933    1
## q[298]   3257    1
## q[299]   2933    1
## q[300]   3465    1
## q[301]   2933    1
## q[302]   3465    1
## q[303]   3257    1
## q[304]   3465    1
## q[305]   2933    1
## q[306]   2933    1
## q[307]   2933    1
## q[308]   2933    1
## q[309]   2933    1
## q[310]   3465    1
## q[311]   2933    1
## q[312]   3465    1
## q[313]   2933    1
## q[314]   3257    1
## q[315]   3465    1
## q[316]   3257    1
## q[317]   2933    1
## q[318]   2933    1
## q[319]   2933    1
## q[320]   3447    1
## q[321]   2850    1
## q[322]   2850    1
## q[323]   2976    1
## q[324]   2850    1
## q[325]   2850    1
## q[326]   3447    1
## q[327]   3447    1
## q[328]   3447    1
## q[329]   2976    1
## q[330]   3447    1
## q[331]   2976    1
## q[332]   2850    1
## q[333]   2850    1
## q[334]   2850    1
## q[335]   2850    1
## q[336]   2976    1
## q[337]   2850    1
## q[338]   2850    1
## q[339]   2850    1
## q[340]   2850    1
## q[341]   2976    1
## q[342]   2850    1
## q[343]   2850    1
## q[344]   2850    1
## q[345]   2850    1
## q[346]   3447    1
## q[347]   2850    1
## q[348]   2850    1
## q[349]   2850    1
## q[350]   2850    1
## q[351]   3447    1
## q[352]   2850    1
## q[353]   3447    1
## q[354]   2976    1
## q[355]   3447    1
## q[356]   3447    1
## q[357]   3447    1
## q[358]   3447    1
## q[359]   2976    1
## q[360]   2850    1
## q[361]   3447    1
## q[362]   2850    1
## q[363]   2976    1
## q[364]   2850    1
## q[365]   2850    1
## q[366]   2976    1
## q[367]   3447    1
## q[368]   2850    1
## q[369]   2850    1
## q[370]   3447    1
## q[371]   2850    1
## q[372]   2850    1
## q[373]   2976    1
## q[374]   3447    1
## q[375]   2850    1
## q[376]   2850    1
## q[377]   2850    1
## q[378]   3447    1
## q[379]   2850    1
## q[380]   2850    1
## q[381]   2850    1
## q[382]   2976    1
## q[383]   2850    1
## q[384]   2976    1
## q[385]   3447    1
## q[386]   2850    1
## q[387]   3447    1
## q[388]   3447    1
## q[389]   2850    1
## q[390]   3447    1
## q[391]   2850    1
## q[392]   2850    1
## q[393]   2850    1
## q[394]   2976    1
## q[395]   2850    1
## q[396]   3767    1
## q[397]   2879    1
## q[398]   2879    1
## q[399]   3204    1
## q[400]   2879    1
## q[401]   2879    1
## q[402]   2879    1
## q[403]   3767    1
## q[404]   3767    1
## q[405]   3767    1
## q[406]   3767    1
## q[407]   3204    1
## q[408]   3767    1
## q[409]   3204    1
## q[410]   2879    1
## q[411]   2879    1
## q[412]   2879    1
## q[413]   2879    1
## q[414]   2879    1
## q[415]   2879    1
## q[416]   3767    1
## q[417]   3204    1
## q[418]   2879    1
## q[419]   2879    1
## q[420]   2879    1
## q[421]   2879    1
## q[422]   2879    1
## q[423]   3767    1
## q[424]   2879    1
## q[425]   2879    1
## q[426]   3767    1
## q[427]   3767    1
## q[428]   2879    1
## q[429]   2879    1
## q[430]   3767    1
## q[431]   3767    1
## q[432]   3767    1
## q[433]   3767    1
## q[434]   2879    1
## q[435]   3204    1
## q[436]   3767    1
## q[437]   2879    1
## q[438]   3767    1
## q[439]   2879    1
## q[440]   3767    1
## q[441]   2879    1
## q[442]   3767    1
## q[443]   3767    1
## q[444]   2879    1
## q[445]   2879    1
## q[446]   2879    1
## q[447]   3767    1
## q[448]   2879    1
## q[449]   2879    1
## q[450]   2879    1
## q[451]   2879    1
## q[452]   2879    1
## q[453]   2879    1
## q[454]   2879    1
## q[455]   2879    1
## q[456]   2879    1
## q[457]   3767    1
## q[458]   2879    1
## q[459]   3204    1
## q[460]   2879    1
## q[461]   3204    1
## q[462]   3767    1
## q[463]   2879    1
## q[464]   2879    1
## q[465]   3204    1
## q[466]   3767    1
## q[467]   2879    1
## q[468]   3767    1
## q[469]   2879    1
## q[470]   2879    1
## q[471]   2879    1
## q[472]   3204    1
## q[473]   2879    1
## q[474]   3790    1
## q[475]   2886    1
## q[476]   2886    1
## q[477]   2886    1
## q[478]   3790    1
## q[479]   3790    1
## q[480]   3209    1
## q[481]   3209    1
## q[482]   3209    1
## q[483]   2886    1
## q[484]   2886    1
## q[485]   2886    1
## q[486]   2886    1
## q[487]   3209    1
## q[488]   2886    1
## q[489]   2886    1
## q[490]   2886    1
## q[491]   3790    1
## q[492]   2886    1
## q[493]   2886    1
## q[494]   2886    1
## q[495]   2886    1
## q[496]   3790    1
## q[497]   2886    1
## q[498]   3790    1
## q[499]   2886    1
## q[500]   2886    1
## q[501]   2886    1
## q[502]   3209    1
## q[503]   3790    1
## q[504]   3790    1
## q[505]   3790    1
## q[506]   3790    1
## q[507]   2886    1
## q[508]   2886    1
## q[509]   3209    1
## q[510]   3209    1
## q[511]   2886    1
## q[512]   3209    1
## q[513]   3209    1
## q[514]   2886    1
## q[515]   2886    1
## q[516]   2886    1
## q[517]   2886    1
## q[518]   3790    1
## q[519]   3790    1
## q[520]   3209    1
## q[521]   2886    1
## q[522]   2886    1
## q[523]   2886    1
## q[524]   2886    1
## q[525]   3209    1
## q[526]   3209    1
## q[527]   3790    1
## q[528]   3790    1
## q[529]   2886    1
## q[530]   2886    1
## q[531]   3790    1
## q[532]   3790    1
## q[533]   2886    1
## q[534]   2886    1
## q[535]   2886    1
## q[536]   3209    1
## q[537]   3209    1
## q[538]   2987    1
## q[539]   2987    1
## q[540]   2987    1
## q[541]   3577    1
## q[542]   3577    1
## q[543]   2987    1
## q[544]   2987    1
## q[545]   3049    1
## q[546]   2987    1
## q[547]   2987    1
## q[548]   2987    1
## q[549]   2987    1
## q[550]   2987    1
## q[551]   3577    1
## q[552]   3049    1
## q[553]   3577    1
## q[554]   3577    1
## q[555]   3049    1
## q[556]   3049    1
## q[557]   2987    1
## q[558]   2987    1
## q[559]   3049    1
## q[560]   3577    1
## q[561]   3049    1
## q[562]   3577    1
## q[563]   2987    1
## q[564]   3577    1
## q[565]   2987    1
## q[566]   3049    1
## q[567]   2987    1
## q[568]   3577    1
## q[569]   2987    1
## q[570]   2987    1
## q[571]   2987    1
## q[572]   2949    1
## q[573]   2949    1
## q[574]   3955    1
## q[575]   3955    1
## q[576]   3258    1
## q[577]   3955    1
## q[578]   2949    1
## q[579]   2949    1
## q[580]   3955    1
## q[581]   3258    1
## q[582]   2949    1
## q[583]   2949    1
## q[584]   3955    1
## q[585]   2949    1
## q[586]   3955    1
## q[587]   2949    1
## q[588]   3955    1
## q[589]   2949    1
## q[590]   2949    1
## q[591]   3955    1
## q[592]   2949    1
## q[593]   2949    1
## q[594]   3258    1
## q[595]   3258    1
## q[596]   3955    1
## q[597]   3955    1
## q[598]   2949    1
## q[599]   3258    1
## q[600]   2949    1
## q[601]   2949    1
## q[602]   2949    1
## q[603]   3955    1
## q[604]   3258    1
## q[605]   3955    1
## q[606]   3955    1
## q[607]   2949    1
## q[608]   2949    1
## q[609]   3955    1
## q[610]   3258    1
## q[611]   3955    1
## q[612]   2949    1
## q[613]   2949    1
## q[614]   3258    1
## q[615]   3258    1
## q[616]   3844    1
## q[617]   3416    1
## q[618]   3416    1
## q[619]   3844    1
## q[620]   3844    1
## q[621]   3199    1
## q[622]   3844    1
## q[623]   3199    1
## q[624]   3416    1
## q[625]   3416    1
## q[626]   3416    1
## q[627]   3416    1
## q[628]   3199    1
## q[629]   3416    1
## q[630]   3199    1
## q[631]   3416    1
## q[632]   3416    1
## q[633]   3416    1
## q[634]   3416    1
## q[635]   3416    1
## q[636]   3416    1
## q[637]   3844    1
## q[638]   3844    1
## q[639]   3416    1
## q[640]   3416    1
## q[641]   3416    1
## q[642]   3844    1
## q[643]   3199    1
## q[644]   3416    1
## q[645]   3416    1
## q[646]   3199    1
## q[647]   3416    1
## q[648]   3199    1
## q[649]   3416    1
## q[650]   3844    1
## q[651]   3416    1
## q[652]   3199    1
## q[653]   3844    1
## q[654]   3199    1
## q[655]   3416    1
## q[656]   3416    1
## q[657]   3844    1
## q[658]   3199    1
## q[659]   3844    1
## q[660]   3844    1
## q[661]   3416    1
## q[662]   3416    1
## q[663]   3844    1
## q[664]   3844    1
## q[665]   3844    1
## q[666]   3416    1
## q[667]   3199    1
## q[668]   3199    1
## q[669]   3465    1
## q[670]   2933    1
## q[671]   3465    1
## q[672]   3257    1
## q[673]   3257    1
## q[674]   3257    1
## q[675]   2933    1
## q[676]   2933    1
## q[677]   2933    1
## q[678]   2933    1
## q[679]   2933    1
## q[680]   2933    1
## q[681]   3465    1
## q[682]   2933    1
## q[683]   3465    1
## q[684]   2933    1
## q[685]   2933    1
## q[686]   2933    1
## q[687]   3465    1
## q[688]   3465    1
## q[689]   3465    1
## q[690]   2933    1
## q[691]   2933    1
## q[692]   3257    1
## q[693]   3257    1
## q[694]   2933    1
## q[695]   2933    1
## q[696]   2933    1
## q[697]   2933    1
## q[698]   3465    1
## q[699]   3257    1
## q[700]   2933    1
## q[701]   2933    1
## q[702]   3257    1
## q[703]   3257    1
## q[704]   3465    1
## q[705]   2933    1
## q[706]   3465    1
## q[707]   2933    1
## q[708]   3257    1
## q[709]   2891    1
## q[710]   2891    1
## q[711]   2891    1
## q[712]   2891    1
## q[713]   3413    1
## q[714]   3413    1
## q[715]   3235    1
## q[716]   3413    1
## q[717]   3413    1
## q[718]   2891    1
## q[719]   2891    1
## q[720]   2891    1
## q[721]   3413    1
## q[722]   3235    1
## q[723]   2891    1
## q[724]   2891    1
## q[725]   3413    1
## q[726]   2891    1
## q[727]   3413    1
## q[728]   2891    1
## q[729]   3413    1
## q[730]   2891    1
## q[731]   2891    1
## q[732]   2891    1
## q[733]   3413    1
## q[734]   2891    1
## q[735]   3235    1
## q[736]   3413    1
## q[737]   2891    1
## q[738]   2891    1
## q[739]   3413    1
## q[740]   3235    1
## q[741]   3235    1
## q[742]   3413    1
## q[743]   3413    1
## q[744]   2891    1
## q[745]   3235    1
## q[746]   2891    1
## q[747]   2891    1
## q[748]   2891    1
## q[749]   2891    1
## q[750]   2891    1
## q[751]   3413    1
## q[752]   3235    1
## q[753]   3413    1
## q[754]   3413    1
## q[755]   2891    1
## q[756]   2891    1
## q[757]   2891    1
## q[758]   3413    1
## q[759]   3235    1
## q[760]   3413    1
## q[761]   2891    1
## q[762]   2891    1
## q[763]   3235    1
## q[764]   3235    1
## q[765]   3569    1
## q[766]   3165    1
## q[767]   3165    1
## q[768]   3165    1
## q[769]   3165    1
## q[770]   3569    1
## q[771]   3569    1
## q[772]   3569    1
## q[773]   3569    1
## q[774]   3086    1
## q[775]   3165    1
## q[776]   3165    1
## q[777]   3165    1
## q[778]   3086    1
## q[779]   3165    1
## q[780]   3165    1
## q[781]   3165    1
## q[782]   3569    1
## q[783]   3165    1
## q[784]   3165    1
## q[785]   3165    1
## q[786]   3569    1
## q[787]   3165    1
## q[788]   3569    1
## q[789]   3569    1
## q[790]   3165    1
## q[791]   3165    1
## q[792]   3569    1
## q[793]   3569    1
## q[794]   3569    1
## q[795]   3165    1
## q[796]   3086    1
## q[797]   3086    1
## q[798]   3165    1
## q[799]   3569    1
## q[800]   3165    1
## q[801]   3569    1
## q[802]   3165    1
## q[803]   3165    1
## q[804]   3569    1
## q[805]   3569    1
## q[806]   3165    1
## q[807]   3086    1
## q[808]   3165    1
## q[809]   3165    1
## q[810]   3165    1
## q[811]   3165    1
## q[812]   3165    1
## q[813]   3165    1
## q[814]   3569    1
## q[815]   3569    1
## q[816]   3086    1
## q[817]   3165    1
## q[818]   3165    1
## q[819]   3569    1
## q[820]   3086    1
## q[821]   3165    1
## q[822]   3165    1
## q[823]   3165    1
## q[824]   2994    1
## q[825]   2994    1
## q[826]   2994    1
## q[827]   3551    1
## q[828]   3551    1
## q[829]   3551    1
## q[830]   2994    1
## q[831]   2994    1
## q[832]   2994    1
## q[833]   3551    1
## q[834]   3285    1
## q[835]   2994    1
## q[836]   3551    1
## q[837]   2994    1
## q[838]   3551    1
## q[839]   2994    1
## q[840]   3551    1
## q[841]   2994    1
## q[842]   3285    1
## q[843]   3551    1
## q[844]   2994    1
## q[845]   3551    1
## q[846]   3551    1
## q[847]   2994    1
## q[848]   2994    1
## q[849]   2994    1
## q[850]   2994    1
## q[851]   2994    1
## q[852]   2994    1
## q[853]   3551    1
## q[854]   3551    1
## q[855]   2994    1
## q[856]   2994    1
## q[857]   3285    1
## q[858]   3551    1
## q[859]   2994    1
## q[860]   2994    1
## q[861]   3285    1
## q[862]   2746    1
## q[863]   2434    1
## q[864]   2434    1
## q[865]   2434    1
## q[866]   2434    1
## q[867]   2434    1
## q[868]   2746    1
## q[869]   2746    1
## q[870]   2746    1
## q[871]   2797    1
## q[872]   2434    1
## q[873]   2434    1
## q[874]   2434    1
## q[875]   2434    1
## q[876]   2434    1
## q[877]   2434    1
## q[878]   2797    1
## q[879]   2434    1
## q[880]   2434    1
## q[881]   2434    1
## q[882]   2746    1
## q[883]   2434    1
## q[884]   2434    1
## q[885]   2746    1
## q[886]   2746    1
## q[887]   2797    1
## q[888]   2434    1
## q[889]   2746    1
## q[890]   2434    1
## q[891]   2746    1
## q[892]   2434    1
## q[893]   2434    1
## q[894]   2434    1
## q[895]   2434    1
## q[896]   2434    1
## q[897]   2797    1
## q[898]   2434    1
## q[899]   2746    1
## q[900]   2434    1
## q[901]   2434    1
## q[902]   2746    1
## q[903]   2434    1
## q[904]   2434    1
## q[905]   2797    1
## q[906]   2434    1
## q[907]   2523    1
## q[908]   2798    1
## q[909]   2975    1
## q[910]   2975    1
## q[911]   2798    1
## q[912]   2523    1
## q[913]   2975    1
## q[914]   2523    1
## q[915]   2523    1
## q[916]   2975    1
## q[917]   2523    1
## q[918]   2975    1
## q[919]   2523    1
## q[920]   2523    1
## q[921]   2975    1
## q[922]   2975    1
## q[923]   2798    1
## q[924]   2975    1
## q[925]   2523    1
## q[926]   2975    1
## q[927]   2975    1
## q[928]   2523    1
## q[929]   2975    1
## q[930]   2798    1
## q[931]   2523    1
## q[932]   2798    1
## q[933]   2975    1
## q[934]   2523    1
## q[935]   2523    1
## q[936]   2975    1
## q[937]   2975    1
## q[938]   2523    1
## q[939]   2523    1
## q[940]   2523    1
## q[941]   2523    1
## q[942]   2523    1
## q[943]   2975    1
## q[944]   2523    1
## q[945]   2523    1
## q[946]   2798    1
## q[947]   2798    1
## q[948]   2523    1
## q[949]   2975    1
## q[950]   2523    1
## q[951]   2523    1
## q[952]   2681    1
## q[953]   2681    1
## q[954]   2681    1
## q[955]   2681    1
## q[956]   2681    1
## q[957]   3216    1
## q[958]   3216    1
## q[959]   2854    1
## q[960]   3216    1
## q[961]   3216    1
## q[962]   2854    1
## q[963]   2681    1
## q[964]   2681    1
## q[965]   2681    1
## q[966]   2681    1
## q[967]   2681    1
## q[968]   2854    1
## q[969]   2681    1
## q[970]   2854    1
## q[971]   2681    1
## q[972]   2681    1
## q[973]   2681    1
## q[974]   2681    1
## q[975]   3216    1
## q[976]   2681    1
## q[977]   2681    1
## q[978]   3216    1
## q[979]   2681    1
## q[980]   2681    1
## q[981]   2681    1
## q[982]   2681    1
## q[983]   2854    1
## q[984]   3216    1
## q[985]   2854    1
## q[986]   3216    1
## q[987]   2854    1
## q[988]   3216    1
## q[989]   3216    1
## q[990]   2681    1
## q[991]   2681    1
## q[992]   3216    1
## q[993]   2854    1
## q[994]   2854    1
## q[995]   2681    1
## q[996]   2681    1
## q[997]   2854    1
## q[998]   2854    1
## q[999]   2681    1
## q[1000]  3216    1
## q[1001]  2681    1
## q[1002]  2854    1
## q[1003]  3216    1
## q[1004]  3216    1
## q[1005]  2854    1
## q[1006]  2681    1
## q[1007]  2681    1
## q[1008]  2681    1
## q[1009]  2681    1
## q[1010]  2681    1
## q[1011]  3216    1
## q[1012]  2681    1
## q[1013]  2681    1
## q[1014]  2854    1
## q[1015]  2854    1
## q[1016]  3216    1
## q[1017]  3216    1
## q[1018]  2681    1
## q[1019]  2681    1
## q[1020]  2681    1
## q[1021]  3216    1
## q[1022]  3216    1
## q[1023]  3216    1
## q[1024]  2681    1
## q[1025]  2681    1
## q[1026]  2681    1
## q[1027]  2854    1
## q[1028]  2854    1
## q[1029]  3152    1
## q[1030]  2890    1
## q[1031]  2890    1
## q[1032]  2890    1
## q[1033]  3152    1
## q[1034]  3152    1
## q[1035]  3152    1
## q[1036]  2945    1
## q[1037]  2945    1
## q[1038]  2890    1
## q[1039]  2890    1
## q[1040]  2945    1
## q[1041]  2890    1
## q[1042]  2890    1
## q[1043]  2890    1
## q[1044]  2890    1
## q[1045]  3152    1
## q[1046]  2890    1
## q[1047]  2890    1
## q[1048]  3152    1
## q[1049]  2890    1
## q[1050]  2890    1
## q[1051]  2890    1
## q[1052]  3152    1
## q[1053]  2890    1
## q[1054]  2890    1
## q[1055]  3152    1
## q[1056]  3152    1
## q[1057]  2890    1
## q[1058]  2945    1
## q[1059]  3152    1
## q[1060]  3152    1
## q[1061]  3152    1
## q[1062]  2890    1
## q[1063]  3152    1
## q[1064]  3152    1
## q[1065]  2945    1
## q[1066]  3152    1
## q[1067]  2890    1
## q[1068]  2945    1
## q[1069]  2890    1
## q[1070]  2890    1
## q[1071]  2945    1
## q[1072]  2890    1
## q[1073]  2890    1
## q[1074]  3152    1
## q[1075]  2890    1
## q[1076]  3152    1
## q[1077]  2890    1
## q[1078]  2945    1
## q[1079]  3152    1
## q[1080]  2890    1
## q[1081]  2890    1
## q[1082]  2890    1
## q[1083]  2890    1
## q[1084]  2890    1
## q[1085]  3152    1
## q[1086]  2890    1
## q[1087]  3152    1
## q[1088]  2890    1
## q[1089]  2945    1
## q[1090]  2890    1
## q[1091]  2945    1
## q[1092]  3152    1
## q[1093]  2945    1
## q[1094]  2890    1
## q[1095]  2890    1
## q[1096]  2890    1
## q[1097]  2890    1
## q[1098]  2890    1
## q[1099]  3343    1
## q[1100]  3343    1
## q[1101]  3343    1
## q[1102]  3343    1
## q[1103]  3343    1
## q[1104]  3737    1
## q[1105]  3162    1
## q[1106]  3737    1
## q[1107]  3737    1
## q[1108]  3737    1
## q[1109]  3737    1
## q[1110]  3162    1
## q[1111]  3343    1
## q[1112]  3343    1
## q[1113]  3343    1
## q[1114]  3162    1
## q[1115]  3343    1
## q[1116]  3343    1
## q[1117]  3737    1
## q[1118]  3343    1
## q[1119]  3343    1
## q[1120]  3343    1
## q[1121]  3737    1
## q[1122]  3343    1
## q[1123]  3737    1
## q[1124]  3343    1
## q[1125]  3343    1
## q[1126]  3737    1
## q[1127]  3343    1
## q[1128]  3343    1
## q[1129]  3343    1
## q[1130]  3737    1
## q[1131]  3737    1
## q[1132]  3343    1
## q[1133]  3162    1
## q[1134]  3737    1
## q[1135]  3162    1
## q[1136]  3737    1
## q[1137]  3737    1
## q[1138]  3343    1
## q[1139]  3343    1
## q[1140]  3737    1
## q[1141]  3162    1
## q[1142]  3343    1
## q[1143]  3162    1
## q[1144]  3343    1
## q[1145]  3737    1
## q[1146]  3343    1
## q[1147]  3737    1
## q[1148]  3162    1
## q[1149]  3737    1
## q[1150]  3343    1
## q[1151]  3162    1
## q[1152]  3343    1
## q[1153]  3343    1
## q[1154]  3343    1
## q[1155]  3343    1
## q[1156]  3343    1
## q[1157]  3343    1
## q[1158]  3737    1
## q[1159]  3343    1
## q[1160]  3737    1
## q[1161]  3162    1
## q[1162]  3162    1
## q[1163]  3343    1
## q[1164]  3162    1
## q[1165]  3737    1
## q[1166]  3343    1
## q[1167]  3343    1
## q[1168]  3737    1
## q[1169]  3737    1
## q[1170]  3162    1
## q[1171]  3343    1
## q[1172]  3343    1
## q[1173]  3343    1
## q[1174]  3343    1
## q[1175]  3162    1
## q[1176]  3703    1
## q[1177]  3309    1
## q[1178]  3309    1
## q[1179]  3148    1
## q[1180]  3703    1
## q[1181]  3148    1
## q[1182]  3703    1
## q[1183]  3148    1
## q[1184]  3703    1
## q[1185]  3309    1
## q[1186]  3309    1
## q[1187]  3703    1
## q[1188]  3309    1
## q[1189]  3309    1
## q[1190]  3309    1
## q[1191]  3703    1
## q[1192]  3309    1
## q[1193]  3309    1
## q[1194]  3703    1
## q[1195]  3309    1
## q[1196]  3703    1
## q[1197]  3309    1
## q[1198]  3703    1
## q[1199]  3703    1
## q[1200]  3309    1
## q[1201]  3309    1
## q[1202]  3703    1
## q[1203]  3309    1
## q[1204]  3148    1
## q[1205]  3703    1
## q[1206]  3703    1
## q[1207]  3309    1
## q[1208]  3703    1
## q[1209]  3309    1
## q[1210]  3309    1
## q[1211]  3309    1
## q[1212]  3148    1
## q[1213]  3148    1
## q[1214]  3309    1
## q[1215]  3148    1
## q[1216]  3703    1
## q[1217]  3148    1
## q[1218]  3309    1
## q[1219]  3703    1
## q[1220]  3309    1
## q[1221]  3703    1
## q[1222]  3703    1
## q[1223]  3309    1
## q[1224]  3148    1
## q[1225]  3309    1
## q[1226]  3309    1
## q[1227]  3703    1
## q[1228]  3309    1
## q[1229]  3309    1
## q[1230]  3148    1
## q[1231]  3703    1
## q[1232]  3309    1
## q[1233]  3703    1
## q[1234]  3148    1
## q[1235]  3703    1
## q[1236]  3309    1
## q[1237]  3309    1
## q[1238]  3148    1
## q[1239]  2939    1
## q[1240]  2939    1
## q[1241]  2939    1
## q[1242]  3025    1
## q[1243]  2939    1
## q[1244]  2939    1
## q[1245]  2939    1
## q[1246]  3534    1
## q[1247]  3534    1
## q[1248]  3534    1
## q[1249]  3534    1
## q[1250]  3534    1
## q[1251]  2939    1
## q[1252]  2939    1
## q[1253]  2939    1
## q[1254]  2939    1
## q[1255]  2939    1
## q[1256]  2939    1
## q[1257]  3534    1
## q[1258]  3025    1
## q[1259]  2939    1
## q[1260]  2939    1
## q[1261]  2939    1
## q[1262]  3534    1
## q[1263]  3534    1
## q[1264]  2939    1
## q[1265]  3534    1
## q[1266]  2939    1
## q[1267]  3534    1
## q[1268]  3025    1
## q[1269]  3534    1
## q[1270]  3534    1
## q[1271]  2939    1
## q[1272]  3534    1
## q[1273]  3025    1
## q[1274]  2939    1
## q[1275]  3534    1
## q[1276]  2939    1
## q[1277]  3534    1
## q[1278]  3025    1
## q[1279]  2939    1
## q[1280]  3025    1
## q[1281]  3534    1
## q[1282]  3534    1
## q[1283]  3534    1
## q[1284]  2939    1
## q[1285]  2939    1
## q[1286]  2939    1
## q[1287]  2939    1
## q[1288]  2939    1
## q[1289]  2939    1
## q[1290]  3534    1
## q[1291]  2939    1
## q[1292]  2939    1
## q[1293]  3025    1
## q[1294]  2939    1
## q[1295]  3534    1
## q[1296]  2939    1
## q[1297]  2939    1
## q[1298]  3025    1
## q[1299]  3534    1
## q[1300]  3534    1
## q[1301]  2939    1
## q[1302]  2939    1
## q[1303]  2939    1
## q[1304]  2939    1
## q[1305]  3025    1
## q[1306]  2939    1
## q[1307]  3417    1
## q[1308]  3417    1
## q[1309]  3417    1
## q[1310]  3839    1
## q[1311]  3839    1
## q[1312]  3839    1
## q[1313]  3839    1
## q[1314]  3417    1
## q[1315]  3417    1
## q[1316]  3417    1
## q[1317]  3417    1
## q[1318]  3417    1
## q[1319]  3198    1
## q[1320]  3839    1
## q[1321]  3198    1
## q[1322]  3417    1
## q[1323]  3417    1
## q[1324]  3839    1
## q[1325]  3417    1
## q[1326]  3839    1
## q[1327]  3417    1
## q[1328]  3839    1
## q[1329]  3417    1
## q[1330]  3198    1
## q[1331]  3198    1
## q[1332]  3839    1
## q[1333]  3417    1
## q[1334]  3839    1
## q[1335]  3417    1
## q[1336]  3839    1
## q[1337]  3839    1
## q[1338]  3198    1
## q[1339]  3417    1
## q[1340]  3417    1
## q[1341]  3417    1
## q[1342]  3417    1
## q[1343]  3417    1
## q[1344]  3417    1
## q[1345]  3417    1
## q[1346]  3839    1
## q[1347]  3839    1
## q[1348]  3839    1
## q[1349]  3417    1
## q[1350]  3417    1
## q[1351]  3839    1
## q[1352]  3198    1
## q[1353]  3839    1
## q[1354]  3417    1
## q[1355]  3417    1
## q[1356]  3198    1
## q[1357]  3294    1
## q[1358]  3294    1
## q[1359]  3294    1
## q[1360]  3294    1
## q[1361]  3294    1
## q[1362]  3805    1
## q[1363]  3170    1
## q[1364]  3805    1
## q[1365]  3805    1
## q[1366]  3170    1
## q[1367]  3294    1
## q[1368]  3294    1
## q[1369]  3294    1
## q[1370]  3294    1
## q[1371]  3294    1
## q[1372]  3294    1
## q[1373]  3805    1
## q[1374]  3294    1
## q[1375]  3294    1
## q[1376]  3294    1
## q[1377]  3294    1
## q[1378]  3805    1
## q[1379]  3170    1
## q[1380]  3805    1
## q[1381]  3805    1
## q[1382]  3805    1
## q[1383]  3294    1
## q[1384]  3294    1
## q[1385]  3805    1
## q[1386]  3170    1
## q[1387]  3170    1
## q[1388]  3294    1
## q[1389]  3294    1
## q[1390]  3805    1
## q[1391]  3170    1
## q[1392]  3294    1
## q[1393]  3294    1
## q[1394]  3294    1
## q[1395]  3294    1
## q[1396]  3170    1
## q[1397]  3170    1
## q[1398]  3294    1
## q[1399]  3170    1
## q[1400]  3805    1
## q[1401]  3294    1
## q[1402]  3294    1
## q[1403]  3805    1
## q[1404]  3294    1
## q[1405]  3294    1
## q[1406]  3294    1
## q[1407]  3170    1
## q[1408]  3317    1
## q[1409]  3055    1
## q[1410]  3055    1
## q[1411]  3753    1
## q[1412]  3753    1
## q[1413]  3317    1
## q[1414]  3055    1
## q[1415]  3055    1
## q[1416]  3055    1
## q[1417]  3055    1
## q[1418]  3055    1
## q[1419]  3753    1
## q[1420]  3753    1
## q[1421]  3753    1
## q[1422]  3055    1
## q[1423]  3753    1
## q[1424]  3753    1
## q[1425]  3055    1
## q[1426]  3055    1
## q[1427]  3055    1
## q[1428]  3055    1
## q[1429]  3055    1
## q[1430]  3317    1
## q[1431]  3055    1
## q[1432]  3317    1
## q[1433]  3055    1
## q[1434]  3753    1
## q[1435]  3055    1
## q[1436]  3055    1
## q[1437]  3055    1
## q[1438]  3753    1
## q[1439]  3317    1
## q[1440]  3055    1
## q[1441]  3753    1
## q[1442]  3055    1
## q[1443]  3753    1
## q[1444]  3055    1
## q[1445]  3753    1
## q[1446]  3055    1
## q[1447]  3055    1
## q[1448]  3753    1
## q[1449]  3317    1
## q[1450]  3753    1
## q[1451]  3055    1
## q[1452]  3753    1
## q[1453]  3753    1
## q[1454]  3753    1
## q[1455]  3317    1
## q[1456]  3055    1
## q[1457]  3317    1
## q[1458]  3055    1
## q[1459]  3055    1
## q[1460]  3753    1
## q[1461]  3753    1
## q[1462]  3055    1
## q[1463]  3055    1
## q[1464]  3055    1
## q[1465]  3055    1
## q[1466]  3055    1
## q[1467]  3753    1
## q[1468]  3055    1
## q[1469]  3317    1
## q[1470]  3317    1
## q[1471]  3055    1
## q[1472]  3055    1
## q[1473]  3055    1
## q[1474]  2739    1
## q[1475]  3439    1
## q[1476]  3066    1
## q[1477]  2739    1
## q[1478]  2739    1
## q[1479]  3439    1
## q[1480]  2739    1
## q[1481]  2739    1
## q[1482]  2739    1
## q[1483]  3066    1
## q[1484]  3066    1
## q[1485]  3439    1
## q[1486]  3066    1
## q[1487]  3066    1
## q[1488]  3439    1
## q[1489]  2739    1
## q[1490]  3439    1
## q[1491]  3066    1
## q[1492]  3515    1
## q[1493]  2974    1
## q[1494]  2974    1
## q[1495]  3515    1
## q[1496]  3515    1
## q[1497]  3277    1
## q[1498]  2974    1
## q[1499]  2974    1
## q[1500]  2974    1
## q[1501]  2974    1
## q[1502]  3515    1
## q[1503]  3277    1
## q[1504]  2974    1
## q[1505]  2974    1
## q[1506]  3515    1
## q[1507]  2974    1
## q[1508]  2974    1
## q[1509]  3515    1
## q[1510]  2974    1
## q[1511]  3515    1
## q[1512]  3515    1
## q[1513]  2974    1
## q[1514]  3277    1
## q[1515]  3515    1
## q[1516]  3515    1
## q[1517]  2974    1
## q[1518]  3515    1
## q[1519]  2974    1
## q[1520]  3277    1
## q[1521]  2974    1
## q[1522]  3277    1
## q[1523]  2974    1
## q[1524]  3515    1
## q[1525]  2974    1
## q[1526]  3515    1
## q[1527]  2974    1
## q[1528]  2974    1
## q[1529]  2974    1
## q[1530]  2974    1
## q[1531]  3515    1
## q[1532]  2974    1
## q[1533]  3515    1
## q[1534]  2974    1
## q[1535]  3277    1
## q[1536]  3515    1
## q[1537]  2974    1
## q[1538]  2974    1
## q[1539]  2974    1
## q[1540]  3277    1
## q[1541]  2555    1
## q[1542]  2555    1
## q[1543]  2555    1
## q[1544]  2724    1
## q[1545]  2955    1
## q[1546]  2955    1
## q[1547]  2724    1
## q[1548]  2955    1
## q[1549]  2955    1
## q[1550]  2724    1
## q[1551]  2555    1
## q[1552]  2555    1
## q[1553]  2555    1
## q[1554]  2955    1
## q[1555]  2724    1
## q[1556]  2555    1
## q[1557]  2555    1
## q[1558]  2555    1
## q[1559]  2555    1
## q[1560]  2955    1
## q[1561]  2555    1
## q[1562]  2955    1
## q[1563]  2555    1
## q[1564]  2955    1
## q[1565]  2555    1
## q[1566]  2555    1
## q[1567]  2955    1
## q[1568]  2555    1
## q[1569]  2955    1
## q[1570]  2555    1
## q[1571]  2724    1
## q[1572]  2955    1
## q[1573]  2555    1
## q[1574]  2955    1
## q[1575]  2955    1
## q[1576]  2555    1
## q[1577]  2955    1
## q[1578]  2555    1
## q[1579]  2724    1
## q[1580]  2724    1
## q[1581]  2555    1
## q[1582]  2724    1
## q[1583]  2955    1
## q[1584]  2724    1
## q[1585]  2555    1
## q[1586]  2555    1
## q[1587]  2955    1
## q[1588]  2955    1
## q[1589]  2955    1
## q[1590]  2555    1
## q[1591]  2724    1
## q[1592]  2555    1
## q[1593]  2555    1
## q[1594]  2555    1
## q[1595]  2555    1
## q[1596]  2555    1
## q[1597]  2955    1
## q[1598]  2555    1
## q[1599]  2555    1
## q[1600]  2724    1
## q[1601]  2724    1
## q[1602]  2955    1
## q[1603]  2955    1
## q[1604]  2555    1
## q[1605]  2555    1
## q[1606]  2955    1
## q[1607]  2724    1
## q[1608]  2955    1
## q[1609]  2555    1
## q[1610]  2955    1
## q[1611]  2555    1
## q[1612]  2555    1
## q[1613]  2555    1
## q[1614]  2724    1
## q[1615]  2724    1
## q[1616]  2699    1
## q[1617]  2699    1
## q[1618]  2804    1
## q[1619]  2699    1
## q[1620]  2699    1
## q[1621]  2818    1
## q[1622]  2804    1
## q[1623]  2699    1
## q[1624]  2804    1
## q[1625]  2699    1
## q[1626]  2699    1
## q[1627]  2699    1
## q[1628]  3640    1
## q[1629]  3640    1
## q[1630]  3082    1
## q[1631]  3060    1
## q[1632]  3060    1
## q[1633]  3082    1
## q[1634]  3060    1
## q[1635]  3060    1
## q[1636]  3060    1
## q[1637]  3060    1
## q[1638]  3640    1
## q[1639]  3060    1
## q[1640]  3640    1
## q[1641]  3082    1
## q[1642]  3060    1
## q[1643]  3060    1
## q[1644]  3060    1
## q[1645]  3640    1
## q[1646]  3060    1
## q[1647]  3082    1
## q[1648]  3060    1
## q[1649]  3640    1
## q[1650]  3640    1
## q[1651]  3839    1
## q[1652]  3839    1
## q[1653]  3190    1
## q[1654]  3364    1
## q[1655]  3364    1
## q[1656]  3190    1
## q[1657]  3839    1
## q[1658]  3364    1
## q[1659]  3364    1
## q[1660]  3839    1
## q[1661]  3364    1
## q[1662]  3364    1
## q[1663]  3839    1
## q[1664]  3839    1
## q[1665]  3364    1
## q[1666]  3839    1
## q[1667]  3190    1
## q[1668]  3839    1
## q[1669]  3364    1
## q[1670]  3364    1
## q[1671]  3839    1
## q[1672]  3364    1
## q[1673]  3839    1
## q[1674]  3190    1
## q[1675]  3364    1
## q[1676]  3364    1
## q[1677]  3364    1
## q[1678]  3364    1
## q[1679]  3364    1
## q[1680]  3839    1
## q[1681]  3364    1
## q[1682]  3839    1
## q[1683]  3190    1
## q[1684]  3839    1
## q[1685]  3190    1
## q[1686]  3364    1
## q[1687]  3364    1
## q[1688]  3970    1
## q[1689]  3051    1
## q[1690]  3051    1
## q[1691]  3051    1
## q[1692]  3970    1
## q[1693]  3970    1
## q[1694]  3970    1
## q[1695]  3970    1
## q[1696]  3325    1
## q[1697]  3051    1
## q[1698]  3051    1
## q[1699]  3051    1
## q[1700]  3051    1
## q[1701]  3325    1
## q[1702]  3051    1
## q[1703]  3051    1
## q[1704]  3051    1
## q[1705]  3970    1
## q[1706]  3325    1
## q[1707]  3051    1
## q[1708]  3051    1
## q[1709]  3051    1
## q[1710]  3051    1
## q[1711]  3970    1
## q[1712]  3051    1
## q[1713]  3970    1
## q[1714]  3970    1
## q[1715]  3051    1
## q[1716]  3970    1
## q[1717]  3970    1
## q[1718]  3970    1
## q[1719]  3051    1
## q[1720]  3325    1
## q[1721]  3051    1
## q[1722]  3051    1
## q[1723]  3051    1
## q[1724]  3970    1
## q[1725]  3970    1
## q[1726]  3051    1
## q[1727]  3325    1
## q[1728]  3051    1
## q[1729]  3051    1
## q[1730]  3051    1
## q[1731]  3051    1
## q[1732]  3051    1
## q[1733]  3970    1
## q[1734]  3970    1
## q[1735]  3325    1
## q[1736]  3051    1
## q[1737]  3970    1
## q[1738]  3051    1
## q[1739]  3970    1
## q[1740]  3325    1
## q[1741]  3970    1
## q[1742]  3051    1
## q[1743]  3051    1
## q[1744]  3051    1
## q[1745]  3325    1
## q[1746]  3051    1
## q[1747]  3020    1
## q[1748]  3020    1
## q[1749]  3305    1
## q[1750]  3020    1
## q[1751]  4041    1
## q[1752]  4041    1
## q[1753]  4041    1
## q[1754]  4041    1
## q[1755]  3305    1
## q[1756]  3020    1
## q[1757]  3020    1
## q[1758]  3020    1
## q[1759]  3020    1
## q[1760]  3020    1
## q[1761]  3305    1
## q[1762]  3305    1
## q[1763]  3020    1
## q[1764]  3020    1
## q[1765]  3020    1
## q[1766]  3020    1
## q[1767]  3020    1
## q[1768]  3020    1
## q[1769]  4041    1
## q[1770]  3305    1
## q[1771]  4041    1
## q[1772]  3305    1
## q[1773]  3020    1
## q[1774]  4041    1
## q[1775]  4041    1
## q[1776]  3020    1
## q[1777]  4041    1
## q[1778]  3020    1
## q[1779]  4041    1
## q[1780]  3020    1
## q[1781]  4041    1
## q[1782]  3020    1
## q[1783]  3305    1
## q[1784]  3020    1
## q[1785]  3020    1
## q[1786]  3020    1
## q[1787]  3020    1
## q[1788]  3020    1
## q[1789]  4041    1
## q[1790]  3020    1
## q[1791]  3020    1
## q[1792]  3305    1
## q[1793]  4041    1
## q[1794]  3020    1
## q[1795]  3020    1
## q[1796]  4041    1
## q[1797]  4041    1
## q[1798]  3020    1
## q[1799]  4041    1
## q[1800]  3020    1
## q[1801]  3305    1
## q[1802]  3685    1
## q[1803]  3290    1
## q[1804]  3290    1
## q[1805]  3290    1
## q[1806]  3290    1
## q[1807]  3290    1
## q[1808]  3290    1
## q[1809]  3685    1
## q[1810]  3140    1
## q[1811]  3685    1
## q[1812]  3685    1
## q[1813]  3685    1
## q[1814]  3140    1
## q[1815]  3140    1
## q[1816]  3290    1
## q[1817]  3290    1
## q[1818]  3290    1
## q[1819]  3290    1
## q[1820]  3290    1
## q[1821]  3685    1
## q[1822]  3290    1
## q[1823]  3290    1
## q[1824]  3685    1
## q[1825]  3290    1
## q[1826]  3290    1
## q[1827]  3290    1
## q[1828]  3685    1
## q[1829]  3290    1
## q[1830]  3685    1
## q[1831]  3290    1
## q[1832]  3290    1
## q[1833]  3685    1
## q[1834]  3685    1
## q[1835]  3290    1
## q[1836]  3290    1
## q[1837]  3290    1
## q[1838]  3290    1
## q[1839]  3140    1
## q[1840]  3685    1
## q[1841]  3685    1
## q[1842]  3685    1
## q[1843]  3290    1
## q[1844]  3685    1
## q[1845]  3140    1
## q[1846]  3685    1
## q[1847]  3685    1
## q[1848]  3290    1
## q[1849]  3685    1
## q[1850]  3290    1
## q[1851]  3290    1
## q[1852]  3685    1
## q[1853]  3140    1
## q[1854]  3140    1
## q[1855]  3290    1
## q[1856]  3140    1
## q[1857]  3140    1
## q[1858]  3290    1
## q[1859]  3290    1
## q[1860]  3290    1
## q[1861]  3685    1
## q[1862]  3290    1
## q[1863]  3685    1
## q[1864]  3685    1
## q[1865]  3290    1
## q[1866]  3140    1
## q[1867]  3290    1
## q[1868]  3290    1
## q[1869]  3290    1
## q[1870]  3290    1
## q[1871]  3290    1
## q[1872]  3290    1
## q[1873]  3685    1
## q[1874]  3290    1
## q[1875]  3140    1
## q[1876]  3140    1
## q[1877]  3290    1
## q[1878]  3140    1
## q[1879]  3685    1
## q[1880]  3290    1
## q[1881]  3290    1
## q[1882]  3685    1
## q[1883]  3140    1
## q[1884]  3290    1
## q[1885]  3290    1
## q[1886]  3290    1
## q[1887]  3290    1
## q[1888]  3290    1
## q[1889]  3140    1
## q[1890]  2912    1
## q[1891]  2912    1
## q[1892]  2912    1
## q[1893]  2912    1
## q[1894]  3439    1
## q[1895]  3439    1
## q[1896]  3439    1
## q[1897]  3247    1
## q[1898]  2912    1
## q[1899]  2912    1
## q[1900]  2912    1
## q[1901]  3439    1
## q[1902]  2912    1
## q[1903]  3439    1
## q[1904]  2912    1
## q[1905]  2912    1
## q[1906]  3439    1
## q[1907]  2912    1
## q[1908]  3439    1
## q[1909]  3439    1
## q[1910]  2912    1
## q[1911]  3247    1
## q[1912]  3439    1
## q[1913]  3439    1
## q[1914]  3439    1
## q[1915]  2912    1
## q[1916]  3439    1
## q[1917]  2912    1
## q[1918]  2912    1
## q[1919]  3439    1
## q[1920]  2912    1
## q[1921]  2912    1
## q[1922]  2912    1
## q[1923]  2912    1
## q[1924]  2912    1
## q[1925]  2912    1
## q[1926]  2912    1
## q[1927]  3439    1
## q[1928]  3247    1
## q[1929]  2912    1
## q[1930]  3247    1
## q[1931]  2912    1
## q[1932]  3247    1
## q[1933]  2912    1
## q[1934]  2912    1
## q[1935]  2912    1
## q[1936]  2912    1
## q[1937]  2891    1
## q[1938]  2906    1
## q[1939]  2906    1
## q[1940]  2519    1
## q[1941]  2519    1
## q[1942]  2519    1
## q[1943]  2519    1
## q[1944]  2519    1
## q[1945]  2891    1
## q[1946]  2519    1
## q[1947]  2891    1
## q[1948]  2891    1
## q[1949]  2891    1
## q[1950]  2519    1
## q[1951]  2519    1
## q[1952]  2519    1
## q[1953]  2519    1
## q[1954]  2519    1
## q[1955]  2519    1
## q[1956]  2519    1
## q[1957]  2906    1
## q[1958]  2519    1
## q[1959]  2870    1
## q[1960]  2506    1
## q[1961]  2889    1
## q[1962]  2506    1
## q[1963]  2506    1
## q[1964]  2870    1
## q[1965]  2889    1
## q[1966]  2870    1
## q[1967]  2506    1
## q[1968]  2506    1
## q[1969]  2506    1
## q[1970]  2506    1
## q[1971]  2506    1
## q[1972]  2506    1
## q[1973]  2870    1
## q[1974]  2506    1
## q[1975]  2506    1
## q[1976]  2870    1
## q[1977]  2506    1
## q[1978]  2870    1
## q[1979]  2889    1
## q[1980]  2870    1
## q[1981]  2870    1
## q[1982]  2870    1
## q[1983]  2506    1
## q[1984]  2506    1
## q[1985]  2889    1
## q[1986]  2506    1
## q[1987]  2889    1
## q[1988]  2870    1
## q[1989]  2506    1
## q[1990]  2506    1
## q[1991]  2870    1
## q[1992]  2506    1
## q[1993]  2506    1
## q[1994]  2889    1
## q[1995]  2506    1
## q[1996]  2870    1
## q[1997]  2506    1
## q[1998]  2506    1
## q[1999]  2506    1
## q[2000]  4027    1
## q[2001]  3041    1
## q[2002]  3316    1
## q[2003]  4027    1
## q[2004]  4027    1
## q[2005]  3316    1
## q[2006]  4027    1
## q[2007]  3041    1
## q[2008]  3041    1
## q[2009]  3316    1
## q[2010]  3041    1
## q[2011]  3041    1
## q[2012]  4027    1
## q[2013]  3041    1
## q[2014]  3041    1
## q[2015]  3041    1
## q[2016]  4027    1
## q[2017]  3041    1
## q[2018]  3041    1
## q[2019]  4027    1
## q[2020]  3041    1
## q[2021]  4027    1
## q[2022]  4027    1
## q[2023]  3041    1
## q[2024]  4027    1
## q[2025]  3316    1
## q[2026]  4027    1
## q[2027]  4027    1
## q[2028]  3041    1
## q[2029]  4027    1
## q[2030]  3316    1
## q[2031]  3041    1
## q[2032]  3041    1
## q[2033]  3316    1
## q[2034]  3041    1
## q[2035]  3041    1
## q[2036]  3316    1
## q[2037]  4027    1
## q[2038]  3041    1
## q[2039]  4027    1
## q[2040]  4027    1
## q[2041]  3041    1
## q[2042]  3316    1
## q[2043]  4027    1
## q[2044]  3041    1
## q[2045]  3041    1
## q[2046]  3041    1
## q[2047]  3041    1
## q[2048]  4027    1
## q[2049]  4027    1
## q[2050]  3041    1
## q[2051]  3041    1
## q[2052]  4027    1
## q[2053]  3316    1
## q[2054]  4027    1
## q[2055]  3041    1
## q[2056]  3041    1
## q[2057]  2998    1
## q[2058]  4018    1
## q[2059]  3292    1
## q[2060]  4018    1
## q[2061]  4018    1
## q[2062]  2998    1
## q[2063]  4018    1
## q[2064]  2998    1
## q[2065]  4018    1
## q[2066]  2998    1
## q[2067]  4018    1
## q[2068]  2998    1
## q[2069]  2998    1
## q[2070]  4018    1
## q[2071]  2998    1
## q[2072]  2998    1
## q[2073]  3292    1
## q[2074]  3292    1
## q[2075]  4018    1
## q[2076]  4018    1
## q[2077]  2998    1
## q[2078]  3292    1
## q[2079]  2998    1
## q[2080]  2998    1
## q[2081]  4018    1
## q[2082]  3292    1
## q[2083]  4018    1
## q[2084]  2998    1
## q[2085]  4018    1
## q[2086]  3292    1
## q[2087]  2998    1
## q[2088]  3292    1
## q[2089]  3858    1
## q[2090]  3058    1
## q[2091]  3058    1
## q[2092]  3058    1
## q[2093]  3858    1
## q[2094]  3326    1
## q[2095]  3858    1
## q[2096]  3858    1
## q[2097]  3326    1
## q[2098]  3058    1
## q[2099]  3058    1
## q[2100]  3058    1
## q[2101]  3858    1
## q[2102]  3058    1
## q[2103]  3058    1
## q[2104]  3058    1
## q[2105]  3858    1
## q[2106]  3058    1
## q[2107]  3858    1
## q[2108]  3058    1
## q[2109]  3858    1
## q[2110]  3858    1
## q[2111]  3058    1
## q[2112]  3058    1
## q[2113]  3058    1
## q[2114]  3858    1
## q[2115]  3858    1
## q[2116]  3858    1
## q[2117]  3058    1
## q[2118]  3058    1
## q[2119]  3058    1
## q[2120]  3326    1
## q[2121]  3326    1
## q[2122]  3058    1
## q[2123]  3858    1
## q[2124]  3058    1
## q[2125]  3858    1
## q[2126]  3058    1
## q[2127]  3326    1
## q[2128]  3058    1
## q[2129]  3058    1
## q[2130]  3858    1
## q[2131]  3326    1
## q[2132]  3326    1
## q[2133]  3058    1
## q[2134]  3858    1
## q[2135]  3058    1
## q[2136]  3858    1
## q[2137]  3326    1
## q[2138]  3058    1
## q[2139]  3058    1
## q[2140]  3058    1
## q[2141]  3326    1
## q[2142]  3722    1
## q[2143]  2864    1
## q[2144]  3191    1
## q[2145]  3191    1
## q[2146]  3722    1
## q[2147]  2864    1
## q[2148]  2864    1
## q[2149]  2864    1
## q[2150]  2864    1
## q[2151]  3722    1
## q[2152]  2864    1
## q[2153]  2864    1
## q[2154]  3722    1
## q[2155]  2864    1
## q[2156]  3722    1
## q[2157]  3191    1
## q[2158]  3722    1
## q[2159]  3722    1
## q[2160]  2864    1
## q[2161]  2864    1
## q[2162]  3191    1
## q[2163]  2864    1
## q[2164]  3191    1
## q[2165]  3722    1
## q[2166]  2864    1
## q[2167]  2864    1
## q[2168]  3722    1
## q[2169]  2864    1
## q[2170]  2864    1
## q[2171]  3722    1
## q[2172]  2864    1
## q[2173]  2546    1
## q[2174]  2546    1
## q[2175]  3027    1
## q[2176]  2829    1
## q[2177]  2546    1
## q[2178]  2546    1
## q[2179]  2546    1
## q[2180]  2546    1
## q[2181]  2829    1
## q[2182]  3027    1
## q[2183]  3027    1
## q[2184]  3027    1
## q[2185]  2546    1
## q[2186]  3027    1
## q[2187]  2546    1
## q[2188]  2546    1
## q[2189]  2546    1
## q[2190]  2546    1
## q[2191]  2546    1
## q[2192]  2546    1
## q[2193]  2829    1
## q[2194]  2546    1
## q[2195]  2546    1
## q[2196]  2806    1
## q[2197]  2806    1
## q[2198]  2806    1
## q[2199]  2806    1
## q[2200]  3303    1
## q[2201]  3185    1
## q[2202]  3303    1
## q[2203]  3185    1
## q[2204]  2806    1
## q[2205]  2806    1
## q[2206]  2806    1
## q[2207]  2806    1
## q[2208]  2806    1
## q[2209]  3303    1
## q[2210]  2806    1
## q[2211]  2806    1
## q[2212]  3303    1
## q[2213]  2806    1
## q[2214]  2806    1
## q[2215]  2806    1
## q[2216]  3185    1
## q[2217]  3303    1
## q[2218]  3303    1
## q[2219]  3303    1
## q[2220]  3303    1
## q[2221]  2806    1
## q[2222]  3185    1
## q[2223]  3185    1
## q[2224]  2806    1
## q[2225]  3185    1
## q[2226]  3185    1
## q[2227]  2806    1
## q[2228]  2806    1
## q[2229]  3303    1
## q[2230]  3303    1
## q[2231]  3185    1
## q[2232]  2806    1
## q[2233]  2806    1
## q[2234]  2806    1
## q[2235]  3185    1
## q[2236]  3185    1
## q[2237]  2806    1
## q[2238]  3185    1
## q[2239]  3303    1
## q[2240]  2806    1
## q[2241]  3303    1
## q[2242]  2806    1
## q[2243]  2806    1
## q[2244]  2806    1
## q[2245]  2806    1
## q[2246]  3185    1
## q[2247]  2486    1
## q[2248]  2365    1
## q[2249]  2365    1
## q[2250]  2365    1
## q[2251]  2257    1
## q[2252]  2365    1
## q[2253]  2257    1
## q[2254]  2365    1
## q[2255]  2365    1
## q[2256]  2365    1
## q[2257]  2257    1
## q[2258]  2257    1
## q[2259]  2365    1
## q[2260]  2365    1
## q[2261]  2257    1
## q[2262]  2257    1
## q[2263]  2257    1
## q[2264]  2365    1
## q[2265]  2257    1
## q[2266]  2486    1
## q[2267]  2365    1
## q[2268]  2257    1
## q[2269]  2847    1
## q[2270]  3676    1
## q[2271]  3676    1
## q[2272]  2847    1
## q[2273]  2847    1
## q[2274]  3176    1
## q[2275]  2847    1
## q[2276]  3676    1
## q[2277]  2847    1
## q[2278]  2847    1
## q[2279]  3676    1
## q[2280]  3676    1
## q[2281]  2847    1
## q[2282]  3676    1
## q[2283]  3176    1
## q[2284]  3676    1
## q[2285]  2847    1
## q[2286]  3676    1
## q[2287]  3176    1
## q[2288]  3176    1
## q[2289]  2847    1
## q[2290]  2847    1
## q[2291]  3176    1
## q[2292]  3676    1
## q[2293]  3676    1
## q[2294]  3176    1
## q[2295]  3676    1
## q[2296]  2847    1
## q[2297]  2847    1
## q[2298]  2847    1
## q[2299]  2847    1
## q[2300]  3676    1
## q[2301]  3676    1
## q[2302]  2847    1
## q[2303]  3676    1
## q[2304]  3176    1
## q[2305]  2847    1
## q[2306]  2847    1
## q[2307]  3137    1
## q[2308]  3082    1
## q[2309]  2685    1
## q[2310]  2685    1
## q[2311]  3082    1
## q[2312]  2685    1
## q[2313]  2685    1
## q[2314]  2685    1
## q[2315]  3137    1
## q[2316]  3082    1
## q[2317]  3137    1
## q[2318]  2685    1
## q[2319]  2685    1
## q[2320]  3137    1
## q[2321]  2685    1
## q[2322]  3082    1
## q[2323]  2685    1
## q[2324]  2685    1
## q[2325]  3137    1
## q[2326]  2685    1
## q[2327]  3082    1
## q[2328]  3137    1
## q[2329]  2685    1
## q[2330]  3569    1
## q[2331]  3165    1
## q[2332]  3165    1
## q[2333]  3165    1
## q[2334]  3165    1
## q[2335]  3569    1
## q[2336]  3569    1
## q[2337]  3086    1
## q[2338]  3569    1
## q[2339]  3569    1
## q[2340]  3086    1
## q[2341]  3165    1
## q[2342]  3165    1
## q[2343]  3165    1
## q[2344]  3165    1
## q[2345]  3569    1
## q[2346]  3086    1
## q[2347]  3165    1
## q[2348]  3165    1
## q[2349]  3165    1
## q[2350]  3165    1
## q[2351]  3569    1
## q[2352]  3165    1
## q[2353]  3569    1
## q[2354]  3165    1
## q[2355]  3569    1
## q[2356]  3569    1
## q[2357]  3165    1
## q[2358]  3165    1
## q[2359]  3165    1
## q[2360]  3165    1
## q[2361]  3569    1
## q[2362]  3569    1
## q[2363]  3165    1
## q[2364]  3086    1
## q[2365]  3165    1
## q[2366]  3569    1
## q[2367]  3165    1
## q[2368]  3165    1
## q[2369]  3569    1
## q[2370]  3086    1
## q[2371]  3086    1
## q[2372]  3165    1
## q[2373]  3569    1
## q[2374]  3165    1
## q[2375]  3569    1
## q[2376]  3165    1
## q[2377]  3086    1
## q[2378]  3165    1
## q[2379]  3165    1
## q[2380]  3165    1
## q[2381]  3165    1
## q[2382]  3165    1
## q[2383]  3569    1
## q[2384]  3086    1
## q[2385]  3569    1
## q[2386]  3569    1
## q[2387]  3165    1
## q[2388]  3165    1
## q[2389]  3165    1
## q[2390]  3569    1
## q[2391]  3086    1
## q[2392]  3569    1
## q[2393]  3165    1
## q[2394]  3165    1
## q[2395]  3086    1
## q[2396]  3086    1
## lp__     1673    1
## 
## Samples were drawn using NUTS(diag_e) at Tue Nov 26 18:02:43 2019.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;檢查模型參數的收斂情況&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;檢查模型參數的收斂情況&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(bayesplot)

color_scheme_set(&amp;quot;mix-brightblue-gray&amp;quot;)

posterior5.5 &amp;lt;- rstan::extract(fit1, inc_warmup = TRUE, permuted = FALSE)

p &amp;lt;- mcmc_trace(posterior5.5, n_warmup = 0, pars = c(&amp;quot;b[1]&amp;quot;, &amp;quot;b[2]&amp;quot;, &amp;quot;b[3]&amp;quot;,   &amp;quot;lp__&amp;quot;), facet_args = list(nrow = 2, labeller = label_parsed))

p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:chapter5-5&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-27-logistic2-rstan_files/figure-html/chapter5-5-1.png&#34; alt=&#34;用 bayesplot包數繪製的模型5-5的MCMC鏈式軌跡圖 (trace plot)。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: 用 bayesplot包數繪製的模型5-5的MCMC鏈式軌跡圖 (trace plot)。
&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- mcmc_acf_bar(posterior5.5, pars = c(&amp;quot;b[1]&amp;quot;, &amp;quot;b[2]&amp;quot;, &amp;quot;b[3]&amp;quot;,   &amp;quot;lp__&amp;quot;))
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:chapter5-5-acf&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-27-logistic2-rstan_files/figure-html/chapter5-5-acf-1.png&#34; alt=&#34;用 bayesplot包數繪製的事後樣本自相關圖(autocorrelation)。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: 用 bayesplot包數繪製的事後樣本自相關圖(autocorrelation)。
&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- mcmc_dens_overlay(posterior5.5, pars = c(&amp;quot;b[1]&amp;quot;, &amp;quot;b[2]&amp;quot;, &amp;quot;b[3]&amp;quot;, &amp;quot;OR1&amp;quot;,  &amp;quot;OR2&amp;quot;, &amp;quot;lp__&amp;quot;), color_chains = T)
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step5-5-density&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-27-logistic2-rstan_files/figure-html/step5-5-density-1.png&#34; alt=&#34;用 bayesplot包數繪製的事後樣本密度分佈圖。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: 用 bayesplot包數繪製的事後樣本密度分佈圖。
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;檢查模型的擬合情況&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;檢查模型的擬合情況&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ms &amp;lt;- rstan::extract(fit1)
set.seed(123)
logistic &amp;lt;- function(x) 1/(1+exp(-x))
X &amp;lt;- 30:200
q_qua &amp;lt;- logistic(t(sapply(1:length(X), function(i) {
  q_mcmc &amp;lt;- ms$b[,1] + ms$b[,3]*X[i]/200
  quantile(q_mcmc, probs=c(0.1, 0.5, 0.9))
})))
d_est &amp;lt;- data.frame(X, q_qua)
colnames(d_est) &amp;lt;- c(&amp;#39;X&amp;#39;, &amp;#39;p10&amp;#39;, &amp;#39;p50&amp;#39;, &amp;#39;p90&amp;#39;)
d$A &amp;lt;- as.factor(d$A)

p &amp;lt;- ggplot(d_est, aes(x=X, y=p50))
p &amp;lt;- p + theme_bw(base_size=18)
p &amp;lt;- p + geom_ribbon(aes(ymin=p10, ymax=p90), fill=&amp;#39;black&amp;#39;, alpha=2/6)
p &amp;lt;- p + geom_line(size=1)
p &amp;lt;- p + geom_point(data=subset(d, A==0 &amp;amp; Weather==&amp;#39;A&amp;#39;), aes(x=Score, y=Y, color=A),
  position=position_jitter(w=0, h=0.1), size=1)
p &amp;lt;- p + labs(x=&amp;#39;Score&amp;#39;, y=&amp;#39;q&amp;#39;)
p &amp;lt;- p + scale_color_manual(values=c(&amp;#39;black&amp;#39;))
p &amp;lt;- p + scale_y_continuous(breaks=seq(0, 1, 0.2))
p &amp;lt;- p + xlim(30, 200)
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:validity-of-model&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-27-logistic2-rstan_files/figure-html/validity-of-model-1.png&#34; alt=&#34;不喜歡打工，且天氣晴天的情況下，分數在 30-200 點之間的學生的出勤概率 q 和它的 80% 可信區間範圍。圖中黑色實線是預測概率的事後分佈的中央值，灰色帶是可信區間範圍。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4: 不喜歡打工，且天氣晴天的情況下，分數在 30-200 點之間的學生的出勤概率 q 和它的 80% 可信區間範圍。圖中黑色實線是預測概率的事後分佈的中央值，灰色帶是可信區間範圍。
&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# ggsave(file=&amp;#39;output/fig5-9.png&amp;#39;, plot=p, dpi=300, w=4.5, h=3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;圖&lt;a href=&#34;#fig:validity-of-model&#34;&gt;4&lt;/a&gt;試圖把分數範圍在 30-200 之間的學生中，通過模型計算獲得的，在天氣晴朗，且不愛打工的孩子們的事後出勤概率的預測值(黑色實線)，和它的事後概率80%可信區間，以及對應的實際觀測值的結果(黑點)。但是，當預測變量越來越多，模型結果的可視化變得越來越困難。下面我們介紹兩種常見的評價邏輯回歸擬合結果的可視化圖。&lt;/p&gt;
&lt;p&gt;首先是圖 &lt;a href=&#34;#fig:validity-of-model1&#34;&gt;5&lt;/a&gt; 顯示的事後出勤概率，和實際觀察出勤結果之間的關係圖。在這個圖中，橫軸是 &lt;span class=&#34;math inline&#34;&gt;\(q[i]\)&lt;/span&gt; 的事後分佈的中央值(每名學生都有自己的事後出勤概率預測，它的中央值)，縱軸是該名學生實際是否在該次課上出勤的觀察結果。如果模型擬合的理想的話，那麼在 &lt;span class=&#34;math inline&#34;&gt;\(Y=0\)&lt;/span&gt;，也就是圖中的下半部分，大多數的預測點應該靠近概率較低的部分(也就是靠近左側)，同時，&lt;span class=&#34;math inline&#34;&gt;\(y = 1\)&lt;/span&gt; 的部分預測概率應該大多數在靠近左側的部分。此圖其實提示我們該模型的擬合效果不理想。不能明顯地將出勤與不出勤較爲準確地區分開來。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)
ms &amp;lt;- rstan::extract(fit1)
d_qua &amp;lt;- t(apply(ms$q, 2, quantile, prob=c(0.1, 0.5, 0.9)))
colnames(d_qua) &amp;lt;- c(&amp;#39;p10&amp;#39;, &amp;#39;p50&amp;#39;, &amp;#39;p90&amp;#39;)
d_qua &amp;lt;- data.frame(d, d_qua)
d_qua$Y &amp;lt;- as.factor(d_qua$Y)
d_qua$A &amp;lt;- as.factor(d_qua$A)

p &amp;lt;- ggplot(data=d_qua, aes(x=Y, y=p50))
p &amp;lt;- p + theme_bw(base_size=18)
p &amp;lt;- p + coord_flip()
p &amp;lt;- p + geom_violin(trim=FALSE, size=1.5, color=&amp;#39;grey80&amp;#39;)
p &amp;lt;- p + geom_point(aes(color=A), position=position_jitter(w=0.4, h=0), size=1)
p &amp;lt;- p + scale_color_manual(values=c(&amp;#39;grey5&amp;#39;, &amp;#39;grey50&amp;#39;))
p &amp;lt;- p + labs(x=&amp;#39;Y&amp;#39;, y=&amp;#39;q&amp;#39;)
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:validity-of-model1&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-27-logistic2-rstan_files/figure-html/validity-of-model1-1.png&#34; alt=&#34;把計算獲得的事後概率的中央值作爲每名學生是否出勤的概率預測(x 軸)，和實際觀察的出勤結果(y 軸)，繪製的散點圖。其中，灰色的小提琴圖其實是根據獲得的事後概率的中央值的概率密度曲線，繪製的上下對稱的圖形，形似小提琴。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 5: 把計算獲得的事後概率的中央值作爲每名學生是否出勤的概率預測(x 軸)，和實際觀察的出勤結果(y 軸)，繪製的散點圖。其中，灰色的小提琴圖其實是根據獲得的事後概率的中央值的概率密度曲線，繪製的上下對稱的圖形，形似小提琴。
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Rstan Wonderful R-(4)</title>
      <link>https://wangcc.me/post/logistic-rstan/</link>
      <pubDate>Sun, 27 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/logistic-rstan/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#邏輯回歸模型的-rstan-貝葉斯實現&#34;&gt;邏輯回歸模型的 Rstan 貝葉斯實現&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#確定分析目的&#34;&gt;確定分析目的&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#確認數據分佈&#34;&gt;確認數據分佈&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#寫下數學模型表達式&#34;&gt;寫下數學模型表達式&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#確認收斂效果&#34;&gt;確認收斂效果&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;邏輯回歸模型的-rstan-貝葉斯實現&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;邏輯回歸模型的 Rstan 貝葉斯實現&lt;/h1&gt;
&lt;p&gt;本小節使用的&lt;a href=&#34;https://raw.githubusercontent.com/MatsuuraKentaro/RStanBook/master/chap05/input/data-attendance-2.txt&#34;&gt;數據&lt;/a&gt;，和前一節的出勤率數據很類似:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d &amp;lt;- read.table(&amp;quot;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap05/input/data-attendance-2.txt&amp;quot;, 
                     sep = &amp;quot;,&amp;quot;, header = T)
head(d)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   PersonID A Score  M  Y
## 1        1 0    69 43 38
## 2        2 1   145 56 40
## 3        3 0   125 32 24
## 4        4 1    86 45 33
## 5        5 1   158 33 23
## 6        6 0   133 61 60&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;PersonID&lt;/code&gt;: 是學生的編號；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;A&lt;/code&gt;, &lt;code&gt;Score&lt;/code&gt;: 和之前一樣用來預測出勤率的兩個預測變量，分別是表示是否喜歡打工的 &lt;code&gt;A&lt;/code&gt;，和表示對學習本身是否喜歡的評分 (滿分200)；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;M&lt;/code&gt;: 過去三個月內，該名學生一共需要上課的總課時數；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Y&lt;/code&gt;: 過去三個月內，該名學生實際上出勤的課時數。&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;確定分析目的&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;確定分析目的&lt;/h1&gt;
&lt;p&gt;需要回答的問題依然是，&lt;span class=&#34;math inline&#34;&gt;\(A, Score\)&lt;/span&gt; 分別在多大程度上預測學生的出勤率？另外，我們希望知道的是，當需要修的課時數固定的事後，這兩個預測變量能準確提供 &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; 的多少信息？&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;確認數據分佈&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;確認數據分佈&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(GGally)

set.seed(1)
d &amp;lt;- d[, -1]
# d &amp;lt;- read.csv(file=&amp;#39;input/data-attendance-2.txt&amp;#39;)[,-1]
d$A &amp;lt;- as.factor(d$A)
d &amp;lt;- transform(d, ratio=Y/M)
N_col &amp;lt;- ncol(d)
ggp &amp;lt;- ggpairs(d, upper=&amp;#39;blank&amp;#39;, diag=&amp;#39;blank&amp;#39;, lower=&amp;#39;blank&amp;#39;)

for(i in 1:N_col) {
  x &amp;lt;- d[,i]
  p &amp;lt;- ggplot(data.frame(x, A=d$A), aes(x))
  p &amp;lt;- p + theme_bw(base_size=14)
  p &amp;lt;- p + theme(axis.text.x=element_text(angle=40, vjust=1, hjust=1))
  if (class(x) == &amp;#39;factor&amp;#39;) {
    p &amp;lt;- p + geom_bar(aes(fill=A), color=&amp;#39;grey20&amp;#39;)
  } else {
    bw &amp;lt;- (max(x)-min(x))/10
    p &amp;lt;- p + geom_histogram(aes(fill=A), color=&amp;#39;grey20&amp;#39;, binwidth=bw)
    p &amp;lt;- p + geom_line(eval(bquote(aes(y=..count..*.(bw)))), stat=&amp;#39;density&amp;#39;)
  }
  p &amp;lt;- p + geom_label(data=data.frame(x=-Inf, y=Inf, label=colnames(d)[i]), aes(x=x, y=y, label=label), hjust=0, vjust=1)
  p &amp;lt;- p + scale_fill_manual(values=alpha(c(&amp;#39;white&amp;#39;, &amp;#39;grey40&amp;#39;), 0.5))
  ggp &amp;lt;- putPlot(ggp, p, i, i)
}

zcolat &amp;lt;- seq(-1, 1, length=81)
zcolre &amp;lt;- c(zcolat[1:40]+1, rev(zcolat[41:81]))

for(i in 1:(N_col-1)) {
  for(j in (i+1):N_col) {
    x &amp;lt;- as.numeric(d[,i])
    y &amp;lt;- as.numeric(d[,j])
    r &amp;lt;- cor(x, y, method=&amp;#39;spearman&amp;#39;, use=&amp;#39;pairwise.complete.obs&amp;#39;)
    zcol &amp;lt;- lattice::level.colors(r, at=zcolat, col.regions=grey(zcolre))
    textcol &amp;lt;- ifelse(abs(r) &amp;lt; 0.4, &amp;#39;grey20&amp;#39;, &amp;#39;white&amp;#39;)
    ell &amp;lt;- ellipse::ellipse(r, level=0.95, type=&amp;#39;l&amp;#39;, npoints=50, scale=c(.2, .2), centre=c(.5, .5))
    p &amp;lt;- ggplot(data.frame(ell), aes(x=x, y=y))
    p &amp;lt;- p + theme_bw() + theme(
      plot.background=element_blank(),
      panel.grid.major=element_blank(), panel.grid.minor=element_blank(),
      panel.border=element_blank(), axis.ticks=element_blank()
    )
    p &amp;lt;- p + geom_polygon(fill=zcol, color=zcol)
    p &amp;lt;- p + geom_text(data=NULL, x=.5, y=.5, label=100*round(r, 2), size=6, col=textcol)
    ggp &amp;lt;- putPlot(ggp, p, i, j)
  }
}

for(j in 1:(N_col-1)) {
  for(i in (j+1):N_col) {
    x &amp;lt;- d[,j]
    y &amp;lt;- d[,i]
    p &amp;lt;- ggplot(data.frame(x, y, gr=d$A), aes(x=x, y=y, fill=gr, shape=gr))
    p &amp;lt;- p + theme_bw(base_size=14)
    p &amp;lt;- p + theme(axis.text.x=element_text(angle=40, vjust=1, hjust=1))
    if (class(x) == &amp;#39;factor&amp;#39;) {
      p &amp;lt;- p + geom_boxplot(aes(group=x), alpha=3/6, outlier.size=0, fill=&amp;#39;white&amp;#39;)
      p &amp;lt;- p + geom_point(position=position_jitter(w=0.4, h=0), size=2)
    } else {
      p &amp;lt;- p + geom_point(size=2)
    }
    p &amp;lt;- p + scale_shape_manual(values=c(21, 24))
    p &amp;lt;- p + scale_fill_manual(values=alpha(c(&amp;#39;white&amp;#39;, &amp;#39;grey40&amp;#39;), 0.5))
    ggp &amp;lt;- putPlot(ggp, p, i, j)
  }
}

ggp&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step1&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-27-logistic-rstan_files/figure-html/step1-1.png&#34; alt=&#34;三個變量的分佈觀察圖，相比之前增加了 $ratio = Y/M$ 列。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: 三個變量的分佈觀察圖，相比之前增加了 &lt;span class=&#34;math inline&#34;&gt;\(ratio = Y/M\)&lt;/span&gt; 列。
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;從圖 &lt;a href=&#34;#fig:step1&#34;&gt;1&lt;/a&gt; 還可以看出，由於總課時數越多，學生實際出勤的課時數也會越多所以 &lt;span class=&#34;math inline&#34;&gt;\(M, Y\)&lt;/span&gt; 兩者之間理應有很強的正相關。另外可能可以推測的是 &lt;span class=&#34;math inline&#34;&gt;\(Ratio\)&lt;/span&gt; 和是否愛學習的分數之間大概有可能有正相關，和是否喜歡打工之間大概可能有負相關。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;寫下數學模型表達式&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;寫下數學模型表達式&lt;/h1&gt;
&lt;p&gt;在 Stan 的語法中，使用的是反邏輯函數 (inverse logit): &lt;code&gt;inv_logit&lt;/code&gt; 來描述下面的邏輯回歸模型 5-4。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{array}{l}
q[n] = \text{inv_logit}(b_1 + b_2 A[n] + b_3Score[n]) &amp;amp; n = 1, 2, \dots, N \\
Y[n] \sim \text{Binomial}(M[n], q[n])                 &amp;amp; n = 1, 2, \dots, N \\
\end{array}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;上面的數學模型，可以被翻譯成下面的 Stan 語言:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data {
  int N; 
  int&amp;lt;lower=0, upper=1&amp;gt; A[N]; 
  real&amp;lt;lower=0, upper=1&amp;gt; Score[N]; 
  int&amp;lt;lower=0&amp;gt; M[N];
  int&amp;lt;lower=0&amp;gt; Y[N];
}

parameters {
  real b1; 
  real b2; 
  real b3;
}

transformed parameters {
  real q[N];
  for (n in 1:N) {
    q[n] = inv_logit(b1 + b2*A[n] + b3*Score[n]);
  }
}

model {
  for (n in 1:N) {
    Y[n] ~ binomial(M[n], q[n]); 
  }
}

generated quantities {
  real y_pred[N]; 
  for (n in 1:N) {
    y_pred[n] = binomial_rng(M[n], q[n]);
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;下面的 R 代碼用來實現對上面 Stan 模型的擬合:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rstan)
d &amp;lt;- read.csv(file=&amp;#39;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap05/input/data-attendance-2.txt&amp;#39;, header = T)
data &amp;lt;- list(N=nrow(d), A=d$A, Score=d$Score/200, M=d$M, Y=d$Y)
fit &amp;lt;- stan(file=&amp;#39;stanfiles/model5-4.stan&amp;#39;, data=data, seed=1234)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &amp;#39;model5-4&amp;#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 1.5e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.129287 seconds (Warm-up)
## Chain 1:                0.13611 seconds (Sampling)
## Chain 1:                0.265397 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &amp;#39;model5-4&amp;#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 8e-06 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.131371 seconds (Warm-up)
## Chain 2:                0.133643 seconds (Sampling)
## Chain 2:                0.265014 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &amp;#39;model5-4&amp;#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 8e-06 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.128284 seconds (Warm-up)
## Chain 3:                0.133711 seconds (Sampling)
## Chain 3:                0.261995 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &amp;#39;model5-4&amp;#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 8e-06 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.128226 seconds (Warm-up)
## Chain 4:                0.140913 seconds (Sampling)
## Chain 4:                0.269139 seconds (Total)
## Chain 4:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Stan model: model5-4.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##                mean se_mean   sd     2.5%      25%      50%      75%    97.5%
## b1             0.09    0.01 0.22    -0.37    -0.05     0.09     0.24     0.53
## b2            -0.62    0.00 0.09    -0.80    -0.68    -0.62    -0.56    -0.44
## b3             1.90    0.01 0.36     1.19     1.67     1.90     2.13     2.64
## q[1]           0.68    0.00 0.02     0.63     0.66     0.68     0.69     0.72
## q[2]           0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[3]           0.78    0.00 0.01     0.76     0.77     0.78     0.79     0.80
## q[4]           0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[5]           0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[6]           0.79    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[7]           0.76    0.00 0.01     0.73     0.75     0.76     0.77     0.78
## q[8]           0.70    0.00 0.02     0.67     0.69     0.70     0.72     0.74
## q[9]           0.81    0.00 0.01     0.79     0.81     0.81     0.82     0.84
## q[10]          0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[11]          0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[12]          0.80    0.00 0.01     0.78     0.79     0.80     0.81     0.82
## q[13]          0.64    0.00 0.01     0.62     0.63     0.64     0.65     0.67
## q[14]          0.76    0.00 0.01     0.73     0.75     0.76     0.77     0.78
## q[15]          0.76    0.00 0.01     0.73     0.75     0.76     0.76     0.78
## q[16]          0.60    0.00 0.02     0.57     0.59     0.60     0.61     0.64
## q[17]          0.76    0.00 0.01     0.74     0.76     0.76     0.77     0.79
## q[18]          0.70    0.00 0.02     0.66     0.69     0.71     0.72     0.74
## q[19]          0.86    0.00 0.02     0.83     0.85     0.86     0.87     0.89
## q[20]          0.72    0.00 0.02     0.69     0.71     0.72     0.73     0.76
## q[21]          0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[22]          0.62    0.00 0.02     0.59     0.61     0.62     0.63     0.65
## q[23]          0.62    0.00 0.02     0.59     0.61     0.62     0.63     0.65
## q[24]          0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[25]          0.64    0.00 0.01     0.61     0.63     0.64     0.65     0.67
## q[26]          0.67    0.00 0.01     0.64     0.66     0.67     0.68     0.69
## q[27]          0.77    0.00 0.01     0.75     0.76     0.77     0.78     0.79
## q[28]          0.77    0.00 0.01     0.75     0.76     0.77     0.78     0.79
## q[29]          0.83    0.00 0.01     0.81     0.83     0.84     0.84     0.86
## q[30]          0.76    0.00 0.01     0.74     0.75     0.76     0.77     0.79
## q[31]          0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.78
## q[32]          0.54    0.00 0.03     0.49     0.53     0.54     0.56     0.60
## q[33]          0.69    0.00 0.01     0.66     0.68     0.69     0.70     0.72
## q[34]          0.66    0.00 0.01     0.63     0.65     0.66     0.67     0.69
## q[35]          0.78    0.00 0.01     0.76     0.78     0.78     0.79     0.81
## q[36]          0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.81
## q[37]          0.62    0.00 0.02     0.58     0.60     0.62     0.63     0.65
## q[38]          0.76    0.00 0.01     0.73     0.75     0.76     0.77     0.78
## q[39]          0.72    0.00 0.02     0.68     0.71     0.72     0.73     0.75
## q[40]          0.72    0.00 0.02     0.68     0.70     0.72     0.73     0.75
## q[41]          0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[42]          0.79    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[43]          0.78    0.00 0.01     0.75     0.77     0.78     0.79     0.80
## q[44]          0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[45]          0.86    0.00 0.02     0.83     0.85     0.86     0.87     0.89
## q[46]          0.75    0.00 0.01     0.72     0.74     0.75     0.76     0.77
## q[47]          0.64    0.00 0.03     0.57     0.62     0.64     0.66     0.70
## q[48]          0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[49]          0.74    0.00 0.01     0.71     0.73     0.74     0.75     0.76
## q[50]          0.60    0.00 0.02     0.57     0.59     0.60     0.61     0.64
## y_pred[1]     29.19    0.05 3.21    23.00    27.00    29.00    31.00    35.00
## y_pred[2]     39.25    0.06 3.56    32.00    37.00    39.00    42.00    46.00
## y_pred[3]     25.07    0.04 2.38    20.00    24.00    25.00    27.00    29.00
## y_pred[4]     25.76    0.06 3.43    19.00    23.00    26.00    28.00    32.00
## y_pred[5]     23.89    0.04 2.62    19.00    22.00    24.00    26.00    29.00
## y_pred[6]     48.46    0.05 3.24    42.00    46.00    49.00    51.00    54.00
## y_pred[7]     37.21    0.05 3.10    31.00    35.00    37.00    39.00    43.00
## y_pred[8]     53.44    0.07 4.12    45.00    51.00    54.00    56.00    61.00
## y_pred[9]     63.52    0.06 3.54    56.00    61.00    64.00    66.00    70.00
## y_pred[10]    51.98    0.05 3.21    45.00    50.00    52.00    54.00    58.00
## y_pred[11]    23.50    0.05 2.73    18.00    22.00    24.00    25.00    29.00
## y_pred[12]    35.26    0.04 2.68    30.00    33.00    35.00    37.00    40.00
## y_pred[13]    34.17    0.06 3.53    27.00    32.00    34.00    37.00    41.00
## y_pred[14]    30.39    0.04 2.73    25.00    29.00    30.00    32.00    35.00
## y_pred[15]    42.23    0.05 3.23    36.00    40.00    42.00    44.00    48.00
## y_pred[16]    35.48    0.06 3.88    28.00    33.00    36.00    38.00    43.00
## y_pred[17]    28.97    0.04 2.69    23.00    27.00    29.00    31.00    34.00
## y_pred[18]    31.67    0.05 3.12    25.00    30.00    32.00    34.00    38.00
## y_pred[19]    38.81    0.04 2.39    34.00    37.00    39.00    40.00    43.00
## y_pred[20]    55.48    0.07 4.26    47.00    53.00    56.00    58.00    63.00
## y_pred[21]    40.16    0.07 4.39    32.00    37.00    40.00    43.00    49.00
## y_pred[22]    47.97    0.08 4.48    39.00    45.00    48.00    51.00    56.03
## y_pred[23]    38.94    0.06 3.89    32.00    36.00    39.00    42.00    46.00
## y_pred[24]    47.35    0.06 3.87    40.00    45.00    47.00    50.00    55.00
## y_pred[25]    32.10    0.06 3.42    25.00    30.00    32.00    34.00    39.00
## y_pred[26]    34.02    0.05 3.38    27.00    32.00    34.00    36.00    40.00
## y_pred[27]    22.42    0.04 2.29    18.00    21.00    23.00    24.00    27.00
## y_pred[28]    28.59    0.04 2.57    23.00    27.00    29.00    30.00    33.00
## y_pred[29]    15.08    0.02 1.59    12.00    14.00    15.00    16.00    18.00
## y_pred[30]    37.37    0.05 3.02    31.00    35.00    37.00    39.00    43.00
## y_pred[31]    55.42    0.07 4.05    47.00    53.00    56.00    58.00    63.00
## y_pred[32]     6.50    0.03 1.75     3.00     5.00     7.00     8.00    10.00
## y_pred[33]    15.82    0.04 2.24    11.00    14.00    16.00    17.00    20.00
## y_pred[34]    24.33    0.05 2.88    19.00    22.00    24.00    26.00    30.00
## y_pred[35]    46.26    0.05 3.27    39.00    44.00    46.00    49.00    52.00
## y_pred[36]    43.51    0.05 3.06    37.00    41.75    44.00    46.00    49.00
## y_pred[37]    54.28    0.08 4.82    45.00    51.00    54.00    58.00    63.00
## y_pred[38]    35.60    0.05 3.04    29.00    34.00    36.00    38.00    41.00
## y_pred[39]    15.84    0.04 2.19    11.00    14.00    16.00    17.00    20.00
## y_pred[40]    29.39    0.05 2.98    23.00    27.00    29.00    31.00    35.00
## y_pred[41]    45.05    0.05 3.14    39.00    43.00    45.00    47.00    51.00
## y_pred[42]    25.43    0.04 2.34    21.00    24.00    26.00    27.00    30.00
## y_pred[43]    41.18    0.05 3.07    35.00    39.00    41.00    43.00    47.00
## y_pred[44]    25.37    0.03 2.16    21.00    24.00    25.00    27.00    29.00
## y_pred[45]    19.76    0.03 1.74    16.00    19.00    20.00    21.00    23.00
## y_pred[46]    38.21    0.05 3.19    32.00    36.00    38.00    40.00    44.00
## y_pred[47]    14.09    0.04 2.37     9.00    12.00    14.00    16.00    18.00
## y_pred[48]    31.20    0.04 2.41    26.00    30.00    31.00    33.00    36.00
## y_pred[49]    16.90    0.03 2.12    12.00    16.00    17.00    18.00    21.00
## y_pred[50]    40.34    0.07 4.25    32.00    37.00    40.00    43.00    48.00
## lp__       -1389.32    0.03 1.20 -1392.42 -1389.91 -1389.01 -1388.42 -1387.95
##            n_eff Rhat
## b1          1443    1
## b2          2052    1
## b3          1519    1
## q[1]        1523    1
## q[2]        2412    1
## q[3]        2751    1
## q[4]        2023    1
## q[5]        2065    1
## q[6]        2718    1
## q[7]        2352    1
## q[8]        2345    1
## q[9]        2443    1
## q[10]       2467    1
## q[11]       2561    1
## q[12]       2648    1
## q[13]       3089    1
## q[14]       2352    1
## q[15]       2267    1
## q[16]       2372    1
## q[17]       2483    1
## q[18]       1617    1
## q[19]       1891    1
## q[20]       2106    1
## q[21]       2023    1
## q[22]       2727    1
## q[23]       2642    1
## q[24]       2484    1
## q[25]       3065    1
## q[26]       3059    1
## q[27]       2648    1
## q[28]       2648    1
## q[29]       2149    1
## q[30]       2439    1
## q[31]       1948    1
## q[32]       1839    1
## q[33]       2682    1
## q[34]       3131    1
## q[35]       2757    1
## q[36]       2743    1
## q[37]       2600    1
## q[38]       2309    1
## q[39]       1707    1
## q[40]       1691    1
## q[41]       2757    1
## q[42]       2718    1
## q[43]       2713    1
## q[44]       2398    1
## q[45]       1913    1
## q[46]       2111    1
## q[47]       1470    1
## q[48]       2354    1
## q[49]       1919    1
## q[50]       2372    1
## y_pred[1]   3738    1
## y_pred[2]   3669    1
## y_pred[3]   3582    1
## y_pred[4]   3362    1
## y_pred[5]   3706    1
## y_pred[6]   3914    1
## y_pred[7]   3706    1
## y_pred[8]   3447    1
## y_pred[9]   3934    1
## y_pred[10]  3907    1
## y_pred[11]  3488    1
## y_pred[12]  4354    1
## y_pred[13]  3685    1
## y_pred[14]  3854    1
## y_pred[15]  3715    1
## y_pred[16]  3841    1
## y_pred[17]  3972    1
## y_pred[18]  3780    1
## y_pred[19]  3683    1
## y_pred[20]  3505    1
## y_pred[21]  3841    1
## y_pred[22]  3432    1
## y_pred[23]  3919    1
## y_pred[24]  3814    1
## y_pred[25]  3828    1
## y_pred[26]  3793    1
## y_pred[27]  3778    1
## y_pred[28]  4052    1
## y_pred[29]  4096    1
## y_pred[30]  3820    1
## y_pred[31]  3352    1
## y_pred[32]  3583    1
## y_pred[33]  3970    1
## y_pred[34]  3628    1
## y_pred[35]  3789    1
## y_pred[36]  3968    1
## y_pred[37]  3355    1
## y_pred[38]  3092    1
## y_pred[39]  3815    1
## y_pred[40]  3738    1
## y_pred[41]  4172    1
## y_pred[42]  4290    1
## y_pred[43]  3624    1
## y_pred[44]  4023    1
## y_pred[45]  3531    1
## y_pred[46]  3976    1
## y_pred[47]  3218    1
## y_pred[48]  3811    1
## y_pred[49]  3951    1
## y_pred[50]  3826    1
## lp__        1381    1
## 
## Samples were drawn using NUTS(diag_e) at Tue Jan  7 14:48:25 2020.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;把獲得的參數事後樣本的均值代入上面的數學模型中可得:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{array}{l}
q[n] = \text{inv_logit}(0.09 - 0.62 A[n] + 1.90Score[n]) &amp;amp; n = 1, 2, \dots, N \\
Y[n] \sim \text{Binomial}(M[n], q[n])                 &amp;amp; n = 1, 2, \dots, N \\
\end{array}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;確認收斂效果&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;確認收斂效果&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(bayesplot)

color_scheme_set(&amp;quot;mix-brightblue-gray&amp;quot;)

posterior2 &amp;lt;- rstan::extract(fit, inc_warmup = TRUE, permuted = FALSE)

p &amp;lt;- mcmc_trace(posterior2, n_warmup = 0, pars = c(&amp;quot;b1&amp;quot;, &amp;quot;b2&amp;quot;, &amp;quot;b3&amp;quot;, &amp;quot;lp__&amp;quot;),
                facet_args = list(nrow = 2, labeller = label_parsed))
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step53&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-27-logistic-rstan_files/figure-html/step53-1.png&#34; alt=&#34;用 bayesplot包數繪製的模型5-3的MCMC鏈式軌跡圖 (trace plot)。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: 用 bayesplot包數繪製的模型5-3的MCMC鏈式軌跡圖 (trace plot)。
&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ms &amp;lt;- rstan::extract(fit)

d_qua &amp;lt;- t(apply(ms$y_pred, 2, quantile, prob=c(0.1, 0.5, 0.9)))
colnames(d_qua) &amp;lt;- c(&amp;#39;p10&amp;#39;, &amp;#39;p50&amp;#39;, &amp;#39;p90&amp;#39;)
d_qua &amp;lt;- data.frame(d, d_qua)
d_qua$A &amp;lt;- as.factor(d_qua$A)

p &amp;lt;- ggplot(data=d_qua, aes(x=Y, y=p50, ymin=p10, ymax=p90, shape=A, fill=A))
p &amp;lt;- p + theme_bw(base_size=18) + theme(legend.key.height=grid::unit(2.5,&amp;#39;line&amp;#39;))
p &amp;lt;- p + coord_fixed(ratio=1, xlim=c(5, 70), ylim=c(5, 70))
p &amp;lt;- p + geom_pointrange(size=0.8, color=&amp;#39;grey5&amp;#39;)
p &amp;lt;- p + geom_abline(aes(slope=1, intercept=0), color=&amp;#39;black&amp;#39;, alpha=3/5, linetype=&amp;#39;31&amp;#39;)
p &amp;lt;- p + scale_shape_manual(values=c(21, 24))
p &amp;lt;- p + scale_fill_manual(values=c(&amp;#39;white&amp;#39;, &amp;#39;grey70&amp;#39;))
p &amp;lt;- p + labs(x=&amp;#39;Observed&amp;#39;, y=&amp;#39;Predicted&amp;#39;)
p &amp;lt;- p + scale_x_continuous(breaks=seq(from=0, to=70, by=20))
p &amp;lt;- p + scale_y_continuous(breaks=seq(from=0, to=70, by=20))
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:fig58&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-27-logistic-rstan_files/figure-html/fig58-1.png&#34; alt=&#34;觀測值(x)，和預測值(y)的散點圖，以及預測值的80%預測區間。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: 觀測值(x)，和預測值(y)的散點圖，以及預測值的80%預測區間。
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Rstan Wonderful R-(3)</title>
      <link>https://wangcc.me/post/rstan-wonderful-r3/</link>
      <pubDate>Tue, 22 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/rstan-wonderful-r3/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#多重回歸-multiple-regression&#34;&gt;多重回歸 multiple regression&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#step-1.-確認數據分佈&#34;&gt;Step 1. 確認數據分佈&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-2.-寫下數學模型&#34;&gt;Step 2. 寫下數學模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-3.-看圖確認模型擬合狀況&#34;&gt;Step 3. 看圖確認模型擬合狀況&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-4.-mcmc-樣本的散點圖矩陣&#34;&gt;Step 4. MCMC 樣本的散點圖矩陣&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;多重回歸-multiple-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;多重回歸 multiple regression&lt;/h1&gt;
&lt;p&gt;本章使用的數據，大學生出勤記錄也是&lt;a href=&#34;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap05/input/data-attendance-1.txt&#34;&gt;架空的數據&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;有大學記錄了50名大學生的出勤狀況：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;A,Score,Y
0,69,0.286
1,145,0.196
0,125,0.261
1,86,0.109
1,158,0.23
0,133,0.35
0,111,0.33
1,147,0.194
0,146,0.413
0,145,0.36
1,141,0.225
0,137,0.423
1,118,0.186
0,111,0.287
...
0,99,0.268
1,99,0.234&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;: 是學生大學二年級時進行的問卷調查時回答是否喜歡打零工的結果（0:不喜歡打工；1:喜歡打工）&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Score\)&lt;/span&gt;: 是大學二年級時進行的問卷調查時計算的該學生對學習是否感興趣的數值評分(200分滿分，分數越高，該學生越熱愛學習)&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;: 是該學生一年內的出勤率&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在本次分析範例中，把&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;出勤率當作是連續型結果變量，我們來用Stan實施多重回歸分析，回答學生喜歡打零工與否，和學生對學習的熱情程度兩個變量能解釋多少出勤率。&lt;/p&gt;
&lt;div id=&#34;step-1.-確認數據分佈&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 1. 確認數據分佈&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# The following figure codes come from the authors website: 
# https://github.com/MatsuuraKentaro/RStanBook/blob/master/chap05/fig5-1.R
library(ggplot2)
library(GGally)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Registered S3 method overwritten by &amp;#39;GGally&amp;#39;:
##   method from   
##   +.gg   ggplot2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)
d &amp;lt;- read.csv(file=&amp;#39;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap05/input/data-attendance-1.txt&amp;#39;, header = T)
d$A &amp;lt;- as.factor(d$A)

N_col &amp;lt;- ncol(d)
ggp &amp;lt;- ggpairs(d, upper=&amp;#39;blank&amp;#39;, diag=&amp;#39;blank&amp;#39;, lower=&amp;#39;blank&amp;#39;)

for(i in 1:N_col) {
  x &amp;lt;- d[,i]
  p &amp;lt;- ggplot(data.frame(x, A=d$A), aes(x))
  p &amp;lt;- p + theme_bw(base_size=14)
  p &amp;lt;- p + theme(axis.text.x=element_text(angle=40, vjust=1, hjust=1))
  if (class(x) == &amp;#39;factor&amp;#39;) {
    p &amp;lt;- p + geom_bar(aes(fill=A), color=&amp;#39;grey5&amp;#39;)
  } else {
    bw &amp;lt;- (max(x)-min(x))/10
    p &amp;lt;- p + geom_histogram(binwidth=bw, aes(fill=A), color=&amp;#39;grey5&amp;#39;) #繪製柱狀圖
    p &amp;lt;- p + geom_line(eval(bquote(aes(y=..count..*.(bw)))), stat=&amp;#39;density&amp;#39;) #添加概率密度曲線
  }
  p &amp;lt;- p + geom_label(data=data.frame(x=-Inf, y=Inf, label=colnames(d)[i]), aes(x=x, y=y, label=label), hjust=0, vjust=1)
  p &amp;lt;- p + scale_fill_manual(values=alpha(c(&amp;#39;white&amp;#39;, &amp;#39;grey40&amp;#39;), 0.5))
  ggp &amp;lt;- putPlot(ggp, p, i, i)
}

zcolat &amp;lt;- seq(-1, 1, length=81)
zcolre &amp;lt;- c(zcolat[1:40]+1, rev(zcolat[41:81]))

for(i in 1:(N_col-1)) {
  for(j in (i+1):N_col) {
    x &amp;lt;- as.numeric(d[,i])
    y &amp;lt;- as.numeric(d[,j])
    r &amp;lt;- cor(x, y, method=&amp;#39;spearman&amp;#39;, use=&amp;#39;pairwise.complete.obs&amp;#39;)
    zcol &amp;lt;- lattice::level.colors(r, at=zcolat, col.regions=grey(zcolre))
    textcol &amp;lt;- ifelse(abs(r) &amp;lt; 0.4, &amp;#39;grey20&amp;#39;, &amp;#39;white&amp;#39;)
    ell &amp;lt;- ellipse::ellipse(r, level=0.95, type=&amp;#39;l&amp;#39;, npoints=50, scale=c(.2, .2), centre=c(.5, .5))
    p &amp;lt;- ggplot(data.frame(ell), aes(x=x, y=y))
    p &amp;lt;- p + theme_bw() + theme(
      plot.background=element_blank(),
      panel.grid.major=element_blank(), panel.grid.minor=element_blank(),
      panel.border=element_blank(), axis.ticks=element_blank()
    )
    p &amp;lt;- p + geom_polygon(fill=zcol, color=zcol)
    p &amp;lt;- p + geom_text(data=NULL, x=.5, y=.5, label=100*round(r, 2), size=6, col=textcol)
    ggp &amp;lt;- putPlot(ggp, p, i, j)
  }
}

for(j in 1:(N_col-1)) {
  for(i in (j+1):N_col) {
    x &amp;lt;- d[,j]
    y &amp;lt;- d[,i]
    p &amp;lt;- ggplot(data.frame(x, y, gr=d$A), aes(x=x, y=y, fill=gr, shape=gr))
    p &amp;lt;- p + theme_bw(base_size=14)
    p &amp;lt;- p + theme(axis.text.x=element_text(angle=40, vjust=1, hjust=1))
    if (class(x) == &amp;#39;factor&amp;#39;) {
      p &amp;lt;- p + geom_boxplot(aes(group=x), alpha=3/6, outlier.size=0, fill=&amp;#39;white&amp;#39;)
      p &amp;lt;- p + geom_point(position=position_jitter(w=0.4, h=0), size=2)
    } else {
      p &amp;lt;- p + geom_point(size=2)
    }
    p &amp;lt;- p + scale_shape_manual(values=c(21, 24))
    p &amp;lt;- p + scale_fill_manual(values=alpha(c(&amp;#39;white&amp;#39;, &amp;#39;grey40&amp;#39;), 0.5))
    ggp &amp;lt;- putPlot(ggp, p, i, j)
  }
}

ggp&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step1&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-22-rstan-wonderful-r-multi-lm_files/figure-html/step1-1.png&#34; alt=&#34;三個變量的分佈觀察圖，對角線上是三個變量各自的柱狀圖 (histogram) 和計算獲得的概率密度函數曲線；左下角三個圖是三個變量的箱式圖和散點圖；右上角三個圖是三個變量兩兩計算獲得的 Spearman 秩相關乘以100之後的數值。對角線上及左下角三個圖中數據點和形狀的不同分別表示學生喜歡(三角形)和不喜歡(圓形)打工。右上角表示秩相關的數值越接近0，顏色越白圖形越接近圓形，相關係數的絕對值越接近1，則顏色越深，橢圓越細長。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: 三個變量的分佈觀察圖，對角線上是三個變量各自的柱狀圖 (histogram) 和計算獲得的概率密度函數曲線；左下角三個圖是三個變量的箱式圖和散點圖；右上角三個圖是三個變量兩兩計算獲得的 Spearman 秩相關乘以100之後的數值。對角線上及左下角三個圖中數據點和形狀的不同分別表示學生喜歡(三角形)和不喜歡(圓形)打工。右上角表示秩相關的數值越接近0，顏色越白圖形越接近圓形，相關係數的絕對值越接近1，則顏色越深，橢圓越細長。
&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# png(file=&amp;#39;output/fig5-1.png&amp;#39;, w=1600, h=1600, res=300)
# print(ggp, left=0.3, bottom=0.3)
# dev.off()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;step-2.-寫下數學模型&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 2. 寫下數學模型&lt;/h2&gt;
&lt;p&gt;Model can be written as (Model5-1):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{array}{l}
Y[n]        = b_1 + b_2A[n] + b_3Sore[n] + \varepsilon [n]&amp;amp;  n = 1,2,\dots,N \\
\varepsilon[n] \sim \text{Normal}(0, \sigma) &amp;amp; n = 1,2,\dots,N \\ 
\end{array}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; 表示學生的人數，&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;則是學生編號的下標；&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(b_1\)&lt;/span&gt; 是回歸直線的截距；&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(b_2\)&lt;/span&gt; 是&lt;span class=&#34;math inline&#34;&gt;\(Score\)&lt;/span&gt;保持不變時，&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;從&lt;span class=&#34;math inline&#34;&gt;\(0\rightarrow 1\)&lt;/span&gt;時出勤率的變化(增加，或者減少)；&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(b_3\)&lt;/span&gt; 是&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;保持不變時，&lt;span class=&#34;math inline&#34;&gt;\(Score\)&lt;/span&gt;增加一個單位時出勤率的變化(增加，或者減少)。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Model can also be written as (Model5-2):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{array}{l}
Y[n]       \sim \text{Normal}(b_1 + b_2A[n] + b_3Score[n], \sigma) &amp;amp;  n = 1,2,\dots,N \\
\end{array}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;如果認爲&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(Score\)&lt;/span&gt;所能預測的出勤率有一個基礎的均值 &lt;span class=&#34;math inline&#34;&gt;\(\mu[n]\)&lt;/span&gt;，剩下的每名學生的出勤率服從這個均值和標準差爲 &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; 的正態分佈，那麼模型又可以繼續改寫成爲下面的 Model 5-3:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{array}{l}
\mu[n]        = b_1 + b_2A[n] + b_3Sore[n] &amp;amp;  n = 1,2,\dots,N \\
Y[n] \sim \text{Normal}(\mu[n], \sigma) &amp;amp; n = 1,2,\dots,N \\ 
\end{array}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;下面的 Stan 模型是按照 Model 5-3 寫的，它的模型參數有四個，&lt;span class=&#34;math inline&#34;&gt;\(b_1, b_2, b_3, \sigma\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(\mu[n]\)&lt;/span&gt;通過 &lt;code&gt;transformed parameter&lt;/code&gt; 計算獲得:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data {
  int N; 
  int&amp;lt;lower=0, upper=1&amp;gt; A[N];
  real&amp;lt;lower=0, upper=1&amp;gt; Score[N];
  real&amp;lt;lower=0, upper=1&amp;gt; Y[N];
}

parameters {
  real b1; 
  real b2;
  real b3;
  real&amp;lt;lower=0&amp;gt; sigma;
}

transformed parameters {
  real mu[N];
  for (n in 1:N) {
    mu[n] = b1 + b2*A[n] + b3*Score[n];
  }
}

model {
  for (n in 1:N) {
    Y[n] ~ normal(mu[n], sigma);
  }
}

generated quantities {
  real y_pred[N];
  for (n in 1:N) {
    y_pred[n] = normal_rng(mu[n], sigma);
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;下面的 R 代碼用來實現對上面 Stan 模型的擬合:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rstan)
d &amp;lt;- read.csv(file=&amp;#39;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap05/input/data-attendance-1.txt&amp;#39;, header = T)
data &amp;lt;- list(N=nrow(d), A=d$A, Score=d$Score/200, Y=d$Y)
fit &amp;lt;- stan(file=&amp;#39;stanfiles/model5-3.stan&amp;#39;, data=data, seed=1234)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &amp;#39;model5-3&amp;#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 2e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.2 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.116273 seconds (Warm-up)
## Chain 1:                0.117482 seconds (Sampling)
## Chain 1:                0.233755 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &amp;#39;model5-3&amp;#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 8e-06 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.114043 seconds (Warm-up)
## Chain 2:                0.109094 seconds (Sampling)
## Chain 2:                0.223137 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &amp;#39;model5-3&amp;#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 6e-06 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.114931 seconds (Warm-up)
## Chain 3:                0.10347 seconds (Sampling)
## Chain 3:                0.218401 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &amp;#39;model5-3&amp;#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 7e-06 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.121109 seconds (Warm-up)
## Chain 4:                0.115413 seconds (Sampling)
## Chain 4:                0.236522 seconds (Total)
## Chain 4:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Stan model: model5-3.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##              mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat
## b1           0.12    0.00 0.03   0.06   0.10   0.12   0.15   0.19  1707    1
## b2          -0.14    0.00 0.02  -0.18  -0.15  -0.14  -0.13  -0.11  2308    1
## b3           0.32    0.00 0.05   0.22   0.29   0.32   0.36   0.42  1747    1
## sigma        0.05    0.00 0.01   0.04   0.05   0.05   0.06   0.06  2065    1
## mu[1]        0.24    0.00 0.02   0.20   0.22   0.24   0.25   0.27  1827    1
## mu[2]        0.22    0.00 0.01   0.19   0.21   0.22   0.22   0.24  2610    1
## mu[3]        0.33    0.00 0.01   0.31   0.32   0.33   0.33   0.35  3101    1
## mu[4]        0.12    0.00 0.02   0.09   0.11   0.12   0.13   0.15  2251    1
## mu[5]        0.24    0.00 0.01   0.21   0.23   0.24   0.25   0.27  2377    1
## mu[6]        0.34    0.00 0.01   0.32   0.33   0.34   0.35   0.36  3091    1
## mu[7]        0.30    0.00 0.01   0.28   0.30   0.30   0.31   0.32  2666    1
## mu[8]        0.22    0.00 0.01   0.19   0.21   0.22   0.23   0.24  2572    1
## mu[9]        0.36    0.00 0.01   0.34   0.35   0.36   0.37   0.38  2841    1
## mu[10]       0.36    0.00 0.01   0.34   0.35   0.36   0.37   0.38  2863    1
## mu[11]       0.21    0.00 0.01   0.18   0.20   0.21   0.22   0.23  2684    1
## mu[12]       0.35    0.00 0.01   0.33   0.34   0.35   0.35   0.37  3032    1
## mu[13]       0.17    0.00 0.01   0.15   0.16   0.17   0.18   0.20  2864    1
## mu[14]       0.30    0.00 0.01   0.28   0.30   0.30   0.31   0.32  2666    1
## mu[15]       0.30    0.00 0.01   0.28   0.29   0.30   0.31   0.32  2592    1
## mu[16]       0.14    0.00 0.01   0.11   0.13   0.14   0.15   0.17  2477    1
## mu[17]       0.31    0.00 0.01   0.29   0.30   0.31   0.31   0.33  2777    1
## mu[18]       0.26    0.00 0.01   0.23   0.25   0.26   0.27   0.28  1935    1
## mu[19]       0.42    0.00 0.02   0.39   0.41   0.42   0.44   0.46  2192    1
## mu[20]       0.23    0.00 0.01   0.20   0.22   0.23   0.24   0.26  2413    1
## mu[21]       0.12    0.00 0.02   0.09   0.11   0.12   0.13   0.15  2251    1
## mu[22]       0.16    0.00 0.01   0.13   0.15   0.16   0.16   0.18  2714    1
## mu[23]       0.15    0.00 0.01   0.13   0.14   0.15   0.16   0.18  2653    1
## mu[24]       0.21    0.00 0.01   0.19   0.20   0.21   0.22   0.24  2647    1
## mu[25]       0.17    0.00 0.01   0.15   0.16   0.17   0.18   0.19  2855    1
## mu[26]       0.19    0.00 0.01   0.16   0.18   0.19   0.20   0.21  2867    1
## mu[27]       0.32    0.00 0.01   0.30   0.31   0.32   0.32   0.34  2994    1
## mu[28]       0.32    0.00 0.01   0.30   0.31   0.32   0.32   0.34  2994    1
## mu[29]       0.38    0.00 0.01   0.36   0.38   0.38   0.39   0.41  2507    1
## mu[30]       0.31    0.00 0.01   0.29   0.30   0.31   0.31   0.33  2740    1
## mu[31]       0.25    0.00 0.02   0.22   0.24   0.25   0.26   0.28  2266    1
## mu[32]       0.10    0.00 0.02   0.07   0.09   0.10   0.11   0.13  2090    1
## mu[33]       0.20    0.00 0.01   0.18   0.20   0.20   0.21   0.23  2737    1
## mu[34]       0.18    0.00 0.01   0.16   0.17   0.18   0.19   0.20  2885    1
## mu[35]       0.33    0.00 0.01   0.31   0.32   0.33   0.33   0.35  3109    1
## mu[36]       0.34    0.00 0.01   0.32   0.33   0.34   0.34   0.36  3109    1
## mu[37]       0.15    0.00 0.01   0.13   0.14   0.15   0.16   0.18  2622    1
## mu[38]       0.30    0.00 0.01   0.28   0.30   0.30   0.31   0.32  2629    1
## mu[39]       0.27    0.00 0.01   0.24   0.26   0.27   0.28   0.29  2033    1
## mu[40]       0.27    0.00 0.01   0.24   0.26   0.27   0.27   0.29  2016    1
## mu[41]       0.33    0.00 0.01   0.31   0.33   0.33   0.34   0.35  3117    1
## mu[42]       0.34    0.00 0.01   0.32   0.33   0.34   0.35   0.36  3091    1
## mu[43]       0.32    0.00 0.01   0.30   0.31   0.32   0.33   0.34  3059    1
## mu[44]       0.36    0.00 0.01   0.34   0.36   0.36   0.37   0.39  2796    1
## mu[45]       0.42    0.00 0.02   0.38   0.41   0.42   0.43   0.45  2220    1
## mu[46]       0.29    0.00 0.01   0.27   0.29   0.29   0.30   0.31  2445    1
## mu[47]       0.21    0.00 0.02   0.17   0.19   0.21   0.22   0.25  1757    1
## mu[48]       0.37    0.00 0.01   0.34   0.36   0.37   0.37   0.39  2752    1
## mu[49]       0.28    0.00 0.01   0.26   0.28   0.28   0.29   0.31  2253    1
## mu[50]       0.14    0.00 0.01   0.11   0.13   0.14   0.15   0.17  2477    1
## y_pred[1]    0.24    0.00 0.05   0.13   0.20   0.24   0.27   0.34  3502    1
## y_pred[2]    0.22    0.00 0.05   0.11   0.18   0.21   0.25   0.32  3776    1
## y_pred[3]    0.33    0.00 0.05   0.22   0.29   0.33   0.36   0.43  4029    1
## y_pred[4]    0.12    0.00 0.05   0.01   0.08   0.12   0.16   0.23  3814    1
## y_pred[5]    0.24    0.00 0.05   0.13   0.20   0.23   0.27   0.35  3842    1
## y_pred[6]    0.34    0.00 0.05   0.24   0.30   0.34   0.37   0.45  4033    1
## y_pred[7]    0.30    0.00 0.05   0.20   0.27   0.30   0.34   0.41  3727    1
## y_pred[8]    0.22    0.00 0.05   0.11   0.18   0.22   0.25   0.32  3946    1
## y_pred[9]    0.36    0.00 0.05   0.26   0.32   0.36   0.39   0.47  3768    1
## y_pred[10]   0.36    0.00 0.05   0.26   0.33   0.36   0.40   0.46  3771    1
## y_pred[11]   0.21    0.00 0.05   0.11   0.17   0.21   0.25   0.31  3793    1
## y_pred[12]   0.35    0.00 0.05   0.24   0.31   0.35   0.38   0.45  3946    1
## y_pred[13]   0.17    0.00 0.05   0.07   0.14   0.17   0.21   0.28  3833    1
## y_pred[14]   0.30    0.00 0.05   0.20   0.27   0.30   0.34   0.41  3925    1
## y_pred[15]   0.30    0.00 0.05   0.20   0.27   0.30   0.34   0.41  3654    1
## y_pred[16]   0.14    0.00 0.05   0.03   0.10   0.14   0.18   0.25  3850    1
## y_pred[17]   0.31    0.00 0.05   0.20   0.27   0.31   0.34   0.41  3951    1
## y_pred[18]   0.26    0.00 0.05   0.15   0.22   0.26   0.29   0.37  4036    1
## y_pred[19]   0.42    0.00 0.06   0.31   0.39   0.42   0.46   0.53  3723    1
## y_pred[20]   0.23    0.00 0.05   0.13   0.20   0.23   0.27   0.34  3819    1
## y_pred[21]   0.12    0.00 0.05   0.01   0.08   0.12   0.15   0.23  3751    1
## y_pred[22]   0.15    0.00 0.05   0.05   0.12   0.15   0.19   0.26  3798    1
## y_pred[23]   0.15    0.00 0.05   0.05   0.12   0.15   0.19   0.26  3729    1
## y_pred[24]   0.21    0.00 0.05   0.11   0.18   0.21   0.25   0.32  4087    1
## y_pred[25]   0.17    0.00 0.05   0.07   0.13   0.17   0.20   0.27  3712    1
## y_pred[26]   0.19    0.00 0.05   0.08   0.15   0.19   0.22   0.29  3800    1
## y_pred[27]   0.32    0.00 0.05   0.21   0.28   0.32   0.35   0.42  3845    1
## y_pred[28]   0.32    0.00 0.05   0.21   0.28   0.32   0.35   0.42  3892    1
## y_pred[29]   0.38    0.00 0.05   0.28   0.35   0.38   0.42   0.49  3995    1
## y_pred[30]   0.31    0.00 0.05   0.20   0.27   0.31   0.34   0.41  3878    1
## y_pred[31]   0.25    0.00 0.05   0.14   0.21   0.25   0.28   0.35  3631    1
## y_pred[32]   0.10    0.00 0.06  -0.01   0.06   0.10   0.14   0.21  3584    1
## y_pred[33]   0.20    0.00 0.05   0.10   0.17   0.20   0.24   0.31  3110    1
## y_pred[34]   0.18    0.00 0.05   0.08   0.15   0.18   0.22   0.29  3853    1
## y_pred[35]   0.33    0.00 0.05   0.22   0.29   0.33   0.36   0.43  4012    1
## y_pred[36]   0.34    0.00 0.05   0.23   0.30   0.34   0.37   0.44  4012    1
## y_pred[37]   0.15    0.00 0.05   0.04   0.11   0.15   0.19   0.25  4071    1
## y_pred[38]   0.30    0.00 0.05   0.20   0.27   0.30   0.34   0.41  4043    1
## y_pred[39]   0.27    0.00 0.05   0.16   0.23   0.27   0.30   0.37  3757    1
## y_pred[40]   0.27    0.00 0.05   0.16   0.23   0.27   0.30   0.37  3630    1
## y_pred[41]   0.33    0.00 0.05   0.22   0.30   0.33   0.37   0.44  3497    1
## y_pred[42]   0.34    0.00 0.05   0.24   0.30   0.34   0.37   0.45  4219    1
## y_pred[43]   0.32    0.00 0.05   0.22   0.29   0.32   0.35   0.43  4143    1
## y_pred[44]   0.36    0.00 0.05   0.26   0.33   0.36   0.40   0.47  4066    1
## y_pred[45]   0.42    0.00 0.05   0.31   0.38   0.42   0.46   0.53  3410    1
## y_pred[46]   0.29    0.00 0.05   0.19   0.26   0.29   0.33   0.40  4211    1
## y_pred[47]   0.21    0.00 0.06   0.10   0.17   0.21   0.24   0.32  3319    1
## y_pred[48]   0.37    0.00 0.05   0.26   0.33   0.37   0.40   0.47  3699    1
## y_pred[49]   0.28    0.00 0.05   0.18   0.25   0.28   0.32   0.39  4135    1
## y_pred[50]   0.14    0.00 0.05   0.03   0.10   0.14   0.17   0.24  3753    1
## lp__       120.89    0.04 1.47 117.17 120.14 121.24 121.98 122.70  1620    1
## 
## Samples were drawn using NUTS(diag_e) at Wed Jan  8 22:23:46 2020.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;上述代碼中值得注意的是我們對 &lt;span class=&#34;math inline&#34;&gt;\(Score\)&lt;/span&gt; 進行了全部除以 &lt;span class=&#34;math inline&#34;&gt;\(200\)&lt;/span&gt; 的數據縮放調整 (scaling)。這樣有助於我們的模型在進行 MCMC 計算時加速其達到收斂時所需要的時間。&lt;/p&gt;
&lt;p&gt;把計算獲得的事後模型參數平均值代入模型 Model 5-3:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{array}{l}
\mu[n]        = 0.12 - 0.14A[n] + 0.32Sore[n] &amp;amp;  n = 1,2,\dots,N \\
Y[n] \sim \text{Normal}(\mu[n], 0.05) &amp;amp; n = 1,2,\dots,N \\ 
\end{array}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;從輸出的結果報告來看，所有的 &lt;code&gt;Rhat&lt;/code&gt; 都小於1.1，可以認爲採樣已經達到收斂效果，再來確認一下軌跡圖：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(bayesplot)

color_scheme_set(&amp;quot;mix-brightblue-gray&amp;quot;)

posterior2 &amp;lt;- rstan::extract(fit, inc_warmup = TRUE, permuted = FALSE)

p &amp;lt;- mcmc_trace(posterior2, n_warmup = 0, pars = c(&amp;quot;b1&amp;quot;, &amp;quot;b2&amp;quot;, &amp;quot;b3&amp;quot;, &amp;quot;sigma&amp;quot;, &amp;quot;lp__&amp;quot;),
                facet_args = list(nrow = 2, labeller = label_parsed))
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step53&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-22-rstan-wonderful-r-multi-lm_files/figure-html/step53-1.png&#34; alt=&#34;用 bayesplot包數繪製的模型5-3的MCMC鏈式軌跡圖 (trace plot)。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: 用 bayesplot包數繪製的模型5-3的MCMC鏈式軌跡圖 (trace plot)。
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;收斂效果很不錯，下面來解釋回歸係數的事後均值的涵義：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;b3&lt;/code&gt;的事後均值是&lt;span class=&#34;math inline&#34;&gt;\(0.32\)&lt;/span&gt;，所以，&lt;span class=&#34;math inline&#34;&gt;\(Score=150\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(Score=50\)&lt;/span&gt;的兩名學生，當他們同時都是喜歡或者同時都不喜歡打工時，&lt;span class=&#34;math inline&#34;&gt;\(Score = 150\)&lt;/span&gt;的學生的出勤率平均比 &lt;span class=&#34;math inline&#34;&gt;\(Score = 50\)&lt;/span&gt; 的學生的出勤率高 &lt;span class=&#34;math inline&#34;&gt;\(0.32 \times (150-50)/200 = 0.16\)&lt;/span&gt;。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;b2&lt;/code&gt;的事後均值是&lt;span class=&#34;math inline&#34;&gt;\(-0.14\)&lt;/span&gt;，所以，同樣地，&lt;span class=&#34;math inline&#34;&gt;\(Score\)&lt;/span&gt;相同的兩名學生，喜歡打工的學生比不喜歡打工的學生出勤率平均要低 &lt;span class=&#34;math inline&#34;&gt;\(0.14\)&lt;/span&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;step-3.-看圖確認模型擬合狀況&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 3. 看圖確認模型擬合狀況&lt;/h2&gt;
&lt;p&gt;下圖繪製了上面貝葉斯多重線性回歸模型計算獲得的事後貝葉斯預測區間，和觀測值&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;出勤率之間的直觀關係：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;source(&amp;quot;commonRstan.R&amp;quot;)

ms &amp;lt;- rstan::extract(fit)

Score_new &amp;lt;- 50:200
N_X &amp;lt;- length(Score_new)
N_mcmc &amp;lt;- length(ms$lp__)

set.seed(1234)
y_base_mcmc &amp;lt;- as.data.frame(matrix(nrow=N_mcmc, ncol=N_X))
y_base_a0_mcmc &amp;lt;- as.data.frame(matrix(nrow = N_mcmc, ncol = N_X))
y_mcmc &amp;lt;- as.data.frame(matrix(nrow=N_mcmc, ncol=N_X))
y_a0_mcmc &amp;lt;- as.data.frame(matrix(nrow=N_mcmc, ncol=N_X))

for (i in 1:N_X) {
  y_base_mcmc[,i] &amp;lt;- ms$b1 + ms$b2 + ms$b3 * Score_new[i]/200
  y_base_a0_mcmc[] &amp;lt;- ms$b1 + ms$b2*0 + ms$b3 * Score_new[i]/200
  y_mcmc[,i] &amp;lt;- rnorm(n=N_mcmc, mean=y_base_mcmc[,i], sd=ms$sigma)
  y_a0_mcmc[,i] &amp;lt;- rnorm(n=N_mcmc, mean=y_base_a0_mcmc[,i], sd=ms$sigma)
}

customize.ggplot.axis &amp;lt;- function(p) {
  p &amp;lt;- p + labs(x=&amp;#39;Score&amp;#39;, y=&amp;#39;Y&amp;#39;)
  p &amp;lt;- p + scale_y_continuous(breaks=seq(from=-0.2, to=0.8, by=0.2))
  p &amp;lt;- p + coord_cartesian(xlim=c(50, 200), ylim=c(-0.2, 0.6))
  return(p)
}

d_est &amp;lt;- data.frame.quantile.mcmc(x=Score_new, y_mcmc=y_mcmc)
d_esta0 &amp;lt;- data.frame.quantile.mcmc(x=Score_new, y_mcmc=y_a0_mcmc)
# p &amp;lt;- ggplot.5quantile(data=d_est)
# p2 &amp;lt;- ggplot.5quantile(data = d_esta0)
# p &amp;lt;- p + geom_point(data=d[d$A==1, ], aes(x=Score, y=Y), shape=24, size=5)
# p2 &amp;lt;- p2 + geom_point(data=d[d$A==0, ], aes(x=Score, y=Y), shape=1, size=5)
# p &amp;lt;- customize.ggplot.axis(p)
# p2 &amp;lt;- customize.ggplot.axis(p2)

visuals = rbind(d_est,d_esta0)
visuals$A=c(rep(1,151),rep(0,151)) # 151 points of each flavour

qn &amp;lt;- colnames(visuals)[-1]
p &amp;lt;- ggplot(data=visuals, aes(x=X, y=p50, group = A))
p &amp;lt;- p + my_theme()
p &amp;lt;- p + geom_ribbon(aes_string(ymin=qn[1], ymax=qn[5]), fill=&amp;#39;black&amp;#39;, alpha=1/6)
p &amp;lt;- p + geom_ribbon(aes_string(ymin=qn[2], ymax=qn[4]), fill=&amp;#39;black&amp;#39;, alpha=2/6)
p &amp;lt;- p + geom_line(size=1)
p &amp;lt;- p + geom_point(data=d[d$A==1, ], aes(x=Score, y=Y), shape=24, size=5)
p &amp;lt;- p + geom_point(data=d[d$A==0, ], aes(x=Score, y=Y), shape=20, size=5)
p &amp;lt;- customize.ggplot.axis(p)
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:fig52&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-22-rstan-wonderful-r-multi-lm_files/figure-html/fig52-1.png&#34; alt=&#34;黑色原點(不喜歡打工)，和無色三角形(喜歡打工)的學生的出勤率，和模型計算獲得的貝葉斯事後預測區間。黑色線是中位數，灰色帶是50%預測區間和95%預測區間。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: 黑色原點(不喜歡打工)，和無色三角形(喜歡打工)的學生的出勤率，和模型計算獲得的貝葉斯事後預測區間。黑色線是中位數，灰色帶是50%預測區間和95%預測區間。
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;上述觀察預測值區間和實際觀測之間的關係的視覺化圖形，在多重線性回歸模型只有兩個預測變量的事後還較爲容易獲得，當模型中有三個或以上的預測變量時，可視化變得困難重重。&lt;/p&gt;
&lt;p&gt;此時我們推薦繪製“實際觀測值和預測值”，以及模型給出的每個預測值的隨機誤差&lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt;分佈範圍，相結合的圖形來判斷模型擬合程度。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d_qua &amp;lt;- t(apply(ms$y_pred, 2, quantile, prob=c(0.1, 0.5, 0.9)))
colnames(d_qua) &amp;lt;- c(&amp;#39;p10&amp;#39;, &amp;#39;p50&amp;#39;, &amp;#39;p90&amp;#39;)
d_qua &amp;lt;- data.frame(d, d_qua)
d_qua$A &amp;lt;- as.factor(d_qua$A)

p &amp;lt;- ggplot(data=d_qua, aes(x=Y, y=p50, ymin=p10, ymax=p90, shape=A, fill=A))
p &amp;lt;- p + theme_bw(base_size=18) + theme(legend.key.height=grid::unit(2.5,&amp;#39;line&amp;#39;))
p &amp;lt;- p + coord_fixed(ratio=1, xlim=c(0, 0.5), ylim=c(0, 0.5))
p &amp;lt;- p + geom_pointrange(size=0.8, color=&amp;#39;grey5&amp;#39;)
p &amp;lt;- p + geom_abline(aes(slope=1, intercept=0), color=&amp;#39;black&amp;#39;, alpha=3/5, linetype=&amp;#39;31&amp;#39;)
p &amp;lt;- p + scale_shape_manual(values=c(21, 24))
p &amp;lt;- p + scale_fill_manual(values=c(&amp;#39;white&amp;#39;, &amp;#39;grey70&amp;#39;))
p &amp;lt;- p + labs(x=&amp;#39;Observed&amp;#39;, y=&amp;#39;Predicted&amp;#39;)
p &amp;lt;- p + scale_x_continuous(breaks=seq(from=0, to=0.5, by=0.1))
p &amp;lt;- p + scale_y_continuous(breaks=seq(from=0, to=0.5, by=0.1))
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:fig53&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-22-rstan-wonderful-r-multi-lm_files/figure-html/fig53-1.png&#34; alt=&#34;觀測值(x)，和預測值(y)的散點圖，以及預測值的80%預測區間。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4: 觀測值(x)，和預測值(y)的散點圖，以及預測值的80%預測區間。
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;從上圖中可以看出，大多數的觀測點和預測點以及預測的80%區間基本都在 &lt;span class=&#34;math inline&#34;&gt;\(y = x\)&lt;/span&gt; 這條對角線上。大致可以認爲本次貝葉斯多重線性回歸擬合效果尚且能夠接受。&lt;/p&gt;
&lt;p&gt;隨機誤差 &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon[n]\)&lt;/span&gt; 被認爲服從 &lt;span class=&#34;math inline&#34;&gt;\(\text{Normal}(0, \sigma)\)&lt;/span&gt; 的正態分佈。從模型中可以計算獲得每個學生出勤率的預測值和實際觀測值之間的差，這就是隨機誤差。貝葉斯框架之下，我們實際獲得的會是每名學生隨機誤差的分佈：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;N_mcmc &amp;lt;- length(ms$lp__)

d_noise &amp;lt;- data.frame(t(-t(ms$mu) + d$Y))
colnames(d_noise) &amp;lt;- paste0(&amp;#39;noise&amp;#39;, 1:nrow(d))
d_est &amp;lt;- data.frame(mcmc=1:N_mcmc, d_noise)
d_melt &amp;lt;- reshape2::melt(d_est, id=c(&amp;#39;mcmc&amp;#39;), variable.name=&amp;#39;X&amp;#39;)

d_mode &amp;lt;- data.frame(t(apply(d_noise, 2, function(x) {
  dens &amp;lt;- density(x)
  mode_i &amp;lt;- which.max(dens$y)
  mode_x &amp;lt;- dens$x[mode_i]
  mode_y &amp;lt;- dens$y[mode_i]
  c(mode_x, mode_y)
})))
colnames(d_mode) &amp;lt;- c(&amp;#39;X&amp;#39;, &amp;#39;Y&amp;#39;)

p &amp;lt;- ggplot()
p &amp;lt;- p + theme_bw(base_size=18)
p &amp;lt;- p + geom_line(data=d_melt, aes(x=value, group=X), stat=&amp;#39;density&amp;#39;, color=&amp;#39;black&amp;#39;, alpha=0.4)
p &amp;lt;- p + geom_segment(data=d_mode, aes(x=X, xend=X, y=Y, yend=0), color=&amp;#39;black&amp;#39;, linetype=&amp;#39;dashed&amp;#39;, alpha=0.4)
p &amp;lt;- p + geom_rug(data=d_mode, aes(x=X), sides=&amp;#39;b&amp;#39;)
p &amp;lt;- p + labs(x=&amp;#39;value&amp;#39;, y=&amp;#39;density&amp;#39;)
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:fig54left&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-22-rstan-wonderful-r-multi-lm_files/figure-html/fig54left-1.png&#34; alt=&#34;每名學生的出勤率隨機誤差的分佈&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 5: 每名學生的出勤率隨機誤差的分佈
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;實際上我們只需要選取每名學生模型計算獲得的事後隨機誤差的代表值，比如可以是平均值，中央值，或者是MAP值（事後確率最大推定値，maximum a posteriori estimate），來觀察就可以了：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s_dens &amp;lt;- density(ms$s)
s_MAP &amp;lt;- s_dens$x[which.max(s_dens$y)]
bw &amp;lt;- 0.01
p &amp;lt;- ggplot(data=d_mode, aes(x=X))
p &amp;lt;- p + theme_bw(base_size=18)
p &amp;lt;- p + geom_histogram(binwidth=bw, color=&amp;#39;black&amp;#39;, fill=&amp;#39;white&amp;#39;)
p &amp;lt;- p + geom_density(eval(bquote(aes(y=..count..*.(bw)))), alpha=0.5, color=&amp;#39;black&amp;#39;, fill=&amp;#39;gray20&amp;#39;)
p &amp;lt;- p + stat_function(fun=function(x) nrow(d)*bw*dnorm(x, mean=0, sd=s_MAP), linetype=&amp;#39;dashed&amp;#39;)
p &amp;lt;- p + labs(x=&amp;#39;value&amp;#39;, y=&amp;#39;count&amp;#39;)
p &amp;lt;- p + xlim(range(density(d_mode$X)$x))
p&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 2 rows containing missing values (geom_bar).&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:fig54right&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-22-rstan-wonderful-r-multi-lm_files/figure-html/fig54right-1.png&#34; alt=&#34;每名學生事後出勤率隨機誤差的MAP值的柱狀圖，和相應的概率密度函數（灰色鐘罩），點狀虛線是均值爲0，標準差是模型計算的事後隨機誤差標準差的 MAP 值的正態分佈的形狀。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 6: 每名學生事後出勤率隨機誤差的MAP值的柱狀圖，和相應的概率密度函數（灰色鐘罩），點狀虛線是均值爲0，標準差是模型計算的事後隨機誤差標準差的 MAP 值的正態分佈的形狀。
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;step-4.-mcmc-樣本的散點圖矩陣&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 4. MCMC 樣本的散點圖矩陣&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(GGally)
library(hexbin)


d &amp;lt;- data.frame(b1=ms$b1, b2=ms$b2, b3=ms$b3, sigma=ms$sigma, mu1=ms$mu[,1], mu50=ms$mu[,50], lp__=ms$lp__)
N_col &amp;lt;- ncol(d)
ggp &amp;lt;- ggpairs(d, upper=&amp;#39;blank&amp;#39;, diag=&amp;#39;blank&amp;#39;, lower=&amp;#39;blank&amp;#39;)

label_list &amp;lt;- list(b1=&amp;#39;b1&amp;#39;, b2=&amp;#39;b2&amp;#39;, b3=&amp;#39;b3&amp;#39;, sigma=&amp;#39;sigma&amp;#39;, mu1=&amp;#39;mu[1]&amp;#39;, mu50=&amp;#39;mu[50]&amp;#39;, lp__=&amp;#39;lp__&amp;#39;)
for(i in 1:N_col) {
  x &amp;lt;- d[,i]
  bw &amp;lt;- (max(x)-min(x))/10
  p &amp;lt;- ggplot(data.frame(x), aes(x))
  p &amp;lt;- p + theme_bw(base_size=14)
  p &amp;lt;- p + theme(axis.text.x=element_text(angle=60, vjust=1, hjust=1))
  p &amp;lt;- p + geom_histogram(binwidth=bw, fill=&amp;#39;white&amp;#39;, color=&amp;#39;grey5&amp;#39;)
  p &amp;lt;- p + geom_line(eval(bquote(aes(y=..count..*.(bw)))), stat=&amp;#39;density&amp;#39;)
  p &amp;lt;- p + geom_label(data=data.frame(x=-Inf, y=Inf, label=label_list[[colnames(d)[i]]]), aes(x=x, y=y, label=label), hjust=0, vjust=1)
  ggp &amp;lt;- putPlot(ggp, p, i, i)
}

zcolat &amp;lt;- seq(-1, 1, length=81)
zcolre &amp;lt;- c(zcolat[1:40]+1, rev(zcolat[41:81]))

for(i in 1:(N_col-1)) {
  for(j in (i+1):N_col) {
    x &amp;lt;- as.numeric(d[,i])
    y &amp;lt;- as.numeric(d[,j])
    r &amp;lt;- cor(x, y, method=&amp;#39;spearman&amp;#39;, use=&amp;#39;pairwise.complete.obs&amp;#39;)
    zcol &amp;lt;- lattice::level.colors(r, at=zcolat, col.regions=grey(zcolre))
    textcol &amp;lt;- ifelse(abs(r) &amp;lt; 0.4, &amp;#39;grey20&amp;#39;, &amp;#39;white&amp;#39;)
    ell &amp;lt;- ellipse::ellipse(r, level=0.95, type=&amp;#39;l&amp;#39;, npoints=50, scale=c(.2, .2), centre=c(.5, .5))
    p &amp;lt;- ggplot(data.frame(ell), aes(x=x, y=y))
    p &amp;lt;- p + theme_bw() + theme(
      plot.background=element_blank(),
      panel.grid.major=element_blank(), panel.grid.minor=element_blank(),
      panel.border=element_blank(), axis.ticks=element_blank()
    )
    p &amp;lt;- p + geom_polygon(fill=zcol, color=zcol)
    p &amp;lt;- p + geom_text(data=NULL, x=.5, y=.5, label=100*round(r, 2), size=6, col=textcol)
    ggp &amp;lt;- putPlot(ggp, p, i, j)
  }
}

for(j in 1:(N_col-1)) {
  for(i in (j+1):N_col) {
    x &amp;lt;- d[,j]
    y &amp;lt;- d[,i]
    p &amp;lt;- ggplot(data.frame(x, y), aes(x=x, y=y))
    p &amp;lt;- p + theme_bw(base_size=14)
    p &amp;lt;- p + theme(axis.text.x=element_text(angle=60, vjust=1, hjust=1))
    p &amp;lt;- p + geom_hex()
    p &amp;lt;- p + scale_fill_gradientn(colours=gray.colors(7, start=0.1, end=0.9))
    ggp &amp;lt;- putPlot(ggp, p, i, j)
  }
}
ggp&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:fig55&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-22-rstan-wonderful-r-multi-lm_files/figure-html/fig55-1.png&#34; alt=&#34;MCMC樣本的事後矩陣。對角線上是各個參數事後樣本的柱狀圖和相應的概率密度曲線。右上角是各個參數事後樣本之間的 Spearman 秩相關係數。左下角是兩兩的散點圖。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 7: MCMC樣本的事後矩陣。對角線上是各個參數事後樣本的柱狀圖和相應的概率密度曲線。右上角是各個參數事後樣本之間的 Spearman 秩相關係數。左下角是兩兩的散點圖。
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Simple linear regression using Rstan--Rstan Wonderful R-(2)</title>
      <link>https://wangcc.me/post/simple-linear-regression-using-rstan/</link>
      <pubDate>Tue, 15 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/simple-linear-regression-using-rstan/</guid>
      <description>
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#step-1-確認數據分佈&#34;&gt;Step 1, 確認數據分佈&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-2-描述線性模型&#34;&gt;Step 2, 描述線性模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-3-寫下stan模型&#34;&gt;Step 3, 寫下Stan模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-4-診斷stan貝葉斯模型的收斂程度&#34;&gt;Step 4, 診斷Stan貝葉斯模型的收斂程度&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-5修改mcmc條件設定&#34;&gt;Step 5，修改MCMC條件設定&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-6-並行平行計算的設定&#34;&gt;Step 6, 並行（平行）計算的設定&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-7-計算貝葉斯可信區間和貝葉斯預測區間&#34;&gt;Step 7, 計算貝葉斯可信區間和貝葉斯預測區間&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#練習題&#34;&gt;練習題&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap04/input/data-salary.txt&#34;&gt;數據 data-salary.txt&lt;/a&gt;是架空的。&lt;/p&gt;
&lt;p&gt;某公司社員的年齡 &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;（歲），和年收入 &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;（萬日元）的數據如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;X,Y
24,472
24,403
26,454
32,575
33,546
35,781
38,750
40,601
40,814
43,792
43,745
44,837
48,868
52,988
56,1092
56,1007
57,1233
58,1202
59,1123
59,1314
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;年收入 &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; 被認爲是由基本年收 &lt;span class=&#34;math inline&#34;&gt;\(y_{base}\)&lt;/span&gt; 和其他影響因素 &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt; 構成。由於該公司是典型的年功序列式的日本傳統企業，所以基本年收本身和社員年齡成正比例。 &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt; 則被認爲是由該員工當年的業績等隨機誤差造成的，但是所有員工的 &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt; 的均值被認爲是零。&lt;/p&gt;
&lt;p&gt;g分析目的：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;借用這個數據來分析並回答如下的問題：在該公司如果採用了一名50歲的員工，他/她的年收入的預期值會是多少。&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;step-1-確認數據分佈&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 1, 確認數據分佈&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Salary &amp;lt;- read.table(&amp;quot;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap04/input/data-salary.txt&amp;quot;, 
                     sep = &amp;quot;,&amp;quot;, header = T)
library(ggplot2)

ggplot(Salary, aes(x = X, y = Y)) + 
  geom_point(shape = 1, size = 4)  + theme(plot.subtitle = element_text(vjust = 1), 
    plot.caption = element_text(vjust = 1), 
    axis.line = element_line(colour = &amp;quot;bisque4&amp;quot;, 
        size = 0.2, linetype = &amp;quot;solid&amp;quot;), 
    axis.ticks = element_line(size = 0.7), 
    axis.title = element_text(size = 16), 
    axis.text = element_text(size = 16, colour = &amp;quot;gray0&amp;quot;), 
    panel.background = element_rect(fill = &amp;quot;gray98&amp;quot;)) +
  scale_y_continuous(limits = c(200, 1400), breaks = c(200, 600, 1000, 1400))&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step1&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-15-simple-linear-regression-using-rstan_files/figure-html/step1-1.png&#34; alt=&#34;橫軸爲 $X$，縱軸爲 $Y$ 的散點圖&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: 橫軸爲 &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;，縱軸爲 &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; 的散點圖
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;從這個散點圖的特徵可以看出年收入確實似乎和年齡呈線性正相關。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-2-描述線性模型&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 2, 描述線性模型&lt;/h2&gt;
&lt;p&gt;這個簡單線性回歸模型的數學表達式可以描述如下：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{array}{l}
Y[n]        = y_{base}[n] + \varepsilon [n]&amp;amp;  n = 1,2,\dots,N \\
y_{base}[n] = a + bX[n]                    &amp;amp;  n = 1,2,\dots,N \\
\varepsilon[n] \sim \text{Normal}(0, \sigma) &amp;amp; n = 1,2,\dots,N \\ 
\end{array}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;同樣的模型你可以簡化描述成爲：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y[n] \sim \text{Normal}(a + bX[n], \sigma)\;\; n = 1,2,\dots,N
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;那麼如果一個統計師只有經過傳統概率論觀點的訓練，他/她會在R裏面這樣來分析這個數據：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res_lm &amp;lt;- lm(Y ~ X, data = Salary)
summary(res_lm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Y ~ X, data = Salary)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -155.471  -51.523   -6.663   52.822  141.349 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) -119.697     68.148  -1.756    0.096 .  
## X             21.904      1.518  14.428 2.47e-11 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 79.1 on 18 degrees of freedom
## Multiple R-squared:  0.9204, Adjusted R-squared:  0.916 
## F-statistic: 208.2 on 1 and 18 DF,  p-value: 2.466e-11&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 用這個線性回歸模型來對上面模型中的參數作出預測：

X_new &amp;lt;- data.frame(X=23:60)
conf_95 &amp;lt;- predict(res_lm, X_new, interval = &amp;quot;confidence&amp;quot;, level = 0.95)
pred_95 &amp;lt;- predict(res_lm, X_new, interval = &amp;quot;prediction&amp;quot;, level = 0.95)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;temp_var &amp;lt;- predict(res_lm, interval=&amp;quot;prediction&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in predict.lm(res_lm, interval = &amp;quot;prediction&amp;quot;): predictions on current data refer to _future_ responses&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_df &amp;lt;- cbind(Salary, temp_var)

ggplot(new_df, aes(x = X, y = Y)) + 
  geom_point(shape = 1, size = 4)  + theme(plot.subtitle = element_text(vjust = 1), 
    plot.caption = element_text(vjust = 1), 
    axis.line = element_line(colour = &amp;quot;bisque4&amp;quot;, 
        size = 0.2, linetype = &amp;quot;solid&amp;quot;), 
    axis.ticks = element_line(size = 0.7), 
    axis.title = element_text(size = 16), 
    axis.text = element_text(size = 16, colour = &amp;quot;gray0&amp;quot;), 
    panel.background = element_rect(fill = &amp;quot;gray98&amp;quot;)) + 
  geom_smooth(method = lm, se=TRUE, size = 0.3)+
  scale_y_continuous(limits = c(200, 1400), breaks = c(200, 600, 1000, 1400)) +
   geom_line(aes(y=lwr), color = &amp;quot;red&amp;quot;, linetype = &amp;quot;dashed&amp;quot;)+
    geom_line(aes(y=upr), color = &amp;quot;red&amp;quot;, linetype = &amp;quot;dashed&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step2&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-15-simple-linear-regression-using-rstan_files/figure-html/step2-1.png&#34; alt=&#34;用簡單線性回歸模型計算的基本年收的信賴區間(灰色陰影)和預測區間(紅色點線)。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: 用簡單線性回歸模型計算的基本年收的信賴區間(灰色陰影)和預測區間(紅色點線)。
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;step-3-寫下stan模型&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 3, 寫下Stan模型&lt;/h2&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;data {
    int N; 
    real X[N]; 
    real Y[N];
}

parameters {
    real a;
    real b;
    real&amp;lt;lower=0&amp;gt; sigma;
}

model {
    for(n in 1:N) {
        Y[n] ~ normal(a + b*X[n], sigma);
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;參數部分 &lt;code&gt;real&amp;lt;lower=0&amp;gt; sigma&lt;/code&gt; 的代碼表示標準差不可採集負數作爲樣本。&lt;/p&gt;
&lt;p&gt;實際運行上面的Stan代碼：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rstan)
data &amp;lt;- list(N=nrow(Salary), X=Salary$X, Y = Salary$Y)
fit &amp;lt;- sampling(model4_5, data, seed = 1234) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &amp;#39;5b73686886069c0bad70513d4ea4141a&amp;#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 1.5e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.093322 seconds (Warm-up)
## Chain 1:                0.054914 seconds (Sampling)
## Chain 1:                0.148236 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &amp;#39;5b73686886069c0bad70513d4ea4141a&amp;#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 4e-06 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.098838 seconds (Warm-up)
## Chain 2:                0.061329 seconds (Sampling)
## Chain 2:                0.160167 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &amp;#39;5b73686886069c0bad70513d4ea4141a&amp;#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 4e-06 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.091017 seconds (Warm-up)
## Chain 3:                0.053765 seconds (Sampling)
## Chain 3:                0.144782 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &amp;#39;5b73686886069c0bad70513d4ea4141a&amp;#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 4e-06 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.100685 seconds (Warm-up)
## Chain 4:                0.062005 seconds (Sampling)
## Chain 4:                0.16269 seconds (Total)
## Chain 4:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Stan model: 5b73686886069c0bad70513d4ea4141a.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##          mean se_mean    sd    2.5%     25%     50%    75%  97.5% n_eff Rhat
## a     -117.45    1.93 71.31 -257.66 -164.65 -119.17 -71.98  23.17  1358 1.00
## b       21.86    0.04  1.60   18.68   20.83   21.89  22.91  24.97  1331 1.00
## sigma   84.51    0.41 15.21   61.09   73.72   82.41  93.18 120.03  1381 1.01
## lp__   -93.61    0.04  1.26  -96.86  -94.19  -93.27 -92.69 -92.14  1164 1.01
## 
## Samples were drawn using NUTS(diag_e) at Mon May 18 14:21:36 2020.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;輸出結果的前三行，是該次MCMC的設定條件，其中模型名稱是Rmarkdown文件中隨機產生的。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;第二行則說明的是該次MCMC進行了4條鏈的採樣，每條鏈2000次，其中前1000次被當作是 burn-in (或者叫 warmup)。可以看到一共獲得了4000個事後樣本。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;接下來的五行是參數的事後樣本的事後分析總結，一共有11列。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第1列是參數名稱，最後一個 &lt;code&gt;lp__&lt;/code&gt;是Stan特有的算法得到的產物，具體解釋爲對數事後概率 (log posterior)，當然它也需要得到收斂才行。&lt;/li&gt;
&lt;li&gt;第2列是獲得的4000個參數的事後樣本的事後平均值(posterior mean)。例如&lt;code&gt;b&lt;/code&gt;（回歸直線的斜率）的事後平均值是21.96，也就是說年齡每增加一歲，基本年收入平均增加21.96萬日元。你可以和之前的概率論算法相比較(&lt;code&gt;b = 21.904&lt;/code&gt;)。&lt;/li&gt;
&lt;li&gt;第3列&lt;code&gt;se_mean&lt;/code&gt;是事後平均值的標準誤(standard error of posterior mean)。說白了是MCMC事後樣本的方差除以第10列的有效樣本量&lt;code&gt;n_eff&lt;/code&gt;之後取根號獲得的值。&lt;/li&gt;
&lt;li&gt;第4列&lt;code&gt;sd&lt;/code&gt;是MCMC事後樣本的標準差(standard deviation of posterior MCMC sample)。&lt;/li&gt;
&lt;li&gt;第5-9列是MCMC事後樣本的四分位點。也就是貝葉斯統計算法獲得的事後可信區間。&lt;/li&gt;
&lt;li&gt;第10列&lt;code&gt;n_eff&lt;/code&gt;是Stan在基於事後樣本自相關程度來判斷的有效事後樣本量大小。爲了有效地計算和繪製事後分佈的統計量，這個有效樣本量需要至少有100個以上吧（作者觀點）。如果報告給出的事後有效樣本量過小的話也是模型收斂不佳的表現之一。&lt;/li&gt;
&lt;li&gt;第11列&lt;code&gt;Rhat&lt;/code&gt;&lt;span class=&#34;math inline&#34;&gt;\((\hat R)\)&lt;/span&gt;是主要用於判斷模型是否達到收斂的重要指標，每個參數都會被計算一個&lt;code&gt;Rhat&lt;/code&gt;值。當MCMC鏈條數在3以上，且同時所有的模型參數的 &lt;code&gt;Rhat &amp;lt; 1.1&lt;/code&gt;的話，可以認爲模型達到了良好的收斂。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;step-4-診斷stan貝葉斯模型的收斂程度&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 4, 診斷Stan貝葉斯模型的收斂程度&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggmcmc)

ggmcmc(ggs(fit, inc_warmup = TRUE, stan_include_auxiliar = TRUE), plot = &amp;quot;traceplot&amp;quot;, dev_type_html = &amp;quot;png&amp;quot;, 
       file = &amp;quot;trace.html&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;上面的代碼，會自動生成四個模型參數的軌跡MCMC鏈式圖報告。&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step4&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../../static/img/traceplot-model4-5.png&#34; alt=&#34;用ggmcmc函數製作而成的MCMC鏈式圖報告。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: 用ggmcmc函數製作而成的MCMC鏈式圖報告。
&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(bayesplot)

color_scheme_set(&amp;quot;mix-brightblue-gray&amp;quot;)

posterior2 &amp;lt;- rstan::extract(fit, inc_warmup = TRUE, permuted = FALSE)

p &amp;lt;- mcmc_trace(posterior2, n_warmup = 0,
                facet_args = list(nrow = 2, labeller = label_parsed))
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step41&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-15-simple-linear-regression-using-rstan_files/figure-html/step41-1.png&#34; alt=&#34;用 bayesplot包數繪製的MCMC鏈式圖。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4: 用 bayesplot包數繪製的MCMC鏈式圖。
&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- mcmc_acf_bar(posterior2)
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step42&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-15-simple-linear-regression-using-rstan_files/figure-html/step42-1.png&#34; alt=&#34;用 bayesplot包數繪製的事後樣本自相關圖(autocorrelation)。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 5: 用 bayesplot包數繪製的事後樣本自相關圖(autocorrelation)。
&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- mcmc_dens_overlay(posterior2, color_chains = T)
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step43&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-15-simple-linear-regression-using-rstan_files/figure-html/step43-1.png&#34; alt=&#34;用 bayesplot包數繪製的事後樣本密度分佈圖。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 6: 用 bayesplot包數繪製的事後樣本密度分佈圖。
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;step-5修改mcmc條件設定&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 5，修改MCMC條件設定&lt;/h2&gt;
&lt;p&gt;進行貝葉斯模型擬合的過程中，常常需要不停地修改模型的條件，例如縮短warm-up等。下面的Rstan代碼可以實現簡便地頻繁修改MCMC條件設定：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# library(rstan) uncomment if run for the first time
data &amp;lt;- list(N=nrow(Salary), X=Salary$X, Y = Salary$Y)
fit2 &amp;lt;- sampling(
    model4_5, 
    data = data, 
    pars = c(&amp;quot;b&amp;quot;, &amp;quot;sigma&amp;quot;), 
    init = function(){
      list(a = runif(1, -10, 10), b = runif(1, 0, 10), sigma = 10)
    },
    seed = 123,
    chains = 3, iter = 1000, warmup = 200, thin = 2
) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &amp;#39;5b73686886069c0bad70513d4ea4141a&amp;#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 1.2e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 1: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 1: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 1: Iteration: 201 / 1000 [ 20%]  (Sampling)
## Chain 1: Iteration: 300 / 1000 [ 30%]  (Sampling)
## Chain 1: Iteration: 400 / 1000 [ 40%]  (Sampling)
## Chain 1: Iteration: 500 / 1000 [ 50%]  (Sampling)
## Chain 1: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 1: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 1: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 1: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 1: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.05316 seconds (Warm-up)
## Chain 1:                0.036218 seconds (Sampling)
## Chain 1:                0.089378 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &amp;#39;5b73686886069c0bad70513d4ea4141a&amp;#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 2e-06 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 2: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 2: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 2: Iteration: 201 / 1000 [ 20%]  (Sampling)
## Chain 2: Iteration: 300 / 1000 [ 30%]  (Sampling)
## Chain 2: Iteration: 400 / 1000 [ 40%]  (Sampling)
## Chain 2: Iteration: 500 / 1000 [ 50%]  (Sampling)
## Chain 2: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 2: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 2: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 2: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 2: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.052191 seconds (Warm-up)
## Chain 2:                0.036956 seconds (Sampling)
## Chain 2:                0.089147 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &amp;#39;5b73686886069c0bad70513d4ea4141a&amp;#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 2e-06 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 3: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 3: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 3: Iteration: 201 / 1000 [ 20%]  (Sampling)
## Chain 3: Iteration: 300 / 1000 [ 30%]  (Sampling)
## Chain 3: Iteration: 400 / 1000 [ 40%]  (Sampling)
## Chain 3: Iteration: 500 / 1000 [ 50%]  (Sampling)
## Chain 3: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 3: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 3: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 3: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 3: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.052083 seconds (Warm-up)
## Chain 3:                0.042977 seconds (Sampling)
## Chain 3:                0.09506 seconds (Total)
## Chain 3:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Stan model: 5b73686886069c0bad70513d4ea4141a.
## 3 chains, each with iter=1000; warmup=200; thin=2; 
## post-warmup draws per chain=400, total post-warmup draws=1200.
## 
##         mean se_mean    sd   2.5%    25%    50%    75%  97.5% n_eff Rhat
## b      21.90    0.06  1.56  18.56  20.96  21.92  22.91  24.83   587 1.00
## sigma  85.49    0.58 15.60  61.11  74.29  83.15  94.85 122.67   727 1.01
## lp__  -93.62    0.05  1.29 -96.82 -94.22 -93.30 -92.70 -92.16   609 1.00
## 
## Samples were drawn using NUTS(diag_e) at Mon May 18 14:27:30 2020.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中&lt;code&gt;fit&lt;/code&gt;的最後一行是修改各種條件的示例：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;chains&lt;/code&gt;至少要三條；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;iter&lt;/code&gt;一開始可以設定在500~1000左右，確定模型可以收斂以後，再加大這個數值以獲得穩定的事後統計量，多多益善；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;warmup&lt;/code&gt;，也就MCMC採樣開始後多少樣本可以丟棄。這個數值需要參考trace plot；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;thin&lt;/code&gt;，通常只需要保持默認值 1。和WinBUGS, JAGS相比Stan算法採集的事後樣本自相關比較低。&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;step-6-並行平行計算的設定&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 6, 並行（平行）計算的設定&lt;/h2&gt;
&lt;p&gt;如果你寫出來的貝葉斯模型需要很長時間的計算和收斂，可以充分利用你的計算機的多核計算，把每條MCMC鏈單獨進行計算加速這個過程：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;parallel::detectCores() #我的桌上型電腦有8個核可以用於平行計算&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;但是平行計算時如果計算中出錯則由於每條鏈都是相互獨立地進行，報錯就減少了。所以如果要使用多核同時計算的話，建議先減少採樣數，確認不會報錯以後再用多核平行計算增加採樣量。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;step-7-計算貝葉斯可信區間和貝葉斯預測區間&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 7, 計算貝葉斯可信區間和貝葉斯預測區間&lt;/h2&gt;
&lt;p&gt;這一步就又回到一開始提出的研究問題上來，我們來計算基本年收的貝葉斯可信區間和貝葉斯預測區間。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ms &amp;lt;- rstan::extract(fit)

quantile(ms$b, probs = c(0.025, 0.975))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     2.5%    97.5% 
## 18.67987 24.97108&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d_mcmc &amp;lt;- data.frame(a = ms$a, b = ms$b, sigma = ms$sigma)

head(d_mcmc)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            a        b    sigma
## 1 -103.92295 22.04743 70.39829
## 2  -30.41656 20.16582 74.35210
## 3  -95.35165 21.32534 75.19714
## 4  -10.88849 19.01547 68.02757
## 5 -177.04183 23.49984 81.40161
## 6 -146.45260 22.73838 95.97706&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1 &amp;lt;- ggplot(d_mcmc, aes(x = a, y = b)) + 
 geom_point(shape = 1, size = 4)

ggExtra::ggMarginal(
  p = p1,
  type = &amp;#39;density&amp;#39;,
  margins = &amp;#39;both&amp;#39;,
  size = 4,
  colour = &amp;#39;black&amp;#39;,
  fill = &amp;#39;#2D077A&amp;#39;
)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step71&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-15-simple-linear-regression-using-rstan_files/figure-html/step71-1.png&#34; alt=&#34;MCMC樣本的兩個模型參數的事後散點圖，及它們之間的邊緣分佈密度圖。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 7: MCMC樣本的兩個模型參數的事後散點圖，及它們之間的邊緣分佈密度圖。
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;從圖&lt;a href=&#34;#fig:step71&#34;&gt;7&lt;/a&gt;中可觀察到該貝葉斯線性模型獲得的事後模型參數樣本中，截距&lt;code&gt;a&lt;/code&gt;，和斜率&lt;code&gt;b&lt;/code&gt;之間呈極強的負相關關係。也就是說，截距是工資的起點（年齡爲0歲時），這個起點的理論值越低，斜率越大（歲年齡增加工資上升的速度越大）。&lt;/p&gt;
&lt;p&gt;根據上面分析的結果，下面的R代碼可以計算一名50歲的人被這家公司採用的時候，她/他的預期基本年收入的分佈（中獲得的MCMC樣本），和她/他的預期總年收的預測分佈（中獲得的MCMC樣本）。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;N_mcmc &amp;lt;- length(ms$lp__)
y50_base &amp;lt;- ms$a + ms$b*50
y50 &amp;lt;- rnorm(n = N_mcmc, mean = y50_base, sd = ms$sigma)
d_mcmc &amp;lt;- data.frame(a = ms$a, b = ms$b, sigma = ms$sigma, y50_base, y50)
head(d_mcmc)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            a        b    sigma y50_base       y50
## 1 -103.92295 22.04743 70.39829 998.4488  953.4024
## 2  -30.41656 20.16582 74.35210 977.8746  861.2176
## 3  -95.35165 21.32534 75.19714 970.9152 1076.7587
## 4  -10.88849 19.01547 68.02757 939.8852  877.2139
## 5 -177.04183 23.49984 81.40161 997.9499 1109.8183
## 6 -146.45260 22.73838 95.97706 990.4664 1063.0656&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# the following codes are also available from the author&amp;#39;s page:
# https://github.com/MatsuuraKentaro/RStanBook/blob/master/chap04/fig4-8.R
# library(ggplot2)
source(&amp;#39;commonRstan.R&amp;#39;)

# load(&amp;#39;output/result-model4-5.RData&amp;#39;)
ms &amp;lt;- rstan::extract(fit)

X_new &amp;lt;- 23:60
N_X &amp;lt;- length(X_new)
N_mcmc &amp;lt;- length(ms$lp__)

set.seed(1234)
y_base_mcmc &amp;lt;- as.data.frame(matrix(nrow=N_mcmc, ncol=N_X))
y_mcmc &amp;lt;- as.data.frame(matrix(nrow=N_mcmc, ncol=N_X))
for (i in 1:N_X) {
  y_base_mcmc[,i] &amp;lt;- ms$a + ms$b * X_new[i]
  y_mcmc[,i] &amp;lt;- rnorm(n=N_mcmc, mean=y_base_mcmc[,i], sd=ms$sigma)
}

customize.ggplot.axis &amp;lt;- function(p) {
  p &amp;lt;- p + labs(x=&amp;#39;X&amp;#39;, y=&amp;#39;Y&amp;#39;)
  p &amp;lt;- p + scale_y_continuous(breaks=seq(from=200, to=1400, by=400))
  p &amp;lt;- p + coord_cartesian(xlim=c(22, 61), ylim=c(200, 1400))
  return(p)
}

d_est &amp;lt;- data.frame.quantile.mcmc(x=X_new, y_mcmc=y_base_mcmc)
p &amp;lt;- ggplot.5quantile(data=d_est)
p &amp;lt;- p + geom_point(data=Salary, aes(x=X, y=Y), shape=1, size=3)
p &amp;lt;- customize.ggplot.axis(p)
# ggsave(file=&amp;#39;output/fig4-8-left.png&amp;#39;, plot=p, dpi=300, w=4, h=3)
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step72&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-15-simple-linear-regression-using-rstan_files/figure-html/step72-1.png&#34; alt=&#34;MCMC樣本計算獲得的基本年收的貝葉斯可信區間。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 8: MCMC樣本計算獲得的基本年收的貝葉斯可信區間。
&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d_est &amp;lt;- data.frame.quantile.mcmc(x=X_new, y_mcmc=y_mcmc)
p &amp;lt;- ggplot.5quantile(data=d_est)
p &amp;lt;- p + geom_point(data=Salary, aes(x=X, y=Y), shape=1, size=3)
p &amp;lt;- customize.ggplot.axis(p)
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step73&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-15-simple-linear-regression-using-rstan_files/figure-html/step73-1.png&#34; alt=&#34;MCMC樣本計算獲得的預期總年收的貝葉斯預測區間。（顏色較深的是50%預測區間帶，黑線是事後樣本的中央值）&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 9: MCMC樣本計算獲得的預期總年收的貝葉斯預測區間。（顏色較深的是50%預測區間帶，黑線是事後樣本的中央值）
&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# ggsave(file=&amp;#39;output/fig4-8-right.png&amp;#39;, plot=p, dpi=300, w=4, h=3)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;練習題&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;練習題&lt;/h2&gt;
&lt;p&gt;用模擬數據來嘗試進行貝葉斯t檢驗&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)
N1 &amp;lt;- 30
N2 &amp;lt;- 20
Y1 &amp;lt;- rnorm(n=N1, mean=0, sd=5)
Y2 &amp;lt;- rnorm(n=N2, mean=1, sd=4)&lt;/code&gt;&lt;/pre&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;請繪製上面代碼生成的兩組數據的示意圖&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d1 &amp;lt;- data.frame(group=1, Y=Y1)
d2 &amp;lt;- data.frame(group=2, Y=Y2)
d &amp;lt;- rbind(d1, d2)
d$group &amp;lt;- as.factor(d$group)

p &amp;lt;- ggplot(data=d, aes(x=group, y=Y, group=group, col=group))
p &amp;lt;- p + geom_boxplot(outlier.size=0)
p &amp;lt;- p + geom_point(position=position_jitter(w=0.4, h=0), size=2)
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:exe11&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-15-simple-linear-regression-using-rstan_files/figure-html/exe11-1.png&#34; alt=&#34;隨機生成的兩組數據的散點圖和箱式圖。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 10: 隨機生成的兩組數據的散點圖和箱式圖。
&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#ggsave(file=&amp;#39;fig-ex1.png&amp;#39;, plot=p, dpi=300, w=4, h=3)&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;寫下相當於t檢驗的數學式，表示各組之間方差或者標準差如果相等時，均值比較的檢驗模型。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;hypotheses:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;observations in each group follow a normal distribution&lt;/li&gt;
&lt;li&gt;all observations are independent&lt;/li&gt;
&lt;li&gt;The two population variance/standard deviations are known (and can be considered equal)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{H}_0: \mu_2 - \mu_1 = 0 \\
\text{H}_1: \mu_2 - \mu_1 \neq 0 \\ 
\text{If H}_0 \text{ is true, then:} \\
Z=\frac{\bar{Y_2} - \bar{Y_1}}{\sqrt{(\sigma_2^2/n_2) + (\sigma_1^2/n_1)}} \\
\text{follows a standard normal distribution with zero mean} \\
\Rightarrow \text{ if two variances are considered the same}\\ 
Y_1[n] \sim N(\mu_1, \sigma) \;\; n = 1,2,\dots,N \\
Y_2[n] \sim N(\mu_2, \sigma) \;\; n = 1,2,\dots,N
\]&lt;/span&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;寫下上一步模型的Stan代碼，並嘗試在R裏運行&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Stan代碼如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data {
  int N1;
  int N2;
  real Y1[N1];
  real Y2[N2];
}

parameters {
  real mu1;
  real mu2;
  real&amp;lt;lower=0&amp;gt; sigma;
}

model {
  for (n in 1:N1)
    Y1[n] ~ normal(mu1, sigma);
  for (n in 1:N2)
    Y2[n] ~ normal(mu2, sigma);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;R代碼如下：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rstan)
data &amp;lt;- list(N1=N1, N2=N2, Y1=Y1, Y2=Y2)
exe13 &amp;lt;- stan_model(file = &amp;quot;stanfiles/ex3.stan&amp;quot;)
fit &amp;lt;- sampling(exe13, data=data, seed=1234)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &amp;#39;ex3&amp;#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 1.6e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.02842 seconds (Warm-up)
## Chain 1:                0.023398 seconds (Sampling)
## Chain 1:                0.051818 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &amp;#39;ex3&amp;#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 5e-06 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.028177 seconds (Warm-up)
## Chain 2:                0.021224 seconds (Sampling)
## Chain 2:                0.049401 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &amp;#39;ex3&amp;#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 3e-06 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.026653 seconds (Warm-up)
## Chain 3:                0.022362 seconds (Sampling)
## Chain 3:                0.049015 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &amp;#39;ex3&amp;#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 5e-06 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.02661 seconds (Warm-up)
## Chain 4:                0.021456 seconds (Sampling)
## Chain 4:                0.048066 seconds (Total)
## Chain 4:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Stan model: ex3.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##         mean se_mean   sd    2.5%    25%    50%    75%  97.5% n_eff Rhat
## mu1    -0.24    0.01 0.84   -1.93  -0.80  -0.23   0.30   1.40  3805    1
## mu2     1.59    0.02 0.99   -0.32   0.92   1.61   2.25   3.50  3739    1
## sigma   4.46    0.01 0.46    3.66   4.14   4.42   4.76   5.45  3402    1
## lp__  -97.74    0.03 1.23 -100.89 -98.32 -97.42 -96.84 -96.33  1879    1
## 
## Samples were drawn using NUTS(diag_e) at Mon May 18 14:28:06 2020.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;從獲取到的事後參數的MCMC樣本計算 &lt;span class=&#34;math inline&#34;&gt;\(\text{Prob}[\mu_1 &amp;lt; \mu_2]\)&lt;/span&gt;：&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ms &amp;lt;- extract(fit)
prob &amp;lt;- mean(ms$mu1 &amp;lt; ms$mu2)  #=&amp;gt; 0.932
prob&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9235&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;所以可以認爲地一組均值，小於第二組均值的事後概率是93.2%&lt;/p&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;如果不能認爲兩組的方差相等的話，模型又該改成什麼樣子？&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y_1[n] \sim N(\mu_1, \sigma_1) \;\; n = 1,2,\dots,N \\
Y_2[n] \sim N(\mu_2, \sigma_2) \;\; n = 1,2,\dots,N
\]&lt;/span&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data {
  int N1;
  int N2;
  real Y1[N1];
  real Y2[N2];
}

parameters {
  real mu1;
  real mu2;
  real&amp;lt;lower=0&amp;gt; sigma1;
  real&amp;lt;lower=0&amp;gt; sigma2;
}

model {
  for (n in 1:N1)
    Y1[n] ~ normal(mu1, sigma1);
  for (n in 1:N2)
    Y2[n] ~ normal(mu2, sigma2);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;下面的代碼相當於實施Welch的t檢驗：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rstan)
data &amp;lt;- list(N1=N1, N2=N2, Y1=Y1, Y2=Y2)
exe15 &amp;lt;- stan_model(file = &amp;quot;stanfiles/ex5.stan&amp;quot;)

fit &amp;lt;- sampling(exe15, data=data, seed=1234)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &amp;#39;ex5&amp;#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 1e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.028096 seconds (Warm-up)
## Chain 1:                0.024392 seconds (Sampling)
## Chain 1:                0.052488 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &amp;#39;ex5&amp;#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 5e-06 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.029221 seconds (Warm-up)
## Chain 2:                0.02848 seconds (Sampling)
## Chain 2:                0.057701 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &amp;#39;ex5&amp;#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 5e-06 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.030482 seconds (Warm-up)
## Chain 3:                0.025653 seconds (Sampling)
## Chain 3:                0.056135 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &amp;#39;ex5&amp;#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 5e-06 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.027327 seconds (Warm-up)
## Chain 4:                0.024552 seconds (Sampling)
## Chain 4:                0.051879 seconds (Total)
## Chain 4:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Stan model: ex5.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##          mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat
## mu1     -0.22    0.01 0.94  -2.07  -0.85  -0.23   0.40   1.68  4084    1
## mu2      1.63    0.01 0.82   0.04   1.09   1.62   2.15   3.25  4068    1
## sigma1   5.12    0.01 0.71   3.94   4.62   5.05   5.53   6.74  3863    1
## sigma2   3.63    0.01 0.64   2.66   3.18   3.55   3.98   5.08  3002    1
## lp__   -95.33    0.03 1.44 -98.86 -96.05 -95.04 -94.28 -93.52  1916    1
## 
## Samples were drawn using NUTS(diag_e) at Mon May 18 14:28:40 2020.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ms &amp;lt;- rstan::extract(fit)
prob &amp;lt;- mean(ms$mu1 &amp;lt; ms$mu2)  #=&amp;gt; 0.93725
prob&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9345&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Rstan Wonderful R-(1)</title>
      <link>https://wangcc.me/post/rstan-wonderful-r/</link>
      <pubDate>Thu, 22 Nov 2018 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/rstan-wonderful-r/</guid>
      <description>
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;P16&lt;/p&gt;
&lt;p&gt;事後分布 &lt;span class=&#34;math inline&#34;&gt;\(p(\theta | Y)\)&lt;/span&gt;の値が最大になる点&lt;span class=&#34;math inline&#34;&gt;\(\theta^*\)&lt;/span&gt;を事後確率最大推定値 (maximum a posteriori estimate)と呼ぶ．略してMAP推定値 (MAP estimate)．&lt;/p&gt;
&lt;p&gt;我們把能夠將事後概率分布取極大值的參數點 &lt;span class=&#34;math inline&#34;&gt;\(\theta^*\)&lt;/span&gt; 稱爲事後概率的最大似然估計值 (maximum a posteriori estimate)，簡稱 MAP估計值 (MAP estimate)。&lt;/p&gt;
&lt;p&gt;P19&lt;/p&gt;
&lt;p&gt;統計建模的一般順序&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;確定分析目的&lt;/li&gt;
&lt;li&gt;確定數據分布&lt;/li&gt;
&lt;li&gt;想象數據產生本身的機制：思考數據與數據之間可能的關系&lt;/li&gt;
&lt;li&gt;寫下你所認爲的數據模型的數學表達式&lt;/li&gt;
&lt;li&gt;用 R 模擬(simulation)並確認前一步寫下的數學模型的性質，特點&lt;/li&gt;
&lt;li&gt;用 Stan 實際進行模型參數的推斷&lt;/li&gt;
&lt;li&gt;獲得推斷結果，解釋其事後概率分布的意義，繪制易於理解的模型示意圖&lt;/li&gt;
&lt;li&gt;繪制成功之後的模型示意圖和最先使用的模型之間進行比對，重新查缺補漏&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;P23&lt;/p&gt;
&lt;p&gt;ただいたずらにモデルを複雑化させるのは解釈のしにくさを招く．&lt;/p&gt;
&lt;p&gt;P30&lt;/p&gt;
&lt;p&gt;最初にmodel ブロックの尤度の部分（と事前分布の部分）を書く．その尤度の部分に登場した変数のうち，データの変数をdataブロックに，残りの変数をparametersブロックに書いていく．&lt;/p&gt;
&lt;p&gt;Stan的基本文法構成&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data {
 數據描述
}

parameters {
 想要進行MCMC事後樣本採集的參數描述
}

model {
 p(Y|theta) 似然的描述
 先驗概率分布 p(theta) 的描述
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;把下面的模型&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
Y[n] &amp;amp; \sim \text{Normal}(\mu, 1) \;\; n = 1, \dots, N \\
\mu  &amp;amp; \sim \text{Normal}(0, 100)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;翻譯成爲 Stan 模型語言是：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data {
  int N;
  real Y[N];
}

parameters {
  real mu;
}

model {
  for (n in 1:N) {
    Y[n] ~ normal(mu, 1);
  }
  mu ~ normal(0, 100);
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中我們按照實際模型書寫的順序 model -&amp;gt; data -&amp;gt; parameter 來逐個解釋：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;model&lt;/code&gt; 模塊中 &lt;code&gt;for (n in 1:N)&lt;/code&gt; 開始的循環部分（三行）對應數學模型的 $Y[n] (, 1) n = 1, , N $　部分。&lt;/li&gt;
&lt;li&gt;Stan 語言中，每一行描述的結尾需要用分號 &lt;code&gt;;&lt;/code&gt; 來結束。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mu ~ normal(0,100)&lt;/code&gt; 則對應數學模型中寫的先驗概率 &lt;span class=&#34;math inline&#34;&gt;\(\mu \sim \text{Normal}(0, 100)\)&lt;/span&gt; 部分。這裏給均值的先驗概率分佈是一個方差很大的無信息先驗概率分佈 (noninformative prior)。事實上在 Stan 軟件語言中，如果不特別指出先驗概率分佈，系統會默認給參數以無信息的先驗概率分佈，這樣即使沒有這一行，模型也是可以跑的。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;data&lt;/code&gt; 模塊中寫明的是 &lt;code&gt;model&lt;/code&gt; 模塊中描述的模型將要使用的數據。它包括宣示數據的個數（樣本量 &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;），以及數據本身。其中 &lt;code&gt;int N&lt;/code&gt; 意爲樣本量的數量是整數個 (integer)，&lt;code&gt;real Y[N]&lt;/code&gt; 則宣示實數有 N 個作爲數據。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;parameter&lt;/code&gt; 模塊是告訴軟件需要採樣且關注的未知參數 (parameter) 是 &lt;code&gt;mu&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;在 Stan 語言中，還可以和其他語言一樣爲模型加註解釋的文字，只需要在想要做註釋的文字最開始的部分增加 &lt;code&gt;//&lt;/code&gt;，如果註釋的文字超過一行，那麼在註釋的模塊前後加上 &lt;code&gt;/*&lt;/code&gt; 和 &lt;code&gt;*/&lt;/code&gt; 即可。&lt;/li&gt;
&lt;li&gt;另外，目前爲止主流的貝葉斯模型軟件中使用精確度 (precision) ，也就是方差的倒數來描述正態分佈 &lt;code&gt;normal(mean, 1/variance)&lt;/code&gt; ，但是在Stan的語法中使用的是 &lt;code&gt;normal(mean, sd)&lt;/code&gt;，也就是用標準差來描述正態分佈。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;寫Stan（或者說寫大多數的代碼）時，請遵守以下的原則：&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;適當縮進，以便於閱讀；&lt;/li&gt;
&lt;li&gt;表示數據的部分用大寫字母，表示參數的部分，用小寫字母；&lt;/li&gt;
&lt;li&gt;每個部分之間至少使用一個空行加以區分；&lt;/li&gt;
&lt;li&gt;請不要用&lt;code&gt;camelCase&lt;/code&gt;這樣的方式（單詞之間用大寫隔開），請在單詞之間用下劃線 &lt;code&gt;camel_case&lt;/code&gt; 的標記方法；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;~&lt;/code&gt;或者&lt;code&gt;=&lt;/code&gt;前後用一個字符大小的空格來隔開。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;最低限度的話，也請依照1,2兩個標準來書寫你的Stan代碼。不爲他人，也爲自己將來再讀代碼時能快速理解其涵義。往Stan的官方論壇投稿時，也必須遵守它們在手冊裏提供的 “Stan Program Style Guide” 代碼書寫規則，也是對其他寫，讀代碼的人的尊重。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Summer Project Schedule</title>
      <link>https://wangcc.me/post/summer-project-schedule/</link>
      <pubDate>Sun, 24 Jun 2018 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/summer-project-schedule/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Data analysis finish by 2018-07-&lt;del&gt;24&lt;/del&gt;31&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Paper structure confirm by 2018-08-01&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Paper draft complete by 2018-08-16&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;2018-06-24
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Read and try to repeat Rll&amp;rsquo;s method in R and familarize the dataset ASAP&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Two papers applying Repeated Measures LCA&lt;/del&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;2018-06-25
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Meeting with supervisor and Susanna&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Confirm the cutoff of carborhydrate consumption&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Talk with Rll ask about the methodology and dataset&lt;/del&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;2018-06-26
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Send the summarised memo of meeting to Supervisor and etc.&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Read the first part fundamentals of LCA.&lt;/del&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;2018-06-27
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;a href=&#34;https://www.londonr.org/&#34;&gt;London R in UCL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Germany lost their game against South Korea, UNBELIEVEABLE&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;2018-06-28
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Read the book collins2010latent - Latent Class and Latent Transition Analysis: With Applications in the Social, Behavioral, and Health Sciences (Done until 4.2)&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Learn how to do LCA in R&lt;/del&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;2018-06-29
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Read the book collins2010latent - Latent Class and Latent Transition Analysis: With Applications in the Social, Behavioral, and Health Sciences (Done until 4.3)&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Data management for NDNS 8 years data (70%)&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Learn how to do LCA in R&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Start to analysis the data according to the discussion on 25th(30%)&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Day1 data analysis results summary&lt;/del&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;2018-07-01
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Relax and do nothing&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Buy some drink to enjoy the night with classmates(HB)&lt;/del&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;2018-07-02/03
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Send some preliminary results to co-authors&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;&lt;a href=&#34;http://www.the-afc.com/competitions/fifa-world-cup/latest/news/japan-fa-president-proud-of-blue-samurai&#34;&gt;Japan lost the game to Belgium, but they are the glory of Asia&amp;ndash;heartbreaking&lt;/a&gt;&lt;/del&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;2018-07-04
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;&amp;quot;consider separating weekdays from weekends if we are not averaging the four days?&amp;ldquo;&lt;/del&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;2018-07-05
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Test and confirm the availability of LCA in SAS&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Learn how to do LCA in SAS with NDNS data&lt;/del&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;2018-07-06
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Learn how to do LCA with random effects in SAS&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Find whether there is any possibility of conducting the same method in R or STATA (no there is no way)&lt;/del&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;2018-07-07~09
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;&amp;quot;Maybe we should try with the threshold at 25% only as per the existing guidelines (although those are per meal)?&amp;ldquo;&lt;/del&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;2018-07-10
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; ~~Meet with tutor;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Start writing about the methodology;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Try to start writing about the introduction;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;2018-07-11
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Try to summarise the meeting memo yesterday;&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Re-analyse the data with new cut-off values (25, 50, 75);&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Re-analyse the data with new cut-off values (50);&lt;/del&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;2018-07-12~22
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Use latent class growth analysis;&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Use multilevel latent class analysis;&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Think about the mathmatical theory behind the mixed LCA, write to PROC LCA group if necessary;&lt;/del&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;2018-07-23~25
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Learn about the survey package in R&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Finish writing about the methodology;&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Write some introduction;&lt;/del&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;2018-07-26
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Let&amp;rsquo;s finish analysis of the classes and health outcomes.&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Read about the carbo-fibre ratio references.&lt;/del&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;2018-08-15
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;PM review&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Finish most of the discussion outlines and 2 pages of them.&lt;/del&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;2018-08-31
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Finish revising the report according to comments from LP and SAM;&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Read RT&amp;rsquo;s report and send the comments;&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Confirm the deadline for funding applications;&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Prepare the abstract for conferences (UK and JP);&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Start preparing the paper for submit (MLCA part alone);&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Think about the schedules and plans after leaving London;&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Finish the post of Scotland trip.&lt;/del&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>對數似然比 Log-likelihood ratio</title>
      <link>https://wangcc.me/post/log-likelihood-ratio/</link>
      <pubDate>Sun, 05 Nov 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/log-likelihood-ratio/</guid>
      <description>
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;對數似然比-log-likelihood-ratio&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;對數似然比 Log-likelihood ratio&lt;/h3&gt;
&lt;p&gt;對數似然比的想法來自於將對數似然方程圖形的 &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; 軸重新調節 (rescale) 使之最大值爲零。這可以通過計算該分佈方程的&lt;strong&gt;對數似然比 (log-likelihood ratio)&lt;/strong&gt; 來獲得：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[llr(\theta)=\ell(\theta|data)-\ell(\hat{\theta}|data)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;由於 &lt;span class=&#34;math inline&#34;&gt;\(\ell(\theta)\)&lt;/span&gt; 的最大值在 &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt; 時， 所以，&lt;span class=&#34;math inline&#34;&gt;\(llr(\theta)\)&lt;/span&gt; 就是個當 &lt;span class=&#34;math inline&#34;&gt;\(\theta=\hat{\theta}\)&lt;/span&gt; 時取最大值，且最大值爲零的方程。很容易理解我們叫這個方程爲對數似然比，因爲這個方程就是將似然比 &lt;span class=&#34;math inline&#34;&gt;\(LR(\theta)=\frac{L(\theta)}{L(\hat{\theta})}\)&lt;/span&gt; 取對數而已。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://winterwang.github.io/post/likelihood/&#34;&gt;之前&lt;/a&gt;我們也確證了，不包含我們感興趣的參數的方程部分可以忽略掉。還是用上一節 10人中4人患病的例子：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[L(\pi|X=4)=\binom{10}{4}\pi^4(1-\pi)^{10-4}\\
\Rightarrow \ell(\pi)=log[\pi^4(1-\pi)^{10-4}]\\
\Rightarrow llr(\pi)=\ell(\pi)-\ell(\hat{\pi})=log\frac{\pi^4(1-\pi)^{10-4}}{0.4^4(1-0.4)^{10-4}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其實由上也可以看出 &lt;span class=&#34;math inline&#34;&gt;\(llr(\theta)\)&lt;/span&gt; 只是將對應的似然方程的 &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; 軸重新調節了一下而已。形狀是沒有改變的：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow=c(1,2))
x &amp;lt;- seq(0,1,by=0.001)
y &amp;lt;- (x^4)*((1-x)^6)/(0.4^4*0.6^6)
z &amp;lt;- log((x^4)*((1-x)^6))-log(0.4^4*0.6^6)
plot(x, y, type = &amp;quot;l&amp;quot;, ylim = c(0,1.1),yaxt=&amp;quot;n&amp;quot;,
     frame.plot = FALSE, ylab = &amp;quot;LR(\U03C0)&amp;quot;, xlab = &amp;quot;\U03C0&amp;quot;)
axis(2, at=seq(0,1, 0.2), las=2)
title(main = &amp;quot;Binomial likelihood ratio&amp;quot;)
abline(h=1.0, lty=2)
segments(x0=0.4, y0=0, x1=0.4, y1=1, lty = 2)
plot(x, z, type = &amp;quot;l&amp;quot;, ylim = c(-10, 1), yaxt=&amp;quot;n&amp;quot;, frame.plot = FALSE,
     ylab = &amp;quot;llr(\U03C0)&amp;quot;, xlab = &amp;quot;\U03C0&amp;quot; )
axis(2, at=seq(-10, 0, 2), las=2)
title(main = &amp;quot;Binomial log-likelihood ratio&amp;quot;)
abline(h=0, lty=2)
segments(x0=0.4, y0=-10, x1=0.4, y1=0, lty = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/post/2017-11-05-log-likelihood-ratio_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;正態分佈數據的最大似然和對數似然比&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;正態分佈數據的最大似然和對數似然比&lt;/h4&gt;
&lt;p&gt;假設單個樣本 &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; 是來自一組服從正態分佈數據的觀察值：&lt;span class=&#34;math inline&#34;&gt;\(Y\sim N(\mu, \tau^2)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;那麼有：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(y|\mu) &amp;amp;= \frac{1}{\sqrt{2\pi\tau^2}}e^{(-\frac{1}{2}(\frac{y-\mu}{\tau})^2)} \\
\Rightarrow L(\mu|y) &amp;amp;=\frac{1}{\sqrt{2\pi\tau^2}}e^{(-\frac{1}{2}(\frac{y-\mu}{\tau})^2)} \\
\Rightarrow \ell(\mu)&amp;amp;=log(\frac{1}{\sqrt{2\pi\tau^2}})-\frac{1}{2}(\frac{y-\mu}{\tau})^2\\
omitting&amp;amp;\;terms\;not\;in\;\mu \\
&amp;amp;= -\frac{1}{2}(\frac{y-\mu}{\tau})^2 \\
\Rightarrow \ell^\prime(\mu) &amp;amp;= 2\cdot[-\frac{1}{2}(\frac{y-\mu}{\tau})\cdot\frac{-1}{\tau}] \\
&amp;amp;=\frac{y-\mu}{\tau^2} \\
let \; \ell^\prime(\mu) &amp;amp;= 0 \\
\Rightarrow \frac{y-\mu}{\tau^2} &amp;amp;= 0 \Rightarrow \hat{\mu} = y\\
\because \ell^{\prime\prime}(\mu) &amp;amp;=  \frac{-1}{\tau^2} &amp;lt; 0 \\
\therefore \hat{\mu} &amp;amp;= y \Rightarrow \ell(\hat{\mu}=y)_{max}=0 \\
llr(\mu)&amp;amp;=\ell(\mu)-\ell(\hat{\mu})=\ell(\mu)\\
&amp;amp;=-\frac{1}{2}(\frac{y-\mu}{\tau})^2
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;n-個獨立正態分佈樣本的對數似然比&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 個獨立正態分佈樣本的對數似然比&lt;/h3&gt;
&lt;p&gt;假設一組觀察值來自正態分佈 &lt;span class=&#34;math inline&#34;&gt;\(X_1,\cdots,X_n\stackrel{i.i.d}{\sim}N(\mu,\sigma^2)\)&lt;/span&gt;，先假設 &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; 已知。將觀察數據 &lt;span class=&#34;math inline&#34;&gt;\(x_1,\cdots, x_n\)&lt;/span&gt; 標記爲 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}\)&lt;/span&gt;。 那麼：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
L(\mu|\underline{x}) &amp;amp;=\prod_{i=1}^nf(x_i|\mu)\\
\Rightarrow \ell(\mu|\underline{x}) &amp;amp;=\sum_{i=1}^nlogf(x_i|\mu)\\
&amp;amp;=\sum_{i=1}^n[-\frac{1}{2}(\frac{x_i-\mu}{\sigma})^2]\\
&amp;amp;=-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2\\
&amp;amp;=-\frac{1}{2\sigma^2}[\sum_{i=1}^n(x_i-\bar{x})^2+\sum_{i=1}^n(\bar{x}-\mu)^2]\\
omitting&amp;amp;\;terms\;not\;in\;\mu \\
&amp;amp;=-\frac{1}{2\sigma^2}\sum_{i=1}^n(\bar{x}-\mu)^2\\
&amp;amp;=-\frac{n}{2\sigma^2}(\bar{x}-\mu)^2 \\
&amp;amp;=-\frac{1}{2}(\frac{\bar{x}-\mu}{\sigma/\sqrt{n}})^2\\
\because \ell(\hat{\mu}) &amp;amp;= 0 \\
\therefore llr(\mu) &amp;amp;= \ell(\mu)-\ell(\hat{\mu}) = \ell(\mu)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;n-個獨立正態分佈樣本的對數似然比的分佈&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 個獨立正態分佈樣本的對數似然比的分佈&lt;/h3&gt;
&lt;p&gt;假設我們用 &lt;span class=&#34;math inline&#34;&gt;\(\mu_0\)&lt;/span&gt; 表示總體均數這一參數的值。要注意的是，每當樣本被重新取樣，似然，對數似然方程，對數似然比都隨着觀察值而變 (即有自己的分佈)。&lt;/p&gt;
&lt;p&gt;考慮一個服從正態分佈的單樣本 &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;: &lt;span class=&#34;math inline&#34;&gt;\(Y\sim N(\mu_0,\tau^2)\)&lt;/span&gt;。那麼它的對數似然比：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[llr(\mu_0|Y)=\ell(\mu_0)-\ell(\hat{\mu})=-\frac{1}{2}(\frac{Y-\mu_0}{\tau})^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;根據&lt;a href=&#34;https://winterwang.github.io/post/chi-square-distribution/&#34;&gt;卡方分佈&lt;/a&gt;的定義：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\because \frac{Y-\mu_0}{\tau}\sim N(0,1)\\
\Rightarrow (\frac{Y-\mu_0}{\tau})^2 \sim \mathcal{X}_1^2\\
\therefore -2llr(\mu_0|Y) \sim \mathcal{X}_1^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;所以，如果有一組服從正態分佈的觀察值：&lt;span class=&#34;math inline&#34;&gt;\(X_1,\cdots,X_n\stackrel{i.i.d}{\sim}N(\mu_0,\sigma^2)\)&lt;/span&gt;，且 &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; 已知的話：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[-2llr(\mu_0|\bar{X})\sim \mathcal{X}_1^2\]&lt;/span&gt;&lt;/p&gt;
根據&lt;a href=&#34;https://winterwang.github.io/post/central-limit-theory/&#34;&gt;中心極限定理&lt;/a&gt;，可以將上面的結論一般化：

&lt;div class=&#34;theorem&#34;&gt;
&lt;span id=&#34;thm:unnamed-chunk-2&#34; class=&#34;theorem&#34;&gt;&lt;strong&gt;Theorem 1  &lt;/strong&gt;&lt;/span&gt;如果 &lt;span class=&#34;math inline&#34;&gt;\(X_1,\cdots,X_n\stackrel{i.i.d}{\sim}f(x|\theta)\)&lt;/span&gt;。 那麼當重複多次從參數爲 &lt;span class=&#34;math inline&#34;&gt;\(\theta_0\)&lt;/span&gt; 的總體中取樣時，那麼統計量 &lt;span class=&#34;math inline&#34;&gt;\(-2llr(\theta_0)\)&lt;/span&gt; 會漸進於自由度爲 &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; 的卡方分佈： &lt;span class=&#34;math display&#34;&gt;\[-2llr(\theta_0)=-2\{\ell(\theta_0)-\ell(\hat{\theta})\}\xrightarrow[n\rightarrow\infty]{}\;\sim \mathcal{X}_1^2\]&lt;/span&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;似然比信賴區間&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;似然比信賴區間&lt;/h3&gt;
&lt;p&gt;如果樣本量 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 足夠大 (通常應該大於 &lt;span class=&#34;math inline&#34;&gt;\(30\)&lt;/span&gt;)，根據上面的定理：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[-2llr(\theta_0)=-2\{\ell(\theta_0)-\ell(\hat{\theta})\}\sim \mathcal{X}_1^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;所以：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Prob(-2llr(\theta_0)\leqslant \mathcal{X}_{1,0.95}^2=3.84) = 0.95\\
\Rightarrow Prob(llr(\theta_0)\geqslant-3.84/2=-1.92) = 0.95\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;故似然比的 &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; 信賴區間就是能夠滿足 &lt;span class=&#34;math inline&#34;&gt;\(llr(\theta)=-1.92\)&lt;/span&gt; 的兩個 &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; 值。&lt;/p&gt;
&lt;div id=&#34;以二項分佈數據爲例&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;以二項分佈數據爲例&lt;/h4&gt;
&lt;p&gt;繼續用本文開頭的例子：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[llr(\pi)=\ell(\pi)-\ell(\hat{\pi})=log\frac{\pi^4(1-\pi)^{10-4}}{0.4^4(1-0.4)^{10-4}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;如果令 &lt;span class=&#34;math inline&#34;&gt;\(llr(\pi)=-1.92\)&lt;/span&gt; 在代數上可能較難獲得答案。然而從圖形上，如果我們在 &lt;span class=&#34;math inline&#34;&gt;\(y=-1.92\)&lt;/span&gt; 畫一條橫線，和該似然比方程曲線相交的兩個點就是我們想要求的信賴區間的上限和下限：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- seq(0,1,by=0.001)
z &amp;lt;- log((x^4)*((1-x)^6))-log(0.4^4*0.6^6)
plot(x, z, type = &amp;quot;l&amp;quot;, ylim = c(-10, 1), yaxt=&amp;quot;n&amp;quot;, frame.plot = FALSE,
     ylab = &amp;quot;llr(\U03C0)&amp;quot;, xlab = &amp;quot;\U03C0&amp;quot; )
axis(2, at=seq(-10, 0, 2), las=2)
abline(h=0, lty=2)
abline(h=-1.92, lty=2)
segments(x0=0.15, y0=-12, x1=0.15, y1=-1.92, lty = 2)
segments(x0=0.7, y0=-12, x1=0.7, y1=-1.92, lty = 2)
axis(1, at=c(0.15,0.7))
text(0.9, -1, &amp;quot;-1.92&amp;quot;)
arrows(0.8, -1.92, 0.8, 0, lty = 1, length = 0.08)
arrows( 0.8, 0, 0.8, -1.92, lty = 1, length = 0.08)
title(main = &amp;quot;Log-likelihood ratio for binomial example, \n with 95% likelihood confidence interval shown&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/post/2017-11-05-log-likelihood-ratio_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;從上圖中可以讀出，&lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; 對數似然比信賴區間就是 &lt;span class=&#34;math inline&#34;&gt;\((0.15, 0.7)\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;以正態分佈數據爲例&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;以正態分佈數據爲例&lt;/h4&gt;
&lt;p&gt;本文前半部分證明過，
&lt;span class=&#34;math inline&#34;&gt;\(X_1,\cdots,X_n\stackrel{i.i.d}{\sim}N(\mu,\sigma^2)\)&lt;/span&gt;，先假設 &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; 已知。將觀察數據 &lt;span class=&#34;math inline&#34;&gt;\(x_1,\cdots, x_n\)&lt;/span&gt; 標記爲 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}\)&lt;/span&gt;。 那麼：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[llr(\mu|\underline{x}) = \ell(\mu|\underline{x})-\ell(\hat{\mu}) = \ell(\mu|\underline{x}) \\
=-\frac{1}{2}(\frac{\bar{x}-\mu}{\sigma/\sqrt{n}})^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;很顯然，這是一個關於 &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; 的二次方程，且最大值在 MLE &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mu}=\bar{x}\)&lt;/span&gt; 時取值 &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;。所以可以通過對數似然比法求出均值的 &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; 信賴區間公式：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[-2\times[-\frac{1}{2}(\frac{\bar{x}-\mu}{\sigma/\sqrt{n}})^2]=3.84\\
\Rightarrow L=\bar{x}-\sqrt{3.84}\frac{\sigma}{\sqrt{n}} \\
U=\bar{x}+\sqrt{3.84}\frac{\sigma}{\sqrt{n}} \\
note: \;\sqrt{3.84}=1.96\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;注意到這和我們&lt;a href=&#34;https://winterwang.github.io/post/frequentist-statistical-inference02/&#34;&gt;之前&lt;/a&gt;求的正態分佈均值的信賴區間公式完全一致。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Exercise&lt;/h3&gt;
&lt;div id=&#34;q1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Q1&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;假設十個對象中有三人死亡，用二項分佈模型來模擬這個例子，求這個例子中參數 &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; 的似然方程和圖形 (likelihood) ?&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;解&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;解&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\begin{aligned}  L(\pi|3) &amp;amp;= \binom{10}{3}\pi^3(1-\pi)^{10-3} \\  omitting\;&amp;amp;terms\;not\;in\;\mu \\  \Rightarrow \ell(\pi|3) &amp;amp;= log[\pi^3(1-\pi)^7] \\  &amp;amp;= 3log\pi+7log(1-\pi)\\  \Rightarrow \ell^\prime(\pi|3)&amp;amp;= \frac{3}{\pi}-\frac{7}{1-\pi} \\  let \; \ell^\prime&amp;amp; =0\\  &amp;amp;\frac{3}{\pi}-\frac{7}{1-\pi} = 0 \\  &amp;amp;\frac{3-10\pi}{\pi(1-\pi)} = 0 \\  \Rightarrow MLE &amp;amp;= \hat\pi = 0.3 \end{aligned}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/post/2017-11-05-log-likelihood-ratio_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;計算似然比，並作圖，注意方程圖形未變，&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; 軸的變化；取對數似然比，並作圖&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;LR &amp;lt;- L/max(L) ; head(LR)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0000000000 0.0004191759 0.0031233631 0.0098110584 0.0216286076
## [6] 0.0392577320&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(pi, LR, type = &amp;quot;l&amp;quot;, ylim = c(0, 1),yaxt=&amp;quot;n&amp;quot;, col=&amp;quot;darkblue&amp;quot;,
     frame.plot = FALSE, ylab = &amp;quot;&amp;quot;, xlab = &amp;quot;\U03C0&amp;quot;)
grid(NA, 5, lwd = 1)
axis(2, at=seq(0,1,0.2), las=2)
title(main = &amp;quot;Binomial likelihood ratio function\n 3 out of 10 subjects&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/post/2017-11-05-log-likelihood-ratio_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logLR &amp;lt;- log(L/max(L))
plot(pi, logLR, type = &amp;quot;l&amp;quot;, ylim = c(-4, 0),yaxt=&amp;quot;n&amp;quot;, col=&amp;quot;darkblue&amp;quot;,
     frame.plot = FALSE, ylab = &amp;quot;&amp;quot;, xlab = &amp;quot;\U03C0&amp;quot;)
grid(NA, 5, lwd = 1)
axis(2, at=seq(-4,0,1), las=2)
title(main = &amp;quot;Binomial log-likelihood ratio function\n 3 out of 10 subjects&amp;quot;)
abline(h=-1.92, lty=1, col=&amp;quot;red&amp;quot;)
axis(4, at=-1.92, las=0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/post/2017-11-05-log-likelihood-ratio_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;q2&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Q2&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;與上面用同樣的模型，但是觀察人數變爲 &lt;span class=&#34;math inline&#34;&gt;\(100\)&lt;/span&gt; 人 患病人數爲 &lt;span class=&#34;math inline&#34;&gt;\(30\)&lt;/span&gt; 人，試作對數似然比方程之圖形，與上圖對比：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/post/2017-11-05-log-likelihood-ratio_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;p&gt;可以看出，兩組數據的 MLE 都是一致的， &lt;span class=&#34;math inline&#34;&gt;\(\hat\pi=0.3\)&lt;/span&gt;，但是對數似然比方程圖形在 樣本量爲 &lt;span class=&#34;math inline&#34;&gt;\(n=100\)&lt;/span&gt; 時比 &lt;span class=&#34;math inline&#34;&gt;\(n=10\)&lt;/span&gt; 時窄很多，由此產生的似然比信賴區間也就窄很多（精確很多）。所以對數似然比方程的曲率（二階導數），反映了觀察獲得數據提供的對總體參數 &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; 推斷過程中的信息量。而且當樣本量較大時，對數似然比方程也更加接近左右對稱的二次方程曲線。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;q3&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Q3&lt;/h4&gt;
&lt;p&gt;在一個實施了160人年的追蹤調查中，觀察到8個死亡案例。使用泊松分佈模型，繪製對數似然比方程圖形，從圖形上目視推測極大似然比的 &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; 信賴區間。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;解-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;解&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\begin{aligned}  d = 8, \;p &amp;amp;= 160\; person\cdot year \\  \Rightarrow D\sim Poi(\mu &amp;amp;=\lambda p) \\  L(\lambda|data) &amp;amp;= Prob(D=d=8) \\  &amp;amp;= e^{-\mu}\frac{\mu^d}{d!} \\  &amp;amp;= e^{-\lambda p}\frac{\lambda^d p^d}{d!} \\  omitting&amp;amp;\;terms\;not\;in\;\lambda \\  &amp;amp;= e^{-\lambda p}\lambda^d \\ \Rightarrow \ell(\lambda|data)&amp;amp;= log(e^{-\lambda p}\lambda^d) \\  &amp;amp;= d\cdot log(\lambda)-\lambda p \\  &amp;amp; = 8\times log(\lambda) - 160\times\lambda \end{aligned}\)&lt;/span&gt;&lt;/p&gt;
&lt;img src=&#34;https://wangcc.me/post/2017-11-05-log-likelihood-ratio_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;480&#34; /&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
lambda
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
LogLR
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.010
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-6.4755033
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.011
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-5.8730219
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.012
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-5.3369308
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.013
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-4.8565892
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.014
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-4.4237254
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.015
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-4.0317824
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.016
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-3.6754743
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.017
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-3.3504773
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.018
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-3.0532100
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.019
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.7806722
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.020
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.5303259
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.021
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.3000045
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;&#34;&gt;
0.022
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;&#34;&gt;
-2.0878444
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;&#34;&gt;
0.023
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;&#34;&gt;
-1.8922303
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.024
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.7117534
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.025
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.5451774
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.026
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.3914117
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.027
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.2494891
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.028
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.1185480
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.029
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.9978174
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.030
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.8866050
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.031
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.7842864
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.032
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.6902968
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.033
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.6041236
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.034
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.5252998
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.035
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.4533996
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.036
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.3880325
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.037
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.3288407
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.038
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.2754948
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.039
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.2276909
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.040
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.1851484
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.041
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.1476075
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.042
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.1148271
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.043
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0865831
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.044
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0626670
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.045
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0428841
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.046
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0270529
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.047
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0150032
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.048
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0065760
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.049
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0016217
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.050
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000000
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.051
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0015790
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.052
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0062343
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.053
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0138487
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.054
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0243117
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.055
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0375186
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.056
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0533705
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.057
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0717739
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.058
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0926400
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.059
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.1158845
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.060
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.1414275
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.061
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.1691931
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.062
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.1991090
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.063
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.2311062
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.064
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.2651194
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.065
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.3010859
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.066
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.3389461
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.067
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.3786431
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.068
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.4201224
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.069
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.4633320
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.070
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.5082221
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.071
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.5547450
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.072
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.6028551
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.073
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.6525085
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.074
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.7036633
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.075
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.7562791
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.076
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.8103173
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.077
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.8657407
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.078
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.9225134
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.079
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.9806012
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.080
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.0399710
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.081
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.1005908
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.082
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.1624301
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.083
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.2254592
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.084
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.2896497
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.085
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.3549740
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.086
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.4214057
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.087
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.4889191
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.088
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.5574895
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.089
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.6270931
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.090
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.6977067
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.091
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.7693080
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.092
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.8418754
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;&#34;&gt;
0.093
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;&#34;&gt;
-1.9153881
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;&#34;&gt;
0.094
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;&#34;&gt;
-1.9898258
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.095
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.0651689
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.096
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.1413985
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.097
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.2184962
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.098
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.2964442
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.099
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.3752252
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.100
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.4548226
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;所以從列表數據結合圖形， 可以找到信賴區間的下限在 0.022~0.023 之間， 上限在 0.093～0.094 之間。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>偉大的中心極限定理</title>
      <link>https://wangcc.me/post/central-limit-theory/</link>
      <pubDate>Thu, 19 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/central-limit-theory/</guid>
      <description>
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;最近明顯可以感覺到課程的步驟開始加速。看我的課表：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/img/IMG_0522.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;手機畫面太小了。早上都是9點半開始，下午基本都是到5點。週一更慘，到7點。週二-週五中午都被統計中心的講座佔據。簡直是非人的生活。&lt;/p&gt;
&lt;p&gt;這周概率論基礎結束。中心極限定理講完以後我們正式進入了 Inference 統計推斷的課程。我們花了一天時間講什麼是樣本估計 (Estimation)，什麼是參數精確度 (Precision)，什麼是自由度 (degree of freedom)，怎樣進行不偏的估計 (unbiased inference)。然後還有似然方程 (likelihood function)。&lt;/p&gt;
&lt;p&gt;今天的更新還是簡單的把概率論掃尾一下。感受一下中心極限定理的偉大。&lt;/p&gt;
&lt;div id=&#34;協方差-covariance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;協方差 Covariance&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://winterwang.github.io/post/probability2-4/&#34;&gt;之前我們定義過&lt;/a&gt;，兩個獨立連續隨機變量 &lt;span class=&#34;math inline&#34;&gt;\(X,Y\)&lt;/span&gt; 之和的方差 Variance ：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Var(X+Y)=Var(X)+Var(Y)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;然而如果他們並不相互獨立的話：&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
Var(X+Y) &amp;amp;= E[((X+Y)-E(X+Y))^2] \\
         &amp;amp;= E[(X+Y)-(E(X)+E(Y))^2] \\
         &amp;amp;= E[(X-E(X)) - (Y-E(Y))^2] \\
         &amp;amp;= E[(X-E(X))^2+(Y-E(Y))^2 \\
         &amp;amp; \;\;\; +2(X-E(X))(Y-E(Y))] \\
         &amp;amp;= Var(X)+Var(Y)+2E[(X-E(X))(Y-E(Y))]
\end{aligned}\]&lt;/span&gt;
&lt;p&gt;可以發現在兩者和的方差公式展開之後多了一部分 &lt;span class=&#34;math inline&#34;&gt;\(E[(X-E(X))(Y-E(Y))]\)&lt;/span&gt;。 這個多出來的一部分就說明了二者 &lt;span class=&#34;math inline&#34;&gt;\((X, Y)\)&lt;/span&gt; 之間的關係。它被定義爲協方差 (Covariance):
&lt;span class=&#34;math display&#34;&gt;\[Cov(X,Y) = E[(X-E(X))(Y-E(Y))]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;所以：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Var(X+Y)=Var(X)+Var(Y)+2Cov(X,Y)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    要記住，協方差只能用於評價&lt;!-- raw HTML omitted --&gt;(X,Y)&lt;!-- raw HTML omitted --&gt;之間的線性關係 (Linear Association)。
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;以下是協方差 (Covariance) 的一些特殊性質：&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Cov(X,X)=Var(X)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Cov(X,Y)=Cov(Y,X)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Cov(aX,bY)=ab\:Cov(X,Y)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Cov(aR+bS,cX+dY)=ac\:Cov(R,X)+ad\:Cov(R,Y)\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+bc\:Cov(S,X)+bd\:Cov(S,Y)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Cov(aX+bY,cX+dY)=ac\:Var(X)+ad\:Var(Y)\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+(ad+bc)Cov(X,Y)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Cov(X+Y,X-Y)=Var(X)-Var(Y)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;If &lt;span class=&#34;math inline&#34;&gt;\(X, Y\)&lt;/span&gt; are independent. &lt;span class=&#34;math inline&#34;&gt;\(Cov(X,Y)=0\)&lt;/span&gt; &lt;span class=&#34;diff_alert&#34;&gt;But not vise-versa !&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;相關-correlation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;相關 Correlation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;協方差雖然&lt;span class=&#34;math inline&#34;&gt;\(Cov(X,Y)\)&lt;/span&gt; 的大小很大程度上會被他們各自的單位和波動大小左右。&lt;/li&gt;
&lt;li&gt;我們將協方差標準化(除以各自的標準差 s.d.) (standardization) 之後，就可以得到相關係數 Corr (&lt;span class=&#34;math inline&#34;&gt;\(-1\sim1\)&lt;/span&gt;):
&lt;span class=&#34;math display&#34;&gt;\[Corr(X,Y)=\frac{Cov(X,Y)}{SD(X)SD(Y)}=\frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;中心極限定理-the-central-limit-theory&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;中心極限定理 the Central Limit Theory&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;diff_add&#34;&gt;&lt;strong&gt;如果從人羣中多次選出樣本量爲 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 的樣本，並計算樣本均值, &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}_n\)&lt;/span&gt;。那麼這個樣本均值 &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}_n\)&lt;/span&gt; 的分佈，會隨着樣本量增加 &lt;span class=&#34;math inline&#34;&gt;\(n\rightarrow\infty\)&lt;/span&gt;，而接近正態分佈。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;偉大的中心極限定理告訴我們：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;diff_alert&#34;&gt;&lt;strong&gt;當樣本量足夠大時，樣本均值 &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}_n\)&lt;/span&gt; 的分佈爲正態分佈，這個特性與樣本來自的人羣的分佈 &lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt; 無關。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;再說一遍：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;如果對象是獨立同分佈 i.i.d (identically and independently distributed)。那麼它的總體期望和方差分別是: &lt;span class=&#34;math inline&#34;&gt;\(E(X)=\mu;\;Var(X)=\sigma^2\)&lt;/span&gt;。
根據中心極限定理，可以得到：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;當樣本量增加，樣本均值的分佈服從正態分佈：
&lt;span class=&#34;math display&#34;&gt;\[\bar{X}_n\sim N(\mu, \frac{\sigma^2}{n})\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;也可以寫作，當樣本量增加：
&lt;span class=&#34;math display&#34;&gt;\[\sum_{i=1}^nX_i \sim N(n\mu,n\sigma^2)\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;有了這個定理，我們可以拋開樣本空間(&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;)的分佈，也不用假定它服從正態分佈。&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;diff_alert&#34;&gt;但是樣本的均值，卻總是服從正態分佈的。&lt;/span&gt;簡直是太完美了！！！！！！&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>你買的彩票中獎概率到底有多少？</title>
      <link>https://wangcc.me/post/probability3/</link>
      <pubDate>Wed, 11 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/probability3/</guid>
      <description>
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;二項分佈的概念-binomial-distribution&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;二項分佈的概念 Binomial distribution&lt;/h3&gt;
&lt;p&gt;二項分佈在醫學研究中至關重要，一組二項分佈的數據，指的通常是 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 次相互獨立的&lt;a href=&#34;https://winterwang.github.io/post/probability2-4/&#34;&gt;成功率爲 &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; 的伯努利實驗&lt;/a&gt; (&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; independent Bernoulli trials) 中成功的次數。&lt;/p&gt;
&lt;p&gt;當 &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 服從二項分佈，記爲 &lt;span class=&#34;math inline&#34;&gt;\(X \sim binomial(n, \pi)\)&lt;/span&gt; 或&lt;span class=&#34;math inline&#34;&gt;\(X \sim bin(n, \pi)\)&lt;/span&gt;。它的(第 &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; 次實驗的)概率被定義爲：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
P(X=x) &amp;amp;= ^nC_x\pi^x(1-\pi)^{n-x} \\
       &amp;amp;= \binom{n}{x}\pi^x(1-\pi)^{n-x} \\
       &amp;amp; for\;\; x = 0,1,2,\dots,n
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;二項分佈的期望和方差&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;二項分佈的期望和方差&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;期望 &lt;span class=&#34;math inline&#34;&gt;\(E(X)\)&lt;/span&gt;
&lt;ul&gt;
&lt;li&gt;若 &lt;span class=&#34;math inline&#34;&gt;\(X \sim bin(n,\pi)\)&lt;/span&gt;，那麼 &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 就是這一系列獨立伯努利實驗中成功的次數。&lt;/li&gt;
&lt;li&gt;用 &lt;span class=&#34;math inline&#34;&gt;\(X_i, i =1,\dots, n\)&lt;/span&gt; 標記每個相互獨立的伯努利實驗。&lt;/li&gt;
&lt;li&gt;那麼我們可以知道 &lt;span class=&#34;math inline&#34;&gt;\(X=\sum_{i=1}^nX_i\)&lt;/span&gt;。
&lt;span class=&#34;math display&#34;&gt;\[\begin{align} E(X) &amp;amp;= E(\sum_{i=1}^nX_i)\\
                   &amp;amp;= E(X_1+X_2+\cdots+X_n) \\
                   &amp;amp;= E(X_1)+E(X_2)+\cdots+E(X_n)\\
                   &amp;amp;= \sum_{i=1}^nE(X_i)\\
                   &amp;amp;= \sum_{i=1}^n\pi \\
                   &amp;amp;= n\pi
\end{align}\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;方差 &lt;span class=&#34;math inline&#34;&gt;\(Var(X)\)&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
Var(X) &amp;amp;= Var(\sum_{i=1}^nX_i) \\
      &amp;amp;= Var(X_i+X_2+\cdots+X_n) \\
      &amp;amp;= Var(X_i)+Var(X_2)+\cdots+Var(X_n) \\
      &amp;amp;= \sum_{i=1}^nVar(X_i) \\
      &amp;amp;= n\pi(1-\pi) \\
\end{align}\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;超幾何分佈-hypergeometric-distribution&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;超幾何分佈 hypergeometric distribution&lt;/h3&gt;
&lt;p&gt;假設我們從總人數爲 &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; 的人羣中，採集一個樣本 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;。假如已知在總體人羣中(&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;)有 &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; 人患有某種疾病。請問採集的樣本 &lt;span class=&#34;math inline&#34;&gt;\(X=n\)&lt;/span&gt; 中患有這種疾病的人，服從怎樣的分佈？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;從人羣(&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;)中取出樣本(&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;)，有 &lt;span class=&#34;math inline&#34;&gt;\(^NC_n\)&lt;/span&gt; 種方法。&lt;/li&gt;
&lt;li&gt;從患病人羣(&lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt;)中取出患有該病的人(&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;)有 &lt;span class=&#34;math inline&#34;&gt;\(^MC_x\)&lt;/span&gt; 種方法。&lt;/li&gt;
&lt;li&gt;樣本中不患病的人(&lt;span class=&#34;math inline&#34;&gt;\(n-x\)&lt;/span&gt;)被採樣的方法有 &lt;span class=&#34;math inline&#34;&gt;\(^{N-M}C_{n-x}\)&lt;/span&gt; 種。&lt;/li&gt;
&lt;li&gt;採集一次 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 人作爲樣本的概率都一樣。因此：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(X=x)=\frac{\binom{M}{x}\binom{N-M}{n-x}}{\binom{N}{n}}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;樂透中獎概率問題&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;樂透中獎概率問題：&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;從數字 &lt;span class=&#34;math inline&#34;&gt;\(1\sim59\)&lt;/span&gt; 中選取 &lt;span class=&#34;math inline&#34;&gt;\(6\)&lt;/span&gt; 個任意號碼&lt;/li&gt;
&lt;li&gt;開獎時從 &lt;span class=&#34;math inline&#34;&gt;\(59\)&lt;/span&gt; 個號碼球中隨機抽取 &lt;span class=&#34;math inline&#34;&gt;\(6\)&lt;/span&gt; 個&lt;/li&gt;
&lt;li&gt;如果六個號碼全部猜中(不分順序)，你可以成爲百萬富翁。請問一次猜中全部 &lt;span class=&#34;math inline&#34;&gt;\(6\)&lt;/span&gt; 個號碼的概率是多少？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;從 &lt;span class=&#34;math inline&#34;&gt;\(59\)&lt;/span&gt; 個號碼中隨機取出任意 &lt;span class=&#34;math inline&#34;&gt;\(6\)&lt;/span&gt; 個號碼的方法有 &lt;span class=&#34;math inline&#34;&gt;\(^{59}C_6\)&lt;/span&gt; 種。
&lt;span class=&#34;math display&#34;&gt;\[^{59}C_6=\frac{59!}{6!(59-6)!}=45,057,474\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;每次選取六個號碼做爲一組的可能性相同，所以，你買了一組樂透號碼，能中獎的概率就是 &lt;span class=&#34;math inline&#34;&gt;\(1/45,057,474 = 0.00000002219\)&lt;/span&gt;。你還會再去買彩票麼？&lt;/p&gt;
&lt;div id=&#34;如果我只想中其中的-3-個號碼概率有多大&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;如果我只想中其中的 &lt;span class=&#34;math inline&#34;&gt;\(3\)&lt;/span&gt; 個號碼，概率有多大？&lt;/h4&gt;
&lt;p&gt;用超幾何分佈的概率公式：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
P(X=3) &amp;amp;= \frac{^6C_3\times ^{53}C_3}{^{59}C_6} \\
       &amp;amp;= 0.010
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;你有 &lt;span class=&#34;math inline&#34;&gt;\(1\%\)&lt;/span&gt; 的可能中獎。換句話說，如果中三個以上的數字算中獎的話，你買的彩票中獎的概率低於 &lt;span class=&#34;math inline&#34;&gt;\(1\%\)&lt;/span&gt;。是不是覺得下次送錢給博彩公司的時候還不如跟我一起喝一杯咖啡划算？&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;泊松分佈-poisson-distribution&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;泊松分佈 Poisson Distribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;當一個事件，在一段時間 (&lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;) 中可能發生的次數是 &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; 。那麼我們可以認爲，經過時間 &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;，該時間發生的期望次數是 &lt;span class=&#34;math inline&#34;&gt;\(E(X)=\lambda T\)&lt;/span&gt;。&lt;/li&gt;
&lt;li&gt;利用微分思想，將這段時間 &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; 等分成 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 個時間段，當 &lt;span class=&#34;math inline&#34;&gt;\(n\rightarrow\infty\)&lt;/span&gt; 直到每個微小的時間段內最多發生一次該事件。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;那麼&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;每個微小的時間段，可以視爲是一個伯努利實驗（有事件發生或者沒有）&lt;/li&gt;
&lt;li&gt;那麼這整段時間 &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; 內發生的事件可以視爲是一個二項分佈實驗。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;令 &lt;span class=&#34;math inline&#34;&gt;\(X=\)&lt;/span&gt; 一次事件發生時所經過的所有時間段。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(X \sim Bin(n, \pi)\)&lt;/span&gt;，其中 &lt;span class=&#34;math inline&#34;&gt;\(n\rightarrow\infty\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 爲時間段。&lt;/li&gt;
&lt;li&gt;在每個分割好的時間段內，事件發生的概率都是：&lt;span class=&#34;math inline&#34;&gt;\(\pi=\frac{\lambda T}{n}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;期望 &lt;span class=&#34;math inline&#34;&gt;\(\mu=\lambda T \Rightarrow \pi=\mu/n\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;所以 &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 的概率方程就是：
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
P(X=x) &amp;amp;= \binom{n}{x}\pi^x(1-\pi)^{n-x} \\
     &amp;amp;= \binom{n}{x}(\frac{\mu}{n})^x(1-\frac{\mu}{n})^{n-x} \\
     &amp;amp;= \frac{n!}{x!(n-x)!}(\frac{\mu}{n})^x(1-\frac{\mu}{n})^{n-x} \\
     &amp;amp;=\frac{n!}{n^x(n-x)!}\frac{\mu^x}{x!}(1-\frac{\mu}{n})^{n-x}\\
當 n\rightarrow\infty   &amp;amp;\; x \ll n (x遠小於n) 時\\
\frac{n!}{n^x(n-x)!} &amp;amp;=\frac{n(n-1)\dots(n-x+1)}{n^x} \rightarrow 1\\
(1-\frac{\mu}{n})^{n-x} &amp;amp;\approx  (1-\frac{\mu}{n})^n \rightarrow e^{-\mu}\\
所以 我們可&amp;amp;以得到泊松分佈的概率公式：   \\
P(X=x) &amp;amp;\rightarrow \frac{\mu^x}{x!}e^{-\mu}
\end{align}\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;當數據服從泊松分佈時，記爲 &lt;span class=&#34;math inline&#34;&gt;\(X\sim Poisson(\mu=\lambda T)\;\; or\;\; X\sim Poi(\mu)\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;證明泊松分佈的參數特徵&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;證明泊松分佈的參數特徵：&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(E(X)=\mu\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
E(X)  &amp;amp;=  \sum_{x=0}^\infty xP(X=x) \\
      &amp;amp;=  \sum_{x=0}^\infty x\frac{\mu^x}{x!}e^{-\mu} \\
      &amp;amp;= 0+ \sum_{x=1}^\infty x\frac{\mu^x}{x!}e^{-\mu} \\
      &amp;amp;=  \sum_{x=1}^\infty \frac{\mu^x}{(x-1)!}e^{-\mu} \\
      &amp;amp;=  \mu\sum_{x=1}^\infty \frac{\mu^{x-1}}{(x-1)!}e^{-\mu} \\
這個時候我們用i&amp;amp;=x-1 替換掉所有的 x \\
      &amp;amp;=  \mu\sum_{i=0}^\infty \frac{\mu^{i}}{i!}e^{-\mu} \\
注意到右半部分 &amp;amp;\sum_{i=0}^\infty \frac{\mu^{i}}{i!}e^{-\mu}=1 是一個\\泊松分佈的所有&amp;amp;概率和 \\
      &amp;amp;= \mu
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Var(x)=\mu\)&lt;/span&gt;
爲了找到 &lt;span class=&#34;math inline&#34;&gt;\(Var(X)\)&lt;/span&gt;，我們用公式 &lt;span class=&#34;math inline&#34;&gt;\(Var(X)=E(X^2)-E(X)^2\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;我們需要找到 &lt;span class=&#34;math inline&#34;&gt;\(E(X^2)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
E(X^2) &amp;amp;= \sum_{x=0}^\infty x^2\frac{\mu^x}{x!}e^{-\mu} \\
       &amp;amp;= \mu \sum_{x=1}^\infty x\frac{\mu^{x-1}}{(x-1)!}e^{-\mu} \\
這個時候我們用i&amp;amp;=x-1 替換掉所有的 x \\
       &amp;amp;= \mu \sum_{i=0}^\infty (i+1)\frac{\mu^{i}}{i!}e^{-\mu} \\
       &amp;amp;= \mu(\sum_{i=0}^\infty i\frac{\mu^i}{i!}e^{-\mu} + \sum_{i=0}^\infty \frac{\mu^i}{i!}e^{-\mu}) \\
       &amp;amp;= \mu(E(X)+1) \\
       &amp;amp;= \mu^2+\mu \\
因此，代入上面&amp;amp;提到的方差公式： \\
Var(X) &amp;amp;= E(X^2) - E(X)^2 \\
       &amp;amp;= \mu^2 + \mu -\mu^2 \\
       &amp;amp;= \mu
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>正態分佈</title>
      <link>https://wangcc.me/post/normal-distribution/</link>
      <pubDate>Wed, 11 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/normal-distribution/</guid>
      <description>
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;概率密度曲線-probability-density-function-pdf&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;概率密度曲線 probability density function， PDF&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;一個隨機連續型變量 &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 它的性質由一個對應的&lt;strong&gt;概率密度方程 (probability density function, PDF)&lt;/strong&gt; 決定。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;在給定的範圍區間內，如 &lt;span class=&#34;math inline&#34;&gt;\(a\sim b, (a &amp;lt; b)\)&lt;/span&gt;，它的概率滿足:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(a\leqslant X \leqslant b) = \int_a^bf(x)dx\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;這個相關的方程，在 &lt;span class=&#34;math inline&#34;&gt;\(a\sim b\)&lt;/span&gt; 區間內的積分，就是這個連續變量在這個區間內取值的概率。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R codes for drawing a standard normal distribution by using ggplot2
library(ggplot2)
p &amp;lt;- ggplot(data.frame(x=c(-3,3)), aes(x=x)) +
  stat_function(fun = dnorm)
p + annotate(&amp;quot;text&amp;quot;, x=2, y=0.3, parse=TRUE, label=&amp;quot;frac(1, sqrt(2*pi)) * e ^(-z^2/2)&amp;quot;) +
  theme(plot.subtitle = element_text(vjust = 1),
        plot.caption = element_text(vjust = 1),
        axis.text.x = element_text(size = 12),
        axis.text.y = element_text(size = 12),
        plot.title = element_text(size = 10, face = &amp;quot;bold&amp;quot;, hjust = 0.5),
        panel.background = element_rect(fill = &amp;quot;ivory&amp;quot;)) +
  labs(title = &amp;quot;Probability density functions \n for standard normal distribution&amp;quot;,
       x = NULL, y = NULL) +
  stat_function(fun = dnorm,
                xlim = c(-1.3,0.4),
                geom = &amp;quot;area&amp;quot;,fill=&amp;quot;#00688B&amp;quot;, alpha= 0.2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/post/2017-10-11-normal-distribution_files/figure-html/normal%20distribution%20graph-1.png&#34; width=&#34;528&#34; /&gt;&lt;/p&gt;
&lt;p&gt;注意：整個方程的曲線下面積等於 &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;：
&lt;span class=&#34;math display&#34;&gt;\[\int_{-\infty}^\infty f(x)dx=1\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;期望 &lt;span class=&#34;math inline&#34;&gt;\(E(X)=\int_{-\infty}^\infty xf(x)dx\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;方差 &lt;span class=&#34;math inline&#34;&gt;\(Var(X)=\int_{-\infty}^\infty (x-\mu)^2f(x)dx\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;正態分佈&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;正態分佈&lt;/h3&gt;
&lt;p&gt;如果一組數據服從正態分佈，我們通常用它的期望（或者叫平均值）&lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;，和它的方差 &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;，來描述這組數據。記爲：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X \sim N(\mu, \sigma^2)\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;它的概率密度方程可以表述爲：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[f(x)=\frac{1}{\sqrt{2\pi\sigma^2}}exp(-\frac{(x-\mu)^2}{2\sigma^2})\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(E(x) =\mu\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Var(x)=\sigma^2\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;標準正態分佈&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;標準正態分佈&lt;/h3&gt;
&lt;p&gt;標準正態分佈的期望（或者均值）爲 &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;，方差爲 &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;記爲：&lt;span class=&#34;math inline&#34;&gt;\(Z \sim N(0,1)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;它的概率密度方程表述爲：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{1}{\sqrt{2\pi}}exp(-\frac{z^2}{2})\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;它的累積分佈方程 (cumulative distribution function， CDF)，是將概率密度方程 (PDF) 積分以後獲得的方程。通常我們記爲 &lt;span class=&#34;math inline&#34;&gt;\(\Phi(z)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;再看一下標準正態分佈的概率密度方程曲線：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/post/2017-10-11-normal-distribution_files/figure-html/normal%20distribution%20graph2-1.png&#34; width=&#34;528&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;95% 的曲線下面積在標準差 standard deviation &lt;span class=&#34;math inline&#34;&gt;\(-1.96\sim1.96\)&lt;/span&gt; 之間的區域。&lt;/li&gt;
&lt;li&gt;而且，&lt;span class=&#34;math inline&#34;&gt;\(\phi(-x)=1-\phi(x)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;任何一個正態分佈都可以通過下面的公式，標準化成爲標準正態分佈：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Z=\frac{X-\mu}{\sigma}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>概率論2</title>
      <link>https://wangcc.me/post/probability2-4/</link>
      <pubDate>Tue, 10 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/probability2-4/</guid>
      <description>
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;bayes-理論的概念&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bayes 理論的概念&lt;/h3&gt;
&lt;p&gt;許多時候，我們需要將概率中的條件相互對調。
例如：
在已知該人羣中有20%的人有吸菸習慣(&lt;span class=&#34;math inline&#34;&gt;\(P(S)\)&lt;/span&gt;)，吸菸的人有9%的概率有哮喘(&lt;span class=&#34;math inline&#34;&gt;\(P(A|S)\)&lt;/span&gt;)，不吸菸的人有7%的概率有哮喘(&lt;span class=&#34;math inline&#34;&gt;\(P(A|\bar{S})\)&lt;/span&gt;)的前提下，有個人前來門診，發現是哮喘患者，那麼這個人有多大的概率是一個菸民？也就是要求 &lt;span class=&#34;math inline&#34;&gt;\(P(S|A)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;這裏先引入貝葉斯的概念：&lt;/p&gt;
&lt;p&gt;我們可以將 &lt;span class=&#34;math inline&#34;&gt;\(P(A\cap S)\)&lt;/span&gt; 寫成：
&lt;span class=&#34;math display&#34;&gt;\[P(A\cap S)=P(A|S)P(S)\\or\\
P(A\cap S)=P(S|A)P(A)\]&lt;/span&gt;
這兩個等式是完全等價的。我們將他們連起來：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(S|A)P(A)=P(A|S)P(S)\\
\Rightarrow P(S|A)=\frac{P(A|S)P(S)}{P(A)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;是不是看起來又像是寫了一堆&lt;strong&gt;廢話&lt;/strong&gt;？
沒錯，你看出來是一堆廢話的時候，證明你也同意這背後的簡單邏輯。&lt;/p&gt;
&lt;p&gt;再繼續，我們可以利用另外一個&lt;strong&gt;廢話&lt;/strong&gt;：&lt;span class=&#34;math inline&#34;&gt;\(\because S+\bar{S}=1\\ \therefore P(A)=P(A\cap S)+P(A\cap\bar{S})\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;用上面的公式替換掉 &lt;span class=&#34;math inline&#34;&gt;\(P(A\cap S)+P(A\cap\bar{S}） \\ \therefore P(A)=P(A|S)P(S)+P(A|\bar{S})P(\bar{S})\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;可以得到&lt;strong&gt;貝葉斯理論公式&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(S|A)=\frac{P(A|S)P(S)}{P(A|S)P(S)+P(A|\bar{S})P(\bar{S})}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;回到上面說到的哮喘人中有多少比例吸菸的問題。可以繼續使用概率樹來方便的計算：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/img/Selection_073.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
P(S|A) &amp;amp;= \frac{P(A|S)P(S)}{P(A|S)P(S)+P(A|\bar{S})P(\bar{S})} \\
        &amp;amp;= \frac{0.09\times0.2}{0.09\times0.2+0.07\times0.8} \\
        &amp;amp;= 0.24
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;所以我們的結論就是，在已知該人羣中有20%的人有吸菸習慣(&lt;span class=&#34;math inline&#34;&gt;\(P(S)\)&lt;/span&gt;)，吸菸的人有9%的概率有哮喘(&lt;span class=&#34;math inline&#34;&gt;\(P(A|S)\)&lt;/span&gt;)，不吸菸的人有7%的概率有哮喘(&lt;span class=&#34;math inline&#34;&gt;\(P(A|\bar{S})\)&lt;/span&gt;)的前提下，有個人前來門診，發現是哮喘患者，那麼這個人有24% 的概率是一個菸民(&lt;span class=&#34;math inline&#34;&gt;\(P(S|A)\)&lt;/span&gt;)。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;期望-expectation-或均值-or-mean-和-方差-variance&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;期望 Expectation (或均值 or mean) 和 方差 Variance&lt;/h3&gt;
&lt;p&gt;期望（或均值）是用來描述一組數據中心位置的指標（另一個是中位數 Median）。
對於離散型隨機變量 &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; (discrete random variables)，它的期望被定義爲：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[E(X)=\sum_x xP(X=x)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;所以就是將所有 &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 可能取到的值乘以相應的概率後求和。這個期望（或均值）常常用希臘字母 &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; 來標記。&lt;/p&gt;
&lt;p&gt;方差 Variance 是衡量一組數據變化幅度(dispersion/variability)的指標之一。 方差的定義是：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Var(X)=E((X-\mu)^2)\\其中，\mu=E(x)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;實際上我們更加常用的是它的另外一個公式：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Var(X)=E(X^2)-E(X)^2\]&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;證明-上面兩個方差公式相等&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;證明 上面兩個方差公式相等&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
Var(x)  &amp;amp;= E((X-\mu)^2) \\
        &amp;amp;= E(X^2-2X\mu+\mu^2)\\
        &amp;amp;= E(X^2) - 2\mu E(X) + \mu^2\\
        &amp;amp;= E(X^2) - 2\mu^2 + \mu^2 \\
        &amp;amp;= E(X^2) - \mu^2 \\
        &amp;amp;= E(X^2) - E(X)^2
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;方差的性質&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;方差的性質：&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Var(X+b)=Var(X)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Var(aX)=a^2Var(X)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Var(aX+b)=a^2Var(X)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;伯努利分佈-bernoulli-distribution&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;伯努利分佈 Bernoulli distribution&lt;/h3&gt;
&lt;p&gt;伯努利分佈，說的就是一個簡單的二分變量 (1, 0)，它取1時的概率如果是 &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt;。那麼我們可以計算這個分佈的期望值:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
E(X) &amp;amp;=\sum_x xP(X=x) \\
     &amp;amp;=1\times\pi + 0\times(1-\pi)\\
     &amp;amp;=\pi
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;由於 &lt;span class=&#34;math inline&#34;&gt;\(x=x^2\)&lt;/span&gt;，因爲 &lt;span class=&#34;math inline&#34;&gt;\(x=0,1\)&lt;/span&gt;, 所以 &lt;span class=&#34;math inline&#34;&gt;\(E[X^2]=E[X]\)&lt;/span&gt;，那麼方差爲：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
Var(X) &amp;amp;=E[X^2]-E[X]^2 \\
       &amp;amp;=E[X]-E[X]^2 \\
       &amp;amp;=\pi - \pi^2 \\
       &amp;amp;=\pi(1-\pi)
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;證明xy-爲互爲獨立的隨機離散變量時-a-exyexey-b-varxyvarxvary&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;證明，&lt;span class=&#34;math inline&#34;&gt;\(X,Y\)&lt;/span&gt; 爲互爲獨立的隨機離散變量時，&lt;br&gt;a) &lt;span class=&#34;math inline&#34;&gt;\(E(XY)=E(X)E(Y)\)&lt;/span&gt; ; &lt;br&gt;b) &lt;span class=&#34;math inline&#34;&gt;\(Var(X+Y)=Var(X)+Var(Y)\)&lt;/span&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;strong&gt;證明&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
E(XY) &amp;amp;= \sum_x\sum_y xyP(X=x, Y=y) \\
\because &amp;amp;\; X,Y are\;independent\;to\;each\;other \\
\therefore &amp;amp;= \sum_x\sum_y xyP(X=x)P(Y=y)\\
      &amp;amp;=\sum_x xP(X=x)\sum_y yP(Y=y)\\
      &amp;amp;=E(X)E(Y)
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;ol start=&#34;2&#34; style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;strong&gt;證明&lt;/strong&gt;
根據方差的定義：
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
Var(X+Y) &amp;amp;= E((X+Y)^2)-E(X+Y)^2 \\
    &amp;amp; \; Expand \\
    &amp;amp;=E(X^2+2XY+Y^2)-(E(X)+E(Y))^2\\
    &amp;amp;=E(X^2)+E(Y^2)+2E(XY)\\
    &amp;amp;\;\;\; - E(X)^2-E(Y)^2-2E(X)E(Y)\\
    &amp;amp;\; We\;just\;showed\; E(XY)=E(X)E(Y)\\
    &amp;amp;=E(X^2)-E(X)^2+E(Y^2)-E(Y)^2 \\
    &amp;amp;=Var(X)+Var(Y)
\end{align}\]&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Matrix Revisions</title>
      <link>https://wangcc.me/post/matrix-revision/</link>
      <pubDate>Sat, 30 Sep 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/matrix-revision/</guid>
      <description>
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;basic-definition-and-notations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Basic Definition and notations:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;An &lt;span class=&#34;math inline&#34;&gt;\(m\times n\)&lt;/span&gt; &lt;strong&gt;matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;&lt;/strong&gt; is a rectangular array of numbers with &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; rows and &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; columns.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;strong&gt;elements&lt;/strong&gt; of a matrix &lt;span class=&#34;math inline&#34;&gt;\(A_{m\times n}\)&lt;/span&gt; are &lt;span class=&#34;math inline&#34;&gt;\(a_{ij}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;strong&gt;order&lt;/strong&gt; of a matrix is the number of rows by the number of columns, i.e. &lt;span class=&#34;math inline&#34;&gt;\(m\times n\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A &lt;strong&gt;column vector&lt;/strong&gt; with &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; elements, &lt;span class=&#34;math inline&#34;&gt;\(y = \left( \begin{array}{c} y_1\\ y_2\\ \vdots\\ y_n \end{array} \right)\)&lt;/span&gt;, is a matrix with only one column i.e. an &lt;span class=&#34;math inline&#34;&gt;\(m\times 1\)&lt;/span&gt; matrix.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A &lt;strong&gt;row vector&lt;/strong&gt; with &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; elements, &lt;span class=&#34;math inline&#34;&gt;\(x=(x_1,x_2,x_3, \cdots, x_n)\)&lt;/span&gt;, is a matrix with only one row, i.e. an &lt;span class=&#34;math inline&#34;&gt;\(1\times n\)&lt;/span&gt; matrix.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Transposed matrix&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(A^T\)&lt;/span&gt; (or &lt;span class=&#34;math inline&#34;&gt;\(A&amp;#39;\)&lt;/span&gt;) arises from the matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; by interchanging the column vectors and the row vectors i.e. &lt;span class=&#34;math inline&#34;&gt;\(a_{ij}^T = a_{ji}\)&lt;/span&gt; (so a column vector is converted into a row vector and vise versa)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;A partitioned matrix&lt;/strong&gt; is a matrix written in terms of sub-matrices. &lt;span class=&#34;math inline&#34;&gt;\(A=\left( \begin{array}{c} A_{11} &amp;amp; A_{12}\\ A_{21} &amp;amp; A_{22}\\ \end{array} \right)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(A_{11},A_{12},A_{21},A_{22}\)&lt;/span&gt; are sub-matrices&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(A_{11}, A_{21}\)&lt;/span&gt; have the same number of columns, so do &lt;span class=&#34;math inline&#34;&gt;\(A_{12}, A_{22}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(A_{11}, A_{12}\)&lt;/span&gt; have the same number of rows, so do &lt;span class=&#34;math inline&#34;&gt;\(A_{21}, A_{22}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;partitioning is not restricted to dividing a matrix into just four sub-matrices&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;A square matrix&lt;/strong&gt; has exactly as many rows as it has columns i.e. the order of the matrix is &lt;span class=&#34;math inline&#34;&gt;\(n\times n\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;The main diagonal&lt;/strong&gt; (or leading diagnonal) of a square matrix &lt;span class=&#34;math inline&#34;&gt;\(A (n\times n)\)&lt;/span&gt; are the elements lying on the diagnoal &lt;strong&gt;from top left to bottom right.&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(a_{11},a_{22},a_{33},\cdots,a_{nn}\)&lt;/span&gt; i.e. all &lt;span class=&#34;math inline&#34;&gt;\(a_{ii}, i= 1,\cdots, n\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;The trace &lt;/strong&gt; of a square matrix is the sum of the diagonal elements &lt;span class=&#34;math inline&#34;&gt;\(tr(A)=a_{11}+a_{22}+\cdots+a_{nn}=\sum_{i=1}^na_{ii}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;special-matrices&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Special matrices&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;A symmetric matrix&lt;/strong&gt; is a square matrix for which the following is true for all the off diagonal elements. &lt;span class=&#34;math inline&#34;&gt;\(a_{ij}=a_{ji}\)&lt;/span&gt; i.e. &lt;span class=&#34;math inline&#34;&gt;\(A^T=A\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Diagonal matrix&lt;/strong&gt; is a square matrix having zero for all the non-diagonal elements i.e. &lt;span class=&#34;math inline&#34;&gt;\(A=\left( \begin{array}{c} a_{11} &amp;amp; \cdots &amp;amp; 0\\ \vdots &amp;amp; \ddots &amp;amp; \vdots\\ 0 &amp;amp; \cdots &amp;amp; a_{nn} \end{array} \right)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Zero matrix&lt;/strong&gt; (null matrix) is a matrix whose all elements are zero&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Identity matrix&lt;/strong&gt; (or unit matrix) is a diagonal matrix having all diagonal elements equal to 1 and off diagonal elements equal to zero. i.e. &lt;span class=&#34;math inline&#34;&gt;\(I=\left( \begin{array}{c} 1 &amp;amp; \cdots &amp;amp; 0\\ \vdots &amp;amp; \ddots &amp;amp; \vdots\\ 0 &amp;amp; \cdots &amp;amp; 1 \end{array} \right)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;“Summing vector”&lt;/strong&gt; is a vector whose every element is 1 i.e. &lt;span class=&#34;math inline&#34;&gt;\(1_{n}=(1\cdots1)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;“J matrix”&lt;/strong&gt; is a matrix (not necessarily square) whose every element is 1 i.e. &lt;span class=&#34;math inline&#34;&gt;\(J_{m\times n}=\left( \begin{array}{c} 1 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 1\\ 1 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 1 \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ 1 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 1 \end{array} \right)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;basic-operations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Basic Operations&lt;/h2&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;!-- raw HTML omitted --&gt;Addition (Substraction)&lt;!-- raw HTML omitted --&gt; can take place only when the matrices involved are of the same order. i.e.
Two matrices can be added (subtracted) only if they have the same numbers of rows and the same numbers of columns.
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(A+B=B+A\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((A+B)+C=A+(B+C)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(A+0=0+A=A\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(A+(-A)=0\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((A+B)^T=A^T+B^T\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Multiplication by scalar:&lt;/strong&gt;
- &lt;span class=&#34;math inline&#34;&gt;\(cA=Ac\)&lt;/span&gt;
- &lt;span class=&#34;math inline&#34;&gt;\(c(dA)=(cd)A\)&lt;/span&gt;
- &lt;span class=&#34;math inline&#34;&gt;\((c\pm d)A=cA\pm dA\)&lt;/span&gt;
- &lt;span class=&#34;math inline&#34;&gt;\(c(A\pm B)=cA \pm cB\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Multiplication of an &lt;span class=&#34;math inline&#34;&gt;\(2\times2\)&lt;/span&gt; matrix by a column vector which has 2 rows yields a column vector with &lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt; rows.&lt;/strong&gt;
&lt;span class=&#34;math display&#34;&gt;\[Ax=\left(
\begin{array}{c}
a_{11} &amp;amp; a_{12}\\
a_{21} &amp;amp; a_{22}\\
\end{array}
\right)\left(
\begin{array}{c}
x_{1}\\
x_{2}\\
\end{array}
\right)=\left(
\begin{array}{c}
a_{11}x_1+a_{12}x_2\\
a_{21}x_1+a_{22}x_2\\
\end{array}
\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;generally&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Generally:&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Multiplication of an &lt;span class=&#34;math inline&#34;&gt;\(m\times n\)&lt;/span&gt; matrix&lt;/strong&gt; by a &lt;strong&gt;column vector&lt;/strong&gt; which has &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; rows &lt;strong&gt;yields a column vector&lt;/strong&gt; with &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; rows.
&lt;span class=&#34;math display&#34;&gt;\[Ax=\left(
\begin{array}{c}
a_{11} &amp;amp; \cdots &amp;amp; a_{1n} \\
\vdots &amp;amp; \ddots &amp;amp; \vdots \\
a_{m1} &amp;amp; \cdots &amp;amp; a_{mn}
\end{array}
\right)\left(
\begin{array}{c}
x_{1}\\
x_{2}\\
\vdots \\
x_{n}
\end{array}
\right)=\left(
\begin{array}{c}
a_{11}x_{1}+a_{12}x_2+\cdots+a_{1n}x_n\\
\vdots \\
a_{m1}x_{1}+a_{m2}x_2+\cdots+a_{mn}x_n
\end{array}
\right)=y \\
i.e. y_i=\sum_{j=1}^na_{ij}x_j, \; i=1,\cdots, m\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://winterwang.github.io/post/2017-02-22/&#34;&gt;Multiplication of matrices&lt;/a&gt;:&lt;/strong&gt; The product &lt;span class=&#34;math inline&#34;&gt;\(AB=C\)&lt;/span&gt; is &lt;strong&gt;defined only when &lt;span class=&#34;math inline&#34;&gt;\(A_{m\times r}\)&lt;/span&gt; has exactly as many columns as &lt;span class=&#34;math inline&#34;&gt;\(B_{r\times n}\)&lt;/span&gt; has rows&lt;/strong&gt;. And the elements of &lt;span class=&#34;math inline&#34;&gt;\(C_{m\times n}\)&lt;/span&gt; are given as
&lt;span class=&#34;math display&#34;&gt;\[c_{ij}=\sum_{l=1}^na_{il}b_{lj}, \;\; i=1,\cdots,m \; and \; j=1,\cdots, n\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(AB \neq BA\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((AB)C=A(BC)=ABC\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(A(B+C)=AB+AC\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((B+C)A=BA+CA\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(IA=AI=A\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;further-definitions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Further definitions&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://winterwang.github.io/post/2017-03-15/&#34;&gt;&lt;strong&gt;The determinant&lt;/strong&gt;&lt;/a&gt; of a second order square matrix is &lt;span class=&#34;math inline&#34;&gt;\(det(A)=|A|=\begin{vmatrix} a_{11} &amp;amp; a_{12} \\ a_{21} &amp;amp; a_{22} \end{vmatrix}=a_{11}a_{22}-a_{12}a_{21}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://winterwang.github.io/post/2017-07-06/&#34;&gt;The inverse of a matrix&lt;/a&gt;&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(A^{-1}\)&lt;/span&gt; if it exists, is a matrix whose product with &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is the identity matrix i.e. &lt;span class=&#34;math inline&#34;&gt;\(AA^{-1}=A^{-1}A=I\)&lt;/span&gt;. (&lt;strong&gt;Note: both &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(A^{-1}\)&lt;/span&gt; have to be square&lt;/strong&gt;) For second order matrices:&lt;span class=&#34;math inline&#34;&gt;\(A^{-1}=\frac{1}{det(A)}\left( \begin{array}{c} a_{22} &amp;amp; -a_{12}\\ -a_{21} &amp;amp; a_{11}\\ \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://winterwang.github.io/post/2017-07-06/&#34;&gt;Singular or non-invertible matrix&lt;/a&gt;&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(det(A)=0\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Idempotent matrices(冪等矩陣)&lt;/strong&gt; are square and the following is true: &lt;span class=&#34;math inline&#34;&gt;\(AA=A^2=A\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://winterwang.github.io/post/2017-03-08/&#34;&gt;Orthogonal matrices&lt;/a&gt;&lt;/strong&gt; have the following property: &lt;span class=&#34;math inline&#34;&gt;\(AA^T=A^TA=I\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記 25</title>
      <link>https://wangcc.me/post/cramers-formula/</link>
      <pubDate>Wed, 09 Aug 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/cramers-formula/</guid>
      <description>
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;克萊姆法則-cramers-formula&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;克萊姆法則 Cramer’s Formula&lt;/h3&gt;
&lt;p&gt;當 &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 爲&lt;a href=&#34;https://winterwang.github.io/post/2017-07-06/&#34;&gt;正則矩陣&lt;/a&gt;（&lt;span class=&#34;math inline&#34;&gt;\(|X|\neq0\)&lt;/span&gt;）時 連立一次方程式：&lt;span class=&#34;math inline&#34;&gt;\(X\underline{a}=\underline{y}\)&lt;/span&gt; 的解可以寫作：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[a_j=\frac{|X_j|}{|X|} (j=1,2,\cdots, n)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中： &lt;span class=&#34;math inline&#34;&gt;\(|X_j|\)&lt;/span&gt; 爲矩陣 &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 的第 &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; 列替換爲 &lt;span class=&#34;math inline&#34;&gt;\(\underline{y}\)&lt;/span&gt; 以後的矩陣的行列式。&lt;/p&gt;
&lt;hr /&gt;
&lt;div id=&#34;練習-解下列連立一次方程式&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;練習 解下列連立一次方程式&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
\left\{
\begin{array}{ll}
a_1+2a_2+a_3  = 2\\
2a_1+a_2+a_3  = 3\\
a_1+a_2+2a_3  = 3
\end{array}
\right.
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;解&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;解&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X=\left(
\begin{array}{c}
1 &amp;amp; 2 &amp;amp; 1 \\
2 &amp;amp; 1 &amp;amp; 1 \\
1 &amp;amp; 1 &amp;amp; 2
\end{array}
\right), \underline{a}=\left(
\begin{array}{c}
a_1 \\
a_2 \\
a_3 \\
\end{array}
\right), \underline{y}=\left(
\begin{array}{c}
2 \\
3 \\
3 \\
\end{array}
\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中 &lt;span class=&#34;math inline&#34;&gt;\(|X|=-4\)&lt;/span&gt; &lt;a href=&#34;https://winterwang.github.io/post/2017-03-15/&#34;&gt;(三次行列式的計算)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 的第一列置換成 &lt;span class=&#34;math inline&#34;&gt;\(\underline{y}\)&lt;/span&gt; 則:
&lt;span class=&#34;math display&#34;&gt;\[|X_1|=\begin{vmatrix}
2 &amp;amp; 2 &amp;amp;  1\\
3 &amp;amp; 1 &amp;amp;  1\\
3 &amp;amp; 1 &amp;amp;  2\\
\end{vmatrix}=-4\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 的第二列置換成 &lt;span class=&#34;math inline&#34;&gt;\(\underline{y}\)&lt;/span&gt; 則:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[|X_2|=\begin{vmatrix}
1 &amp;amp; 2 &amp;amp;  1\\
2 &amp;amp; 3 &amp;amp;  1\\
1 &amp;amp; 3 &amp;amp;  2\\
\end{vmatrix}=0\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 的第三列置換成 &lt;span class=&#34;math inline&#34;&gt;\(\underline{y}\)&lt;/span&gt; 則:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[|X_3|=\begin{vmatrix}
1 &amp;amp; 2 &amp;amp;  2\\
2 &amp;amp; 1 &amp;amp;  3\\
1 &amp;amp; 1 &amp;amp;  3\\
\end{vmatrix}=-4\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\therefore a_1=\frac{|X_1|}{|X|}=1, \\
a_2=\frac{|X_2|}{|X|}=0, \\
a_3=\frac{|X_3|}{|X|}=1\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記 24</title>
      <link>https://wangcc.me/post/inverse-matrix-method/</link>
      <pubDate>Sun, 06 Aug 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/inverse-matrix-method/</guid>
      <description>
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;逆矩陣法解連立一次方程式&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;逆矩陣法解連立一次方程式&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 為&lt;a href=&#34;https://winterwang.github.io/post/2017-07-06/&#34;&gt;正則矩陣&lt;/a&gt;時(&lt;span class=&#34;math inline&#34;&gt;\(|X|\neq0\)&lt;/span&gt;)，給 &lt;span class=&#34;math inline&#34;&gt;\(X\underline{a}=\underline{y}\)&lt;/span&gt; 等式兩邊同時乘以 &lt;span class=&#34;math inline&#34;&gt;\(X^{-1}\)&lt;/span&gt;，可以得到 &lt;span class=&#34;math inline&#34;&gt;\(X^{-1}X\underline{a}=X^{-1}\underline{y}\rightarrow E\underline{a}=X^{-1}\underline{y}\)&lt;/span&gt;。由此方法可以得到 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=X^{-1}\underline{y}\)&lt;/span&gt;。&lt;/p&gt;
&lt;hr /&gt;
&lt;div id=&#34;練習-解下列連立一次方程式&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;練習 解下列連立一次方程式&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
\left\{
\begin{array}{ll}
a_1+2a_2+a_3  = 2\\
2a_1+a_2+a_3  = 3\\
a_1+a_2+2a_3  = 3
\end{array}
\right.
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;解&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;解&lt;/h4&gt;
&lt;p&gt;元連立方程式可以寫作&lt;span class=&#34;math inline&#34;&gt;\(X\underline{a}=\underline{y}\)&lt;/span&gt;，其中
&lt;span class=&#34;math display&#34;&gt;\[X=\left(
\begin{array}{c}
1 &amp;amp; 2 &amp;amp; 1 \\
2 &amp;amp; 1 &amp;amp; 1 \\
1 &amp;amp; 1 &amp;amp; 2
\end{array}
\right), \underline{a}=\left(
\begin{array}{c}
a_1 \\
a_2 \\
a_3 \\
\end{array}
\right), \underline{y}=\left(
\begin{array}{c}
2 \\
3 \\
3 \\
\end{array}
\right)\]&lt;/span&gt;
之前我們已經用&lt;a href=&#34;https://winterwang.github.io/post/2017-07-07/&#34;&gt;行的基本變形法&lt;/a&gt;和&lt;a href=&#34;https://winterwang.github.io/post/inverse-matrix/&#34;&gt;逆矩陣法&lt;/a&gt;分別計算過了 &lt;span class=&#34;math inline&#34;&gt;\(X^{-1}\)&lt;/span&gt; ：
&lt;span class=&#34;math display&#34;&gt;\[X^{-1}=\left(\begin{array}{c}
-1/4 &amp;amp; 3/4 &amp;amp; -1/4\\
3/4 &amp;amp; -1/4 &amp;amp; -1/4\\
-1/4 &amp;amp; -1/4 &amp;amp; -3/4\\
\end{array}\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\therefore\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
\underline{a} &amp;amp; =X^{-1}\underline{y} \\
&amp;amp; =\left(\begin{array}{c}
-1/4 &amp;amp; 3/4 &amp;amp; -1/4\\
3/4 &amp;amp; -1/4 &amp;amp; -1/4\\
-1/4 &amp;amp; -1/4 &amp;amp; 3/4\\
\end{array}\right)\left(
\begin{array}{c}
2 \\
3 \\
3 \\
\end{array}
\right)\\
&amp;amp;=\left(
\begin{array}{c}
-1/4\times2+3/4\times3-1/4\times3 \\
3/4\times1+(-1/4)\times3-1/4\times3 \\
-1/4\times2-1/4\times3+3/4\times3 \\
\end{array}
\right) \\
&amp;amp; = \left(
\begin{array}{c}
1 \\
0 \\
1 \\
\end{array}
\right)
\end{align} \]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記22</title>
      <link>https://wangcc.me/post/inverse-matrix/</link>
      <pubDate>Thu, 03 Aug 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/inverse-matrix/</guid>
      <description>&lt;p&gt;正方形矩陣 $A$ 的行列式滿足 $|A| \neq 0$ 時，逆矩陣可以表達爲(當 $|A|=0$ 時，正方形矩陣 $A$ 沒有逆矩陣)：
$$A^{-1}=\frac{1}{|A|}adj(A)=\frac{1}{|A|}(A_{ij})^t$$&lt;/p&gt;
&lt;p&gt;$$=\frac{1}{|A|}\lbrace(-1)^{i+j}D_{ij}\rbrace^t$$&lt;/p&gt;
&lt;p&gt;其中:&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$adj(A)$ 爲&lt;a href=&#34;https://winterwang.github.io/post/2017-03-15/&#34;&gt;餘因子矩陣&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;$A_{ij}$ 爲&lt;a href=&#34;https://winterwang.github.io/post/2017-03-15/&#34;&gt;餘因子&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;$D_{ij}$ &lt;a href=&#34;https://winterwang.github.io/post/2017-03-15/&#34;&gt;爲小行列式&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(1) 之前舉過的例子再拿來試試看：&lt;/p&gt;
&lt;p&gt;$$X=\left(
\begin{array}{c}
1 &amp;amp; 2 &amp;amp; 1 \newline
2 &amp;amp; 1 &amp;amp; 1 \newline
1 &amp;amp; 1 &amp;amp; 2
\end{array}
\right)=\left(\begin{array}{c}
x_{11} &amp;amp; x_{12} &amp;amp; x_{13} \newline
x_{21} &amp;amp; x_{22} &amp;amp; x_{23} \newline
x_{31} &amp;amp; x_{32} &amp;amp; x_{33}
\end{array}\right)$$
&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;p&gt;元素 $x_{ij}$ 的餘因子 $X_{ij}(i,j=1,2,3)$ 爲：&lt;/p&gt;
&lt;p&gt;$$X_{11}=(-1)^{1+1}\left|
\begin{array}{c}
1 &amp;amp; 1 \newline
1 &amp;amp; 2
\end{array}\right|=1$$&lt;/p&gt;
&lt;p&gt;$$X_{12}=(-1)^{1+2}\left|
\begin{array}{c}
2 &amp;amp; 1 \newline
1 &amp;amp; 2
\end{array}\right|=-3$$&lt;/p&gt;
&lt;p&gt;$$X_{13}=(-1)^{1+3}\left|
\begin{array}{c}
2 &amp;amp; 1 \newline
1 &amp;amp; 1
\end{array}\right|=1$$&lt;/p&gt;
&lt;p&gt;$$X_{21}=(-1)^{2+1}\left|
\begin{array}{c}
2 &amp;amp; 1 \newline
1 &amp;amp; 2
\end{array}\right|=-3$$&lt;/p&gt;
&lt;p&gt;$$X_{22}=(-1)^{2+2}\left|
\begin{array}{c}
1 &amp;amp; 1 \newline
1 &amp;amp; 2
\end{array}\right|=1$$&lt;/p&gt;
&lt;p&gt;$$X_{23}=(-1)^{2+3}\left|
\begin{array}{c}
1 &amp;amp; 2 \newline
1 &amp;amp; 1
\end{array}\right|=1$$&lt;/p&gt;
&lt;p&gt;$$X_{31}=(-1)^{3+1}\left|
\begin{array}{c}
2 &amp;amp; 1 \newline
1 &amp;amp; 1
\end{array}\right|=1$$&lt;/p&gt;
&lt;p&gt;$$X_{32}=(-1)^{3+2}\left|
\begin{array}{c}
1 &amp;amp; 1 \newline
2 &amp;amp; 1
\end{array}\right|=1$$&lt;/p&gt;
&lt;p&gt;$$X_{33}=(-1)^{3+3}\left|
\begin{array}{c}
1 &amp;amp; 2 \newline
2 &amp;amp; 1
\end{array}\right|=-3$$&lt;/p&gt;
&lt;p&gt;因此餘因子矩陣爲：$adj(X)=\left(
\begin{array}{c}
1 &amp;amp; -3 &amp;amp; 1 \newline
-3 &amp;amp; 1 &amp;amp; 1 \newline
1 &amp;amp; 1 &amp;amp; -3
\end{array}
\right)^t=\left(
\begin{array}{c}
1 &amp;amp; -3 &amp;amp; 1 \newline
-3 &amp;amp; 1 &amp;amp; 1 \newline
1 &amp;amp; 1 &amp;amp; -3
\end{array}
\right)$&lt;/p&gt;
&lt;p&gt;我們看見這個餘因子矩陣是一個對稱矩陣，這是由於原矩陣 $X$ 本身就是一個對稱矩陣。另外，行列式爲：&lt;/p&gt;
&lt;p&gt;$$\begin{align}|X|&amp;amp;=1\times X_{11}+2\times X_{12}+1\times X_{13}\newline&amp;amp;=1\times1+2\times(-3)+1\times1\newline&amp;amp;=-4\end{align}$$&lt;/p&gt;
&lt;p&gt;因此所求的逆矩陣爲：&lt;/p&gt;
&lt;p&gt;$$\begin{align}X^{-1}&amp;amp;=\frac{1}{|X|}adj(X)\newline
&amp;amp;=\frac{1}{-4}\left(
\begin{array}{c}
1 &amp;amp; -3 &amp;amp; 1 \newline
-3 &amp;amp; 1 &amp;amp; 1 \newline
1 &amp;amp; 1 &amp;amp; -3
\end{array}
\right)\newline
&amp;amp;=\left(
\begin{array}{c}
-\frac{1}{4} &amp;amp; \frac{3}{4} &amp;amp; -\frac{1}{4} \newline
\frac{3}{4} &amp;amp; -\frac{1}{4} &amp;amp; -\frac{1}{4} \newline
-\frac{1}{4} &amp;amp; -\frac{1}{4} &amp;amp; \frac{3}{4}
\end{array}
\right)\end{align}$$&lt;/p&gt;
&lt;p&gt;(2) 試求矩陣 $A=\left(
\begin{array}{c}
1 &amp;amp; 2 &amp;amp; 1 \newline
2 &amp;amp; 3 &amp;amp; 1 \newline
1 &amp;amp; 2 &amp;amp; 2
\end{array}
\right)=\left(
\begin{array}{c}
a_{11} &amp;amp; a_{12}  &amp;amp; a_{13} \newline
a_{21} &amp;amp; a_{22}  &amp;amp; a_{23} \newline
a_{31} &amp;amp; a_{32}  &amp;amp; a_{33}
\end{array}
\right)$ 的逆矩陣 $A^{-1}$:&lt;/p&gt;
&lt;p&gt;$$\begin{array}
=A_{11}=6-2=4, &amp;amp; A_{12}=-(4-1)=-3, &amp;amp; A_{13}=4-3=1 \newline
A_{21}=-(4-2)=-2, &amp;amp; A_{22}=2-1=1, &amp;amp; A_{23}=-(2-2)=0 \newline
A_{31}=2-3=-1, &amp;amp; A_{32}=-(1-2)=1, &amp;amp; A_{33}=3-4=-1
\end{array}$$&lt;/p&gt;
&lt;p&gt;$$adj(A)=\left(
\begin{array}{c}
4 &amp;amp; -3 &amp;amp; 1 \newline
-2 &amp;amp; 1 &amp;amp; 0 \newline
-1 &amp;amp; 1 &amp;amp; -1
\end{array}
\right)^t=\left(
\begin{array}{c}
4 &amp;amp; -2 &amp;amp; -1 \newline
-3 &amp;amp; 1 &amp;amp; 1 \newline
1 &amp;amp; 0 &amp;amp; -1
\end{array}
\right)$$&lt;/p&gt;
&lt;p&gt;$$\begin{align}
|A| &amp;amp;=1\times A_{11}+2\times A_{12}+1\times A_{13} \newline
&amp;amp;=1\times4+2\times(-3)+1\times1 \newline
&amp;amp;=4-6+1 \newline
&amp;amp;=-1
\end{align}$$&lt;/p&gt;
&lt;p&gt;$$
\therefore
\begin{align}
A^{-1} &amp;amp;= \frac{1}{(-1)}\left(
\begin{array}{c}
4 &amp;amp; -2 &amp;amp; -1 \newline
-3 &amp;amp; 1 &amp;amp; 1 \newline
1 &amp;amp; 0 &amp;amp; -1
\end{array}
\right) \newline
&amp;amp;=\left(
\begin{array}{c}
-4 &amp;amp; 2 &amp;amp; 1 \newline
3 &amp;amp; -1 &amp;amp; -1 \newline
-1 &amp;amp; 0 &amp;amp; 1
\end{array}
\right)
\end{align}$$&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>無條件 offer, CAS, 和宿舍抽籤結果</title>
      <link>https://wangcc.me/post/unconditional/</link>
      <pubDate>Sat, 29 Jul 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/unconditional/</guid>
      <description>&lt;p&gt;言而總之，總而言之，我的4月5月6月7月在無盡的等待中度過。期間投稿了一篇論文。和西山一起進行了磕磕絆絆的GWAS數據分析。&lt;/p&gt;
&lt;p&gt;本來以爲我的 offer 條件僅僅衹是把我原先名古屋大學的博士學位證書，中英文的原本郵寄給 LSHTM 負責確認就可以了。&lt;/p&gt;
&lt;p&gt;結果6月8日那天收到郵件催促我快點滿足 offer 條件：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/fig/meetingcondition.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;可以看到資金證明是我必須提供的條件。所以，我立刻開始著手資金的準備，存款全部移到一個賬戶中去，然後開了一個存款證明。結果就是這個新開的存款證明，後來拖了我一個多月的腿。差點害我以爲可能這次留學計劃就要泡湯了。我原本告訴 LSHTM 的簽證詢問小組（visa-enquiries）說，我的生活費由我的大學支付的工資來做擔保，然後大學還有資助我的一部分旅費和住宿費。因此我還要求我工作的大學給我速速給我開具了上述證明。結果後來被證明這些都不如一張自己賬戶上有錢的證明來得簡單。&lt;/p&gt;
&lt;p&gt;因爲英國留學簽證(Tier 4 student)對 sponsor (資金贊助者)極爲嚴格：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;For visa purposes, an Official Financial Sponsor is only one of the following: Her Majesty’s Government, your home government, the British Council or any international organisation, international company, university or an Independent School&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我原以爲我開的三個證明完全足夠了吧。結果過了一個月告訴我說：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Your documents didn’t meet the requirements because: &lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol&gt;
&lt;li&gt;The salary expectancy is not admissible &lt;!-- raw HTML omitted --&gt;&lt;/li&gt;
&lt;li&gt;The statement you have provided only shows the balance on a single day and we therefore recommend a bank letter to show funds held for 28 days. Please find attached an example bank letter. &lt;!-- raw HTML omitted --&gt;&lt;/li&gt;
&lt;li&gt;The bank statements did not include the bank name and logo.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;不知道爲什麽，未來的工資單證明不被接受，然後資金證明必須證明說我擁有足夠的資金并且保持了4周時間。而且還要求資金證明上面有銀行的logo。WTF!&lt;/p&gt;
&lt;p&gt;這些都好說。可是日本的銀行，&lt;strong&gt;沒有&lt;/strong&gt;這種類型的證明書（我也是第一次知道日本銀行不給開這樣子的證明）。所以許多人的解決辦法是讓銀行開一個月的流水賬單，要命的是這個證明不能開英文的，然後再去找翻譯公司翻譯流水。當然我也可以這麽辦。衹是，當我知道我的三個證明書都不能作爲有效的資金證明的時候，我離7月31日祗剩下不到2周時間了。在此奉勸后來者，一定要先準備好自己的資金證明書。最好能按照下面的樣本，讓銀行開具類似的證明書：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/fig/bankstatement.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;我在接到證明書不滿足條件的郵件的第二天，立刻去了銀行，接待我的銀行經理先是打報告給總部請示。毫無意外被擋回來。說如果是客人自己要求的樣式的證明書，無法給加銀行logo，也不能蓋章，衹能簽字。在我一個多小時的軟磨硬泡以後，經理鬆動了。竟然主動想辦法，她提議說，可以辦理bank statement，不過我看了他們給的bank statement樣本也是一個時間點的賬戶存款而已，無法滿足28天的資金維持證明。看我面有難色，日本人經理還是挺善解人意的，說，我可以把日期改成，從xx月xx日-xx月xx日（28天）的最低資金證明。這樣就能解決問題了。而且bank statement本身自帶銀行logo。謝天謝地，一項死板不能變通的日本人讓我從此刮目相看。解決了我的燃眉之急。也不必再去找翻譯公司翻譯賬戶流水了。有驚無險。第二天我拿到開好的證明，立刻掃描PDF郵件發給LSHTM，期待他們能馬上給開來 CAS (Confirmation of Acceptance of Studies)。等了一周，還是左等不來右等不來，距離7月31日還剩下不到10天了。終於無法忍耐等待的我，打電話去倫敦詢問我的情況。對方接電話的是個年輕女聲，優雅的倫敦音告訴我，不要着急，先無視學校的提醒滿足條件的郵件吧。我們會儘快看你的檔案。無奈我衹好作罷，挂了電話繼續等待。&lt;/p&gt;
&lt;p&gt;結果第二天晚上就收到了確認函，說你的CAS很快就能發給你了。oh yeah！半夜裏我就收到了發來的新鮮剛出爐的CAS號碼以及新的無條件錄取證明：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/fig/InkedCAS_LI.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/fig/uncon.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;我很早以前就在&lt;a href=&#34;https://www.visa4uk.fco.gov.uk/home/welcome&#34;&gt;visa4uk&lt;/a&gt;上註冊好了全部的信息，就等着學校發來 CAS 的文件了。於是我再花了半個小時把 CAS 上的內容填寫到簽證申請的網站上去。在申請的網站上，會中途跳出來讓你支付一年醫療保險的頁面（£150），付完保險費以後會收到自己的保險號碼。估計以後在英國如果需要看病的話報自己的保險號碼就OK了。於是乎我以迅雷不及掩耳之勢立刻預約了7月31日去大阪的簽證申請中心遞交簽證材料。&lt;/p&gt;
&lt;p&gt;等待去辦簽證的過程中，又收到好消息，宿舍抽籤中了。於是我就成了倫敦準市民之一拉。哈哈哈哈。今兒真高興阿，今兒真高興。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/gif/shuang.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;我抽中的是&lt;a href=&#34;http://halls.london.ac.uk/international-hall&#34;&gt;International Hall&lt;/a&gt;的單人間。仔細閱讀了條款後發現，每週2百鎊的房租確實有點小貴，但是呢，確是包了早餐晚餐和週末的四餐的。我想這將會大大減輕時間和金錢的成本。畢竟只有一年的留學時間。將就將就吧，每天都是炸魚和薯條估計吃一週就會讓人瘋了誒。。。先做好心理準備。對伙食不應有太高期待。&lt;/p&gt;
&lt;p&gt;萬事具備，&lt;del&gt;只差簽證了。&lt;/del&gt; 於是就到了預約機票的時候，查了半天各種中介的網站，結果都是什麼中轉三四次的，要不就是繞地球一大圈的，才能有價格比較便宜的。索性打電話去日本航空詢問有沒有給留學生準備的往返一年，時間靈活的機票。果然不問不知道，一問嚇一跳阿，電話接線員小哥樂呵呵:-)說，哎呀你這電話打的太是時候了，我們日本航空正好最近上線了歐洲航線的特價機票，而且專門針對你這樣要待三個月以上的客戶。一問價格，我的媽呀，出發行程已定，歸程未定的叫做半靈活機票 (semi-flexi)，日本航空的這個折扣價爲12萬日元。比全日空便宜了一半，比其他的可疑航空減少了飛行時間，還有什麼好說的，果斷就訂了。結果呢，準備付錢了小哥告訴我說，您現在先別付定金，我這裏已經幫你把機票預留好了，您等8月1日以後再上網站上打開訂單支付，因爲8月1日後的燃油稅機場時用費等雜費由於匯率等變化會再便宜一萬日元左右。&lt;strong&gt;W!T!F!&lt;/strong&gt; 感動得熱淚盈眶有沒有，簡直就想穿過電話線去擁抱這位小哥了。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/img/jal.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;這一週簡直了，從前幾個月的無盡等待到讓人懷疑人生，懷疑自己還能不能去英國，瞬間轉到材料全備齊，訂了飛機票，而且還額外中了一個獎學金（日本的財團）。快要樂不攏嘴了。。。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>留學筆記</title>
      <link>https://wangcc.me/post/2017-03-16/</link>
      <pubDate>Thu, 16 Mar 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/2017-03-16/</guid>
      <description>&lt;h2 id=&#34;尋找並確定合適自己的大學合適的課程&#34;&gt;尋找並確定合適自己的大學，合適的課程&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;英國，還是美國？ 這是一個問題
&lt;ul&gt;
&lt;li&gt;我能獲得現在工作的大學的經費（其實就是保留職位，工資照發）支持的條件是，最長的出差/留學不能超過一年。&lt;/li&gt;
&lt;li&gt;上面這個條件是最硬的了，沒有銀子，啥都辦不成是吧。美國的碩士基本都是兩年，而且每年的學費都是英國的兩倍左右。真是羨慕嫉妒自費去英美讀書的大陸籍學生們，你們都是行走的美金符號 $。&lt;/li&gt;
&lt;li&gt;加上美國目前爲止去了3-4次了，對北美大陸除了加拿大(溫哥華)印象非常好以外，美帝給人的感覺就是一個自由化了的中國大陸。沒有任何親切感，或者吸引我個人再去長久居住的地方。當然去美國的機會以後可能還有。故覺得去正在經歷激盪變幻莫測歷史的英國也是不錯的選擇。脫歐愈演愈烈，不知道英國會不會有什麼波瀾壯闊的變化，如果能碰巧做個見證人，也是不錯的。將來可以跟我兒子說，看當年大英帝國被踢出歐萌的時候，爸爸在那親眼看着呢。&lt;/li&gt;
&lt;li&gt;另外就是大學的選擇了。當然可選擇的大學有很多，奈何我之前跟大學申請這個例外項目的時候說的是倫敦大學。因此什麼劍橋牛津都是浮雲了。還好我沒明確說，其實倫敦大學底下一大堆大學，UCL和LSHTM是我的申請重點。因爲論醫學統計學課程，大家可以參考這篇&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pubmed/7754267&#34;&gt;文章&lt;/a&gt;^[Pocock, S. J. Life as an academic medical statistician and how to survive it. Statist. Med. 14, 209–222 (1995).] 。儘管時間有點久遠，但是英國國內大學有開設醫學統計課程的大概就那麼幾個，估計沒什麼太大變化，摘錄Pro. Pocock總結的各家特色如下：&lt;!-- raw HTML omitted --&gt;我們可以看到，從最上面的劍橋大學，到最下面的LSHTM(有人翻譯成倫敦衛校😅)按照教學內容偏重理論還是實際進行了排序。所以，LSHTM最偏重實際應用的名氣，是由來已久的。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Theory&lt;!-- raw HTML omitted --&gt; (偏重理論)&lt;/td&gt;
&lt;td&gt;Cambridge&lt;/td&gt;
&lt;td&gt;Mathematical Statistics&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$\downdownarrows$&lt;/td&gt;
&lt;td&gt;Sheffield&lt;/td&gt;
&lt;td&gt;Statistics&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$\downdownarrows$&lt;/td&gt;
&lt;td&gt;University College London&lt;/td&gt;
&lt;td&gt;Applied Stochastic Systems&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$\downdownarrows$&lt;/td&gt;
&lt;td&gt;Oxford&lt;/td&gt;
&lt;td&gt;Applied Statistics&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$\downdownarrows$&lt;/td&gt;
&lt;td&gt;Kent&lt;/td&gt;
&lt;td&gt;Statistics&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$\downdownarrows$&lt;/td&gt;
&lt;td&gt;Reading&lt;/td&gt;
&lt;td&gt;Biometry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$\downdownarrows$&lt;/td&gt;
&lt;td&gt;Southampton&lt;/td&gt;
&lt;td&gt;Statistics with Application in Medicine&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$\downdownarrows$&lt;/td&gt;
&lt;td&gt;Leicester&lt;/td&gt;
&lt;td&gt;Medical Statistics &amp;amp; Information Technology&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Applications (偏重實踐)&lt;/td&gt;
&lt;td&gt;London School of Hygiene &amp;amp; Tropical Medicine&lt;/td&gt;
&lt;td&gt;Medical Statitics&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;確認申請時間申請要點雅思成績要求是否有面試推薦信&#34;&gt;確認申請時間，申請要點（雅思成績要求，是否有面試，推薦信）&lt;/h2&gt;
&lt;p&gt;決定了申請 LSHTM 以後，便要開始準備材料，確定截止時間，以及雅思成績的要求等。&lt;/p&gt;
&lt;p&gt;我之前並無申請歐美大學的經驗，許多都是這次申請過程中自己摸索的。總結一下就是，留學申請這種事，自己來就可以搞定了。經過仔細鑽研LSHTM的醫學統計碩士課程&lt;a href=&#34;http://www.lshtm.ac.uk/study/masters/msms.html&#34;&gt;網站&lt;/a&gt;，確認雅思成績要求總分不低於7，聽說讀單項最低不低於5.5，寫作不低於6.5以後，便着手開始集中複習英語的計劃。&lt;/p&gt;
&lt;p&gt;至於申請截止時間，&lt;a href=&#34;http://www.lshtm.ac.uk/study/applications/index.html&#34;&gt;網站&lt;/a&gt;說的8月1日，沃天。。。9月底開學8月還能申請。不過，6月1日以後的申請要交£100的過遲申請費用。不管怎麼說，越早越好。我是2016年10月開始計劃申請的，那時候自己給自己定下目標，1月7日雅思成績如果達標，1月份之內就完成所有申請步驟。&lt;/p&gt;
&lt;p&gt;面試的情況後面會再多說一些，其他課程不太瞭解，醫學統計學的碩士課程是對有可能成爲學生的人進行面試的 (potential students will be invited to join an interview)。所以估計材料交了以後很久都沒有面試的通知的話，那就可以安心在家當作自己沒有申請過，該幹嘛幹嘛了。儘量保持低調嘛。我還跟他們負責招生的人發郵件確認了，材料遞交6周左右會給面試通知。估計不錄取也是在這個時間點給通知的。&lt;/p&gt;
&lt;p&gt;最後一個就是最重要的推薦人的選擇了。我邀請之前博士階段的導師，以及現在的同事。聽說美國大學要三個推薦人。英國是只要兩人的。關於如何選擇推薦人，LSHTM的網站上說的是，如果申請人正在就學，那就需要兩個都是對你的學業/學術十分瞭解的人。如果申請人已經就業，那就填最高學歷時期的導師一名，及現在的同事一名或者老闆/上司。當然，在把自己要寫的推薦人姓名信息等填入申請表格之前，要跟他們打個招呼才是。&lt;/p&gt;
&lt;p&gt;至於推薦信的內容。我的博士導師收到我的邀請郵件以後欣然同意，然而那之後我並沒有收到他給我的個人評價或者推薦信內容/稿件。我想，大概(有些)認真的日本人認爲這個推薦信對申請人本人來說也應該保密的。不過我對我的導師有充分的信任，不至於在推薦信裏寫我不愛讀書行爲不端之類害我的話。他一直都是實事求是認真做事的人。另外一封推薦信來自我的同事，他對自己英文不太有自信，而且他每天就坐在我隔壁，寫了稿子就讓我看，我又請native speaker幫忙校對了以後提交的。所以我對這個第二封推薦信的內容是掌握的。&lt;/p&gt;
&lt;h2 id=&#34;3個月突擊雅思8分&#34;&gt;3個月突擊，雅思8分&lt;/h2&gt;
&lt;p&gt;我以前考過兩次託福。都是裸考。一次是大學期間跟風考的，大夥兒都忙忙碌碌，準備考研啦，準備託福GRE出國拉，所以我也想說考一個，看看這些英語考試都考什麼內容。如果您來我這裏想瞭解託福雅思考試的祕籍，抱歉出門左轉去&lt;a href=&#34;https://www.hujiang.com/&#34;&gt;滬江外語&lt;/a&gt;吧。我每日也都是用的他家的APP和資源（主要是聽寫BBC新聞）。另外推薦一個背單詞的軟件：&lt;a href=&#34;https://www.baicizhan.com/&#34;&gt;百詞斬&lt;/a&gt;。&lt;a href=&#34;https://www.shanbay.com/&#34;&gt;扇貝單詞&lt;/a&gt;也不錯。不過個人還是對百詞斬比較偏愛。也許是先入爲主吧。第一次打開時，設置自己的背誦單詞表（雅思詞彙）然後設定好時間，和背詞計劃。我是設定了每日100個單詞。每天堅持一百個，直到考試前一天。百詞斬的app會再每天第一次打開app的時候提醒，並且複習昨天或者最近背誦過的生詞。感覺他們應該是用了一些算法的，大約是根據個人背誦單詞的記錄（傳說中的記憶曲線？），以及錯誤次數來選出每天複習的詞彙的，這一點百詞斬很厲害。&lt;/p&gt;
&lt;p&gt;除了背單詞，就是尋找合適的老師練習寫作和口語了。在此我就不去給某寶作廣告了。我找了兩個雅思作文老師練習，每天都有寫作的作業，一天 task 1 第二天 task 2 這樣。有的老師只提供作文修改和點評，有的還會給你上課，當然費用就比前一種稍微貴一些。能提供授課服務的老師基本上就是具有新東方，環球雅思等授課經驗的作文老師。作文老師推薦的教材可以在此介紹一下:
&lt;a href=&#34;https://book.douban.com/subject/11596223/&#34;&gt;&lt;img src=&#34;https://wangcc.me/img/ieltswriting.jpg&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;至於口語，某寶上的口語外教中介等類似商品就更多了。基本上應該都是菲律賓的口語老師。一開始我也抱着忐忑的心情預約試聽了以下，擔心菲律賓的口語老師可能會有類似印度人的難懂口音。後來的事實證明自己完全是多慮了。至少在我聽課的那幾位菲律賓的外教的口音都較爲純正。況且每個人的口音（應該）都是天生的/後天跟父母學的，不必擔心自己上了幾天口語可就變成怪怪的阿三口音。另外記得以前看過文章說英語母語者能辨別很多不同的口音，所以關鍵不是口音影響一個人的表達，而是你到底真的會不會表達。而且如果你的有點異國口音的英語常常還會被認爲很有趣，很性感，或者很有特點。個人認爲典型的中國人的口音其實多數情況下不太性感，但是你也可以變得像下面這個人一樣風趣幽默（點擊圖片可以看到他講的中式腔調的英語笑話，老外一樣被逗得一樂一樂的）:
&lt;a href=&#34;https://www.youtube.com/watch?v=JTE0-UY9_T0&#34;&gt;&lt;img src=&#34;https://wangcc.me/img/joewong.jpg&#34; alt=&#34;&#34;&gt;&lt;/a&gt;
於是我的口語課就固定爲每天早晨9點鐘開始一個小時，和老師練習過去口語考過的題目。各種常見/不常見話題的切磋和準備。許多話題是根本想不到的。比如，“請描述一次你參加過的婚禮”，或者“請用英語講一個中國歷史上的有趣的故事”這樣的題目，讓我用中文來講述我還要愣上個1分鐘，更不要說在分秒必爭的口語考試中被問道這樣的題目，基本就等於告訴你回去準備再交錢考試了。&lt;/p&gt;
&lt;p&gt;備考雅思是一段辛苦的過程。堅持每日練習才能保持良好的考試/競技狀態，口語和作文是中國人的短板。聽力和寫作常常有不少人（包括我）可以拿到接近滿分。我一開始備考時也是覺得要把過去劍橋雅思的4-11套&lt;a href=&#34;https://book.douban.com/subject/1479127/&#34;&gt;全真練習題&lt;/a&gt;全部過一邊，題海戰術嘛。後來被寫作的老師敲了警鐘。他說：「聽力和閱讀如果每天都花過多的時間去做的話，對於你來說提高很有限，因爲你都只有錯很少的題目，只是自己刷高分滿足自己的虛榮心而已。到頭來短板的作文和口語都沒有時間練習的話，總分還是上不去。」於是我聽從了寫作老師的話，改爲三四天做一套聽力和閱讀。當然每次都是用考試時的標準來。所以其實一直到了考前，我也沒有把4-11的所有過去試題都練習完。只是挑着做了一些。關於考場的真實感受和我的考分。可以看我之前的&lt;a href=&#34;https://winterwang.github.io/post/2017-01-07-ielts-test/&#34;&gt;文章&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&#34;1個月集中文件準備&#34;&gt;1個月集中，文件準備&lt;/h2&gt;
&lt;p&gt;考完雅思考試以後等待考試成績公佈的這段時間，我便開始着手準備申請所需要的各種文件。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;最近的大學院（就是我的博士課程）成績單，英文版。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;博士學位證書，和名古屋大學的畢業證書的英文版。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1和2由於是要開英文版的，聯繫名古屋大學的留學生辦公室，申請郵寄辦理（無手續費），然後附上回信的信封和郵票就可以了。&lt;/li&gt;
&lt;li&gt;另外，爲了以防萬一，我又拜託之前本科階段上海交大的指導老師幫忙開具了本科階段的學位證書，畢業證書的英文版，以及當年的成績單。（看了當時的成績單，不禁回想當年在上海求學的日子。曾經有段時間，在中國訪問facebook是不需要任何技巧的。那個時候，我們還有google reader，還有google.cn。。。）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;個人簡歷&lt;a href=&#34;https://github.com/winterwang/markdown_cv/raw/master/Rmarkdown/rap-2pg-cv.pdf&#34;&gt;cv&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;查找了許多模板，后来选择了(&lt;a href=&#34;http://svmiller.com/blog/2016/03/svm-r-markdown-cv/&#34;&gt;这一款&lt;/a&gt;)。&lt;code&gt;Fork&lt;/code&gt;过来以后打开&lt;code&gt;Rmd&lt;/code&gt;文件，写上自己的内容，&lt;code&gt;knit&lt;/code&gt;，pdf就生成了。生活从未如此简单与快乐。告别Micro$oft，你会更轻松。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;個人陳述(Personal Statement)寫作，修改，寫作，修改。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;格式依然是用&lt;a href=&#34;https://github.com/rohanarora/SoP&#34;&gt;模板&lt;/a&gt;，然后内容的写作和修改，确实费了一番脑筋和功夫。先是寫了初稿，然後給了曾經上过LSHTM醫學統計課程的日本人前輩看，然後修改，又給寫推薦信的兩位導師看，然後再修改，之後再給曾經在UCL留學的高中同學，以及他認識的 native speaker 看，之後再修改。此後又给目前在UCL任教的曾經的高中同學看。最后又花錢送去潤色和校對一遍，才決定最後作爲申請文書遞交給LSHTM。六個不同的人給的意見自然會有不同，最終還是要自己作決斷和取捨的。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;跟以前的導師，現在的上司請求推薦信&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;上面提到個人陳述的時候也說到把稿子給了兩位寫推薦信的導師看。我覺得這一點十分重要。畢竟寫推薦信的導師，他要知道你自己在個人陳述中自我推薦了什麼，才能再在推薦信裏加以強調。深以爲然。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;備齊材料終於可以申請了&#34;&gt;備齊材料，終於可以申請了！&lt;/h2&gt;
&lt;p&gt;上面的各種文件備齊了以後，就是直接在線填寫申請表格了。表格中仍有部分內容需要自己填寫的。在此不再贅述。 按下申請按鈕之後，LSHTM發來確認信。估計是系統自動發送的。之後便是等待兩位推薦人在線遞交推薦信了。兩位推薦人交齊了推薦信，已經是我申請提交之後一個月左右的事了。之後該是進入和文書審查階段。&lt;/p&gt;
&lt;h2 id=&#34;面試來了面試真的來了&#34;&gt;面試來了！面試真的來了！&lt;/h2&gt;
&lt;p&gt;過了兩到三週。課程的聯絡人(Admissions Administrator)發來郵件說安排一下Skype面試的時間。&lt;/p&gt;
&lt;h2 id=&#34;我被錄取了&#34;&gt;我被錄取了！&lt;/h2&gt;
&lt;h2 id=&#34;補交畢業證書的原件&#34;&gt;補交畢業證書的原件&lt;/h2&gt;
</description>
    </item>
    
  </channel>
</rss>
