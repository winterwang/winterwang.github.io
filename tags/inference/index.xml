<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>inference on Be ambitious</title>
    <link>https://winterwang.github.io/tags/inference/</link>
    <description>Recent content in inference on Be ambitious</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Chaochen Wang | 王超辰</copyright>
    <lastBuildDate>Wed, 08 Nov 2017 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://winterwang.github.io/tags/inference/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>徒手打造一個假設檢驗</title>
      <link>https://winterwang.github.io/post/construction-of-a-hypothesis-test/</link>
      <pubDate>Wed, 08 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/construction-of-a-hypothesis-test/</guid>
      <description>什麼是假設檢驗 Hypothesis testing一般來說，我們的假設（或者叫假說）是對與我們實驗觀察數據來自的總體（或人羣）的概率分佈的描述。在參數檢驗的背景下，就是要檢驗描述這個總體（或人羣）的概率分佈的參數 (parameters)。最典型的情況是，我們提出兩個互補的假設，一個叫作零假設（或者叫原假設），null hypothesis (\(H_0\))；另一個是與之對應的（互補的）替代假設，althernative hypothesis (\(H_1/H_A\))。
例如，若 \(X\) 是一個服從二項分佈的隨機離散變量 \(X\sim Bin(5, \theta)\)。可以考慮如下的零假設和替代假設：\(H_0: \theta=\frac{1}{2}; H_1: \theta=\frac{2}{3}\)。
當建立了零假設和替代假設以後，假設檢驗就是要建立如下的規則以確定：
從樣本中計算所得的參數估計值爲多少時，拒絕零假設。（接受替代假設爲“真”）從樣本中計算所得的參數估計值爲多少時，零假設不被拒絕。（接受零假設爲“真”）注意：（這一段很繞）
上面的例子是零假設和替代假設均爲簡單假設的情況，實際操作中常常會設計更加複雜的（不對稱的）假設：即簡單的 \(H_0\)，複雜的 \(H_1\)。如此一來當零假設 \(H_0\) 不被拒絕時，我們並不一定就接受之。因爲無證據證明 \(H_1\) 不等於有證據證明 \(H_0\)。(Absence of evidence is not evidence of absence). 換句話說，無證據讓我們拒絕 \(H_0\) 本身並不成爲支持 \(H_0\) 爲“真”的證據。因爲在實際操作中，當我們設定的簡單的零假設沒有被拒絕，可能還存在其他符合樣本數據的零假設；相反地，當樣本數據的計算結果拒絕了零假設，我們只能接受替代假設。所以，反對零假設的證據，同時就是支持替代假設的證據。
在樣本空間 sample space 中，決定了零假設 \(H_0\) 會被拒絕的子集 subset，被命名爲拒絕域 rejection region 或者 判別區域 critical region，用 \(\mathfrak{R}\) 來標記。
錯誤概率和效能方程這一部分可以參考之前臨牀試驗樣本量計算的部分。
Table 1: Definition of Type I and Type II errorSAMPLE\(\underline{x} \notin \mathfrak{R}\) Accept \(H_0\)\(\underline{x} \in \mathfrak{R}\) Reject \(H_0\)TRUTH\(H_0\) is true\(\checkmark\)\(\alpha\) Type I error\(H_1\) is true\(\beta\) Type II error\(\checkmark\)假如一個假設檢驗是關於總體參數 \(\theta\) 的：</description>
    </item>
    
    <item>
      <title>二次方程近似法求對數似然比 approximate log-likelihood ratios</title>
      <link>https://winterwang.github.io/post/approximate-log-likelihood-ratios/</link>
      <pubDate>Tue, 07 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/approximate-log-likelihood-ratios/</guid>
      <description>爲什麼要用二次方程近似對數似然比方程？
上節也看到，我們會碰上難以用代數學計算獲得對數似然比信賴區間的情況 (binomial example)。我們同時知道，對數似然比方程會隨着樣本量增加而越來越漸進於二次方程，且左右對稱。所以，我們考慮當樣本量足夠大時，用二次方程來近似對數似然比方程從而獲得參數估計的信賴區間。正態近似法求對數似然 Normal approximation to the log-likelihood根據前一節，如果樣本均數的分佈符合正態分佈：\(\bar{X}\sim N(\mu, \sigma^2/n)\)。那麼樣本均數的對數似然比爲：
\[llr(\mu|\bar{X})=\ell(\mu|\bar{X})=-\frac{1}{2}(\frac{\bar{x}-\mu}{\sigma/\sqrt{n}})^2\]
其中， \(\bar{x}\) 是正態分佈總體均數 \(\mu\) 的極大似然估計 (maximum likelihood estimator, MLE)。如果已知總體的方差參數，那麼 \(\sigma/\sqrt{n}\) 是 \(\bar{x}\) 的標準誤 (standard error)。
因此，假設 \(\theta\) 是我們想尋找的總體參數。有些人提議可以使用下面的關於 \(\theta\) 的二次方程來做近似：
\[f(\theta|data)=-\frac{1}{2}(\frac{\theta-M}{S})^2\]
上述方程具有一個正態二次對數似然 (比) 的形式，而且該方程的極大似然估計(MLE)， \(M\) 的標準誤爲 \(S\)。如果我們正確地選用 \(M\) 和 \(S\)，那我們就可以用這樣的方程來近似求真實觀察數據的似然 \(\ell(\theta|data)\)。
通過近似正態對數似然比，\(M\) 應當選用使方程取最大值時，參數 \(\theta\) 的極大似然估計 \(M=\hat{\Theta}\)。
但是在選用標準誤 \(S\) 上必須滿足下列條件：
\(S\) 是極大似然估計 \(\hat{\Theta}\) 的標準誤。被選擇的 \(S\) 必須儘可能的使該二次方程形成一個十分接近真實的對數似然比方程。特別是在最大值的部分必須與之無限接近或者一致。所以二者在 MLE 的位置應當有相同的曲率（二階導數）。由於，一個方程的曲率是該方程的二階導數（斜線斜率變化的速度）。所以對數似然比方程在 MLE 取最大值時的曲率（二階導數）爲：</description>
    </item>
    
    <item>
      <title>對數似然比 Log-likelihood ratio</title>
      <link>https://winterwang.github.io/post/log-likelihood-ratio/</link>
      <pubDate>Sun, 05 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/log-likelihood-ratio/</guid>
      <description>對數似然比 Log-likelihood ratio對數似然比的想法來自於將對數似然方程圖形的 \(y\) 軸重新調節 (rescale) 使之最大值爲零。這可以通過計算該分佈方程的對數似然比 (log-likelihood ratio) 來獲得：
\[llr(\theta)=\ell(\theta|data)-\ell(\hat{\theta}|data)\]
由於 \(\ell(\theta)\) 的最大值在 \(\hat{\theta}\) 時， 所以，\(llr(\theta)\) 就是個當 \(\theta=\hat{\theta}\) 時取最大值，且最大值爲零的方程。很容易理解我們叫這個方程爲對數似然比，因爲這個方程就是將似然比 \(LR(\theta)=\frac{L(\theta)}{L(\hat{\theta})}\) 取對數而已。
之前我們也確證了，不包含我們感興趣的參數的方程部分可以忽略掉。還是用上一節 10人中4人患病的例子：
\[L(\pi|X=4)=\binom{10}{4}\pi^4(1-\pi)^{10-4}\\\Rightarrow \ell(\pi)=log[\pi^4(1-\pi)^{10-4}]\\\Rightarrow llr(\pi)=\ell(\pi)-\ell(\hat{\pi})=log\frac{\pi^4(1-\pi)^{10-4}}{0.4^4(1-0.4)^{10-4}}\]
其實由上也可以看出 \(llr(\theta)\) 只是將對應的似然方程的 \(y\) 軸重新調節了一下而已。形狀是沒有改變的：
par(mfrow=c(1,2))x &amp;lt;- seq(0,1,by=0.001)y &amp;lt;- (x^4)*((1-x)^6)/(0.4^4*0.6^6)z &amp;lt;- log((x^4)*((1-x)^6))-log(0.4^4*0.6^6)plot(x, y, type = &amp;quot;l&amp;quot;, ylim = c(0,1.1),yaxt=&amp;quot;n&amp;quot;,frame.plot = FALSE, ylab = &amp;quot;LR(\U03C0)&amp;quot;, xlab = &amp;quot;\U03C0&amp;quot;)axis(2, at=seq(0,1, 0.2), las=2)title(main = &amp;quot;Binomial likelihood ratio&amp;quot;)abline(h=1.</description>
    </item>
    
    <item>
      <title>似然非然 Likelihood</title>
      <link>https://winterwang.github.io/post/likelihood/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/likelihood/</guid>
      <description>概率 vs. 推斷/Probability vs. Inference在概率論的環境下，我們常常被告知的前提是：某某事件發生的概率是多少。例如： 一枚硬幣正面朝上的概率是 \(0.5\; Prob(coin\;landing\;heads)=0.5\)。然後在這個前提下，我們又繼續去計算複雜的事件發生的概率（例如，10次投擲硬幣以後4次正面朝上的概率是多少？）。
\[\binom{10}{4}\times(0.5^4)\times(0.5^{10-4}) = 0.205\]
dbinom(4, 10, 0.5)## [1] 0.2050781# or you can calculate by hand:factorial(10)*(0.5^10)/(factorial(4)*(factorial(6)))## [1] 0.2050781在統計推斷的理論中，我們考慮實際的情況，這樣的實際情況就是，我們通過觀察獲得數據，然而我們並不知道某事件發生的概率到底是多少（神如果存在話，只有神知道）。故這個 \(Prob(coin\;landing\;heads)\) 的概率大小對於“人類”來說是未知的。我們可能觀察到投擲了10次硬幣，其中有4次是正面朝上的。那麼我們從這一次觀察實驗中，需要計算的是能夠符合觀察結果的“最佳”概率估計 (best estimate)。在這種情況下，似然法 (likelihood) 就是我們進行參數估計的最佳手段。
似然和極大似然估計此處用二項分佈的例子來理解似然法的概念：假設我們觀察到10個對象中有4個患病，我們假定這個患病的概率爲 \(\pi\)。於是我們就有了下面的模型：
模型： 我們假定患病與否是一個服從二項分佈的隨機變量，\(X\sim Bin(10,\pi)\)。同時也默認每個人之間是否患病是相互獨立的。
數據： 觀察到的數據是，10人中有4人患病。於是 \(x=4\)。
現在按照觀察到的數據，參數 \(\pi\) 變成了未知數：
\[Prob(X=4|\pi)=\binom{10}{4}\pi^4(1-\pi)^{10-4}\]
此時我們會很自然的考慮，當 \(\pi\) 是未知數的時候，它取值爲多大的時候才能讓這個事件（即：10人中4人患病）發生的概率最大？ 所以我們可以將不同的數值代入 \(\pi\) 來計算該事件在不同概率的情況下發生的可能性到底是多少：
Table 1: The probability of observing \(X=4\)\(\pi\)事件 \(X=4\) 發生的概率0.</description>
    </item>
    
    <item>
      <title>卡方分佈 chi square distribution</title>
      <link>https://winterwang.github.io/post/chi-square-distribution/</link>
      <pubDate>Thu, 26 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/chi-square-distribution/</guid>
      <description>卡方分佈的期望和方差的證明：當 \(X\sim N(0,1)\) 時， \(X^2\sim \mathcal{X}_1^2\)
如果 \(X_1, \dots, X_n\stackrel{i.i.d}{\sim} N(0,1)\)，那麼 \(\sum_{i=1}^nX_i^2\sim\mathcal{X}_n^2\)
其中： \(\mathcal{X}_n^2\) 表示自由度爲 \(n\) 的卡方分佈。
且 \(X_m^2+X_n^2=\mathcal{X}_{m+n}^2\)
卡方分佈的期望：\[E(X_1^2)=Var(X)+[E(X)]^2=1+0=1\]
\[\Rightarrow E(X_n^2)=n\]
卡方分佈的方差：\[\begin{aligned}Var(X_1^2) &amp;amp;= E(X_1^{2^2}) - E(X_1^2)^2 \\&amp;amp;= E(X_1^4)-1\end{aligned}\]
下面來求 \(E(X_1^4)\)\[\begin{aligned}\because E(X_1) &amp;amp;= \int_{-\infty}^{+\infty} xf(x)dx \\\therefore E(X_1^4) &amp;amp;= \int_{-\infty}^{+\infty} x^4f(x)dx\end{aligned}\]
已知： \(f(x)=\frac{1}{\sqrt{2\pi}}e^{(-\frac{x^2}{2})}\) 代入上式：
\[\begin{aligned}E(X_1^4) &amp;amp;= \int_{-\infty}^{+\infty} x^4f(x)dx \\&amp;amp;= \int_{-\infty}^{+\infty} x^4\frac{1}{\sqrt{2\pi}}e^{(-\frac{x^2}{2})}dx\\&amp;amp;=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}x^4e^{(-\frac{x^2}{2})}dx\\&amp;amp;=\frac{-1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}x^3(-x)e^{(-\frac{x^2}{2})}dx\end{aligned}\]</description>
    </item>
    
    <item>
      <title>估計和精確度的概念</title>
      <link>https://winterwang.github.io/post/frequentist-statistical-inference02/</link>
      <pubDate>Tue, 24 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/frequentist-statistical-inference02/</guid>
      <description>估計量和他們的樣本分佈例子： 最大呼氣量 (Forced Expoiratory Volume in one second, FEV1) 用於測量一個人的肺功能，它的測量值是連續的。我們從前來門診的人中隨機抽取 \(n\) 人作爲樣本，用這個樣本的 FEV1 平均值來估計這個診所的患者的平均肺功能。
模型假設： 在這個例子中，我們的假設有如下：每個隨機抽取的 FEV1 測量值都是從同一個總體（人羣）中抽取，每一個觀察值 \(Y_i\) 都互相獨立互不影響。我們用縮寫 iid 表示這些隨機抽取的樣本是服從獨立同分佈 (independent and identically distributed)。另外，總體的分佈也假定爲正態分佈，且總體均值爲 \(\mu\)，總體方差爲 \(\sigma^2\)。那麼這個模型可以簡單的被寫成：
\[Y_i \stackrel{i.i.d}{\sim} N(\mu, \sigma^2), i=1,2,\dots,n\]
總體均值 \(\mu\) 的估計量： 顯然算術平均值: \(\bar{Y}=\frac{1}{n}\sum_{i=1}^ny_i\) 是我們用於估計總體均值的估計量。
估計量的樣本分佈：\[\bar{Y}\stackrel{i.i.d}{\sim}N(\mu, \frac{\sigma^2}{n})\]
證明\[\begin{aligned}E(\bar{Y}) &amp;amp;= E(\frac{1}{n}\sum Y_i) \\&amp;amp;= \frac{1}{n}E(\sum Y_i) \\&amp;amp;= \frac{1}{n}\sum E(Y_i) \\&amp;amp;= \frac{1}{n}n\mu = \mu \\Var(\bar{Y}) &amp;amp;= Var(\frac{1}{n}\sum Y_i) \\\because Y_i \;are &amp;amp;\; independent \\&amp;amp;= \frac{1}{n^2}\sum Var(Y_i) \\&amp;amp;= \frac{1}{n^2} n Var(Y_i) \\&amp;amp;= \frac{\sigma^2}{n}\end{aligned}\]</description>
    </item>
    
    <item>
      <title>概率論者統計推斷入門之-被門夾住</title>
      <link>https://winterwang.github.io/post/frequentist-statistical-inference01/</link>
      <pubDate>Tue, 24 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/frequentist-statistical-inference01/</guid>
      <description>人羣與樣本 (population and sample)討論樣本時，需考慮下面幾個問題：
樣本是否具有代表性？人羣被準確定義了嗎？我們感興趣的“人羣”是否可以是無限大（多）的？我們研究的樣本，是僅僅用來觀察，亦或是計劃對之進行某種干預呢？我們從所有可能的人羣中抽樣了嗎？樣本和統計量 (sample and statistic)通常我們在進行實驗或觀察時只是獲得了樣本的數據。而希望從樣本數據去推斷 (inference) 總體（或人羣）的一些特徵。我們也許只是想用樣本的平均值來估計整體人羣的某個特徵的平均值。不管是何種估計和推斷，都是基於對樣本數據的計算，從樣本中獲得想要推斷總體的統計量 (statistics)。我們用已知樣本去推斷未知總體的過程就叫做估計 (estimate)。這個想要被推斷的總體或人羣的值，被叫做參數 (parameter)，常常使用希臘字母來標記。用來估計總體或人羣的，從樣本數據計算得來的統計量，叫做估計量 (estimator)。
所有的統計量，都有樣本分佈 (sampling distributions，意爲重複無限次取樣後獲得的無限次統計量的分佈)。推斷的過程歸納如下：
從總體或人羣中抽樣 (樣本量 \(n\))計算這個樣本的合適統計量，從而用於估計它在整體或人羣中的值。我們還需要決定計算獲得的統計量的樣本分佈（假定會抽樣無數次）。一旦可以精確地確認樣本分佈，我們就可以定量地計算出使用步驟2中獲得的統計量估計總體或人羣的參數時的準確度。估計 Estimation從樣本的均值，推斷總體或人羣的均值是一種估計。我們的目的是，從已知樣本中計算一個儘可能接近那個未知的總體或人羣參數的值。一個估計量有兩個與生俱來的性質 (properties)：1) 偏倚 (bias); 2) 精確度 (precision)。這兩個性質都可以從樣本分佈和估計量獲得。
偏倚： 偏倚簡單說就是樣本分佈的均值，也就是我們從樣本中計算獲得的估計量，和我們想要拿它來估計的總體或人羣的參數之間的差距。(The bias is the difference between the mean of the sampling distribution – the expected or average value of the estimator – and the population parameter being estimated.</description>
    </item>
    
    <item>
      <title>偉大的中心極限定理</title>
      <link>https://winterwang.github.io/post/central-limit-theory/</link>
      <pubDate>Thu, 19 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/central-limit-theory/</guid>
      <description>最近明顯可以感覺到課程的步驟開始加速。看我的課表：
手機畫面太小了。早上都是9點半開始，下午基本都是到5點。週一更慘，到7點。週二-週五中午都被統計中心的講座佔據。簡直是非人的生活。
這周概率論基礎結束。中心極限定理講完以後我們正式進入了 Inference 統計推斷的課程。我們花了一天時間講什麼是樣本估計 (Estimation)，什麼是參數精確度 (Precision)，什麼是自由度 (degree of freedom)，怎樣進行不偏的估計 (unbiased inference)。然後還有似然方程 (likelihood function)。
今天的更新還是簡單的把概率論掃尾一下。感受一下中心極限定理的偉大。
協方差 Covariance之前我們定義過，兩個獨立連續隨機變量 \(X,Y\) 之和的方差 Variance ：
\[Var(X+Y)=Var(X)+Var(Y)\]
然而如果他們並不相互獨立的話：
\[\begin{aligned}Var(X+Y) &amp;amp;= E[((X+Y)-E(X+Y))^2] \\&amp;amp;= E[(X+Y)-(E(X)+E(Y))^2] \\&amp;amp;= E[(X-E(X)) - (Y-E(Y))^2] \\&amp;amp;= E[(X-E(X))^2+(Y-E(Y))^2 \\&amp;amp; \;\;\; +2(X-E(X))(Y-E(Y))] \\&amp;amp;= Var(X)+Var(Y)+2E[(X-E(X))(Y-E(Y))]\end{aligned}\]可以發現在兩者和的方差公式展開之後多了一部分 \(E[(X-E(X))(Y-E(Y))]\)。 這個多出來的一部分就說明了二者 \((X, Y)\) 之間的關係。它被定義爲協方差 (Covariance):\[Cov(X,Y) = E[(X-E(X))(Y-E(Y))]\]
所以：
\[Var(X+Y)=Var(X)+Var(Y)+2Cov(X,Y)\]
要記住，協方差只能用於評價(X,Y)之間的線性關係 (Linear Association)。
以下是協方差 (Covariance) 的一些特殊性質：
\(Cov(X,X)=Var(X)\)\(Cov(X,Y)=Cov(Y,X)\)\(Cov(aX,bY)=ab\:Cov(X,Y)\)\(Cov(aR+bS,cX+dY)=ac\:Cov(R,X)+ad\:Cov(R,Y)\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+bc\:Cov(S,X)+bd\:Cov(S,Y)\)\(Cov(aX+bY,cX+dY)=ac\:Var(X)+ad\:Var(Y)\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+(ad+bc)Cov(X,Y)\)\(Cov(X+Y,X-Y)=Var(X)-Var(Y)\)If \(X, Y\) are independent.</description>
    </item>
    
    <item>
      <title>你會用概率論來賭博嗎？</title>
      <link>https://winterwang.github.io/post/probability-gambling/</link>
      <pubDate>Sun, 08 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/probability-gambling/</guid>
      <description>轉眼我已經進入課程的第二週了，總體來說，我們一半的時間都在電腦房練習 Stata 的數據清理和簡單的描述統計 (descriptive statistics)。從我個人的經驗來說，數據分析的過程，其實一大半的時間是消耗在 data cleaning 上的，即使手頭拿到了所謂的乾淨的數據，到真正要分析的時候就會發現一大堆的問題在裏面，需要重新整理，重新添加標記以使之變得更加讓人類可以讀懂。電腦是機器，他是不管你的數據是否乾淨的。只要你放了數據進去，邏輯還可以，沒有編程上的語法錯誤，它總歸會出來一些報告和結果的。如果就這麼直接用的話，大部分的人就會掉進陷阱。畢竟數據不光會說出事實真相，更多的情況下還會把真相給掩蓋住了。
我的其餘大部分時間都用在了複習高等數學的微積分上了。感覺好似回到了高中時代。其實大學的時候線性代數得分還是接近滿分的。後來多年不用，生疏了。剛打開複習的書的時候，許多微分積分的規則都已經忘記。通過這一週的辛苦練習，終於是找回了一點狀態。如果你也想有空的時候複習以下高中數學知識，這本書可以推薦給你：
Quick Calculus: Short Manual of Self-instruction
上面這本書的內容可以一邊閱讀，一邊練習。實在是複習的一本好書。我花了一週的課餘時間，從頭到尾把裏面的習題和解答全部完成。收穫很大。感覺年輕時的數學思維又開始在大腦裏復甦了。一身輕鬆。
下面想介紹一下上週學習的概率的基礎問題。
首先是最基礎的三個概率的公理：對於任意事件 \(A\)，它發生的概率 \(P(A)\) 滿足這樣的不等式： \(0 \leqslant P(A) \leqslant 1\)\(P(\Omega)=1\) , \(\Omega\) 是全樣本空間 (total sample space)對於互斥（相互獨立）的事件 \(A_1, A_2, \dots, A_n\) 有如下的等式關係： \(P(A_1\cup A_2 \cup \cdots \cup A_n)=P(A_1)+P(A_2)+\cdots+P(A_n)\)你是不是覺得上面三條公理都是廢話。不用擔心，我也是這麼覺得的。因爲所有人都認同的道理，才能成爲公理 (axiom)，因爲它們是不需要證明的自然而然形成的人人都接受的觀念。(axiom: a saying that is widely accepted on its own merits; its truth is assumed to be self-evident)
然而，正是這樣顯而易見的道理，確是拿來建築理論的基石，千萬不能小看了他們。例如，我們看下面這個看似也應該成爲公理的公式，你能證明嗎：
\(P(A_1\cup A_2) = P(A_1) + P(A_2) - P(A_1 \cap A_2)\)</description>
    </item>
    
  </channel>
</rss>