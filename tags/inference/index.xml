<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Inference on Be ambitious</title>
    <link>https://winterwang.github.io/tags/inference/</link>
    <description>Recent content in Inference on Be ambitious</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Chaochen Wang | 王超辰</copyright>
    <lastBuildDate>Tue, 07 Nov 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/tags/inference/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>二次方程近似法求對數似然比 approximate log-likelihood ratios</title>
      <link>https://winterwang.github.io/post/approximate-log-likelihood-ratios/</link>
      <pubDate>Tue, 07 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/approximate-log-likelihood-ratios/</guid>
      <description>&lt;p&gt;爲什麼要用二次方程近似對數似然比方程？&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;上節也看到，我們會碰上難以用代數學計算獲得對數似然比信賴區間的情況 (&lt;a href=&#34;https://winterwang.github.io/post/log-likelihood-ratio/&#34;&gt;binomial example&lt;/a&gt;)。&lt;/li&gt;
&lt;li&gt;我們同時知道，對數似然比方程會隨着樣本量增加而越來越漸進於二次方程，且左右對稱。&lt;/li&gt;
&lt;li&gt;所以，我們考慮當樣本量足夠大時，用二次方程來近似對數似然比方程從而獲得參數估計的信賴區間。&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;-normal-approximation-to-the-log-likelihood&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;正態近似法求對數似然 Normal approximation to the log-likelihood&lt;/h3&gt;
&lt;p&gt;根據&lt;a href=&#34;https://winterwang.github.io/post/log-likelihood-ratio/&#34;&gt;前一節&lt;/a&gt;，如果樣本均數的分佈符合正態分佈：&lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\sim N(\mu, \sigma^2/n)\)&lt;/span&gt;。那麼樣本均數的對數似然比爲：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[llr(\mu|\bar{X})=\ell(\mu|\bar{X})=-\frac{1}{2}(\frac{\bar{x}-\mu}{\sigma/\sqrt{n}})^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中， &lt;span class=&#34;math inline&#34;&gt;\(\bar{x}\)&lt;/span&gt; 是正態分佈總體均數 &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; 的極大似然估計 (maximum likelihood estimator, MLE)。如果已知總體的方差參數，那麼 &lt;span class=&#34;math inline&#34;&gt;\(\sigma/\sqrt{n}\)&lt;/span&gt; 是 &lt;span class=&#34;math inline&#34;&gt;\(\bar{x}\)&lt;/span&gt; 的標準誤 (standard error)。&lt;/p&gt;
&lt;p&gt;因此，假設 &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; 是我們想尋找的總體參數。有些人提議可以使用下面的關於 &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; 的二次方程來做近似：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[f(\theta|data)=-\frac{1}{2}(\frac{\theta-M}{S})^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;上述方程具有一個正態二次對數似然 (比) 的形式，而且該方程的極大似然估計(MLE)， &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; 的標準誤爲 &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;。如果我們正確地選用 &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; 和 &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;，那我們就可以用這樣的方程來近似求真實觀察數據的似然 &lt;span class=&#34;math inline&#34;&gt;\(\ell(\theta|data)\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;通過近似正態對數似然比，&lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; 應當選用使方程取最大值時，參數 &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; 的極大似然估計 &lt;span class=&#34;math inline&#34;&gt;\(M=\hat{\Theta}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;但是在選用標準誤 &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; 上必須滿足下列條件：&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; 是極大似然估計 &lt;span class=&#34;math inline&#34;&gt;\(\hat{\Theta}\)&lt;/span&gt; 的標準誤。&lt;/li&gt;
&lt;li&gt;被選擇的 &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; 必須儘可能的使該二次方程形成一個十分接近真實的對數似然比方程。特別是在最大值的部分必須與之無限接近或者一致。所以二者在 MLE 的位置應當有相同的曲率（二階導數）。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;由於，一個方程的曲率是該方程的二階導數（斜線斜率變化的速度）。所以對數似然比方程在 MLE 取最大值時的曲率（二階導數）爲：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\left.\frac{d^2}{d\theta^2}\ell(\theta)\right\vert_{\theta=\hat{\theta}}=\ell^{\prime\prime}(\hat{\theta})=-\frac{1}{S^2}\\
\Rightarrow S^2=\left.-\frac{1}{\ell^{\prime\prime}(\theta)}\right\vert_{\theta=\hat{\theta}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;在正態分佈的例子下，&lt;span class=&#34;math inline&#34;&gt;\(M=\bar{x}, S=\sigma/\sqrt{n}\)&lt;/span&gt;。對數似然比方程最大值時的曲率（二階導數）恰好就爲標準誤的平方的負倒數：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\ell^{\prime\prime}(\theta)=-\frac{1}{SE^2}\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\Rightarrow\)&lt;/span&gt; 被叫做 &lt;strong&gt;Fisher information&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;稍微總結一下：&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;任意的對數似然比方程 &lt;span class=&#34;math inline&#34;&gt;\(llr(\theta)\)&lt;/span&gt; 都可以考慮用一個二次方程來近似： &lt;span class=&#34;math display&#34;&gt;\[f(\theta|data)=-\frac{1}{2}(\frac{\theta-M}{S})^2\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;其中&lt;br&gt; &lt;span class=&#34;math inline&#34;&gt;\(\begin{aligned} &amp;amp;M=\hat\theta\\ &amp;amp;S^2=\left.-\frac{1}{\ell^{\prime\prime}(\theta)}\right\vert_{\theta=\hat{\theta}}\\ &amp;amp;when \\  &amp;amp; n\rightarrow\infty \Rightarrow  \begin{cases}  S^2\rightarrow Var(\hat\theta) \\  S\rightarrow SE(\hat\theta)  \end{cases} \end{aligned}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;section level4&#34;&gt;
&lt;h4&gt;近似法估算對數似然比的信賴區間&lt;/h4&gt;
&lt;p&gt;一旦我們決定了使用正態近似法來模擬對數似然比方程，對數似然比的信賴區間算法就回到了前一節中我們算過的方法，也就是：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[-2f(\theta)&amp;lt;\mathcal{X}_{1,(1-\alpha)}^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;故信賴區間爲： &lt;span class=&#34;math inline&#34;&gt;\(m\pm\sqrt{\mathcal{X}_{1,(1-\alpha)}^2}S\)&lt;/span&gt;。求&lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; 水平的信賴區間時，&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{X}_{1,0.95}^2=3.84\)&lt;/span&gt;，所以就又看到了熟悉的 &lt;span class=&#34;math inline&#34;&gt;\(M\pm1.96S\)&lt;/span&gt;。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level4&#34;&gt;
&lt;h4&gt;以泊松分佈爲例&lt;/h4&gt;
&lt;p&gt;一個被追蹤的樣本，經過了 &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; 人年的觀察，記錄到了 &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; 個我們要研究的事件：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[D\sim po(\mu), where \mu=\lambda p\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Step 1. 找極大似然估計 (MLE)，&lt;a href=&#34;https://winterwang.github.io/post/likelihood/&#34;&gt;之前介紹似然方程時推導過的泊松分佈的似然方程&lt;/a&gt;：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
P(D=d|\lambda) &amp;amp;= \frac{e^{-\mu}\cdot\mu^d}{d!} \\
 &amp;amp;=\frac{e^{-\lambda p}\cdot\lambda^d p^d}{d!} \\
omitting&amp;amp;\;terms\;not\;in\;\mu \\
&amp;amp;\Rightarrow \ell(\lambda) = dlog\lambda - \lambda p \\
&amp;amp;\Rightarrow \ell^\prime(\lambda) = \frac{d}{\lambda} -p \\
&amp;amp;\Rightarrow \hat\lambda=\frac{d}{p} = \textbf{M}
\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Step 2. 求似然方程的二階導數，確認 MLE 是使方程獲得最大值的點，然後確定 &lt;span class=&#34;math inline&#34;&gt;\(S^2\)&lt;/span&gt;：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
&amp;amp; \ell^\prime(\lambda) = \frac{d}{\lambda} -p \\
&amp;amp; \Rightarrow \ell^{\prime\prime}(\lambda) = -\frac{d}{\lambda^2}&amp;lt;0 \Rightarrow \textbf{MLE is maximum} \\
&amp;amp; S^2 = \left.-\frac{1}{\ell^{\prime\prime}(\lambda)}\right\vert_{\lambda=\hat{\lambda}=d/p} = -\frac{1}{-d/\hat\lambda^2} = -\frac{1}{-d/(d/p)^2} \\
&amp;amp;\Rightarrow S^2 = \frac{d}{p^2} \\
\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Step 3. 把前兩部求得的 &lt;span class=&#34;math inline&#34;&gt;\(MLE\)&lt;/span&gt; 和 &lt;span class=&#34;math inline&#34;&gt;\(S^2\)&lt;/span&gt; 代入近似的二次方程：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
&amp;amp; \hat\lambda=\frac{d}{p}=M,\; S^2 = \frac{d}{p^2}  \\
&amp;amp; using\;approximate\;quadratic\;llr \\
&amp;amp; q(\lambda) = -\frac{1}{2}(\frac{\lambda-M}{S})^2\\
&amp;amp;\Rightarrow q(\lambda) = -\frac{1}{2}(\frac{\lambda-\frac{d}{p}}{\frac{\sqrt{d}}{p}})^2\\
&amp;amp; let \; q(\lambda)=-1.92\\
&amp;amp;\Rightarrow -\frac{1}{2}(\frac{\lambda-\frac{d}{p}}{\frac{\sqrt{d}}{p}})^2=-1.92\\
&amp;amp;(\frac{\lambda-\frac{d}{p}}{\frac{\sqrt{d}}{p}})^2=3.84\\
&amp;amp;\frac{\lambda-\frac{d}{p}}{\frac{\sqrt{d}}{p}} = \pm1.96\\
&amp;amp;\Rightarrow 95\%CI \;for \;\lambda = \frac{d}{p}\pm1.96\frac{\sqrt{d}}{p}
\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;結論就是： 發病（死亡）率 &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; 的 &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; 信賴區間爲： &lt;span class=&#34;math inline&#34;&gt;\(M\pm1.96S\)&lt;/span&gt;。所以我們不需要每次都代入對數似然比方程，只要算出 &lt;span class=&#34;math inline&#34;&gt;\(MLE = M\)&lt;/span&gt; 和 &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; 之後代入這個公式就可以用二次方程近似法算出信賴區間。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level4&#34;&gt;
&lt;h4&gt;以二項分佈爲例&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[K\sim Bin(n,\pi)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Step 1. 找極大似然估計 (MLE)：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp; Prob(K=k) = \pi^k(1-\pi)\binom{n}{k}\\
&amp;amp;\Rightarrow L(\pi|k) = \pi^k(1-\pi)\binom{n}{k}\\
&amp;amp;omitting\;terms\;not\;in\;\pi \\
&amp;amp;\Rightarrow \ell(\pi) = k\:log\pi+(n-k)log(1-\pi) \\
&amp;amp;\ell^\prime(\pi) = \frac{k}{\pi}-\frac{n-k}{1-\pi} \\
&amp;amp; let\;\ell^\prime(\hat\pi) =0 \\
&amp;amp;\Rightarrow \frac{k}{\hat\pi}-\frac{n-k}{1-\hat\pi}=0\\
&amp;amp;\Rightarrow \frac{\hat\pi}{1-\hat\pi}=\frac{k}{n-k}\\
&amp;amp;\Rightarrow \frac{\hat\pi}{1-\hat\pi}=\frac{k/n}{1-k/n}\\
&amp;amp;\Rightarrow \hat\pi=\frac{k}{n} = p = \textbf{M}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Step 2. 將對數似然方程的二次微分 (二階導數)，確認在 MLE 爲極大值，並確認 &lt;span class=&#34;math inline&#34;&gt;\(S^2\)&lt;/span&gt;：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\ell^\prime(\pi) = \frac{k}{\pi}-\frac{n-k}{1-\pi} \\
&amp;amp;\ell^{\prime\prime}(\pi)=\frac{-k}{\pi^2}-\frac{n-k}{(1-\pi)^2} &amp;lt;0 \\
&amp;amp;\therefore at\;\textbf{MLE}\;\ell(\pi)\;has\;maximum \\
S^2&amp;amp;=\left.-\frac{1}{\ell^{\prime\prime}(\pi)}\right\vert_{\hat\pi=\hat{\hat\pi}=k/n=p}\\
&amp;amp;=\frac{1}{\frac{k}{\hat\pi^2}+\frac{n-k}{(1-\hat\pi)^2}}\\
&amp;amp;=\frac{\hat\pi^2(1-\hat\pi)^2}{k(1-\hat\pi)^2+(n-k)\hat\pi^2}\\
&amp;amp;=\frac{P^2(1-P)^2}{np(1-p)^2+(n-np)p^2}\\
&amp;amp;=\frac{p(1-p)}{n(1-p)+np}\\
&amp;amp;=\frac{p(1-p)}{n}\\
&amp;amp;\Rightarrow S=\sqrt{\frac{p(1-p)}{n}}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Step 3. 將求得的 MLE 和 &lt;span class=&#34;math inline&#34;&gt;\(S^2\)&lt;/span&gt; 代入近似信賴區間：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
95\% CI \;for \; \pi:\\
M\pm1.96S=p\pm1.96\sqrt{\frac{p(1-p)}{n}}\\
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;-parameter-transformations&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;參數轉化 parameter transformations&lt;/h3&gt;
&lt;p&gt;如果將參數 &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; 通過某種數學方程轉化成 &lt;span class=&#34;math inline&#34;&gt;\(g(\theta)\)&lt;/span&gt;，那麼我們可以認爲，轉化後的方程的 MLE 爲 &lt;span class=&#34;math inline&#34;&gt;\(g(\hat\theta)\)&lt;/span&gt;，其中 &lt;span class=&#34;math inline&#34;&gt;\(\hat\theta\)&lt;/span&gt; 是參數 &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; 的 MLE。&lt;/p&gt;
&lt;p&gt;類似地，如果 &lt;span class=&#34;math inline&#34;&gt;\(\theta_1 \sim \theta_2\)&lt;/span&gt; 是參數 &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; 的似然比信賴區間，那麼 &lt;span class=&#34;math inline&#34;&gt;\(g(\theta_1)\sim g(\theta_2)\)&lt;/span&gt; 就是 &lt;span class=&#34;math inline&#34;&gt;\(g(\theta)\)&lt;/span&gt; 的似然比信賴區間。&lt;/p&gt;
&lt;p&gt;以下爲轉換參數以後獲取信賴區間的步驟：&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;將參數通過某些數學方程（通常是取對數）轉化，使新的對數似然比方程更加接近二次方程的對稱圖形。&lt;br&gt; Transform parameter so that &lt;span class=&#34;math inline&#34;&gt;\(llr\)&lt;/span&gt; is closer to a quadratic shape.&lt;/li&gt;
&lt;li&gt;用本節學到的二次方程近似法，求得轉化後的參數的似然比信賴區間。 &lt;br&gt; Use our quadratic approximation on the transformed parameter to calculate our likelihood ratio confidence intervals.&lt;/li&gt;
&lt;li&gt;將第2步計算獲得的似然比信賴區間再通過轉化參數時的逆函數轉換回去，以獲得原參數的似然比信賴區間。&lt;br&gt; Transform the confidence intervals back, or to any scale we wish – they remain valid.&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;以泊松分佈爲例&lt;/h4&gt;
&lt;p&gt;當我們用泊松分佈模擬事件在某段時間內發生率 &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; 時，注意到這個事件發生率必須滿足 &lt;span class=&#34;math inline&#34;&gt;\(\lambda&amp;gt;0\)&lt;/span&gt;。當事件發生次數較低時，會讓似然方程的圖形被擠壓在低值附近。如果嘗試用對數轉換 &lt;span class=&#34;math inline&#34;&gt;\(\lambda \rightarrow log(\lambda)\)&lt;/span&gt; 此時 &lt;span class=&#34;math inline&#34;&gt;\(log(\lambda)\)&lt;/span&gt; 就不再被限制與 &lt;span class=&#34;math inline&#34;&gt;\(&amp;gt;0\)&lt;/span&gt;。下面我們嘗試尋找對數轉換過後的 &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; 和 &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;令 &lt;span class=&#34;math inline&#34;&gt;\(\beta=log(\lambda), \Rightarrow e^\beta=\lambda\)&lt;/span&gt; 從本文上半部分中我們已知 &lt;span class=&#34;math inline&#34;&gt;\(\hat\lambda=\frac{d}{p}\)&lt;/span&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;對數轉換以後的 &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; 是什麼? &lt;br&gt;根據定義，&lt;span class=&#34;math inline&#34;&gt;\(MLE(\beta)=MLE[log(\lambda)=log(\hat\lambda)]\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\Rightarrow M=\hat\beta=log(\frac{d}{p})\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;對數轉換以後的 &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; 是什麼? &lt;br&gt; 泊松分佈的對數似然方程是：&lt;span class=&#34;math inline&#34;&gt;\(\ell(\lambda|d)=d log(\lambda) - \lambda p\)&lt;/span&gt; 用 &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; 替換掉 &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;&lt;/p&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\begin{aligned} &amp;amp; \ell(\beta|d)=d \beta - pe^\beta\\ &amp;amp; \Rightarrow \ell^\prime(\beta)=d-pe^\beta \Rightarrow \ell^{\prime\prime}(\beta)=-pe^\beta \\ &amp;amp; S^2 = \left.-\frac{1}{\ell^{\prime\prime}(\beta)}\right\vert_{\beta=\hat{\beta}} = \left.\frac{1}{pe^\beta}\right\vert_{\beta=\hat{\beta}} = \frac{1}{pe^{log(d/p)}}\\ &amp;amp;\Rightarrow S^2=\frac{1}{d} \therefore S=\frac{1}{\sqrt{d}} \end{aligned}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;轉換後的近似二次方程：&lt;br&gt; &lt;span class=&#34;math inline&#34;&gt;\(\begin{aligned} &amp;amp; q(\beta) = -\frac{1}{2}(\frac{\beta-M}{S})^2 = -\frac{1}{2}(\frac{\beta-log(\frac{d}{p})}{\frac{1}{\sqrt{d}}})^2 \end{aligned}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; 的 &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; 信賴區間 &lt;span class=&#34;math inline&#34;&gt;\(=log(\frac{d}{p})\pm1.96\frac{1}{\sqrt{d}}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; 的 &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; 信賴區間 &lt;span class=&#34;math inline&#34;&gt;\(=exp(log(\frac{d}{p})\pm1.96\frac{1}{\sqrt{d}})\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;以二項分佈爲例&lt;/h4&gt;
&lt;p&gt;在研究對象 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 人中觀察到 &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; 個人患有某種疾病。&lt;/p&gt;
&lt;p&gt;令 &lt;span class=&#34;math inline&#34;&gt;\(\beta=log(\pi) \Rightarrow \pi=e^\beta\)&lt;/span&gt; 從上文的推倒也已知 &lt;span class=&#34;math inline&#34;&gt;\(\hat\pi=\frac{k}{n}=p\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\begin{aligned} &amp;amp;\Rightarrow \ell(\beta)=klog\pi+(n-k)log(1-\pi)=k\beta+(n-k)log(1-e^\beta) \\ &amp;amp;\Rightarrow \ell^{\prime}(\beta)=k-\frac{(n-k)(e^\beta)}{1-e^\beta} \\ &amp;amp;\Rightarrow \ell^{\prime\prime}(\beta)=-(n-k)\frac{e^\beta(1-e^\beta)+e^{2\beta}}{(1-e^\beta)^2} \\ &amp;amp; \ell^{\prime\prime}(\beta)= -(n-k)\frac{e^\beta}{(1-e^\beta)^2}\\ &amp;amp;\Rightarrow S^2 = \left.-\frac{1}{\ell^{\prime\prime}(\beta)}\right\vert_{\beta=\hat{\beta}} = \frac{(1-e^{\hat\beta})^2}{(n-k)e^{\hat\beta}} \\ &amp;amp;\because \hat\beta=log(\hat\pi) \\ &amp;amp;\therefore e^{\hat\beta} = \frac{k}{n}\\ &amp;amp;\Rightarrow S^2=\frac{(1-\frac{k}{n})^2}{(n-k)\frac{k}{n}}=\frac{n-k}{nk}=\frac{1}{k}-\frac{1}{n}\\ &amp;amp; \Rightarrow S=\sqrt{\frac{1}{k}-\frac{1}{n}}\\ \end{aligned}\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Exercise&lt;/h3&gt;
&lt;div id=&#34;q1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Q1&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;在&lt;span class=&#34;math inline&#34;&gt;\(n=100\)&lt;/span&gt;人中觀察到有&lt;span class=&#34;math inline&#34;&gt;\(k=40\)&lt;/span&gt;人患病，假設每個人只有患病，不患病兩個狀態，用二項分佈來模擬這個數據，&lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; 爲患病的概率。下面是 &lt;span class=&#34;math inline&#34;&gt;\(\pi \in [0.2,0.6]\)&lt;/span&gt; 區間的對數似然比方程曲線。&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pi &amp;lt;- seq(0.2, 0.6, by=0.01)
L &amp;lt;- (pi^40)*((1-pi)^60)
Lmax &amp;lt;- rep(max(L), 41)
LR &amp;lt;- L/Lmax
logLR &amp;lt;- log(LR)

plot(pi, logLR, type = &amp;quot;l&amp;quot;, ylim = c(-11, 0),yaxt=&amp;quot;n&amp;quot;,
     frame.plot = FALSE, ylab = &amp;quot;logLR(\U03C0)&amp;quot;, xlab = &amp;quot;\U03C0&amp;quot;)
grid(NA, 5, lwd = 2) # add some horizontal grid on the background
axis(2, at=seq(-12,0,2), las=2)
title(main = &amp;quot;Figure 1. Binomial log-likelihood ratio&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://winterwang.github.io/post/2017-11-07-approximate-log-likelihood-ratios_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;用一個二次方程來模擬上面的對數似然比曲線：&lt;span class=&#34;math inline&#34;&gt;\(f(\pi)=-\frac{(\pi-M)^2}{2S^2}\)&lt;/span&gt;，其中 &lt;span class=&#34;math inline&#34;&gt;\(M=\hat\pi=\frac{k}{n}=0.4\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(S^2=\frac{p(1-p)}{n}=0.0024\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mai = c(1.2, 0.5, 1, 0.7))
quad &amp;lt;- -(pi-0.4)^2/(2*0.0024)
plot(pi, quad, type = &amp;quot;l&amp;quot;, ylim = c(-4, 0),yaxt=&amp;quot;n&amp;quot;, col=&amp;quot;red&amp;quot;,
     frame.plot = FALSE, ylab = &amp;quot;&amp;quot;, xlab = &amp;quot;\U03C0&amp;quot;)
lines(pi, logLR, col=&amp;quot;black&amp;quot;)
grid(NA, 4, lwd = 1) # add some horizontal grid on the background
axis(2, at=seq(-4,0,1), las=2)
title(main = &amp;quot;Figure 2. Quadratic approximation\n of binomial log-likelihood ratio \n 40 out of 100 subjects&amp;quot;)
abline(h=-1.92, lty=1, col=&amp;quot;red&amp;quot;)
axis(4, at=-1.92, las=2)

legend(x=0.27, y= -5.5 ,xpd = TRUE,  legend=c(&amp;quot;logLR&amp;quot;,&amp;quot;Quadratic&amp;quot;), bty = &amp;quot;n&amp;quot;,
       col=c(&amp;quot;black&amp;quot;,&amp;quot;red&amp;quot;), lty=c(1,1), horiz = TRUE) #the legend is below the graph&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://winterwang.github.io/post/2017-11-07-approximate-log-likelihood-ratios_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;q2&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Q2&lt;/h4&gt;
&lt;p&gt;依舊使用二項分佈數據來模擬，觀察不同的事件數量和樣本量對近似計算的影響。&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;類比上面的問題，用同樣的 &lt;span class=&#34;math inline&#34;&gt;\(\hat\pi=0.4\)&lt;/span&gt;，但是 &lt;span class=&#34;math inline&#34;&gt;\(n=10, k=4\)&lt;/span&gt; 時的圖形：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://winterwang.github.io/post/2017-11-07-approximate-log-likelihood-ratios_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat\pi=0.4, n=1000, k=400\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://winterwang.github.io/post/2017-11-07-approximate-log-likelihood-ratios_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat\pi=0.01, n=100, k=1\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;注意此圖中紅線提示的近似二次曲線，信賴區間的下限已經低於0，是無法接受的近似。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://winterwang.github.io/post/2017-11-07-approximate-log-likelihood-ratios_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat\pi=0.01, n=1000, k=10\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://winterwang.github.io/post/2017-11-07-approximate-log-likelihood-ratios_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat\pi=0.01, n=10000, k=100\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://winterwang.github.io/post/2017-11-07-approximate-log-likelihood-ratios_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat\pi=0.99, n=100, k=99\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;注意此圖中紅線提示的近似二次曲線，信賴區間的上限已經大於1，和上面的 Figure 5. 一樣也是無法接受的近似。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://winterwang.github.io/post/2017-11-07-approximate-log-likelihood-ratios_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;總結： 二次方程近似時，在二項分佈的情況下，隨着 &lt;span class=&#34;math inline&#34;&gt;\(n, k\)&lt;/span&gt; 增加，近似越理想。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>對數似然比 Log-likelihood ratio</title>
      <link>https://winterwang.github.io/post/log-likelihood-ratio/</link>
      <pubDate>Sun, 05 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/log-likelihood-ratio/</guid>
      <description>&lt;div id=&#34;-log-likelihood-ratio&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;對數似然比 Log-likelihood ratio&lt;/h3&gt;
&lt;p&gt;對數似然比的想法來自於將對數似然方程圖形的 &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; 軸重新調節 (rescale) 使之最大值爲零。這可以通過計算該分佈方程的&lt;strong&gt;對數似然比 (log-likelihood ratio)&lt;/strong&gt; 來獲得：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[llr(\theta)=\ell(\theta|data)-\ell(\hat{\theta}|data)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;由於 &lt;span class=&#34;math inline&#34;&gt;\(\ell(\theta)\)&lt;/span&gt; 的最大值在 &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt; 時， 所以，&lt;span class=&#34;math inline&#34;&gt;\(llr(\theta)\)&lt;/span&gt; 就是個當 &lt;span class=&#34;math inline&#34;&gt;\(\theta=\hat{\theta}\)&lt;/span&gt; 時取最大值，且最大值爲零的方程。很容易理解我們叫這個方程爲對數似然比，因爲這個方程就是將似然比 &lt;span class=&#34;math inline&#34;&gt;\(LR(\theta)=\frac{L(\theta)}{L(\hat{\theta})}\)&lt;/span&gt; 取對數而已。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://winterwang.github.io/post/likelihood/&#34;&gt;之前&lt;/a&gt;我們也確證了，不包含我們感興趣的參數的方程部分可以忽略掉。還是用上一節 10人中4人患病的例子：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[L(\pi|X=4)=\binom{10}{4}\pi^4(1-\pi)^{10-4}\\
\Rightarrow \ell(\pi)=log[\pi^4(1-\pi)^{10-4}]\\
\Rightarrow llr(\pi)=\ell(\pi)-\ell(\hat{\pi})=log\frac{\pi^4(1-\pi)^{10-4}}{0.4^4(1-0.4)^{10-4}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其實由上也可以看出 &lt;span class=&#34;math inline&#34;&gt;\(llr(\theta)\)&lt;/span&gt; 只是將對應的似然方程的 &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; 軸重新調節了一下而已。形狀是沒有改變的：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow=c(1,2))
x &amp;lt;- seq(0,1,by=0.001)
y &amp;lt;- (x^4)*((1-x)^6)/(0.4^4*0.6^6)
z &amp;lt;- log((x^4)*((1-x)^6))-log(0.4^4*0.6^6)
plot(x, y, type = &amp;quot;l&amp;quot;, ylim = c(0,1.1),yaxt=&amp;quot;n&amp;quot;,
     frame.plot = FALSE, ylab = &amp;quot;LR(\U03C0)&amp;quot;, xlab = &amp;quot;\U03C0&amp;quot;)
axis(2, at=seq(0,1, 0.2), las=2)
title(main = &amp;quot;Binomial likelihood ratio&amp;quot;)
abline(h=1.0, lty=2)
segments(x0=0.4, y0=0, x1=0.4, y1=1, lty = 2)
plot(x, z, type = &amp;quot;l&amp;quot;, ylim = c(-10, 1), yaxt=&amp;quot;n&amp;quot;, frame.plot = FALSE,
     ylab = &amp;quot;llr(\U03C0)&amp;quot;, xlab = &amp;quot;\U03C0&amp;quot; )
axis(2, at=seq(-10, 0, 2), las=2)
title(main = &amp;quot;Binomial log-likelihood ratio&amp;quot;)
abline(h=0, lty=2)
segments(x0=0.4, y0=-10, x1=0.4, y1=0, lty = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://winterwang.github.io/post/2017-11-05-log-likelihood-ratio_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div class=&#34;section level4&#34;&gt;
&lt;h4&gt;正態分佈數據的最大似然和對數似然比&lt;/h4&gt;
&lt;p&gt;假設單個樣本 &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; 是來自一組服從正態分佈數據的觀察值：&lt;span class=&#34;math inline&#34;&gt;\(Y\sim N(\mu, \tau^2)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;那麼有：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(y|\mu) &amp;amp;= \frac{1}{\sqrt{2\pi\tau^2}}e^{(-\frac{1}{2}(\frac{y-\mu}{\tau})^2)} \\
\Rightarrow L(\mu|y) &amp;amp;=\frac{1}{\sqrt{2\pi\tau^2}}e^{(-\frac{1}{2}(\frac{y-\mu}{\tau})^2)} \\
\Rightarrow \ell(\mu)&amp;amp;=log(\frac{1}{\sqrt{2\pi\tau^2}})-\frac{1}{2}(\frac{y-\mu}{\tau})^2\\
omitting&amp;amp;\;terms\;not\;in\;\mu \\
&amp;amp;= -\frac{1}{2}(\frac{y-\mu}{\tau})^2 \\
\Rightarrow \ell^\prime(\mu) &amp;amp;= 2\cdot[-\frac{1}{2}(\frac{y-\mu}{\tau})\cdot\frac{-1}{\tau}] \\
&amp;amp;=\frac{y-\mu}{\tau^2} \\
let \; \ell^\prime(\mu) &amp;amp;= 0 \\
\Rightarrow \frac{y-\mu}{\tau^2} &amp;amp;= 0 \Rightarrow \hat{\mu} = y\\
\because \ell^{\prime\prime}(\mu) &amp;amp;=  \frac{-1}{\tau^2} &amp;lt; 0 \\
\therefore \hat{\mu} &amp;amp;= y \Rightarrow \ell(\hat{\mu}=y)_{max}=0 \\
llr(\mu)&amp;amp;=\ell(\mu)-\ell(\hat{\mu})=\ell(\mu)\\
&amp;amp;=-\frac{1}{2}(\frac{y-\mu}{\tau})^2
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;n-&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 個獨立正態分佈樣本的對數似然比&lt;/h3&gt;
&lt;p&gt;假設一組觀察值來自正態分佈 &lt;span class=&#34;math inline&#34;&gt;\(X_1,\cdots,X_n\stackrel{i.i.d}{\sim}N(\mu,\sigma^2)\)&lt;/span&gt;，先假設 &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; 已知。將觀察數據 &lt;span class=&#34;math inline&#34;&gt;\(x_1,\cdots, x_n\)&lt;/span&gt; 標記爲 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}\)&lt;/span&gt;。 那麼：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
L(\mu|\underline{x}) &amp;amp;=\prod_{i=1}^nf(x_i|\mu)\\
\Rightarrow \ell(\mu|\underline{x}) &amp;amp;=\sum_{i=1}^nlogf(x_i|\mu)\\
&amp;amp;=\sum_{i=1}^n[-\frac{1}{2}(\frac{x_i-\mu}{\sigma})^2]\\
&amp;amp;=-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2\\
&amp;amp;=-\frac{1}{2\sigma^2}[\sum_{i=1}^n(x_i-\bar{x})^2+\sum_{i=1}^n(\bar{x}-\mu)^2]\\
omitting&amp;amp;\;terms\;not\;in\;\mu \\
&amp;amp;=-\frac{1}{2\sigma^2}\sum_{i=1}^n(\bar{x}-\mu)^2\\
&amp;amp;=-\frac{n}{2\sigma^2}(\bar{x}-\mu)^2 \\
&amp;amp;=-\frac{1}{2}(\frac{\bar{x}-\mu}{\sigma/\sqrt{n}})^2\\
\because \ell(\hat{\mu}) &amp;amp;= 0 \\
\therefore llr(\mu) &amp;amp;= \ell(\mu)-\ell(\hat{\mu}) = \ell(\mu)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;n-&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 個獨立正態分佈樣本的對數似然比的分佈&lt;/h3&gt;
&lt;p&gt;假設我們用 &lt;span class=&#34;math inline&#34;&gt;\(\mu_0\)&lt;/span&gt; 表示總體均數這一參數的值。要注意的是，每當樣本被重新取樣，似然，對數似然方程，對數似然比都隨着觀察值而變 (即有自己的分佈)。&lt;/p&gt;
&lt;p&gt;考慮一個服從正態分佈的單樣本 &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;: &lt;span class=&#34;math inline&#34;&gt;\(Y\sim N(\mu_0,\tau^2)\)&lt;/span&gt;。那麼它的對數似然比：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[llr(\mu_0|Y)=\ell(\mu_0)-\ell(\hat{\mu})=-\frac{1}{2}(\frac{Y-\mu_0}{\tau})^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;根據&lt;a href=&#34;https://winterwang.github.io/post/chi-square-distribution/&#34;&gt;卡方分佈&lt;/a&gt;的定義：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\because \frac{Y-\mu_0}{\tau}\sim N(0,1)\\
\Rightarrow (\frac{Y-\mu_0}{\tau})^2 \sim \mathcal{X}_1^2\\
\therefore -2llr(\mu_0|Y) \sim \mathcal{X}_1^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;所以，如果有一組服從正態分佈的觀察值：&lt;span class=&#34;math inline&#34;&gt;\(X_1,\cdots,X_n\stackrel{i.i.d}{\sim}N(\mu_0,\sigma^2)\)&lt;/span&gt;，且 &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; 已知的話：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[-2llr(\mu_0|\bar{X})\sim \mathcal{X}_1^2\]&lt;/span&gt;&lt;/p&gt;
根據&lt;a href=&#34;https://winterwang.github.io/post/central-limit-theory/&#34;&gt;中心極限定理&lt;/a&gt;，可以將上面的結論一般化： 
&lt;div class=&#34;theorem&#34;&gt;
&lt;span id=&#34;thm:unnamed-chunk-2&#34; class=&#34;theorem&#34;&gt;&lt;strong&gt;Theorem 1  &lt;/strong&gt;&lt;/span&gt;如果 &lt;span class=&#34;math inline&#34;&gt;\(X_1,\cdots,X_n\stackrel{i.i.d}{\sim}f(x|\theta)\)&lt;/span&gt;。 那麼當重複多次從參數爲 &lt;span class=&#34;math inline&#34;&gt;\(\theta_0\)&lt;/span&gt; 的總體中取樣時，那麼統計量 &lt;span class=&#34;math inline&#34;&gt;\(-2llr(\theta_0)\)&lt;/span&gt; 會漸進於自由度爲 &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; 的卡方分佈： &lt;span class=&#34;math display&#34;&gt;\[-2llr(\theta_0)=-2\{\ell(\theta_0)-\ell(\hat{\theta})\}\xrightarrow[n\rightarrow\infty]{}\;\sim \mathcal{X}_1^2\]&lt;/span&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;似然比信賴區間&lt;/h3&gt;
&lt;p&gt;如果樣本量 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 足夠大 (通常應該大於 &lt;span class=&#34;math inline&#34;&gt;\(30\)&lt;/span&gt;)，根據上面的定理：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[-2llr(\theta_0)=-2\{\ell(\theta_0)-\ell(\hat{\theta})\}\sim \mathcal{X}_1^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;所以：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Prob(-2llr(\theta_0)\leqslant \mathcal{X}_{1,0.95}^2=3.84) = 0.95\\
\Rightarrow Prob(llr(\theta_0)\geqslant-3.84/2=-1.92) = 0.95\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;故似然比的 &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; 信賴區間就是能夠滿足 &lt;span class=&#34;math inline&#34;&gt;\(llr(\theta)=-1.92\)&lt;/span&gt; 的兩個 &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; 值。&lt;/p&gt;
&lt;div class=&#34;section level4&#34;&gt;
&lt;h4&gt;以二項分佈數據爲例&lt;/h4&gt;
&lt;p&gt;繼續用本文開頭的例子：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[llr(\pi)=\ell(\pi)-\ell(\hat{\pi})=log\frac{\pi^4(1-\pi)^{10-4}}{0.4^4(1-0.4)^{10-4}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;如果令 &lt;span class=&#34;math inline&#34;&gt;\(llr(\pi)=-1.92\)&lt;/span&gt; 在代數上可能較難獲得答案。然而從圖形上，如果我們在 &lt;span class=&#34;math inline&#34;&gt;\(y=-1.92\)&lt;/span&gt; 畫一條橫線，和該似然比方程曲線相交的兩個點就是我們想要求的信賴區間的上限和下限：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- seq(0,1,by=0.001)
z &amp;lt;- log((x^4)*((1-x)^6))-log(0.4^4*0.6^6)
plot(x, z, type = &amp;quot;l&amp;quot;, ylim = c(-10, 1), yaxt=&amp;quot;n&amp;quot;, frame.plot = FALSE,
     ylab = &amp;quot;llr(\U03C0)&amp;quot;, xlab = &amp;quot;\U03C0&amp;quot; )
axis(2, at=seq(-10, 0, 2), las=2)
abline(h=0, lty=2)
abline(h=-1.92, lty=2)
segments(x0=0.15, y0=-12, x1=0.15, y1=-1.92, lty = 2)
segments(x0=0.7, y0=-12, x1=0.7, y1=-1.92, lty = 2)
axis(1, at=c(0.15,0.7))
text(0.9, -1, &amp;quot;-1.92&amp;quot;)
arrows(0.8, -1.92, 0.8, 0, lty = 1, length = 0.08)
arrows( 0.8, 0, 0.8, -1.92, lty = 1, length = 0.08)
title(main = &amp;quot;Log-likelihood ratio for binomial example, \n with 95% likelihood confidence interval shown&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://winterwang.github.io/post/2017-11-05-log-likelihood-ratio_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;從上圖中可以讀出，&lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; 對數似然比信賴區間就是 &lt;span class=&#34;math inline&#34;&gt;\((0.15, 0.7)\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level4&#34;&gt;
&lt;h4&gt;以正態分佈數據爲例：&lt;/h4&gt;
&lt;p&gt;本文前半部分證明過， &lt;span class=&#34;math inline&#34;&gt;\(X_1,\cdots,X_n\stackrel{i.i.d}{\sim}N(\mu,\sigma^2)\)&lt;/span&gt;，先假設 &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; 已知。將觀察數據 &lt;span class=&#34;math inline&#34;&gt;\(x_1,\cdots, x_n\)&lt;/span&gt; 標記爲 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}\)&lt;/span&gt;。 那麼：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[llr(\mu|\underline{x}) = \ell(\mu|\underline{x})-\ell(\hat{\mu}) = \ell(\mu|\underline{x}) \\
=-\frac{1}{2}(\frac{\bar{x}-\mu}{\sigma/\sqrt{n}})^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;很顯然，這是一個關於 &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; 的二次方程，且最大值在 MLE &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mu}=\bar{x}\)&lt;/span&gt; 時取值 &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;。所以可以通過對數似然比法求出均值的 &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; 信賴區間公式：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[-2\times[-\frac{1}{2}(\frac{\bar{x}-\mu}{\sigma/\sqrt{n}})^2]=3.84\\
\Rightarrow L=\bar{x}-\sqrt{3.84}\frac{\sigma}{\sqrt{n}} \\
U=\bar{x}+\sqrt{3.84}\frac{\sigma}{\sqrt{n}} \\
note: \;\sqrt{3.84}=1.96\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;注意到這和我們&lt;a href=&#34;https://winterwang.github.io/post/frequentist-statistical-inference02/&#34;&gt;之前&lt;/a&gt;求的正態分佈均值的信賴區間公式完全一致。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>似然非然 Likelihood</title>
      <link>https://winterwang.github.io/post/likelihood/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/likelihood/</guid>
      <description>&lt;div id=&#34;-vs.probability-vs.inference&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;概率 vs. 推斷/Probability vs. Inference&lt;/h3&gt;
&lt;p&gt;在概率論的環境下，我們常常被告知的前提是：某某事件發生的概率是多少。例如： 一枚硬幣正面朝上的概率是 &lt;span class=&#34;math inline&#34;&gt;\(0.5\; Prob(coin\;landing\;heads)=0.5\)&lt;/span&gt;。然後在這個前提下，我們又繼續去計算複雜的事件發生的概率（例如，10次投擲硬幣以後4次正面朝上的概率是多少？）。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\binom{10}{4}\times(0.5^4)\times(0.5^{10-4}) = 0.205
\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dbinom(4, 10, 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2050781&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# or you can calculate by hand:
factorial(10)*(0.5^10)/(factorial(4)*(factorial(6)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2050781&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在統計推斷的理論中，我們考慮實際的情況，這樣的實際情況就是，我們通過觀察獲得數據，然而我們並不知道某事件發生的概率到底是多少（神如果存在話，只有神知道）。故這個 &lt;span class=&#34;math inline&#34;&gt;\(Prob(coin\;landing\;heads)\)&lt;/span&gt; 的概率大小對於“人類”來說是未知的。我們可能觀察到投擲了10次硬幣，其中有4次是正面朝上的。那麼我們從這一次觀察實驗中，需要計算的是能夠符合觀察結果的“最佳”概率估計 (best estimate)。在這種情況下，&lt;strong&gt;似然法 (likelihood)&lt;/strong&gt; 就是我們進行參數估計的最佳手段。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;似然和極大似然估計&lt;/h3&gt;
&lt;p&gt;此處用二項分佈的例子來理解似然法的概念：假設我們觀察到10個對象中有4個患病，我們假定這個患病的概率爲 &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt;。於是我們就有了下面的模型：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;模型：&lt;/strong&gt; 我們假定患病與否是一個服從&lt;strong&gt;二項分佈的隨機變量&lt;/strong&gt;，&lt;span class=&#34;math inline&#34;&gt;\(X\sim Bin(10,\pi)\)&lt;/span&gt;。同時也默認每個人之間是否患病是相互獨立的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;數據：&lt;/strong&gt; 觀察到的數據是，10人中有4人患病。於是 &lt;span class=&#34;math inline&#34;&gt;\(x=4\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;現在按照觀察到的數據，參數 &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; 變成了未知數：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Prob(X=4|\pi)=\binom{10}{4}\pi^4(1-\pi)^{10-4}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;此時我們會很自然的考慮，當 &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; 是未知數的時候，&lt;strong&gt;它取值爲多大的時候才能讓這個事件（即：10人中4人患病）發生的概率最大？&lt;/strong&gt; 所以我們可以將不同的數值代入 &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; 來計算該事件在不同概率的情況下發生的可能性到底是多少：&lt;/p&gt;
&lt;table class=&#34;table table-striped table-bordered&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;caption&gt;
Table 1: The probability of observing &lt;span class=&#34;math inline&#34;&gt;\(X=4\)&lt;/span&gt;
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
事件 &lt;span class=&#34;math inline&#34;&gt;\(X=4\)&lt;/span&gt; 發生的概率
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.000
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.088
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;0.4&lt;/strong&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;0.251&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.205
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.6
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.111
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.8
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.006
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.000
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;很顯然，如果 &lt;span class=&#34;math inline&#34;&gt;\(\pi=0.4\)&lt;/span&gt; 時，我們觀察到的事件發生的概率要比 &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; 取其它值時更大。於是小總結一下目前爲止的步驟如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;觀察到實驗數據（10人中4個患病）；&lt;/li&gt;
&lt;li&gt;假定這數據服從二項分佈的概率模型，計算不同（&lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; 的取值不同的）情況下，該事件按照假定模型發生的概率；&lt;/li&gt;
&lt;li&gt;通過比較，我們選擇了能夠讓觀察事件發生概率最高的參數取值 (&lt;span class=&#34;math inline&#34;&gt;\(\pi=0.4\)&lt;/span&gt;)。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;至此，我們可以知道，似然方程，是一個關於未知參數 &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; 的函數，我們目前位置做的就是找到這個函數的最大值 (maximised)，和使之成爲最大值時的 &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; ：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[L(\pi|X=4)=\binom{10}{4}\pi^4(1-\pi)^{10-4}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;我們可以畫出這個似然方程的形狀， &lt;span class=&#34;math inline&#34;&gt;\(\pi\in[0,1]\)&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- seq(0,1,by=0.001)
y &amp;lt;- (factorial(10)/(factorial(4)*(factorial(6))))*(x^4)*((1-x)^6)
plot(x, y, type = &amp;quot;l&amp;quot;, ylim = c(0,0.3), ylab = &amp;quot;L(\U03C0)&amp;quot;, xlab = &amp;quot;\U03C0&amp;quot;)
title(&amp;quot;Figure 1. Binomial Likelihood&amp;quot;)
abline(h=0.251, lty=2)
abline(v=0.4, lty=2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://winterwang.github.io/post/2017-11-02-likelihood_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;p&gt;從圖形上我們也能確認，&lt;span class=&#34;math inline&#34;&gt;\(\pi=0.4\)&lt;/span&gt; 時能夠讓這個似然方程取得最大值。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;似然方程的一般化定義&lt;/h3&gt;
&lt;p&gt;對於一個概率模型，如果其參數爲 &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;，那麼在給定觀察數據 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}\)&lt;/span&gt; 時，該參數的似然方程被定義爲：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(L(\theta|\underline{x})=P(\underline{x}|\theta)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;注意：&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(\underline{x}|\theta)\)&lt;/span&gt; 可以是概率（離散分佈）方程，也可以是概率密度（連續型變量）方程。對於此方程，&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; 是給定的，然後再計算某些事件發生的概率。&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(L(\theta|\underline{x})\)&lt;/span&gt; 是一個關於參數 &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; 的方程，此時，&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}\)&lt;/span&gt; 是固定不變的（觀察值）。我們希望通過這個方程求出能夠使觀察到的事件發生概率最大的參數值。&lt;/li&gt;
&lt;li&gt;似然方程&lt;strong&gt;不是&lt;/strong&gt;一個概率密度方程。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;另一個例子：&lt;/p&gt;
&lt;p&gt;有一組觀察數據是離散型隨機變量 &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;，它符合概率方程 &lt;span class=&#34;math inline&#34;&gt;\(f(x|\theta)\)&lt;/span&gt;。下表羅列了當 &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; 分別取值 &lt;span class=&#34;math inline&#34;&gt;\(1,2,3\)&lt;/span&gt; 時的概率方程的值，試求每個觀察值 &lt;span class=&#34;math inline&#34;&gt;\(X = 0,1,2,3,4\)&lt;/span&gt; 的最大似然參數估計：&lt;/p&gt;
&lt;table class=&#34;table table-striped table-bordered&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;caption&gt;
Exercise 1
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(f(x|1)\)&lt;/span&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(f(x|2)\)&lt;/span&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(f(x|3)\)&lt;/span&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1/3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1/4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1/3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1/4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1/4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1/6
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1/6
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1/4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1/2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1/6
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1/3
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class=&#34;table table-striped table-bordered&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;caption&gt;
Exercise 1 answer
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(f(x|1)\)&lt;/span&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(f(x|2)\)&lt;/span&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(f(x|3)\)&lt;/span&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1/3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1/4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;1&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1/3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1/4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;1&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1/4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1/6
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;2&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1/6
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1/4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1/2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;3&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1/6
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1/3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;3&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;-log-likelihood&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;對數似然方程 log-likelihood&lt;/h3&gt;
&lt;p&gt;似然方程的最大值，可通過求 &lt;span class=&#34;math inline&#34;&gt;\(L(\theta|data)\)&lt;/span&gt; 的最大值獲得，也可以通過求該方程的對數方程 &lt;span class=&#34;math inline&#34;&gt;\(\ell(\theta|data)\)&lt;/span&gt; 的最大值獲得。傳統上，我們估計最大方程的最大值的時候，會給參數戴一頂“帽子”（因爲這是觀察獲得的數據告訴我們的參數）： &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt;。並且我們發現對數似然方程比一般的似然方程更加容易微分，因此求似然方程的最大值就變成了求對數似然方程的最大值：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{d\ell}{d\theta}=\ell^\prime(\theta)=0\\
AND\\
\frac{d^2\ell}{d\theta^2}&amp;lt;0\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;要注意的是，微分不一定總是能幫助我們求得似然方程的最大值。如果說參數本身的定義域是有界限的話，微分就行不通了：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- seq(0,3,by=0.001)
y &amp;lt;- (x-1)^2-5
plot(x, y, type = &amp;quot;l&amp;quot;, ylim = c(-5,0-1), ylab = &amp;quot;L(\U03B8)&amp;quot;, xlab = &amp;quot;\U03B8&amp;quot;)
title(&amp;quot;Figure 2. Likelihood function with \n a limited domain&amp;quot;)
abline(v=3, lty=2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://winterwang.github.io/post/2017-11-02-likelihood_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;-lthetadata--ellthetadata-&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;證明：當 &lt;span class=&#34;math inline&#34;&gt;\(L(\theta|data)\)&lt;/span&gt; 取最大值時，該方程的對數方程 &lt;span class=&#34;math inline&#34;&gt;\(\ell(\theta|data)\)&lt;/span&gt; 也是最大值：&lt;/h4&gt;
&lt;p&gt;如果似然方程是連續可導，只有一個最大值，且可以二次求導，假設 &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt; 使該方程取最大值，那麼：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{dL}{d\theta}=0, \frac{d^2L}{d\theta^2}&amp;lt;0 \Rightarrow \theta=\hat{\theta}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;令 &lt;span class=&#34;math inline&#34;&gt;\(\ell=logL\)&lt;/span&gt; 那麼 &lt;span class=&#34;math inline&#34;&gt;\(\frac{d\ell}{dL}=\ell^\prime=\frac{1}{L}\)&lt;/span&gt;：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{d\ell}{d\theta}=\frac{d\ell}{dL}\cdot\frac{dL}{d\theta}=\frac{1}{L}\cdot\frac{dL}{d\theta}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;當 &lt;span class=&#34;math inline&#34;&gt;\(\ell(\theta|data)\)&lt;/span&gt; 取最大值時：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{d\ell}{d\theta}=0\Leftrightarrow\frac{1}{L}\cdot\frac{dL}{d\theta}=0\\
\because \frac{1}{L}\neq0 \\
\therefore \frac{dL}{d\theta}=0\\
\Leftrightarrow \theta=\hat{\theta}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\frac{d^2\ell}{d\theta^2} &amp;amp;= \frac{d}{d\theta}(\frac{d\ell}{dL}\cdot\frac{dL}{d\theta})\\
 &amp;amp;= \frac{d\ell}{dL}\cdot\frac{d^2L}{d\theta^2} + \frac{dL}{d\theta}\cdot\frac{d}{d\theta}(\frac{d\ell}{dL})
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;當 &lt;span class=&#34;math inline&#34;&gt;\(\theta=\hat{\theta}\)&lt;/span&gt; 時，&lt;span class=&#34;math inline&#34;&gt;\(\frac{dL}{d\theta}=0\)&lt;/span&gt; 且 &lt;span class=&#34;math inline&#34;&gt;\(\frac{d^2L}{d\theta^2}&amp;lt;0 \Rightarrow \frac{d^2\ell}{d\theta^2}&amp;lt;0\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;所以，求獲得 &lt;span class=&#34;math inline&#34;&gt;\(\ell(\theta|data)\)&lt;/span&gt; 最大值的 &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; 即可令 &lt;span class=&#34;math inline&#34;&gt;\(L(\theta|data)\)&lt;/span&gt; 獲得最大值。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;-maximum-likelihood-estimator-mle-&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;極大似然估計 (maximum likelihood estimator, MLE) 的性質：&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;漸進無偏 Asymptotically unbiased: &lt;br&gt; &lt;span class=&#34;math inline&#34;&gt;\(n\rightarrow \infty \Rightarrow E(\hat{\Theta}) \rightarrow \theta\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;漸進最高效能 Asymptotically efficient: &lt;br&gt; &lt;span class=&#34;math inline&#34;&gt;\(n\rightarrow \infty \Rightarrow Var(\hat{\Theta})\)&lt;/span&gt; 是所有參數中方差最小的估計&lt;/li&gt;
&lt;li&gt;漸進正態分佈 Asymptotically normal: &lt;br&gt; &lt;span class=&#34;math inline&#34;&gt;\(n\rightarrow \infty \Rightarrow \hat{\Theta} \sim N(\theta, Var(\hat{\Theta}))\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;變形後依然保持不變 Transformation invariant: &lt;br&gt; &lt;span class=&#34;math inline&#34;&gt;\(\hat{\Theta}\)&lt;/span&gt; 是 &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; 的MLE時 &lt;span class=&#34;math inline&#34;&gt;\(\Rightarrow g(\hat{\Theta})\)&lt;/span&gt; 是 &lt;span class=&#34;math inline&#34;&gt;\(g(\theta)\)&lt;/span&gt; 的 MLE&lt;/li&gt;
&lt;li&gt;信息足夠充分 Sufficient：&lt;br&gt; &lt;span class=&#34;math inline&#34;&gt;\(\hat{\Theta}\)&lt;/span&gt; 包含了觀察數據中所有的能夠用於估計參數的信息&lt;/li&gt;
&lt;li&gt;始終不變 consistent: &lt;br&gt; &lt;span class=&#34;math inline&#34;&gt;\(n\rightarrow\infty\Rightarrow\hat{\Theta}\rightarrow\theta\)&lt;/span&gt; 或者可以寫成：&lt;span class=&#34;math inline&#34;&gt;\(\varepsilon&amp;gt;0, lim_{n\rightarrow\infty}P(|\hat{\Theta}-\theta|&amp;gt;\varepsilon)=0\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;-likelihood-for-a-rate&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;率的似然估計 Likelihood for a rate&lt;/h3&gt;
&lt;p&gt;如果在一項研究中，參與者有各自不同的追蹤隨訪時間（長度），那麼我們應該把事件（疾病）的發病率用率的形式（多少事件每單位人年, e.g. per person year of observation）。如果這個發病率的參數用 &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; 來表示，所有參與對象的隨訪時間之和爲 &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; 人年。那麼這段時間內的期望事件（疾病發病）次數爲：&lt;span class=&#34;math inline&#34;&gt;\(\mu=\lambda p\)&lt;/span&gt;。假設事件（疾病發病）發生是相互獨立的，可以使用泊松分佈來模擬期望事件（疾病發病）次數 &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt;：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[D\sim Poi(\mu)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;假設我們觀察到了 &lt;span class=&#34;math inline&#34;&gt;\(D=d\)&lt;/span&gt; 個事件，我們獲得這個觀察值的概率應該用以下的模型：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Prob(D=d)=e^{-\mu}\frac{\mu^d}{d!}=e^{-\lambda p}\frac{\lambda^dp^d}{d!}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;因此，&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; 的似然方程是：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[L(\lambda|observed \;data)=e^{-\lambda p}\frac{\lambda^dp^d}{d!}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;所以，&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; 的對數似然方程是：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\ell(\lambda|observed\;data) &amp;amp;= log(e^{-\lambda p}\frac{\lambda^dp^d}{d!}) \\
  &amp;amp;= -\lambda p+d\:log(\lambda)+d\:log(p)-log(d!) \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;解 &lt;span class=&#34;math inline&#34;&gt;\(\ell^\prime(\lambda|data)=0\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\ell^\prime(\lambda|data) &amp;amp;= -p+\frac{d}{\lambda}=0\\
\Rightarrow \hat{\lambda} &amp;amp;= \frac{d}{p} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;注意：&lt;/strong&gt; 在對數似然方程中，不包含參數的部分，對與似然方程的形狀不產生任何影響，我們在微分對數似然方程的時候，這部分也都自動消失。所以不包含參數的部分，與我們如何獲得極大似然估計是無關的。因此，我們常常在寫對數似然方程的時候就把其中沒有參數的部分直接忽略了。例如上面泊松分佈的似然方程中，&lt;span class=&#34;math inline&#34;&gt;\(d\:log(p)-log(d!)\)&lt;/span&gt; 不包含參數 &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; 可以直接不寫出來。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;-n-&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;有 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 個獨立觀察時的似然方程和對數似然方程&lt;/h3&gt;
&lt;p&gt;當有多個獨立觀察時，總體的似然方程等於各個觀察值的似然方程之&lt;strong&gt;乘積&lt;/strong&gt;。如果 &lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\stackrel{i.i.d}{\sim}f(\cdot|\theta)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[L(\theta|x_1,\cdots,x_n)=f(x_1,\cdots,x_n|\theta)=\prod_{i=1}^nf(x_i|\theta)\\
\Rightarrow \ell(\theta|x_1,\cdots,x_n)=\sum_{i=1}^nlog(f(x_i|\theta))\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>卡方分佈 chi square distribution</title>
      <link>https://winterwang.github.io/post/chi-square-distribution/</link>
      <pubDate>Thu, 26 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/chi-square-distribution/</guid>
      <description>&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;卡方分佈的期望和方差的證明：&lt;/h3&gt;
&lt;p&gt;當 &lt;span class=&#34;math inline&#34;&gt;\(X\sim N(0,1)\)&lt;/span&gt; 時， &lt;span class=&#34;math inline&#34;&gt;\(X^2\sim \mathcal{X}_1^2\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;如果 &lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots, X_n\stackrel{i.i.d}{\sim} N(0,1)\)&lt;/span&gt;， 那麼 &lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^nX_i^2\sim\mathcal{X}_n^2\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中： &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{X}_n^2\)&lt;/span&gt; 表示自由度爲 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 的卡方分佈。&lt;/p&gt;
&lt;p&gt;且 &lt;span class=&#34;math inline&#34;&gt;\(X_m^2+X_n^2=\mathcal{X}_{m+n}^2\)&lt;/span&gt;&lt;/p&gt;
&lt;div class=&#34;section level4&#34;&gt;
&lt;h4&gt;卡方分佈的期望：&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[E(X_1^2)=Var(X)+[E(X)]^2=1+0=1\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\Rightarrow E(X_n^2)=n\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level4&#34;&gt;
&lt;h4&gt;卡方分佈的方差：&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
Var(X_1^2) &amp;amp;= E(X_1^{2^2}) - E(X_1^2)^2 \\
           &amp;amp;= E(X_1^4)-1
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;-ex_14&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;下面來求 &lt;span class=&#34;math inline&#34;&gt;\(E(X_1^4)\)&lt;/span&gt;&lt;/h5&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\because E(X_1) &amp;amp;= \int_{-\infty}^{+\infty} xf(x)dx \\
\therefore E(X_1^4) &amp;amp;= \int_{-\infty}^{+\infty} x^4f(x)dx
\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;已知： &lt;span class=&#34;math inline&#34;&gt;\(f(x)=\frac{1}{\sqrt{2\pi}}e^{(-\frac{x^2}{2})}\)&lt;/span&gt; 代入上式：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
E(X_1^4) &amp;amp;= \int_{-\infty}^{+\infty} x^4f(x)dx \\
         &amp;amp;= \int_{-\infty}^{+\infty} x^4\frac{1}{\sqrt{2\pi}}e^{(-\frac{x^2}{2})}dx\\
         &amp;amp;=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}x^4e^{(-\frac{x^2}{2})}dx\\
         &amp;amp;=\frac{-1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}x^3(-x)e^{(-\frac{x^2}{2})}dx
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;令 &lt;span class=&#34;math inline&#34;&gt;\(u=x^3, v=e^{(-\frac{x^2}{2})},t=-\frac{x^2}{2}\)&lt;/span&gt; 可以推導：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\frac{dv}{dx} &amp;amp;= \frac{dv}{dt}\frac{dt}{dx} \\
              &amp;amp;= e^t(-\frac{1}{2}\times2x) \\
              &amp;amp;= (-x)e^{(-\frac{x^2}{2})} \\
\Rightarrow dv &amp;amp;= (-x)e^{(-\frac{x^2}{2})}dx
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;再代入上面的式子：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
E(X_1^4) &amp;amp;= \frac{-1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}u\:dv \\
integrate\; &amp;amp;by\; parts:\\
E(X_1^4) &amp;amp;= \frac{-1}{\sqrt{2\pi}}\{[u\:v] \rvert_{-\infty}^{+\infty}-\int_{-\infty}^{+\infty}v\:du\} \\
&amp;amp;= \frac{-1}{\sqrt{2\pi}}\{[x^3e^{(-\frac{x^2}{2})}]\rvert_{-\infty}^{+\infty} -\int_{-\infty}^{+\infty}v\:du\} \\
&amp;amp;=\frac{-1}{\sqrt{2\pi}}\{0-0-\int_{-\infty}^{+\infty}e^{(-\frac{x^2}{2})}dx^3\} \\
&amp;amp;=\frac{-1}{\sqrt{2\pi}}[-3\int_{-\infty}^{+\infty}x^2e^{(-\frac{x^2}{2})}dx] \\
&amp;amp;=\frac{-3}{\sqrt{2\pi}}[\int_{-\infty}^{+\infty}x(-x)e^{(-\frac{x^2}{2})}dx] \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;再來一次分部積分：&lt;/p&gt;
&lt;p&gt;令 &lt;span class=&#34;math inline&#34;&gt;\(a=x,b=e^{(-\frac{x^2}{2})},d\:b = (-x)e^{(-\frac{x^2}{2})}dx\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
E(X_1^4) &amp;amp;= \frac{-3}{\sqrt{2\pi}}\{[a\:b] \rvert_{-\infty}^{+\infty} - \int_{-\infty}^{+\infty}b\:da\} \\
&amp;amp;=\frac{-3}{\sqrt{2\pi}}\{[xe^{(-\frac{x^2}{2})}]\rvert_{-\infty}^{+\infty} -\int_{-\infty}^{+\infty}b\:da\} \\
&amp;amp;=\frac{-3}{\sqrt{2\pi}}\{0-0-\int_{-\infty}^{+\infty}e^{(-\frac{x^2}{2})}dx\} \\
&amp;amp;=\frac{-3}{\sqrt{2\pi}}[-\int_{-\infty}^{+\infty}e^{(-\frac{x^2}{2})}dx] \\
&amp;amp;=\frac{3}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}e^{(-\frac{x^2}{2})}dx
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;下面令 &lt;span class=&#34;math inline&#34;&gt;\(I=\int_{-\infty}^{+\infty}e^{(-\frac{x^2}{2})}dx\\ \Rightarrow I^2=\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}e^{(-\frac{x^2+y^2}{2})}dxdy\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;接下來需要用到 &lt;a href=&#34;https://www.youtube.com/watch?v=r0fv9V9GHdo&#34;&gt;座標轉換&lt;/a&gt;的知識，將 &lt;span class=&#34;math inline&#34;&gt;\(x,y\)&lt;/span&gt; 表示的笛卡爾座標，轉換爲用角度 &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; 和半徑 &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; 表示的形式。之後的證明可以在&lt;a href=&#34;https://www.youtube.com/watch?v=fWOGfzC3IeY&#34;&gt;油管&lt;/a&gt;上看到，但是我還是繼續證明下去。&lt;/p&gt;
&lt;p&gt;直角座標系 (cartesian coordinators) 和 極座標系 (polar coordinators) 之間轉換的關係如下：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
x&amp;amp;=r\:cos\theta\\
y&amp;amp;=r\:sin\theta\\
r^2&amp;amp;=x^2+y^2\\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;座標轉換以後可以繼續求 &lt;span class=&#34;math inline&#34;&gt;\(E(X_1^4)\)&lt;/span&gt;。 在那之前我們先求 &lt;span class=&#34;math inline&#34;&gt;\(I^2\)&lt;/span&gt;。 注意轉換座標系統以後，&lt;span class=&#34;math inline&#34;&gt;\(\theta\in[0,2\pi], r\in[0,+\infty]\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I^2 &amp;amp;= \int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}e^{(-\frac{x^2+y^2}{2})}dxdy \\
&amp;amp;= \int_{0}^{+\infty}\int_{0}^{2\pi}e^{(-\frac{r^2}{2})}rd\theta dr \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;由於先從中間的 &lt;span class=&#34;math inline&#34;&gt;\(\int_{0}^{2\pi}e^{(-\frac{r^2}{2})}rd\theta\)&lt;/span&gt; 開始積分，&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; 以外都可以視爲常數，那麼這個 &lt;span class=&#34;math inline&#34;&gt;\([0,2\pi]\)&lt;/span&gt; 上的積分就的等於 &lt;span class=&#34;math inline&#34;&gt;\(2\pi e^{(-\frac{r^2}{2})}r\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;因此上面的式子又變爲：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I^2 &amp;amp;=  2\pi\int_{0}^{+\infty}e^{(-\frac{r^2}{2})}r\:dr \\
\because \frac{d(e^{\frac{-r^2}{2}})}{dr} &amp;amp;= -e^{(-\frac{r^2}{2})}r \\
\therefore I^2 &amp;amp;= 2\pi(-e^{\frac{-r^2}{2}})\rvert_0^{+\infty} \\
               &amp;amp;= 0-(2\pi\times(-1)) \\
               &amp;amp;= 2\pi\\
\Rightarrow I  &amp;amp;= \sqrt{2\pi}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;所以，&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
E(X_1^4) &amp;amp;= \frac{3}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}e^{(-\frac{x^2}{2})}dx \\
&amp;amp;= \frac{3}{\sqrt{2\pi}}\times I \\
&amp;amp;= 3 \\
\Rightarrow Var(X_1^2) &amp;amp;= E(X_1^4) - 1 \\
                       &amp;amp;= 3-1 =2 \\
\Rightarrow Var(X_n^2) &amp;amp;= 2n
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;結論：&lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots, X_n\stackrel{i.i.d}{\sim} N(0,1)\)&lt;/span&gt; 時，&lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^nX_i^2\sim\mathcal{X}_n^2\)&lt;/span&gt; 服從卡方分佈，其期望 &lt;span class=&#34;math inline&#34;&gt;\(E(X_n^2)=n\)&lt;/span&gt;，方差 &lt;span class=&#34;math inline&#34;&gt;\(Var(X_n^2)=2n\)&lt;/span&gt;。 根據&lt;a href=&#34;https://winterwang.github.io/post/central-limit-theory/&#34;&gt;中心極限定理&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[n\rightarrow \infty, X_n^2\sim N(n, 2n)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>估計和精確度的概念</title>
      <link>https://winterwang.github.io/post/frequentist-statistical-inference02/</link>
      <pubDate>Tue, 24 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/frequentist-statistical-inference02/</guid>
      <description>&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;估計量和他們的樣本分佈&lt;/h3&gt;
&lt;p&gt;例子： 最大呼氣量 (Forced Expoiratory Volume in one second, FEV1) 用於測量一個人的肺功能，它的測量值是連續的。我們從前來門診的人中隨機抽取 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 人作爲樣本，用這個樣本的 FEV1 平均值來估計這個診所的患者的平均肺功能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;模型假設：&lt;/strong&gt; 在這個例子中，我們的假設有如下：每個隨機抽取的 FEV1 測量值都是從同一個總體（人羣）中抽取，每一個觀察值 &lt;span class=&#34;math inline&#34;&gt;\(Y_i\)&lt;/span&gt; 都互相獨立互不影響。我們用縮寫 iid 表示這些隨機抽取的樣本是服從獨立同分佈 (independent and identically distributed)。另外，總體的分佈也假定爲正態分佈，且總體均值爲 &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;，總體方差爲 &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;。那麼這個模型可以簡單的被寫成：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y_i \stackrel{i.i.d}{\sim} N(\mu, \sigma^2), i=1,2,\dots,n\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;總體均值 &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; 的估計量：&lt;/strong&gt; 顯然算術平均值: &lt;span class=&#34;math inline&#34;&gt;\(\bar{Y}=\frac{1}{n}\sum_{i=1}^ny_i\)&lt;/span&gt; 是我們用於估計總體均值的估計量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;估計量的樣本分佈：&lt;/strong&gt; &lt;span class=&#34;math display&#34;&gt;\[\bar{Y}\stackrel{i.i.d}{\sim}N(\mu, \frac{\sigma^2}{n})\]&lt;/span&gt;&lt;/p&gt;
&lt;div class=&#34;section level4&#34;&gt;
&lt;h4&gt;證明&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
E(\bar{Y}) &amp;amp;= E(\frac{1}{n}\sum Y_i) \\
           &amp;amp;= \frac{1}{n}E(\sum Y_i) \\
           &amp;amp;= \frac{1}{n}\sum E(Y_i) \\
           &amp;amp;= \frac{1}{n}n\mu = \mu \\
Var(\bar{Y}) &amp;amp;= Var(\frac{1}{n}\sum Y_i) \\
\because Y_i \;are &amp;amp;\; independent   \\
            &amp;amp;= \frac{1}{n^2}\sum Var(Y_i) \\
            &amp;amp;= \frac{1}{n^2} n Var(Y_i) \\
            &amp;amp;= \frac{\sigma^2}{n}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;-zfracbary-musqrtvarbary--zsim-n01&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;證明當 &lt;span class=&#34;math inline&#34;&gt;\(Z=\frac{\bar{Y}-\mu}{\sqrt{Var(\bar{Y})}}\)&lt;/span&gt; 時， &lt;span class=&#34;math inline&#34;&gt;\(Z\sim N(0,1)\)&lt;/span&gt;:&lt;/h4&gt;
&lt;p&gt;由式子可知， &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; 只是由一組服從正態分佈的數據 &lt;span class=&#34;math inline&#34;&gt;\(\bar{Y}\)&lt;/span&gt; 線性轉換 (linear transformation) 而來，所以 &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; 本身也服從正態分佈 &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
E(Z) &amp;amp;= \frac{1}{\sqrt{Var(\bar{Y})}}E[\bar{Y}-\mu] \\
     &amp;amp;= \frac{1}{\sqrt{Var(\bar{Y})}}[\mu-\mu] = 0 \\
Var(Z) &amp;amp;= \frac{1}{Var(\bar{Y})}Var[\bar{Y}-\mu] \\
       &amp;amp;= \frac{1}{Var(\bar{Y})}Var(\bar{Y}) =1 \\
\therefore Z \;&amp;amp;\sim N(0,1)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;均值 &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; 的信賴區間：&lt;/strong&gt; 上節說道，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;信賴區間通常是成對成對的出現的，即有上限和下限。這樣的一對從樣本數據中計算得來的統計量，同樣也是有樣本分佈的。&lt;strong&gt;每次我們重新從總體或人羣中抽樣，計算獲得的信賴區間都不同，這些信賴區間就組成了信賴區間的樣本分佈。總體和人羣的參數落在這些信賴區間範圍內的概率，就是我們常說的信賴區間的水平（&lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt;）。&lt;/strong&gt; 常用的這個概率值就是 &lt;span class=&#34;math inline&#34;&gt;\(95\%, 90\%, 99\%\)&lt;/span&gt;。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;假定我們用 &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; 作爲信賴區間的水平。那麼下面我們嘗試推導一下信賴區間的計算公式。從長遠來說（也就是假設我們從總體中抽樣無數次，每次都進行信賴區間的計算，也獲得無數個信賴區間），這些信賴區間中有 &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; 是包含了總體的真實均值（但是卻是未知）的，而且這些信賴區間由於是從一個服從正態分佈的數據而來，它們也服從正態分佈（對真實均值左右對稱）。所以我們有理由相信，可以找到一個數值 &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Prob(\bar{Y} &amp;gt; \mu+c) = 0.025 \\
  Prob(\bar{Y} &amp;lt; \mu-c) = 0.025\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;因此，我們可以定義 &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; 信賴區間的上限和下限分別是：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[L=\bar{Y}-c \Rightarrow Prob(L&amp;gt;\mu)=0.025 \\
  U=\bar{Y}+c \Rightarrow Prob(U&amp;lt;\mu)=0.025\]&lt;/span&gt;&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://winterwang.github.io/img/Selection_082.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;接下來就是推倒（故意的）&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; 的過程啦：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
Prob(\bar{Y}&amp;gt;\mu+c)=Prob(\bar{Y}-\mu&amp;gt;c) \;&amp;amp;= 0.025 \\
\Rightarrow Prob(\frac{\bar{Y}-\mu}{\sqrt{Var(\bar{Y})}} &amp;gt; \frac{c}{\sqrt{Var(\bar{Y})}}) \;&amp;amp;= 0.025 \\
\Rightarrow Prob(Z&amp;gt;\frac{c}{\sqrt{Var(\bar{Y})}}) \;&amp;amp;= 0.025 \\
上面已經證明了 Z\sim N(0,1) \\
而且我們也已知 Prob(Z&amp;gt;1.96) \;&amp;amp;= 0.025 \\
所以只要令 \frac{c}{\sqrt{Var(\bar{Y})}} =1.96 \\
\Rightarrow c=1.96\sqrt{Var(\bar{Y})} \\
所以總體均值\mu 的 95\% 信賴區間就是: \\
\mu = \bar{Y}\pm1.96\sqrt{Var(\bar{Y})}=\bar{Y}\pm &amp;amp; 1.96\frac{\sigma}{\sqrt{n}}\\
其中，\sqrt{Var(\bar{Y})} 就是我們熟知的估計量 \bar{Y} &amp;amp;的標準誤。
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;估計量的特質&lt;/h3&gt;
&lt;p&gt;考慮以下的問題：&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;什麼因素決定了一個估計量 (estimator) 的好壞，是否實用？&lt;/li&gt;
&lt;li&gt;如果有其他的可選擇估計量，該如何取捨呢？&lt;/li&gt;
&lt;li&gt;當情況複雜的時候，我們該如何尋找合適的估計量？&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;section level4&#34;&gt;
&lt;h4&gt;偏倚&lt;/h4&gt;
&lt;p&gt;假設 &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; 是我們估計總體參數 &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; 的一個估計量。一般來說我們希望估計量的樣本分佈可以在 &lt;code&gt;“正確的位置”&lt;/code&gt; 左右均勻分佈。換句話說我們希望：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[E(T)=\theta\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;如果實現了這個條件，我們說這樣的估計量是無偏的 (&lt;code&gt;unbiased&lt;/code&gt;)。然而，天下哪有這等好事，我們叫真實值和估計量之間的差距爲偏倚：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[bias(T) = E(T)-\theta\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其實偏倚完全等於零並不是最重要，許多常見的估計量都是有偏倚的。重要的是，這個偏倚會隨着樣本量的增加而逐漸趨近於零。所以我們就可以認爲這樣的估計量是漸進無偏的 (asymptotically unbiased)：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[T\;is\;an\;\textbf{unbiased}\;estimator\;for\;\theta\;if\;\\E(T)=\theta\\
T\;is\;an\;\textbf{asymptotically unbiased}\;estimator\;for\;\theta\;if\;\\lim_{n\rightarrow\infty}E(T)=\theta\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;-efficiency&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;估計量的效能 Efficiency&lt;/h4&gt;
&lt;p&gt;通常，我們希望一個估計量 (estimator) 的偏倚要小，同時，它的樣本分佈也希望能儘可能的不要波動太大。換句話說，我們還希望估計量的方差越小越好。&lt;/p&gt;
&lt;p&gt;如果說，兩個估計量有相同的偏倚，均可以選擇來推斷總體，我們說，其中樣本分佈的方差小的那個（波動幅度小）的那個估計量是相對更好的。因爲樣本分佈方差越小，說明可以&lt;strong&gt;更加精確的&lt;/strong&gt;估計總體參數。這兩個估計量的方差之比：&lt;span class=&#34;math inline&#34;&gt;\(Var(S)/Var(T)\)&lt;/span&gt; 被叫做這兩個估計量的&lt;strong&gt;相對效能 (relative efficiency)&lt;/strong&gt;。所以我們用估計量去推斷總體時，需要選用效能最高，精確度最好的估計量 &lt;strong&gt;(the minimum variance unbiased estimator/an efficient estimator)&lt;/strong&gt;。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level4&#34;&gt;
&lt;h4&gt;均值和中位數的相對效能&lt;/h4&gt;
&lt;p&gt;在一個服從 &lt;span class=&#34;math inline&#34;&gt;\(N(\mu,\sigma^2)\)&lt;/span&gt; 正態分佈的數據中，中位數和均值是一樣的，也都同時等於總體均值參數 &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;。而且，樣本均數 &lt;span class=&#34;math inline&#34;&gt;\(\bar{Y}\)&lt;/span&gt; 和樣本中位數 &lt;span class=&#34;math inline&#34;&gt;\(\dot{Y}\)&lt;/span&gt; 都是對總體均值的無偏估計量。那麼應該選用中位數還是平均值呢？&lt;/p&gt;
&lt;p&gt;之前證明過當 &lt;span class=&#34;math inline&#34;&gt;\(Y_i \sim N(\mu,\sigma^2)\)&lt;/span&gt; 時， &lt;span class=&#34;math inline&#34;&gt;\(Var(\bar{Y}=\sigma^2/n)\)&lt;/span&gt;。然而，當 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 較大的時候，可以證明的是：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Var(\dot{Y})=\frac{\pi}{2}\frac{\sigma^2}{n}\approx1.571\frac{\sigma^2}{n}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;因此，這兩個估計量的相對效能就是：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{Var(\dot{Y})}{Var(\bar{Y})}\approx1.571\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;所以總體是正態分佈時，平均值就是較中位數更適合用來估計總體的估計量。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;-mean-square-error-mse&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;均方差 mean square error (MSE)&lt;/h3&gt;
&lt;p&gt;兩個估計量的偏倚不同時，可以比較他們和總體參數之間的差距，這被叫做均方差, Mean Square Error (MSE)。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[MSE(T)=E[(T-\theta)^2]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;這裏用一個數學技巧，將式子中的估計量和總體參數之間的差，分成兩個部分：一是估計量本身的方差 (&lt;span class=&#34;math inline&#34;&gt;\(T-E(T)\)&lt;/span&gt;)，一是估計量的偏倚 (&lt;span class=&#34;math inline&#34;&gt;\(E(T)-\theta\)&lt;/span&gt;)。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
MSE(T) &amp;amp;= E[(T-\theta)^2] \\
       &amp;amp;= E\{[T-E(T)+E(T)-\theta]^2\} \\
       &amp;amp;= E\{[T-E(T)]^2+[E(T)-\theta]^2 \\
       &amp;amp; \;\;\;\;\; \;\;+2[T-E(T)][E(T)-\theta]\} \\
       &amp;amp;= E\{[T-E(T)]^2\}+E\{[E(T)-\theta]^2\} + 0\\
       &amp;amp;= Var(T) + [bias(T)^2]
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;總體方差的估計，自由度&lt;/h3&gt;
&lt;p&gt;如果 &lt;span class=&#34;math inline&#34;&gt;\(Y_i \sim (\mu, \sigma^2)\)&lt;/span&gt;，並不需要默認或者假定它服從正態分佈或者任何分佈。那麼它的方差我們會用：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[V_{\mu}=\frac{1}{n}\sum_{i=1}^n(Y_i-\mu)\]&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;-v_mu--sigma2-&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;證明 &lt;span class=&#34;math inline&#34;&gt;\(V_{\mu}\)&lt;/span&gt; 是 &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; 的無偏估計：&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
V_{\mu} &amp;amp;= \frac{1}{n}\sum_{i=1}^n(Y_i-\mu) \\
需要證明 &amp;amp;E(V_{\mu}) = \sigma^2 \\
\Rightarrow E(V_{\mu}) &amp;amp;= \frac{1}{n}\sum_{i=1}^nE(Y_i-\mu)^2 \\
        &amp;amp;= \frac{1}{n}\sum_{i=1}^nVar(Y_i) \\
        &amp;amp;= \frac{1}{n}\sum_{i=1}^n\sigma^2 \\
        &amp;amp;= \sigma^2
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;然而通常情況下，我們並不知道總體的均值 &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;。因此，只好用樣本的均值 &lt;span class=&#34;math inline&#34;&gt;\(\bar{Y}\)&lt;/span&gt; 來估計 &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;。所以上面的方程就變成了：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[V_{\mu}=\frac{1}{n}\sum_{i=1}^n(Y_i-\bar{Y})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;你如果仔細觀察認真思考，就會發現，上面這個式子是&lt;code&gt;有問題的&lt;/code&gt;。這個大問題就在於，&lt;span class=&#34;math inline&#34;&gt;\(Y_i-\bar{Y}\)&lt;/span&gt; 中我們忽略掉了樣本均值 &lt;span class=&#34;math inline&#34;&gt;\(\bar{Y}\)&lt;/span&gt; 和總體均值 &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; 之間的差 (&lt;span class=&#34;math inline&#34;&gt;\(\bar{Y}-\mu\)&lt;/span&gt;)。因此上面的計算式來估計總體方差時，很顯然是會低估平均平方差，從而低估了總體方差。&lt;/p&gt;
&lt;p&gt;這裏需要引入&lt;strong&gt;自由度 (degree of freedom)&lt;/strong&gt; 在參數估計中的概念。&lt;/p&gt;
&lt;p&gt;字面上可以理解爲：自由度是估計過程中使用了多少互相獨立的信息。所以在上面第一個公式中：&lt;span class=&#34;math inline&#34;&gt;\(V_{\mu}=\frac{1}{n}\sum_{i=1}^n(Y_i-\mu)\)&lt;/span&gt;。所有的 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 個觀察值互相獨立，不僅如此，他們還對總體均值獨立。然而在第二個我們用 &lt;span class=&#34;math inline&#34;&gt;\(\bar{Y}\)&lt;/span&gt; 取代了 &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; 的公式中，樣本均數則與觀察值不互相獨立。因爲&lt;strong&gt;樣本均數必然總是落在觀察值的中間&lt;/strong&gt;。然而總體均數並不一定就會落在觀察值中間。總體均數，和觀察值之間是自由，獨立的。因此，當我們觀察到 &lt;span class=&#34;math inline&#34;&gt;\(n-1\)&lt;/span&gt; 個觀察值時，剩下的最後一個觀察值，決定了樣本均值的大小。所以說，樣本均值的自由度，是 &lt;span class=&#34;math inline&#34;&gt;\(n-1\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;所以，加入了自由度的討論，我們可以相信，用樣本估計總體的方差時，使用下面的公式將會是總體方差的無偏估計：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[V_{n-1}=\frac{1}{n-1}\sum_{i=1}^n(Y_i-\bar{Y})=\frac{n}{n-1}V_n\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;證明&lt;/h4&gt;
&lt;p&gt;利用上面也用到過的證明方法 – 把樣本和總體均值之間的差分成兩部分：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
V_{\mu} &amp;amp;= \frac{1}{n}\sum_{i=1}^n(Y_i-\mu)^2 \\
        &amp;amp;= \frac{1}{n}\sum_{i=1}^n[(Y_i-\bar{Y})+(\bar{Y}-\mu)]^2 \\
        &amp;amp;= \frac{1}{n}\sum_{i=1}^n[(Y_i-\bar{Y})^2+(\bar{Y}-\mu)^2\\
        &amp;amp;\;\;\;\;\;\;\;\;\;\;\;\;+2(Y_i-\bar{Y})(\bar{Y}-\mu)]\\
        &amp;amp;=\frac{1}{n}\sum_{i=1}^n(Y_i-\bar{Y})^2+\frac{1}{n}\sum_{i=1}^n(\bar{Y}-\mu)^2\\
        &amp;amp;\;\;\;\;\;\;\;\;\;\;\;\;+\frac{2}{n}(\bar{Y}-\mu)\sum_{i=1}^n(Y_i-\bar{Y}) \\
        &amp;amp;= V_n+(\bar{Y}-\mu)^2 \\ &amp;amp;\;\;\;\;\;\;\;\;\;\;\;\;(note\;that\;\sum_{i=1}^n(Y_i-\bar{Y})=0) \\
\Rightarrow  V_n &amp;amp;= V_{\mu}-(\bar{Y}-\mu)^2  \\
\therefore E(V_n)&amp;amp;= E(V_{\mu}) - E[(\bar{Y}-\mu)^2] \\
                 &amp;amp;= Var(Y)-Var(\bar{Y}) \\
                 &amp;amp;= \sigma^2-\frac{\sigma^2}{n} \\
                 &amp;amp;= \sigma^2(\frac{n-1}{n})
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;因此，我們看見 &lt;span class=&#34;math inline&#34;&gt;\(V_n\)&lt;/span&gt; 正如上面討論的那樣，是低估了總體方差的。雖然當 &lt;span class=&#34;math inline&#34;&gt;\(n\rightarrow\infty\)&lt;/span&gt; 時無限接近 &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; 但是依然是低估了的。所以，我們可以對之進行修正：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
E[\frac{n}{n-1}V_n]     &amp;amp;= \frac{n}{n-1}E[V_n] =\sigma^2 \\
\Rightarrow E[V_{n-1}]  &amp;amp;= \sigma^2
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;樣本方差的樣本分佈&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(S^2\)&lt;/span&gt; 常用來標記樣本方差，取代上面我們用到的 &lt;span class=&#34;math inline&#34;&gt;\(V_{n-1}\)&lt;/span&gt;：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[S^2=\frac{1}{n-1}\sum_{i=1}^n(Y_i-\bar{Y})^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;而且上面也證明了，&lt;span class=&#34;math inline&#34;&gt;\(E(S^2)=\sigma^2\)&lt;/span&gt; 是總體方差的無偏估計。然而，要注意的是，樣本標準差 &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{S^2}\)&lt;/span&gt; 卻不是總體標準差 &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; 的無偏估計（因爲並不是線性變換，而是開了根號）。&lt;/p&gt;
&lt;div id=&#34;-s--sigma-&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;證明樣本標準差 &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; 不是總體標準差 &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; 的無偏估計&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
Var(S)               &amp;amp;=E(S^2)-[E(S)]^2 \\
\Rightarrow [E(S)]^2 &amp;amp;=E(S^2)-Var(S) \\
\because E(S^2)      &amp;amp;=\sigma^2 \\
\therefore   [E(S)]^2 &amp;amp;=\sigma^2-Var(S) \\
             E(S)     &amp;amp;=\sqrt{\sigma^2-Var(S)} \\
\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;可見樣本標準差是低估了總體標準差的。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;另外可以被證明的是：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{n-1}{\sigma^2}S^2\sim \mathcal{X}_{n-1}^2\\
Var(S^2)=\frac{2\sigma^4}{n-1}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{X}^2_m\)&lt;/span&gt;： 自由度爲 &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; 的&lt;a href=&#34;https://winterwang.github.io/post/chi-square-distribution/&#34;&gt;卡方分佈&lt;/a&gt;。是在圖形上向右歪曲的分佈。當自由度增加時，會越來越接近正態分佈。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>概率論者統計推斷入門之-被門夾住</title>
      <link>https://winterwang.github.io/post/frequentist-statistical-inference01/</link>
      <pubDate>Tue, 24 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/frequentist-statistical-inference01/</guid>
      <description>&lt;div id=&#34;-population-and-sample&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;人羣與樣本 (population and sample)&lt;/h3&gt;
&lt;p&gt;討論樣本時，需考慮下面幾個問題：&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;樣本是否具有代表性？&lt;/li&gt;
&lt;li&gt;人羣被準確定義了嗎？&lt;/li&gt;
&lt;li&gt;我們感興趣的“人羣”是否可以是無限大（多）的？&lt;/li&gt;
&lt;li&gt;我們研究的樣本，是僅僅用來觀察，亦或是計劃對之進行某種干預呢？&lt;/li&gt;
&lt;li&gt;我們從所有可能的人羣中抽樣了嗎？&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;-sample-and-statistic&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;樣本和統計量 (sample and statistic)&lt;/h3&gt;
&lt;p&gt;通常我們在進行實驗或觀察時只是獲得了樣本的數據。而希望從樣本數據去推斷 (inference) 總體（或人羣）的一些特徵。我們也許只是想用樣本的平均值來估計整體人羣的某個特徵的平均值。不管是何種估計和推斷，都是基於對樣本數據的計算，從樣本中獲得想要推斷總體的&lt;strong&gt;統計量 (statistics)&lt;/strong&gt;。我們用已知樣本去推斷未知總體的過程就叫做&lt;strong&gt;估計 (estimate)&lt;/strong&gt;。這個想要被推斷的總體或人羣的值，被叫做&lt;strong&gt;參數 (parameter)&lt;/strong&gt;，常常使用希臘字母來標記。用來估計總體或人羣的，從樣本數據計算得來的統計量，叫做&lt;strong&gt;估計量 (estimator)&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;所有的統計量，都有&lt;strong&gt;樣本分佈 (sampling distributions，意爲重複無限次取樣後獲得的無限次統計量的分佈)&lt;/strong&gt;。推斷的過程歸納如下：&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;從總體或人羣中抽樣 (樣本量 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;)&lt;/li&gt;
&lt;li&gt;計算這個樣本的合適統計量，從而用於估計它在整體或人羣中的值。&lt;/li&gt;
&lt;li&gt;我們還需要決定計算獲得的統計量的樣本分佈（假定會抽樣無數次）。&lt;/li&gt;
&lt;li&gt;一旦可以精確地確認樣本分佈，我們就可以定量地計算出使用步驟2中獲得的統計量估計總體或人羣的參數時的準確度。&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;-estimation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;估計 Estimation&lt;/h3&gt;
&lt;p&gt;從樣本的均值，推斷總體或人羣的均值是一種估計。我們的目的是，從已知樣本中計算一個儘可能接近那個未知的總體或人羣參數的值。一個估計量有兩個與生俱來的性質 (properties)：1) 偏倚 (bias); 2) 精確度 (precision)。這兩個性質都可以從樣本分佈和估計量獲得。&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;偏倚： 偏倚簡單說就是樣本分佈的均值，也就是我們從樣本中計算獲得的估計量，和我們想要拿它來估計的總體或人羣的參數之間的差距。(The bias is the difference between the mean of the sampling distribution – the expected or average value of the estimator – and the population parameter being estimated.) 一個小的偏倚，確保了我們從樣本中計算獲得的估計值（假設我們抽樣無數次，計算無數個樣本估計值）&lt;strong&gt;均勻地&lt;/strong&gt;分佈在總體或人羣參數的左右兩邊。偏倚本身並不是太大的問題，但是假如樣本量增加，偏倚依然存在（估計量不一致, inconsistent），那常常意味着是抽樣過程出現了問題。例如：&lt;br&gt;用簡單隨機抽樣法獲得的樣本均值，就是總體或人羣均值的無偏估計 (unbiased estimator)。如果抽樣時由於某些主觀客觀的原因導致較小的樣本很少被抽樣（抽樣過程出了問題，脫離了簡單隨機抽樣原則），那麼此時得到的樣本均值就會是一個過高的估計值 (upward biased estimator)。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;精確度：估計值的精確度可以通過樣本分佈的方差或標準差來評價（簡單說是樣本分佈的方差越低，波動越小，精確度越高）。樣本分佈的標準差被定義爲估計值的標準誤。假如估計量是樣本均值，那麼樣本分佈的標準差（估計量的標準誤）和樣本數據之間有如下的關係： &lt;span class=&#34;math display&#34;&gt;\[均值的標準誤 = \frac{樣本數據的標準差}{\sqrt{樣本量大小}}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在一些簡單的情況下，通常估計值的選用不言自明（例如均值，或者百分比）。但是在複雜的情況下，我們可能可以有多個不同類型的估計量可以選擇，他們也常常各有利弊，需要我們做出取捨。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;-confidence-intervals&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;信賴區間 confidence intervals&lt;/h3&gt;
&lt;p&gt;從樣本中計算估計量獲得的一個估計值，只是一個&lt;strong&gt;點估計 (point estimate)&lt;/strong&gt;。對比之下，信賴區間就是一個對這個點估計的精確度的體現。信賴區間越窄，說明我們對於總體或人羣的參數的可能取值的範圍估計越精確。&lt;/p&gt;
&lt;p&gt;信賴區間通常是成對成對的出現的，即有上限和下限。這樣的一對從樣本數據中計算得來的統計量，同樣也是有樣本分佈的。&lt;strong&gt;每次我們重新從總體或人羣中抽樣，計算獲得的信賴區間都不同，這些信賴區間就組成了信賴區間的樣本分佈。總體和人羣的參數落在這些信賴區間範圍內的概率，就是我們常說的信賴區間的水平（&lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt;）。&lt;/strong&gt; 常用的這個概率值就是 &lt;span class=&#34;math inline&#34;&gt;\(95\%, 90\%, 99\%\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;當從樣本數據計算獲得的估計量的信賴區間很寬，說明了這個收集來的數據提供了很少的參數信息，導致估計變得很不精確。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;看到這裏的都是好漢一條啊！ 我不知道你暈了麼有，反正我是已經暈了。。。。&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>偉大的中心極限定理</title>
      <link>https://winterwang.github.io/post/central-limit-theory/</link>
      <pubDate>Thu, 19 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/central-limit-theory/</guid>
      <description>&lt;p&gt;最近明顯可以感覺到課程的步驟開始加速。看我的課表：&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://winterwang.github.io/img/IMG_0522.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;手機畫面太小了。早上都是9點半開始，下午基本都是到5點。週一更慘，到7點。週二-週五中午都被統計中心的講座佔據。簡直是非人的生活。&lt;/p&gt;
&lt;p&gt;這周概率論基礎結束。中心極限定理講完以後我們正式進入了 Inference 統計推斷的課程。我們花了一天時間講什麼是樣本估計 (Estimation)，什麼是參數精確度 (Precision)，什麼是自由度 (degree of freedom)，怎樣進行不偏的估計 (unbiased inference)。然後還有似然方程 (likelihood function)。&lt;/p&gt;
&lt;p&gt;今天的更新還是簡單的把概率論掃尾一下。感受一下中心極限定理的偉大。&lt;/p&gt;
&lt;div id=&#34;-covariance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;協方差 Covariance&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://winterwang.github.io/post/probability2-4/&#34;&gt;之前我們定義過&lt;/a&gt;，兩個獨立連續隨機變量 &lt;span class=&#34;math inline&#34;&gt;\(X,Y\)&lt;/span&gt; 之和的方差 Variance ：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Var(X+Y)=Var(X)+Var(Y)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;然而如果他們並不相互獨立的話：&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
Var(X+Y) &amp;amp;= E[((X+Y)-E(X+Y))^2] \\
         &amp;amp;= E[(X+Y)-(E(X)+E(Y))^2] \\
         &amp;amp;= E[(X-E(X)) - (Y-E(Y))^2] \\
         &amp;amp;= E[(X-E(X))^2+(Y-E(Y))^2 \\
         &amp;amp; \;\;\; +2(X-E(X))(Y-E(Y))] \\
         &amp;amp;= Var(X)+Var(Y)+2E[(X-E(X))(Y-E(Y))]
\end{aligned}\]&lt;/span&gt;
&lt;p&gt;可以發現在兩者和的方差公式展開之後多了一部分 &lt;span class=&#34;math inline&#34;&gt;\(E[(X-E(X))(Y-E(Y))]\)&lt;/span&gt;。 這個多出來的一部分就說明了二者 &lt;span class=&#34;math inline&#34;&gt;\((X, Y)\)&lt;/span&gt; 之間的關係。它被定義爲協方差 (Covariance): &lt;span class=&#34;math display&#34;&gt;\[Cov(X,Y) = E[(X-E(X))(Y-E(Y))]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;所以：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Var(X+Y)=Var(X)+Var(Y)+2Cov(X,Y)\]&lt;/span&gt;&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;要記住，協方差只能用於評價&lt;span class=&#34;math inline&#34;&gt;(X,Y)&lt;/span&gt;之間的線性關係 (Linear Association)。&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;以下是協方差 (Covariance) 的一些特殊性質：&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Cov(X,X)=Var(X)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Cov(X,Y)=Cov(Y,X)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Cov(aX,bY)=ab\:Cov(X,Y)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Cov(aR+bS,cX+dY)=ac\:Cov(R,X)+ad\:Cov(R,Y)\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+bc\:Cov(S,X)+bd\:Cov(S,Y)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Cov(aX+bY,cX+dY)=ac\:Var(X)+ad\:Var(Y)\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+(ad+bc)Cov(X,Y)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Cov(X+Y,X-Y)=Var(X)-Var(Y)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;If &lt;span class=&#34;math inline&#34;&gt;\(X, Y\)&lt;/span&gt; are independent. &lt;span class=&#34;math inline&#34;&gt;\(Cov(X,Y)=0\)&lt;/span&gt; &lt;span class=&#34;diff_alert&#34;&gt;But not vise-versa !&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;-correlation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;相關 Correlation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;協方差雖然&lt;span class=&#34;math inline&#34;&gt;\(Cov(X,Y)\)&lt;/span&gt; 的大小很大程度上會被他們各自的單位和波動大小左右。&lt;/li&gt;
&lt;li&gt;我們將協方差標準化(除以各自的標準差 s.d.) (standardization) 之後，就可以得到相關係數 Corr (&lt;span class=&#34;math inline&#34;&gt;\(-1\sim1\)&lt;/span&gt;): &lt;span class=&#34;math display&#34;&gt;\[Corr(X,Y)=\frac{Cov(X,Y)}{SD(X)SD(Y)}=\frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;-the-central-limit-theory&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;中心極限定理 the Central Limit Theory&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;diff_add&#34;&gt;&lt;strong&gt;如果從人羣中多次選出樣本量爲 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 的樣本，並計算樣本均值, &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}_n\)&lt;/span&gt;。那麼這個樣本均值 &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}_n\)&lt;/span&gt; 的分佈，會隨着樣本量增加 &lt;span class=&#34;math inline&#34;&gt;\(n\rightarrow\infty\)&lt;/span&gt;，而接近正態分佈。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;偉大的中心極限定理告訴我們：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;diff_alert&#34;&gt;&lt;strong&gt;當樣本量足夠大時，樣本均值 &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}_n\)&lt;/span&gt; 的分佈爲正態分佈，這個特性與樣本來自的人羣的分佈 &lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt; 無關。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;再說一遍：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;如果對象是獨立同分佈 i.i.d (identically and independently distributed)。那麼它的總體期望和方差分別是: &lt;span class=&#34;math inline&#34;&gt;\(E(X)=\mu;\;Var(X)=\sigma^2\)&lt;/span&gt;。 根據中心極限定理，可以得到：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;當樣本量增加，樣本均值的分佈服從正態分佈： &lt;span class=&#34;math display&#34;&gt;\[\bar{X}_n\sim N(\mu, \frac{\sigma^2}{n})\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;也可以寫作，當樣本量增加： &lt;span class=&#34;math display&#34;&gt;\[\sum_{i=1}^nX_i \sim N(n\mu,n\sigma^2)\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;有了這個定理，我們可以拋開樣本空間(&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;)的分佈，也不用假定它服從正態分佈。&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;diff_alert&#34;&gt;但是樣本的均值，卻總是服從正態分佈的。&lt;/span&gt;簡直是太完美了！！！！！！&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>你會用概率論來賭博嗎？</title>
      <link>https://winterwang.github.io/post/probability-gambling/</link>
      <pubDate>Sun, 08 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/probability-gambling/</guid>
      <description>&lt;p&gt;轉眼我已經進入課程的第二週了，總體來說，我們一半的時間都在電腦房練習 Stata 的數據清理和簡單的描述統計 (descriptive statistics)。從我個人的經驗來說，數據分析的過程，其實一大半的時間是消耗在 data cleaning 上的，即使手頭拿到了所謂的乾淨的數據，到真正要分析的時候就會發現一大堆的問題在裏面，需要重新整理，重新添加標記以使之變得更加讓人類可以讀懂。電腦是機器，他是不管你的數據是否乾淨的。只要你放了數據進去，邏輯還可以，沒有編程上的語法錯誤，它總歸會出來一些報告和結果的。如果就這麼直接用的話，大部分的人就會掉進陷阱。畢竟數據不光會說出事實真相，&lt;strong&gt;更多的情況下還會把真相給掩蓋住了。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;我的其餘大部分時間都用在了複習高等數學的微積分上了。感覺好似回到了高中時代。其實大學的時候線性代數得分還是接近滿分的。後來多年不用，生疏了。剛打開複習的書的時候，許多微分積分的規則都已經忘記。通過這一週的辛苦練習，終於是找回了一點狀態。如果你也想有空的時候複習以下高中數學知識，這本書可以推薦給你：&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.amazon.co.uk/gp/product/0471827223/ref=oh_aui_detailpage_o04_s00?ie=UTF8&amp;amp;psc=1&#34;&gt;Quick Calculus: Short Manual of Self-instruction&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://winterwang.github.io/img/Selection_070.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;上面這本書的內容可以一邊閱讀，一邊練習。實在是複習的一本好書。我花了一週的課餘時間，從頭到尾把裏面的習題和解答全部完成。收穫很大。感覺年輕時的數學思維又開始在大腦裏復甦了。一身輕鬆。&lt;/p&gt;
&lt;p&gt;下面想介紹一下上週學習的概率的基礎問題。&lt;/p&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;首先是最基礎的&lt;strong&gt;三個概率的公理&lt;/strong&gt;：&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;對於任意事件 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;，它發生的概率 &lt;span class=&#34;math inline&#34;&gt;\(P(A)\)&lt;/span&gt; 滿足這樣的不等式： &lt;span class=&#34;math inline&#34;&gt;\(0 \leqslant P(A) \leqslant 1\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(\Omega)=1\)&lt;/span&gt; , &lt;span class=&#34;math inline&#34;&gt;\(\Omega\)&lt;/span&gt; 是全樣本空間 (total sample space)&lt;/li&gt;
&lt;li&gt;對於互斥（相互獨立）的事件 &lt;span class=&#34;math inline&#34;&gt;\(A_1, A_2, \dots, A_n\)&lt;/span&gt; 有如下的等式關係： &lt;span class=&#34;math inline&#34;&gt;\(P(A_1\cup A_2 \cup \cdots \cup A_n)=P(A_1)+P(A_2)+\cdots+P(A_n)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;你是不是覺得上面三條公理都是&lt;strong&gt;廢話&lt;/strong&gt;。 不用擔心，我也是這麼覺得的。因爲所有人都認同的道理，才能成爲公理 (axiom)，因爲它們是不需要證明的自然而然形成的人人都接受的觀念。&lt;code&gt;(axiom: a saying that is widely accepted on its own merits; its truth is assumed to be self-evident)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;然而，正是這樣顯而易見的道理，確是拿來建築理論的基石，千萬不能小看了他們。例如，我們看下面這個看似也應該成爲公理的公式，你能證明嗎：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(A_1\cup A_2) = P(A_1) + P(A_2) - P(A_1 \cap A_2)\)&lt;/span&gt;&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://winterwang.github.io/img/venngram.png&#34; /&gt;

&lt;/div&gt;
&lt;div class=&#34;section level4&#34;&gt;
&lt;h4&gt;證明：&lt;/h4&gt;
&lt;p&gt;先考慮 &lt;span class=&#34;math inline&#34;&gt;\(A_1 \cup A_2\)&lt;/span&gt; 是什麼（拆分成三個互斥事件）&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(A_1 \cup A_2 = (A_1\cap \bar{A_2})\cup(\bar{A_1}\cap A_2)\cup(A_1\cap A_2)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;運用上面的公理&lt;del&gt;2&lt;/del&gt; 3&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\therefore P(A_1 \cup A_2) = P(A_1\cap \bar{A_2}) + P(\bar{A_1}\cap A_2) + P(A_1\cap A_2) \;\;\;\;\;\;(1)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;再考慮 &lt;span class=&#34;math inline&#34;&gt;\(A_1=(A_1\cap A_2)\cup(A_1\cap\bar{A_2})\)&lt;/span&gt; 繼續拆分成兩個互斥事件&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\therefore P(A_1)=P(A_1\cap A_2)+P(A_1\cap\bar{A_2})\)&lt;/span&gt; 整理一下：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(A_1\cap\bar{A_2})=P(A_1)-P(A_1\cap A_2)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;同理可得: &lt;span class=&#34;math inline&#34;&gt;\(P(\bar{A_1}\cap A_2)=P(A_2)-P(A_1\cap A_2)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;代入上面第(1)式可得：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(A_1 \cup A_2) =P(A_1)-P(A_1\cap A_2)\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+P(A_2)-P(A_1\cap A_2)\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+P(A_1\cap A_2)\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;=P(A_1) + P(A_2) - P(A_1 \cap A_2)\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;-conditional-probability&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;條件概率 Conditional probability&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(A|S)=\frac{P(A\cap S)}{P(S)}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(A\cap S) = P(A|S)P(S)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;-independence-&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;獨立 (independence) 的定義&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;兩個事件定義爲互爲獨立時 (&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; are said to be independent &lt;strong&gt;if and only if&lt;/strong&gt;) &lt;span class=&#34;math display&#34;&gt;\[P(A\cap B)=P(A)P(B)\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;因爲從條件概率的概念我們已知&lt;br&gt; &lt;span class=&#34;math inline&#34;&gt;\(P(A\cap B) = P(A|B)P(B)\)&lt;/span&gt; &lt;br&gt;所以&lt;span class=&#34;math inline&#34;&gt;\(P(A|B)=P(A)\)&lt;/span&gt; 即：事件 &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; 無法提供事件 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 的任何有效訊息 (&lt;strong&gt;&lt;span class=&#34;math inline&#34;&gt;\(A, B\)&lt;/span&gt; 互相獨立&lt;/strong&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;賭博問題&lt;/h2&gt;
&lt;p&gt;終於來到本次話題的重點了。我要扣題了哦。語文老師快在此加分。&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://winterwang.github.io/img/Selection_071.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;假設你在一個電視遊戲節目。有上圖一樣的三扇門。其中一扇門後面有一輛保時捷，另兩扇門後面則是&lt;a href=&#34;https://winterwang.github.io/post/black-meal/&#34;&gt;(味道奇特的)山羊&lt;/a&gt;。遊戲規則是主持人會讓你先選擇其中一扇門（先不打開你選的這扇門）。主持人隨後打開另外兩扇門中的一扇沒有保時捷的門。主持人問你，你要堅持選擇之前選中的那扇門，還是要改變主意換一扇門去猜是否可以猜中保時捷。 請問，堅持選擇之前選中的門猜中保時捷的概率高，還是主持人打開一扇門以後改變主意猜中保時捷的概率更高呢？&lt;/p&gt;
&lt;p&gt;答案明天揭曉。&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
