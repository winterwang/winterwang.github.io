<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Inference on Be ambitious</title>
    <link>https://winterwang.github.io/tags/inference/</link>
    <description>Recent content in Inference on Be ambitious</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Chaochen Wang | 王超辰</copyright>
    <lastBuildDate>Thu, 02 Nov 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/tags/inference/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>似然非然 Likelihood</title>
      <link>https://winterwang.github.io/post/likelihood/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/likelihood/</guid>
      <description>&lt;div id=&#34;-vs.probability-vs.inference&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;概率 vs. 推斷/Probability vs. Inference&lt;/h3&gt;
&lt;p&gt;在概率論的環境下，我們常常被告知的前提是：某某事件發生的概率是多少。例如： 一枚硬幣正面朝上的概率是 &lt;span class=&#34;math inline&#34;&gt;\(0.5\; Prob(coin\;landing\;heads)=0.5\)&lt;/span&gt;。然後在這個前提下，我們又繼續去計算複雜的事件發生的概率（例如，10次投擲硬幣以後4次正面朝上的概率是多少？）。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\binom{10}{4}\times(0.5^4)\times(0.5^{10-4}) = 0.205
\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dbinom(4, 10, 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2050781&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# or you can calculate by hand:
factorial(10)*(0.5^10)/(factorial(4)*(factorial(6)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2050781&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在統計推斷的理論中，我們考慮實際的情況，這樣的實際情況就是，我們通過觀察獲得數據，然而我們並不知道某事件發生的概率到底是多少（神如果存在話，只有神知道）。所以這個 &lt;span class=&#34;math inline&#34;&gt;\(Prob(coin\;landing\;heads)\)&lt;/span&gt; 的概率大小對於“人類”來說是未知的。所以我們可能觀察到投擲了10次硬幣，其中有4次是正面朝上的。所以我們從這一次觀察實驗中，需要計算的是能夠符合觀察結果的“最佳”概率估計 (best estimate)。所以，這種情況下，&lt;strong&gt;似然法 (likelihood)&lt;/strong&gt; 就是我們進行參數估計的最佳手段&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;似然和最大似然估計&lt;/h3&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>卡方分佈 chi square distribution</title>
      <link>https://winterwang.github.io/post/chi-square-distribution/</link>
      <pubDate>Thu, 26 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/chi-square-distribution/</guid>
      <description>&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;卡方分佈的期望和方差的證明：&lt;/h3&gt;
&lt;p&gt;當 &lt;span class=&#34;math inline&#34;&gt;\(X\sim N(0,1)\)&lt;/span&gt; 時， &lt;span class=&#34;math inline&#34;&gt;\(X^2\sim \mathcal{X}_1^2\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;如果 &lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots, X_n\stackrel{i.i.d}{\sim} N(0,1)\)&lt;/span&gt;， 那麼 &lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^nX_i^2\sim\mathcal{X}_n^2\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中： &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{X}_n^2\)&lt;/span&gt; 表示自由度爲 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 的卡方分佈。&lt;/p&gt;
&lt;p&gt;且 &lt;span class=&#34;math inline&#34;&gt;\(X_m^2+X_n^2=\mathcal{X}_{m+n}^2\)&lt;/span&gt;&lt;/p&gt;
&lt;div class=&#34;section level4&#34;&gt;
&lt;h4&gt;卡方分佈的期望：&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[E(X_1^2)=Var(X)+[E(X)]^2=1+0=1\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\Rightarrow E(X_n^2)=n\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level4&#34;&gt;
&lt;h4&gt;卡方分佈的方差：&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
Var(X_1^2) &amp;amp;= E(X_1^{2^2}) - E(X_1^2)^2 \\
           &amp;amp;= E(X_1^4)-1
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;-ex_14&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;下面來求 &lt;span class=&#34;math inline&#34;&gt;\(E(X_1^4)\)&lt;/span&gt;&lt;/h5&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\because E(X_1) &amp;amp;= \int_{-\infty}^{+\infty} xf(x)dx \\
\therefore E(X_1^4) &amp;amp;= \int_{-\infty}^{+\infty} x^4f(x)dx
\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;已知： &lt;span class=&#34;math inline&#34;&gt;\(f(x)=\frac{1}{\sqrt{2\pi}}e^{(-\frac{x^2}{2})}\)&lt;/span&gt; 代入上式：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
E(X_1^4) &amp;amp;= \int_{-\infty}^{+\infty} x^4f(x)dx \\
         &amp;amp;= \int_{-\infty}^{+\infty} x^4\frac{1}{\sqrt{2\pi}}e^{(-\frac{x^2}{2})}dx\\
         &amp;amp;=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}x^4e^{(-\frac{x^2}{2})}dx\\
         &amp;amp;=\frac{-1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}x^3(-x)e^{(-\frac{x^2}{2})}dx
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;令 &lt;span class=&#34;math inline&#34;&gt;\(u=x^3, v=e^{(-\frac{x^2}{2})},t=-\frac{x^2}{2}\)&lt;/span&gt; 可以推導：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\frac{dv}{dx} &amp;amp;= \frac{dv}{dt}\frac{dt}{dx} \\
              &amp;amp;= e^t(-\frac{1}{2}\times2x) \\
              &amp;amp;= (-x)e^{(-\frac{x^2}{2})} \\
\Rightarrow dv &amp;amp;= (-x)e^{(-\frac{x^2}{2})}dx
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;再代入上面的式子：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
E(X_1^4) &amp;amp;= \frac{-1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}u\:dv \\
integrate\; &amp;amp;by\; parts:\\
E(X_1^4) &amp;amp;= \frac{-1}{\sqrt{2\pi}}\{[u\:v] \rvert_{-\infty}^{+\infty}-\int_{-\infty}^{+\infty}v\:du\} \\
&amp;amp;= \frac{-1}{\sqrt{2\pi}}\{[x^3e^{(-\frac{x^2}{2})}]\rvert_{-\infty}^{+\infty} -\int_{-\infty}^{+\infty}v\:du\} \\
&amp;amp;=\frac{-1}{\sqrt{2\pi}}\{0-0-\int_{-\infty}^{+\infty}e^{(-\frac{x^2}{2})}dx^3\} \\
&amp;amp;=\frac{-1}{\sqrt{2\pi}}[-3\int_{-\infty}^{+\infty}x^2e^{(-\frac{x^2}{2})}dx] \\
&amp;amp;=\frac{-3}{\sqrt{2\pi}}[\int_{-\infty}^{+\infty}x(-x)e^{(-\frac{x^2}{2})}dx] \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;再來一次分部積分：&lt;/p&gt;
&lt;p&gt;令 &lt;span class=&#34;math inline&#34;&gt;\(a=x,b=e^{(-\frac{x^2}{2})},d\:b = (-x)e^{(-\frac{x^2}{2})}dx\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
E(X_1^4) &amp;amp;= \frac{-3}{\sqrt{2\pi}}\{[a\:b] \rvert_{-\infty}^{+\infty} - \int_{-\infty}^{+\infty}b\:da\} \\
&amp;amp;=\frac{-3}{\sqrt{2\pi}}\{[xe^{(-\frac{x^2}{2})}]\rvert_{-\infty}^{+\infty} -\int_{-\infty}^{+\infty}b\:da\} \\
&amp;amp;=\frac{-3}{\sqrt{2\pi}}\{0-0-\int_{-\infty}^{+\infty}e^{(-\frac{x^2}{2})}dx\} \\
&amp;amp;=\frac{-3}{\sqrt{2\pi}}[-\int_{-\infty}^{+\infty}e^{(-\frac{x^2}{2})}dx] \\
&amp;amp;=\frac{3}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}e^{(-\frac{x^2}{2})}dx
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;下面令 &lt;span class=&#34;math inline&#34;&gt;\(I=\int_{-\infty}^{+\infty}e^{(-\frac{x^2}{2})}dx\\ \Rightarrow I^2=\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}e^{(-\frac{x^2+y^2}{2})}dxdy\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;接下來需要用到 &lt;a href=&#34;https://www.youtube.com/watch?v=r0fv9V9GHdo&#34;&gt;座標轉換&lt;/a&gt;的知識，將 &lt;span class=&#34;math inline&#34;&gt;\(x,y\)&lt;/span&gt; 表示的笛卡爾座標，轉換爲用角度 &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; 和半徑 &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; 表示的形式。之後的證明可以在&lt;a href=&#34;https://www.youtube.com/watch?v=fWOGfzC3IeY&#34;&gt;油管&lt;/a&gt;上看到，但是我還是繼續證明下去。&lt;/p&gt;
&lt;p&gt;直角座標系 (cartesian coordinators) 和 極座標系 (polar coordinators) 之間轉換的關係如下：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
x&amp;amp;=r\:cos\theta\\
y&amp;amp;=r\:sin\theta\\
r^2&amp;amp;=x^2+y^2\\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;座標轉換以後可以繼續求 &lt;span class=&#34;math inline&#34;&gt;\(E(X_1^4)\)&lt;/span&gt;。 在那之前我們先求 &lt;span class=&#34;math inline&#34;&gt;\(I^2\)&lt;/span&gt;。 注意轉換座標系統以後，&lt;span class=&#34;math inline&#34;&gt;\(\theta\in[0,2\pi], r\in[0,+\infty]\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I^2 &amp;amp;= \int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}e^{(-\frac{x^2+y^2}{2})}dxdy \\
&amp;amp;= \int_{0}^{+\infty}\int_{0}^{2\pi}e^{(-\frac{r^2}{2})}rd\theta dr \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;由於先從中間的 &lt;span class=&#34;math inline&#34;&gt;\(\int_{0}^{2\pi}e^{(-\frac{r^2}{2})}rd\theta\)&lt;/span&gt; 開始積分，&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; 以外都可以視爲常數，那麼這個 &lt;span class=&#34;math inline&#34;&gt;\([0,2\pi]\)&lt;/span&gt; 上的積分就的等於 &lt;span class=&#34;math inline&#34;&gt;\(2\pi e^{(-\frac{r^2}{2})}r\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;因此上面的式子又變爲：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I^2 &amp;amp;=  2\pi\int_{0}^{+\infty}e^{(-\frac{r^2}{2})}r\:dr \\
\because \frac{d(e^{\frac{-r^2}{2}})}{dr} &amp;amp;= -e^{(-\frac{r^2}{2})}r \\
\therefore I^2 &amp;amp;= 2\pi(-e^{\frac{-r^2}{2}})\rvert_0^{+\infty} \\
               &amp;amp;= 0-(2\pi\times(-1)) \\
               &amp;amp;= 2\pi\\
\Rightarrow I  &amp;amp;= \sqrt{2\pi}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;所以，&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
E(X_1^4) &amp;amp;= \frac{3}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}e^{(-\frac{x^2}{2})}dx \\
&amp;amp;= \frac{3}{\sqrt{2\pi}}\times I \\
&amp;amp;= 3 \\
\Rightarrow Var(X_1^2) &amp;amp;= E(X_1^4) - 1 \\
                       &amp;amp;= 3-1 =2 \\
\Rightarrow Var(X_n^2) &amp;amp;= 2n
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;結論：&lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots, X_n\stackrel{i.i.d}{\sim} N(0,1)\)&lt;/span&gt; 時，&lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^nX_i^2\sim\mathcal{X}_n^2\)&lt;/span&gt; 服從卡方分佈，其期望 &lt;span class=&#34;math inline&#34;&gt;\(E(X_n^2)=n\)&lt;/span&gt;，方差 &lt;span class=&#34;math inline&#34;&gt;\(Var(X_n^2)=2n\)&lt;/span&gt;。 根據&lt;a href=&#34;https://winterwang.github.io/post/central-limit-theory/&#34;&gt;中心極限定理&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[n\rightarrow \infty, X_n^2\sim N(n, 2n)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>估計和精確度的概念</title>
      <link>https://winterwang.github.io/post/frequentist-statistical-inference02/</link>
      <pubDate>Tue, 24 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/frequentist-statistical-inference02/</guid>
      <description>&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;估計量和他們的樣本分佈&lt;/h3&gt;
&lt;p&gt;例子： 最大呼氣量 (Forced Expoiratory Volume in one second, FEV1) 用於測量一個人的肺功能，它的測量值是連續的。我們從前來門診的人中隨機抽取 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 人作爲樣本，用這個樣本的 FEV1 平均值來估計這個診所的患者的平均肺功能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;模型假設：&lt;/strong&gt; 在這個例子中，我們的假設有如下：每個隨機抽取的 FEV1 測量值都是從同一個總體（人羣）中抽取，每一個觀察值 &lt;span class=&#34;math inline&#34;&gt;\(Y_i\)&lt;/span&gt; 都互相獨立互不影響。我們用縮寫 iid 表示這些隨機抽取的樣本是服從獨立同分佈 (independent and identically distributed)。另外，總體的分佈也假定爲正態分佈，且總體均值爲 &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;，總體方差爲 &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;。那麼這個模型可以簡單的被寫成：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y_i \stackrel{i.i.d}{\sim} N(\mu, \sigma^2), i=1,2,\dots,n\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;總體均值 &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; 的估計量：&lt;/strong&gt; 顯然算術平均值: &lt;span class=&#34;math inline&#34;&gt;\(\bar{Y}=\frac{1}{n}\sum_{i=1}^ny_i\)&lt;/span&gt; 是我們用於估計總體均值的估計量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;估計量的樣本分佈：&lt;/strong&gt; &lt;span class=&#34;math display&#34;&gt;\[\bar{Y}\stackrel{i.i.d}{\sim}N(\mu, \frac{\sigma^2}{n})\]&lt;/span&gt;&lt;/p&gt;
&lt;div class=&#34;section level4&#34;&gt;
&lt;h4&gt;證明&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
E(\bar{Y}) &amp;amp;= E(\frac{1}{n}\sum Y_i) \\
           &amp;amp;= \frac{1}{n}E(\sum Y_i) \\
           &amp;amp;= \frac{1}{n}\sum E(Y_i) \\
           &amp;amp;= \frac{1}{n}n\mu = \mu \\
Var(\bar{Y}) &amp;amp;= Var(\frac{1}{n}\sum Y_i) \\
\because Y_i \;are &amp;amp;\; independent   \\
            &amp;amp;= \frac{1}{n^2}\sum Var(Y_i) \\
            &amp;amp;= \frac{1}{n^2} n Var(Y_i) \\
            &amp;amp;= \frac{\sigma^2}{n}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;-zfracbary-musqrtvarbary--zsim-n01&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;證明當 &lt;span class=&#34;math inline&#34;&gt;\(Z=\frac{\bar{Y}-\mu}{\sqrt{Var(\bar{Y})}}\)&lt;/span&gt; 時， &lt;span class=&#34;math inline&#34;&gt;\(Z\sim N(0,1)\)&lt;/span&gt;:&lt;/h4&gt;
&lt;p&gt;由式子可知， &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; 只是由一組服從正態分佈的數據 &lt;span class=&#34;math inline&#34;&gt;\(\bar{Y}\)&lt;/span&gt; 線性轉換 (linear transformation) 而來，所以 &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; 本身也服從正態分佈 &lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
E(Z) &amp;amp;= \frac{1}{\sqrt{Var(\bar{Y})}}E[\bar{Y}-\mu] \\
     &amp;amp;= \frac{1}{\sqrt{Var(\bar{Y})}}[\mu-\mu] = 0 \\
Var(Z) &amp;amp;= \frac{1}{Var(\bar{Y})}Var[\bar{Y}-\mu] \\
       &amp;amp;= \frac{1}{Var(\bar{Y})}Var(\bar{Y}) =1 \\
\therefore Z \;&amp;amp;\sim N(0,1)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;均值 &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; 的信賴區間：&lt;/strong&gt; 上節說道，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;信賴區間通常是成對成對的出現的，即有上限和下限。這樣的一對從樣本數據中計算得來的統計量，同樣也是有樣本分佈的。&lt;strong&gt;每次我們重新從總體或人羣中抽樣，計算獲得的信賴區間都不同，這些信賴區間就組成了信賴區間的樣本分佈。總體和人羣的參數落在這些信賴區間範圍內的概率，就是我們常說的信賴區間的水平（&lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt;）。&lt;/strong&gt; 常用的這個概率值就是 &lt;span class=&#34;math inline&#34;&gt;\(95\%, 90\%, 99\%\)&lt;/span&gt;。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;假定我們用 &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; 作爲信賴區間的水平。那麼下面我們嘗試推導一下信賴區間的計算公式。從長遠來說（也就是假設我們從總體中抽樣無數次，每次都進行信賴區間的計算，也獲得無數個信賴區間），這些信賴區間中有 &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; 是包含了總體的真實均值（但是卻是未知）的，而且這些信賴區間由於是從一個服從正態分佈的數據而來，它們也服從正態分佈（對真實均值左右對稱）。所以我們有理由相信，可以找到一個數值 &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Prob(\bar{Y} &amp;gt; \mu+c) = 0.025 \\
  Prob(\bar{Y} &amp;lt; \mu-c) = 0.025\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;因此，我們可以定義 &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; 信賴區間的上限和下限分別是：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[L=\bar{Y}-c \Rightarrow Prob(L&amp;gt;\mu)=0.025 \\
  U=\bar{Y}+c \Rightarrow Prob(U&amp;lt;\mu)=0.025\]&lt;/span&gt;&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://winterwang.github.io/img/Selection_082.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;接下來就是推倒（故意的）&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; 的過程啦：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
Prob(\bar{Y}&amp;gt;\mu+c)=Prob(\bar{Y}-\mu&amp;gt;c) \;&amp;amp;= 0.025 \\
\Rightarrow Prob(\frac{\bar{Y}-\mu}{\sqrt{Var(\bar{Y})}} &amp;gt; \frac{c}{\sqrt{Var(\bar{Y})}}) \;&amp;amp;= 0.025 \\
\Rightarrow Prob(Z&amp;gt;\frac{c}{\sqrt{Var(\bar{Y})}}) \;&amp;amp;= 0.025 \\
上面已經證明了 Z\sim N(0,1) \\
而且我們也已知 Prob(Z&amp;gt;1.96) \;&amp;amp;= 0.025 \\
所以只要令 \frac{c}{\sqrt{Var(\bar{Y})}} =1.96 \\
\Rightarrow c=1.96\sqrt{Var(\bar{Y})} \\
所以總體均值\mu 的 95\% 信賴區間就是: \\
\mu = \bar{Y}\pm1.96\sqrt{Var(\bar{Y})}=\bar{Y}\pm &amp;amp; 1.96\frac{\sigma}{\sqrt{n}}\\
其中，\sqrt{Var(\bar{Y})} 就是我們熟知的估計量 \bar{Y} &amp;amp;的標準誤。
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;估計量的特質&lt;/h3&gt;
&lt;p&gt;考慮以下的問題：&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;什麼因素決定了一個估計量 (estimator) 的好壞，是否實用？&lt;/li&gt;
&lt;li&gt;如果有其他的可選擇估計量，該如何取捨呢？&lt;/li&gt;
&lt;li&gt;當情況複雜的時候，我們該如何尋找合適的估計量？&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;section level4&#34;&gt;
&lt;h4&gt;偏倚&lt;/h4&gt;
&lt;p&gt;假設 &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; 是我們估計總體參數 &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; 的一個估計量。一般來說我們希望估計量的樣本分佈可以在 &lt;code&gt;“正確的位置”&lt;/code&gt; 左右均勻分佈。換句話說我們希望：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[E(T)=\theta\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;如果實現了這個條件，我們說這樣的估計量是無偏的 (&lt;code&gt;unbiased&lt;/code&gt;)。然而，天下哪有這等好事，我們叫真實值和估計量之間的差距爲偏倚：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[bias(T) = E(T)-\theta\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其實偏倚完全等於零並不是最重要，許多常見的估計量都是有偏倚的。重要的是，這個偏倚會隨着樣本量的增加而逐漸趨近於零。所以我們就可以認爲這樣的估計量是漸進無偏的 (asymptotically unbiased)：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[T\;is\;an\;\textbf{unbiased}\;estimator\;for\;\theta\;if\;\\E(T)=\theta\\
T\;is\;an\;\textbf{asymptotically unbiased}\;estimator\;for\;\theta\;if\;\\lim_{n\rightarrow\infty}E(T)=\theta\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;-efficiency&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;估計量的效能 Efficiency&lt;/h4&gt;
&lt;p&gt;通常，我們希望一個估計量 (estimator) 的偏倚要小，同時，它的樣本分佈也希望能儘可能的不要波動太大。換句話說，我們還希望估計量的方差越小越好。&lt;/p&gt;
&lt;p&gt;如果說，兩個估計量有相同的偏倚，均可以選擇來推斷總體，我們說，其中樣本分佈的方差小的那個（波動幅度小）的那個估計量是相對更好的。因爲樣本分佈方差越小，說明可以&lt;strong&gt;更加精確的&lt;/strong&gt;估計總體參數。這兩個估計量的方差之比：&lt;span class=&#34;math inline&#34;&gt;\(Var(S)/Var(T)\)&lt;/span&gt; 被叫做這兩個估計量的&lt;strong&gt;相對效能 (relative efficiency)&lt;/strong&gt;。所以我們用估計量去推斷總體時，需要選用效能最高，精確度最好的估計量 &lt;strong&gt;(the minimum variance unbiased estimator/an efficient estimator)&lt;/strong&gt;。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level4&#34;&gt;
&lt;h4&gt;均值和中位數的相對效能&lt;/h4&gt;
&lt;p&gt;在一個服從 &lt;span class=&#34;math inline&#34;&gt;\(N(\mu,\sigma^2)\)&lt;/span&gt; 正態分佈的數據中，中位數和均值是一樣的，也都同時等於總體均值參數 &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;。而且，樣本均數 &lt;span class=&#34;math inline&#34;&gt;\(\bar{Y}\)&lt;/span&gt; 和樣本中位數 &lt;span class=&#34;math inline&#34;&gt;\(\dot{Y}\)&lt;/span&gt; 都是對總體均值的無偏估計量。那麼應該選用中位數還是平均值呢？&lt;/p&gt;
&lt;p&gt;之前證明過當 &lt;span class=&#34;math inline&#34;&gt;\(Y_i \sim N(\mu,\sigma^2)\)&lt;/span&gt; 時， &lt;span class=&#34;math inline&#34;&gt;\(Var(\bar{Y}=\sigma^2/n)\)&lt;/span&gt;。然而，當 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 較大的時候，可以證明的是：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Var(\dot{Y})=\frac{\pi}{2}\frac{\sigma^2}{n}\approx1.571\frac{\sigma^2}{n}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;因此，這兩個估計量的相對效能就是：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{Var(\dot{Y})}{Var(\bar{Y})}\approx1.571\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;所以總體是正態分佈時，平均值就是較中位數更適合用來估計總體的估計量。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;-mean-square-error-mse&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;均方差 mean square error (MSE)&lt;/h3&gt;
&lt;p&gt;兩個估計量的偏倚不同時，可以比較他們和總體參數之間的差距，這被叫做均方差, Mean Square Error (MSE)。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[MSE(T)=E[(T-\theta)^2]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;這裏用一個數學技巧，將式子中的估計量和總體參數之間的差，分成兩個部分：一是估計量本身的方差 (&lt;span class=&#34;math inline&#34;&gt;\(T-E(T)\)&lt;/span&gt;)，一是估計量的偏倚 (&lt;span class=&#34;math inline&#34;&gt;\(E(T)-\theta\)&lt;/span&gt;)。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
MSE(T) &amp;amp;= E[(T-\theta)^2] \\
       &amp;amp;= E\{[T-E(T)+E(T)-\theta]^2\} \\
       &amp;amp;= E\{[T-E(T)]^2+[E(T)-\theta]^2 \\
       &amp;amp; \;\;\;\;\; \;\;+2[T-E(T)][E(T)-\theta]\} \\
       &amp;amp;= E\{[T-E(T)]^2\}+E\{[E(T)-\theta]^2\} + 0\\
       &amp;amp;= Var(T) + [bias(T)^2]
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;總體方差的估計，自由度&lt;/h3&gt;
&lt;p&gt;如果 &lt;span class=&#34;math inline&#34;&gt;\(Y_i \sim (\mu, \sigma^2)\)&lt;/span&gt;，並不需要默認或者假定它服從正態分佈或者任何分佈。那麼它的方差我們會用：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[V_{\mu}=\frac{1}{n}\sum_{i=1}^n(Y_i-\mu)\]&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;-v_mu--sigma2-&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;證明 &lt;span class=&#34;math inline&#34;&gt;\(V_{\mu}\)&lt;/span&gt; 是 &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; 的無偏估計：&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
V_{\mu} &amp;amp;= \frac{1}{n}\sum_{i=1}^n(Y_i-\mu) \\
需要證明 &amp;amp;E(V_{\mu}) = \sigma^2 \\
\Rightarrow E(V_{\mu}) &amp;amp;= \frac{1}{n}\sum_{i=1}^nE(Y_i-\mu)^2 \\
        &amp;amp;= \frac{1}{n}\sum_{i=1}^nVar(Y_i) \\
        &amp;amp;= \frac{1}{n}\sum_{i=1}^n\sigma^2 \\
        &amp;amp;= \sigma^2
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;然而通常情況下，我們並不知道總體的均值 &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;。因此，只好用樣本的均值 &lt;span class=&#34;math inline&#34;&gt;\(\bar{Y}\)&lt;/span&gt; 來估計 &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;。所以上面的方程就變成了：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[V_{\mu}=\frac{1}{n}\sum_{i=1}^n(Y_i-\bar{Y})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;你如果仔細觀察認真思考，就會發現，上面這個式子是&lt;code&gt;有問題的&lt;/code&gt;。這個大問題就在於，&lt;span class=&#34;math inline&#34;&gt;\(Y_i-\bar{Y}\)&lt;/span&gt; 中我們忽略掉了樣本均值 &lt;span class=&#34;math inline&#34;&gt;\(\bar{Y}\)&lt;/span&gt; 和總體均值 &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; 之間的差 (&lt;span class=&#34;math inline&#34;&gt;\(\bar{Y}-\mu\)&lt;/span&gt;)。因此上面的計算式來估計總體方差時，很顯然是會低估平均平方差，從而低估了總體方差。&lt;/p&gt;
&lt;p&gt;這裏需要引入&lt;strong&gt;自由度 (degree of freedom)&lt;/strong&gt; 在參數估計中的概念。&lt;/p&gt;
&lt;p&gt;字面上可以理解爲：自由度是估計過程中使用了多少互相獨立的信息。所以在上面第一個公式中：&lt;span class=&#34;math inline&#34;&gt;\(V_{\mu}=\frac{1}{n}\sum_{i=1}^n(Y_i-\mu)\)&lt;/span&gt;。所有的 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 個觀察值互相獨立，不僅如此，他們還對總體均值獨立。然而在第二個我們用 &lt;span class=&#34;math inline&#34;&gt;\(\bar{Y}\)&lt;/span&gt; 取代了 &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; 的公式中，樣本均數則與觀察值不互相獨立。因爲&lt;strong&gt;樣本均數必然總是落在觀察值的中間&lt;/strong&gt;。然而總體均數並不一定就會落在觀察值中間。總體均數，和觀察值之間是自由，獨立的。因此，當我們觀察到 &lt;span class=&#34;math inline&#34;&gt;\(n-1\)&lt;/span&gt; 個觀察值時，剩下的最後一個觀察值，決定了樣本均值的大小。所以說，樣本均值的自由度，是 &lt;span class=&#34;math inline&#34;&gt;\(n-1\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;所以，加入了自由度的討論，我們可以相信，用樣本估計總體的方差時，使用下面的公式將會是總體方差的無偏估計：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[V_{n-1}=\frac{1}{n-1}\sum_{i=1}^n(Y_i-\bar{Y})=\frac{n}{n-1}V_n\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;證明&lt;/h4&gt;
&lt;p&gt;利用上面也用到過的證明方法 – 把樣本和總體均值之間的差分成兩部分：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
V_{\mu} &amp;amp;= \frac{1}{n}\sum_{i=1}^n(Y_i-\mu)^2 \\
        &amp;amp;= \frac{1}{n}\sum_{i=1}^n[(Y_i-\bar{Y})+(\bar{Y}-\mu)]^2 \\
        &amp;amp;= \frac{1}{n}\sum_{i=1}^n[(Y_i-\bar{Y})^2+(\bar{Y}-\mu)^2\\
        &amp;amp;\;\;\;\;\;\;\;\;\;\;\;\;+2(Y_i-\bar{Y})(\bar{Y}-\mu)]\\
        &amp;amp;=\frac{1}{n}\sum_{i=1}^n(Y_i-\bar{Y})^2+\frac{1}{n}\sum_{i=1}^n(\bar{Y}-\mu)^2\\
        &amp;amp;\;\;\;\;\;\;\;\;\;\;\;\;+\frac{2}{n}(\bar{Y}-\mu)\sum_{i=1}^n(Y_i-\bar{Y}) \\
        &amp;amp;= V_n+(\bar{Y}-\mu)^2 \\ &amp;amp;\;\;\;\;\;\;\;\;\;\;\;\;(note\;that\;\sum_{i=1}^n(Y_i-\bar{Y})=0) \\
\Rightarrow  V_n &amp;amp;= V_{\mu}-(\bar{Y}-\mu)^2  \\
\therefore E(V_n)&amp;amp;= E(V_{\mu}) - E[(\bar{Y}-\mu)^2] \\
                 &amp;amp;= Var(Y)-Var(\bar{Y}) \\
                 &amp;amp;= \sigma^2-\frac{\sigma^2}{n} \\
                 &amp;amp;= \sigma^2(\frac{n-1}{n})
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;因此，我們看見 &lt;span class=&#34;math inline&#34;&gt;\(V_n\)&lt;/span&gt; 正如上面討論的那樣，是低估了總體方差的。雖然當 &lt;span class=&#34;math inline&#34;&gt;\(n\rightarrow\infty\)&lt;/span&gt; 時無限接近 &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; 但是依然是低估了的。所以，我們可以對之進行修正：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
E[\frac{n}{n-1}V_n]     &amp;amp;= \frac{n}{n-1}E[V_n] =\sigma^2 \\
\Rightarrow E[V_{n-1}]  &amp;amp;= \sigma^2
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;樣本方差的樣本分佈&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(S^2\)&lt;/span&gt; 常用來標記樣本方差，取代上面我們用到的 &lt;span class=&#34;math inline&#34;&gt;\(V_{n-1}\)&lt;/span&gt;：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[S^2=\frac{1}{n-1}\sum_{i=1}^n(Y_i-\bar{Y})^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;而且上面也證明了，&lt;span class=&#34;math inline&#34;&gt;\(E(S^2)=\sigma^2\)&lt;/span&gt; 是總體方差的無偏估計。然而，要注意的是，樣本標準差 &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{S^2}\)&lt;/span&gt; 卻不是總體標準差 &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; 的無偏估計（因爲並不是線性變換，而是開了根號）。&lt;/p&gt;
&lt;div id=&#34;-s--sigma-&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;證明樣本標準差 &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; 不是總體標準差 &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; 的無偏估計&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
Var(S)               &amp;amp;=E(S^2)-[E(S)]^2 \\
\Rightarrow [E(S)]^2 &amp;amp;=E(S^2)-Var(S) \\
\because E(S^2)      &amp;amp;=\sigma^2 \\
\therefore   [E(S)]^2 &amp;amp;=\sigma^2-Var(S) \\
             E(S)     &amp;amp;=\sqrt{\sigma^2-Var(S)} \\
\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;可見樣本標準差是低估了總體標準差的。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;另外可以被證明的是：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{n-1}{\sigma^2}S^2\sim \mathcal{X}_{n-1}^2\\
Var(S^2)=\frac{2\sigma^4}{n-1}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{X}^2_m\)&lt;/span&gt;： 自由度爲 &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; 的&lt;a href=&#34;https://winterwang.github.io/post/chi-square-distribution/&#34;&gt;卡方分佈&lt;/a&gt;。是在圖形上向右歪曲的分佈。當自由度增加時，會越來越接近正態分佈。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>概率論者統計推斷入門之-被門夾住</title>
      <link>https://winterwang.github.io/post/frequentist-statistical-inference01/</link>
      <pubDate>Tue, 24 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/frequentist-statistical-inference01/</guid>
      <description>&lt;div id=&#34;-population-and-sample&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;人羣與樣本 (population and sample)&lt;/h3&gt;
&lt;p&gt;討論樣本時，需考慮下面幾個問題：&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;樣本是否具有代表性？&lt;/li&gt;
&lt;li&gt;人羣被準確定義了嗎？&lt;/li&gt;
&lt;li&gt;我們感興趣的“人羣”是否可以是無限大（多）的？&lt;/li&gt;
&lt;li&gt;我們研究的樣本，是僅僅用來觀察，亦或是計劃對之進行某種干預呢？&lt;/li&gt;
&lt;li&gt;我們從所有可能的人羣中抽樣了嗎？&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;-sample-and-statistic&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;樣本和統計量 (sample and statistic)&lt;/h3&gt;
&lt;p&gt;通常我們在進行實驗或觀察時只是獲得了樣本的數據。而希望從樣本數據去推斷 (inference) 總體（或人羣）的一些特徵。我們也許只是想用樣本的平均值來估計整體人羣的某個特徵的平均值。不管是何種估計和推斷，都是基於對樣本數據的計算，從樣本中獲得想要推斷總體的&lt;strong&gt;統計量 (statistics)&lt;/strong&gt;。我們用已知樣本去推斷未知總體的過程就叫做&lt;strong&gt;估計 (estimate)&lt;/strong&gt;。這個想要被推斷的總體或人羣的值，被叫做&lt;strong&gt;參數 (parameter)&lt;/strong&gt;，常常使用希臘字母來標記。用來估計總體或人羣的，從樣本數據計算得來的統計量，叫做&lt;strong&gt;估計量 (estimator)&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;所有的統計量，都有&lt;strong&gt;樣本分佈 (sampling distributions，意爲重複無限次取樣後獲得的無限次統計量的分佈)&lt;/strong&gt;。推斷的過程歸納如下：&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;從總體或人羣中抽樣 (樣本量 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;)&lt;/li&gt;
&lt;li&gt;計算這個樣本的合適統計量，從而用於估計它在整體或人羣中的值。&lt;/li&gt;
&lt;li&gt;我們還需要決定計算獲得的統計量的樣本分佈（假定會抽樣無數次）。&lt;/li&gt;
&lt;li&gt;一旦可以精確地確認樣本分佈，我們就可以定量地計算出使用步驟2中獲得的統計量估計總體或人羣的參數時的準確度。&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;-estimation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;估計 Estimation&lt;/h3&gt;
&lt;p&gt;從樣本的均值，推斷總體或人羣的均值是一種估計。我們的目的是，從已知樣本中計算一個儘可能接近那個未知的總體或人羣參數的值。一個估計量有兩個與生俱來的性質 (properties)：1) 偏倚 (bias); 2) 精確度 (precision)。這兩個性質都可以從樣本分佈和估計量獲得。&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;偏倚： 偏倚簡單說就是樣本分佈的均值，也就是我們從樣本中計算獲得的估計量，和我們想要拿它來估計的總體或人羣的參數之間的差距。(The bias is the difference between the mean of the sampling distribution – the expected or average value of the estimator – and the population parameter being estimated.) 一個小的偏倚，確保了我們從樣本中計算獲得的估計值（假設我們抽樣無數次，計算無數個樣本估計值）&lt;strong&gt;均勻地&lt;/strong&gt;分佈在總體或人羣參數的左右兩邊。偏倚本身並不是太大的問題，但是假如樣本量增加，偏倚依然存在（估計量不一致, inconsistent），那常常意味着是抽樣過程出現了問題。例如：&lt;br&gt;用簡單隨機抽樣法獲得的樣本均值，就是總體或人羣均值的無偏估計 (unbiased estimator)。如果抽樣時由於某些主觀客觀的原因導致較小的樣本很少被抽樣（抽樣過程出了問題，脫離了簡單隨機抽樣原則），那麼此時得到的樣本均值就會是一個過高的估計值 (upward biased estimator)。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;精確度：估計值的精確度可以通過樣本分佈的方差或標準差來評價（簡單說是樣本分佈的方差越低，波動越小，精確度越高）。樣本分佈的標準差被定義爲估計值的標準誤。假如估計量是樣本均值，那麼樣本分佈的標準差（估計量的標準誤）和樣本數據之間有如下的關係： &lt;span class=&#34;math display&#34;&gt;\[均值的標準誤 = \frac{樣本數據的標準差}{\sqrt{樣本量大小}}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在一些簡單的情況下，通常估計值的選用不言自明（例如均值，或者百分比）。但是在複雜的情況下，我們可能可以有多個不同類型的估計量可以選擇，他們也常常各有利弊，需要我們做出取捨。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;-confidence-intervals&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;信賴區間 confidence intervals&lt;/h3&gt;
&lt;p&gt;從樣本中計算估計量獲得的一個估計值，只是一個&lt;strong&gt;點估計 (point estimate)&lt;/strong&gt;。對比之下，信賴區間就是一個對這個點估計的精確度的體現。信賴區間越窄，說明我們對於總體或人羣的參數的可能取值的範圍估計越精確。&lt;/p&gt;
&lt;p&gt;信賴區間通常是成對成對的出現的，即有上限和下限。這樣的一對從樣本數據中計算得來的統計量，同樣也是有樣本分佈的。&lt;strong&gt;每次我們重新從總體或人羣中抽樣，計算獲得的信賴區間都不同，這些信賴區間就組成了信賴區間的樣本分佈。總體和人羣的參數落在這些信賴區間範圍內的概率，就是我們常說的信賴區間的水平（&lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt;）。&lt;/strong&gt; 常用的這個概率值就是 &lt;span class=&#34;math inline&#34;&gt;\(95\%, 90\%, 99\%\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;當從樣本數據計算獲得的估計量的信賴區間很寬，說明了這個收集來的數據提供了很少的參數信息，導致估計變得很不精確。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;看到這裏的都是好漢一條啊！ 我不知道你暈了麼有，反正我是已經暈了。。。。&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>偉大的中心極限定理</title>
      <link>https://winterwang.github.io/post/central-limit-theory/</link>
      <pubDate>Thu, 19 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/central-limit-theory/</guid>
      <description>&lt;p&gt;最近明顯可以感覺到課程的步驟開始加速。看我的課表：&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://winterwang.github.io/img/IMG_0522.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;手機畫面太小了。早上都是9點半開始，下午基本都是到5點。週一更慘，到7點。週二-週五中午都被統計中心的講座佔據。簡直是非人的生活。&lt;/p&gt;
&lt;p&gt;這周概率論基礎結束。中心極限定理講完以後我們正式進入了 Inference 統計推斷的課程。我們花了一天時間講什麼是樣本估計 (Estimation)，什麼是參數精確度 (Precision)，什麼是自由度 (degree of freedom)，怎樣進行不偏的估計 (unbiased inference)。然後還有似然方程 (likelihood function)。&lt;/p&gt;
&lt;p&gt;今天的更新還是簡單的把概率論掃尾一下。感受一下中心極限定理的偉大。&lt;/p&gt;
&lt;div id=&#34;-covariance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;協方差 Covariance&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://winterwang.github.io/post/probability2-4/&#34;&gt;之前我們定義過&lt;/a&gt;，兩個獨立連續隨機變量 &lt;span class=&#34;math inline&#34;&gt;\(X,Y\)&lt;/span&gt; 之和的方差 Variance ：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Var(X+Y)=Var(X)+Var(Y)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;然而如果他們並不相互獨立的話：&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
Var(X+Y) &amp;amp;= E[((X+Y)-E(X+Y))^2] \\
         &amp;amp;= E[(X+Y)-(E(X)+E(Y))^2] \\
         &amp;amp;= E[(X-E(X)) - (Y-E(Y))^2] \\
         &amp;amp;= E[(X-E(X))^2+(Y-E(Y))^2 \\
         &amp;amp; \;\;\; +2(X-E(X))(Y-E(Y))] \\
         &amp;amp;= Var(X)+Var(Y)+2E[(X-E(X))(Y-E(Y))]
\end{aligned}\]&lt;/span&gt;
&lt;p&gt;可以發現在兩者和的方差公式展開之後多了一部分 &lt;span class=&#34;math inline&#34;&gt;\(E[(X-E(X))(Y-E(Y))]\)&lt;/span&gt;。 這個多出來的一部分就說明了二者 &lt;span class=&#34;math inline&#34;&gt;\((X, Y)\)&lt;/span&gt; 之間的關係。它被定義爲協方差 (Covariance): &lt;span class=&#34;math display&#34;&gt;\[Cov(X,Y) = E[(X-E(X))(Y-E(Y))]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;所以：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Var(X+Y)=Var(X)+Var(Y)+2Cov(X,Y)\]&lt;/span&gt;&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;要記住，協方差只能用於評價&lt;span class=&#34;math inline&#34;&gt;(X,Y)&lt;/span&gt;之間的線性關係 (Linear Association)。&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;以下是協方差 (Covariance) 的一些特殊性質：&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Cov(X,X)=Var(X)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Cov(X,Y)=Cov(Y,X)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Cov(aX,bY)=ab\:Cov(X,Y)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Cov(aR+bS,cX+dY)=ac\:Cov(R,X)+ad\:Cov(R,Y)\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+bc\:Cov(S,X)+bd\:Cov(S,Y)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Cov(aX+bY,cX+dY)=ac\:Var(X)+ad\:Var(Y)\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+(ad+bc)Cov(X,Y)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Cov(X+Y,X-Y)=Var(X)-Var(Y)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;If &lt;span class=&#34;math inline&#34;&gt;\(X, Y\)&lt;/span&gt; are independent. &lt;span class=&#34;math inline&#34;&gt;\(Cov(X,Y)=0\)&lt;/span&gt; &lt;span class=&#34;diff_alert&#34;&gt;But not vise-versa !&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;-correlation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;相關 Correlation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;協方差雖然&lt;span class=&#34;math inline&#34;&gt;\(Cov(X,Y)\)&lt;/span&gt; 的大小很大程度上會被他們各自的單位和波動大小左右。&lt;/li&gt;
&lt;li&gt;我們將協方差標準化(除以各自的標準差 s.d.) (standardization) 之後，就可以得到相關係數 Corr (&lt;span class=&#34;math inline&#34;&gt;\(-1\sim1\)&lt;/span&gt;): &lt;span class=&#34;math display&#34;&gt;\[Corr(X,Y)=\frac{Cov(X,Y)}{SD(X)SD(Y)}=\frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;-the-central-limit-theory&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;中心極限定理 the Central Limit Theory&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;diff_add&#34;&gt;&lt;strong&gt;如果從人羣中多次選出樣本量爲 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 的樣本，並計算樣本均值, &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}_n\)&lt;/span&gt;。那麼這個樣本均值 &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}_n\)&lt;/span&gt; 的分佈，會隨着樣本量增加 &lt;span class=&#34;math inline&#34;&gt;\(n\rightarrow\infty\)&lt;/span&gt;，而接近正態分佈。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;偉大的中心極限定理告訴我們：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;diff_alert&#34;&gt;&lt;strong&gt;當樣本量足夠大時，樣本均值 &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}_n\)&lt;/span&gt; 的分佈爲正態分佈，這個特性與樣本來自的人羣的分佈 &lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt; 無關。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;再說一遍：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;如果對象是獨立同分佈 i.i.d (identically and independently distributed)。那麼它的總體期望和方差分別是: &lt;span class=&#34;math inline&#34;&gt;\(E(X)=\mu;\;Var(X)=\sigma^2\)&lt;/span&gt;。 根據中心極限定理，可以得到：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;當樣本量增加，樣本均值的分佈服從正態分佈： &lt;span class=&#34;math display&#34;&gt;\[\bar{X}_n\sim N(\mu, \frac{\sigma^2}{n})\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;也可以寫作，當樣本量增加： &lt;span class=&#34;math display&#34;&gt;\[\sum_{i=1}^nX_i \sim N(n\mu,n\sigma^2)\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;有了這個定理，我們可以拋開樣本空間(&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;)的分佈，也不用假定它服從正態分佈。&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;diff_alert&#34;&gt;但是樣本的均值，卻總是服從正態分佈的。&lt;/span&gt;簡直是太完美了！！！！！！&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>你會用概率論來賭博嗎？</title>
      <link>https://winterwang.github.io/post/probability-gambling/</link>
      <pubDate>Sun, 08 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/probability-gambling/</guid>
      <description>&lt;p&gt;轉眼我已經進入課程的第二週了，總體來說，我們一半的時間都在電腦房練習 Stata 的數據清理和簡單的描述統計 (descriptive statistics)。從我個人的經驗來說，數據分析的過程，其實一大半的時間是消耗在 data cleaning 上的，即使手頭拿到了所謂的乾淨的數據，到真正要分析的時候就會發現一大堆的問題在裏面，需要重新整理，重新添加標記以使之變得更加讓人類可以讀懂。電腦是機器，他是不管你的數據是否乾淨的。只要你放了數據進去，邏輯還可以，沒有編程上的語法錯誤，它總歸會出來一些報告和結果的。如果就這麼直接用的話，大部分的人就會掉進陷阱。畢竟數據不光會說出事實真相，&lt;strong&gt;更多的情況下還會把真相給掩蓋住了。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;我的其餘大部分時間都用在了複習高等數學的微積分上了。感覺好似回到了高中時代。其實大學的時候線性代數得分還是接近滿分的。後來多年不用，生疏了。剛打開複習的書的時候，許多微分積分的規則都已經忘記。通過這一週的辛苦練習，終於是找回了一點狀態。如果你也想有空的時候複習以下高中數學知識，這本書可以推薦給你：&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.amazon.co.uk/gp/product/0471827223/ref=oh_aui_detailpage_o04_s00?ie=UTF8&amp;amp;psc=1&#34;&gt;Quick Calculus: Short Manual of Self-instruction&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://winterwang.github.io/img/Selection_070.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;上面這本書的內容可以一邊閱讀，一邊練習。實在是複習的一本好書。我花了一週的課餘時間，從頭到尾把裏面的習題和解答全部完成。收穫很大。感覺年輕時的數學思維又開始在大腦裏復甦了。一身輕鬆。&lt;/p&gt;
&lt;p&gt;下面想介紹一下上週學習的概率的基礎問題。&lt;/p&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;首先是最基礎的&lt;strong&gt;三個概率的公理&lt;/strong&gt;：&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;對於任意事件 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;，它發生的概率 &lt;span class=&#34;math inline&#34;&gt;\(P(A)\)&lt;/span&gt; 滿足這樣的不等式： &lt;span class=&#34;math inline&#34;&gt;\(0 \leqslant P(A) \leqslant 1\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(\Omega)=1\)&lt;/span&gt; , &lt;span class=&#34;math inline&#34;&gt;\(\Omega\)&lt;/span&gt; 是全樣本空間 (total sample space)&lt;/li&gt;
&lt;li&gt;對於互斥（相互獨立）的事件 &lt;span class=&#34;math inline&#34;&gt;\(A_1, A_2, \dots, A_n\)&lt;/span&gt; 有如下的等式關係： &lt;span class=&#34;math inline&#34;&gt;\(P(A_1\cup A_2 \cup \cdots \cup A_n)=P(A_1)+P(A_2)+\cdots+P(A_n)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;你是不是覺得上面三條公理都是&lt;strong&gt;廢話&lt;/strong&gt;。 不用擔心，我也是這麼覺得的。因爲所有人都認同的道理，才能成爲公理 (axiom)，因爲它們是不需要證明的自然而然形成的人人都接受的觀念。&lt;code&gt;(axiom: a saying that is widely accepted on its own merits; its truth is assumed to be self-evident)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;然而，正是這樣顯而易見的道理，確是拿來建築理論的基石，千萬不能小看了他們。例如，我們看下面這個看似也應該成爲公理的公式，你能證明嗎：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(A_1\cup A_2) = P(A_1) + P(A_2) - P(A_1 \cap A_2)\)&lt;/span&gt;&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://winterwang.github.io/img/venngram.png&#34; /&gt;

&lt;/div&gt;
&lt;div class=&#34;section level4&#34;&gt;
&lt;h4&gt;證明：&lt;/h4&gt;
&lt;p&gt;先考慮 &lt;span class=&#34;math inline&#34;&gt;\(A_1 \cup A_2\)&lt;/span&gt; 是什麼（拆分成三個互斥事件）&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(A_1 \cup A_2 = (A_1\cap \bar{A_2})\cup(\bar{A_1}\cap A_2)\cup(A_1\cap A_2)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;運用上面的公理&lt;del&gt;2&lt;/del&gt; 3&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\therefore P(A_1 \cup A_2) = P(A_1\cap \bar{A_2}) + P(\bar{A_1}\cap A_2) + P(A_1\cap A_2) \;\;\;\;\;\;(1)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;再考慮 &lt;span class=&#34;math inline&#34;&gt;\(A_1=(A_1\cap A_2)\cup(A_1\cap\bar{A_2})\)&lt;/span&gt; 繼續拆分成兩個互斥事件&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\therefore P(A_1)=P(A_1\cap A_2)+P(A_1\cap\bar{A_2})\)&lt;/span&gt; 整理一下：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(A_1\cap\bar{A_2})=P(A_1)-P(A_1\cap A_2)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;同理可得: &lt;span class=&#34;math inline&#34;&gt;\(P(\bar{A_1}\cap A_2)=P(A_2)-P(A_1\cap A_2)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;代入上面第(1)式可得：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(A_1 \cup A_2) =P(A_1)-P(A_1\cap A_2)\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+P(A_2)-P(A_1\cap A_2)\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+P(A_1\cap A_2)\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;=P(A_1) + P(A_2) - P(A_1 \cap A_2)\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;-conditional-probability&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;條件概率 Conditional probability&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(A|S)=\frac{P(A\cap S)}{P(S)}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(A\cap S) = P(A|S)P(S)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;-independence-&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;獨立 (independence) 的定義&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;兩個事件定義爲互爲獨立時 (&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; are said to be independent &lt;strong&gt;if and only if&lt;/strong&gt;) &lt;span class=&#34;math display&#34;&gt;\[P(A\cap B)=P(A)P(B)\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;因爲從條件概率的概念我們已知&lt;br&gt; &lt;span class=&#34;math inline&#34;&gt;\(P(A\cap B) = P(A|B)P(B)\)&lt;/span&gt; &lt;br&gt;所以&lt;span class=&#34;math inline&#34;&gt;\(P(A|B)=P(A)\)&lt;/span&gt; 即：事件 &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; 無法提供事件 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 的任何有效訊息 (&lt;strong&gt;&lt;span class=&#34;math inline&#34;&gt;\(A, B\)&lt;/span&gt; 互相獨立&lt;/strong&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;賭博問題&lt;/h2&gt;
&lt;p&gt;終於來到本次話題的重點了。我要扣題了哦。語文老師快在此加分。&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://winterwang.github.io/img/Selection_071.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;假設你在一個電視遊戲節目。有上圖一樣的三扇門。其中一扇門後面有一輛保時捷，另兩扇門後面則是&lt;a href=&#34;https://winterwang.github.io/post/black-meal/&#34;&gt;(味道奇特的)山羊&lt;/a&gt;。遊戲規則是主持人會讓你先選擇其中一扇門（先不打開你選的這扇門）。主持人隨後打開另外兩扇門中的一扇沒有保時捷的門。主持人問你，你要堅持選擇之前選中的那扇門，還是要改變主意換一扇門去猜是否可以猜中保時捷。 請問，堅持選擇之前選中的門猜中保時捷的概率高，還是主持人打開一扇門以後改變主意猜中保時捷的概率更高呢？&lt;/p&gt;
&lt;p&gt;答案明天揭曉。&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
