<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>statistics | Be Ambitious</title>
    <link>https://wangcc.me/categories/statistics/</link>
      <atom:link href="https://wangcc.me/categories/statistics/index.xml" rel="self" type="application/rss+xml" />
    <description>statistics</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2017-2019 Chaochen Wang | 王超辰</copyright><lastBuildDate>Fri, 16 Aug 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://wangcc.me/img/icon-192.png</url>
      <title>statistics</title>
      <link>https://wangcc.me/categories/statistics/</link>
    </image>
    
    <item>
      <title>等級線性回歸模型的 Rstan 貝葉斯實現</title>
      <link>https://wangcc.me/post/multilevel-model-rstan/</link>
      <pubDate>Fri, 16 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/multilevel-model-rstan/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#多層等級線性回歸模型混合效應模型-multilevelmixed-effect-regression-model&#34;&gt;多層(等級)線性回歸模型/混合效應模型 multilevel/mixed effect regression model&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#適用於等級線性回歸模型的數據&#34;&gt;適用於等級線性回歸模型的數據&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#確認數據分佈&#34;&gt;確認數據分佈&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#如果不考慮組間公司間差異&#34;&gt;如果不考慮組間(公司間)差異&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#如果要考慮組間差異&#34;&gt;如果要考慮組間差異&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#等級線性回歸的貝葉斯實現&#34;&gt;等級線性回歸的貝葉斯實現&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#模型機制-mechanism&#34;&gt;模型機制 mechanism&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;多層等級線性回歸模型混合效應模型-multilevelmixed-effect-regression-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;多層(等級)線性回歸模型/混合效應模型 multilevel/mixed effect regression model&lt;/h2&gt;
&lt;p&gt;關於等級線性回歸的基本知識和概念，請參考&lt;a href=&#34;https://wangcc.me/LSHTMlearningnote/Hierarchical.html&#34;&gt;讀書筆記58-60章節&lt;/a&gt;。簡單來說，等級線性回歸通過給數據內部可能存在或者已知存在的結構或者層級增加隨機截距或者隨機斜率的方式來輔助解釋組間差異和組內的差異。&lt;/p&gt;
&lt;div id=&#34;適用於等級線性回歸模型的數據&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;適用於等級線性回歸模型的數據&lt;/h3&gt;
&lt;p&gt;本章節使用的數據是四家大公司40名社員的年齡和年收入數據：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d &amp;lt;- read.csv(file=&amp;#39;../../static/files/data-salary-2.txt&amp;#39;)
d&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     X   Y KID
## 1   7 457   1
## 2  10 482   1
## 3  16 518   1
## 4  25 535   1
## 5   5 427   1
## 6  25 603   1
## 7  26 610   1
## 8  18 484   1
## 9  17 508   1
## 10  1 380   1
## 11  5 453   1
## 12  4 391   1
## 13 19 559   1
## 14 10 453   1
## 15 21 517   1
## 16 12 553   2
## 17 17 653   2
## 18 22 763   2
## 19  9 538   2
## 20 18 708   2
## 21 21 740   2
## 22  6 437   2
## 23 15 646   2
## 24  4 422   2
## 25  7 444   2
## 26 10 504   2
## 27  2 376   2
## 28 15 522   3
## 29 27 623   3
## 30 14 515   3
## 31 18 542   3
## 32 20 529   3
## 33 18 540   3
## 34 11 411   3
## 35 26 666   3
## 36 22 641   3
## 37 25 592   3
## 38 28 722   4
## 39 24 726   4
## 40 22 728   4&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;X&lt;/code&gt;: 社員年齡減去23獲得的數據（23歲是大部分人大學畢業入職時的年齡）&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Y&lt;/code&gt;: 年收入（萬日元）&lt;/li&gt;
&lt;li&gt;&lt;code&gt;KID&lt;/code&gt;: 公司編號&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我們認爲，年收入 &lt;code&gt;Y&lt;/code&gt;，是基本平均年收入和隨機誤差（服從均值爲零，方差是 &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; 的正態分佈）之和。且基本平均年收入和年齡成正比（年功序列型企業）。但是呢，因爲不同的公司入職時的基本收入可能不同，且可能隨着年齡增加而增長薪水的速度可能也不一樣。那麼由於不同公司所造成的差異，可以被認爲是組間差異。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;確認數據分佈&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;確認數據分佈&lt;/h3&gt;
&lt;p&gt;這次分析的目的是要瞭解「每個公司&lt;code&gt;KID&lt;/code&gt;內隨着年齡的增加而增長的薪水幅度是多少」，那麼我們要在結果報告中體現的就是每家公司的基本年收入，新入職時的年收入，以及隨着年齡增長而上升的薪水的事後分佈。&lt;/p&gt;
&lt;p&gt;我們先來看把四家公司職員放在一起時的整體圖形：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)

d$KID &amp;lt;- as.factor(d$KID)
res_lm &amp;lt;- lm(Y ~ X, data=d)
coef &amp;lt;- as.numeric(res_lm$coefficients)

p &amp;lt;- ggplot(d, aes(X, Y, shape=KID))
p &amp;lt;- p + theme_bw(base_size=18)
p &amp;lt;- p + geom_abline(intercept=coef[1], slope=coef[2], size=2, alpha=0.3)
p &amp;lt;- p + geom_point(size=2)
p &amp;lt;- p + scale_shape_manual(values=c(16, 2, 4, 9))
p &amp;lt;- p + labs(x=&amp;#39;X (age-23)&amp;#39;, y=&amp;#39;Y (10,000 Yen/year)&amp;#39;)
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:fig8-1&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-08-16-multilevel-model-rstan_files/figure-html/fig8-1-1.png&#34; alt=&#34;年齡和年收入的散點圖，不同點的形狀代表不同的公司編號。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: 年齡和年收入的散點圖，不同點的形狀代表不同的公司編號。
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;從總體的散點圖 &lt;a href=&#34;#fig:fig8-1&#34;&gt;1&lt;/a&gt; 來看，似乎年收入確實是隨着年齡增長而呈現直線增加的趨勢。但是公司編號 &lt;code&gt;KID = 4&lt;/code&gt; 的三名社員薪水似乎是在同一水平的並無明顯變化。這一點可以把四家公司社員的數據分開來看更加清晰:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- ggplot(d, aes(X, Y, shape=KID))
p &amp;lt;- p + theme_bw(base_size=20)
p &amp;lt;- p + geom_abline(intercept=coef[1], slope=coef[2], size=2, alpha=0.3)
p &amp;lt;- p + facet_wrap(~KID)
p &amp;lt;- p + geom_line(stat=&amp;#39;smooth&amp;#39;, method=&amp;#39;lm&amp;#39;, se=FALSE, size=1, color=&amp;#39;black&amp;#39;, linetype=&amp;#39;31&amp;#39;, alpha=0.8)
p &amp;lt;- p + geom_point(size=3)
p &amp;lt;- p + scale_shape_manual(values=c(16, 2, 4, 9))
p &amp;lt;- p + labs(x=&amp;#39;X (age-23)&amp;#39;, y=&amp;#39;Y (10,000 Yen/year)&amp;#39;)
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:fig8-2&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-08-16-multilevel-model-rstan_files/figure-html/fig8-2-1.png&#34; alt=&#34;年齡和年收入的散點圖，不同的公司在四個平面中展示,黑色點線是每家公司數據單獨使用線性回歸時獲得的直線。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: 年齡和年收入的散點圖，不同的公司在四個平面中展示,黑色點線是每家公司數據單獨使用線性回歸時獲得的直線。
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;如果不考慮組間公司間差異&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;如果不考慮組間(公司間)差異&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;模型的數學描述&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
              Y[n] &amp;amp; = y_{\text{base}}[n] + \varepsilon[n] &amp;amp; n = 1, \dots, N \\
y_{\text{base}}[n] &amp;amp; = a + bX[n]                           &amp;amp; n = 1, \dots, N \\
    \varepsilon[n] &amp;amp; \sim \text{Normal}(0, \sigma_Y^2)     &amp;amp; n = 1, \dots, N \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;當然，如果你想，模型可以直接簡化成：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y[n] \sim \text{Normal}(a + bX[n], \sigma^2_Y) \;\;\;\;\;\; n = 1, \dots, N \\
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;上述簡化版的模型，翻譯成Stan語言如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data {
  int N;
  real X[N];
  real Y[N];
}

parameters{
  real a;
  real b;
  real&amp;lt;lower = 0&amp;gt; s_Y;
}

model {
  for (n in 1 : N)
  Y[n] = normal(a + b * X[n], s_Y);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;下面的 R 代碼用來實現對上面 Stan 模型的擬合:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rstan)
d &amp;lt;- read.csv(file=&amp;#39;../../static/files/data-salary-2.txt&amp;#39;)
d$KID &amp;lt;- as.factor(d$KID)

data &amp;lt;- list(N=nrow(d), X=d$X, Y=d$Y)
fit &amp;lt;- stan(file=&amp;#39;stanfiles/model8-1.stan&amp;#39;, data=data, seed=1234)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &amp;#39;model8-1&amp;#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 8e-06 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.062709 seconds (Warm-up)
## Chain 1:                0.061853 seconds (Sampling)
## Chain 1:                0.124562 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &amp;#39;model8-1&amp;#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 4e-06 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.066626 seconds (Warm-up)
## Chain 2:                0.032892 seconds (Sampling)
## Chain 2:                0.099518 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &amp;#39;model8-1&amp;#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 4e-06 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.056179 seconds (Warm-up)
## Chain 3:                0.033966 seconds (Sampling)
## Chain 3:                0.090145 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &amp;#39;model8-1&amp;#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 4e-06 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.078616 seconds (Warm-up)
## Chain 4:                0.050848 seconds (Sampling)
## Chain 4:                0.129464 seconds (Total)
## Chain 4:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Stan model: model8-1.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##         mean se_mean    sd    2.5%     25%     50%     75%   97.5% n_eff Rhat
## a     376.97    0.58 24.54  329.21  360.86  377.27  393.00  425.66  1811    1
## b      10.99    0.03  1.41    8.25   10.07   10.98   11.94   13.78  1865    1
## s_Y    68.85    0.19  8.21   54.92   63.07   68.08   73.79   86.74  1938    1
## lp__ -184.12    0.03  1.29 -187.39 -184.69 -183.79 -183.17 -182.66  1450    1
## 
## Samples were drawn using NUTS(diag_e) at Tue Nov 26 15:35:18 2019.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;現在有更加方便的 &lt;code&gt;rstanarm&lt;/code&gt; 包可以幫助我們省去寫 Stan 模型的過程：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rstanarm)

rstanarm_results = stan_glm(Y ~ X, data=d, iter=2000, warmup=1000, cores=4)
summary(rstanarm_results, probs=c(.025, .975), digits=3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Model Info:
##  function:     stan_glm
##  family:       gaussian [identity]
##  formula:      Y ~ X
##  algorithm:    sampling
##  sample:       4000 (posterior sample size)
##  priors:       see help(&amp;#39;prior_summary&amp;#39;)
##  observations: 40
##  predictors:   2
## 
## Estimates:
##               mean    sd      2.5%    97.5%
## (Intercept) 376.487  24.016 329.488 423.410
## X            11.037   1.377   8.225  13.740
## sigma        67.743   8.119  53.733  85.183
## 
## Fit Diagnostics:
##            mean    sd      2.5%    97.5%
## mean_PPD 547.968  15.618 517.363 578.565
## 
## The mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help(&amp;#39;summary.stanreg&amp;#39;)).
## 
## MCMC diagnostics
##               mcse  Rhat  n_eff
## (Intercept)   0.386 1.000 3862 
## X             0.022 1.000 3972 
## sigma         0.136 1.000 3548 
## mean_PPD      0.263 1.000 3535 
## log-posterior 0.031 1.002 1651 
## 
## For each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;可以看到強制不同公司社員的年收入來自同一個正態分佈時，方差顯得非常的大。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;如果要考慮組間差異&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;如果要考慮組間差異&lt;/h3&gt;
&lt;p&gt;我們認爲每家公司社員新入職時的起點薪水不同(截距不同-隨機截距)，進入公司之後隨年齡增加的薪水幅度也不同(斜率不同-隨機斜率)。因此，用 &lt;span class=&#34;math inline&#34;&gt;\(a[1]\sim a[K], K = 1, 2, 3, 4\)&lt;/span&gt; 表示每家公司的截距，用 &lt;span class=&#34;math inline&#34;&gt;\(b[1] \sim b[K], K = 1, 2, 3, 4\)&lt;/span&gt; 表示每家公司薪水上升的斜率。那麼每家公司的薪水年齡線性回歸模型可以寫作是 &lt;span class=&#34;math inline&#34;&gt;\(a[K] + b[K] X, K = 1, 2, 3, 4\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;模型數學描述&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y[n] \sim \text{Normal}(a[\text{KID[n]}] + b[\text{KID}[n]] X[n], \sigma^2_Y) \\ n = 1, \dots, N
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;上述模型的 Stan 譯文如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data {
  int N;
  int K;
  real X[N];
  real Y[N];
  int&amp;lt;lower = 1, upper = K&amp;gt; KID[N];
}

parameters {
  real a[K];
  real b[K];
  real&amp;lt;lower = 0&amp;gt; s_Y; 
}

model {
  for (n in 1:N)
  Y[n] ~ normal(a[KID[n]] + b[KID[n]] * X[n], s_Y);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;下面的 R 代碼用來實現上面貝葉斯多組不同截距不同斜率線性回歸模型的擬合:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rstan)
d &amp;lt;- read.csv(file=&amp;#39;../../static/files/data-salary-2.txt&amp;#39;)

data &amp;lt;- list(N=nrow(d), X=d$X, Y=d$Y, KID = d$KID, K = 4)
fit &amp;lt;- stan(file=&amp;#39;stanfiles/model8-2.stan&amp;#39;, data=data, seed=1234)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &amp;#39;model8-2&amp;#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 1.1e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.280227 seconds (Warm-up)
## Chain 1:                0.238368 seconds (Sampling)
## Chain 1:                0.518595 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &amp;#39;model8-2&amp;#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 4e-06 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.298175 seconds (Warm-up)
## Chain 2:                0.182623 seconds (Sampling)
## Chain 2:                0.480798 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &amp;#39;model8-2&amp;#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 4e-06 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.310218 seconds (Warm-up)
## Chain 3:                0.168221 seconds (Sampling)
## Chain 3:                0.478439 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &amp;#39;model8-2&amp;#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 4e-06 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.318433 seconds (Warm-up)
## Chain 4:                0.193179 seconds (Sampling)
## Chain 4:                0.511612 seconds (Total)
## Chain 4:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Stan model: model8-2.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##         mean se_mean     sd    2.5%     25%     50%     75%   97.5% n_eff Rhat
## a[1]  387.11    0.27  14.17  359.31  377.78  387.20  396.47  415.13  2828    1
## a[2]  329.03    0.33  16.43  296.89  317.97  328.89  340.33  360.46  2517    1
## a[3]  314.11    0.75  34.43  246.03  291.69  313.70  335.87  382.48  2129    1
## a[4]  748.39    2.88 153.50  446.07  643.82  746.53  853.26 1054.33  2834    1
## b[1]    7.52    0.02   0.87    5.82    6.92    7.52    8.10    9.26  2604    1
## b[2]   19.82    0.02   1.21   17.47   19.01   19.82   20.63   22.20  2438    1
## b[3]   12.44    0.03   1.70    9.08   11.34   12.44   13.57   15.82  2595    1
## b[4]   -0.93    0.12   6.20  -13.37   -5.14   -0.91    3.22   11.19  2827    1
## s_Y    27.17    0.07   3.58   21.23   24.68   26.82   29.30   35.16  2695    1
## lp__ -148.01    0.06   2.40 -153.78 -149.35 -147.61 -146.26 -144.55  1414    1
## 
## Samples were drawn using NUTS(diag_e) at Tue Nov 26 15:36:07 2019.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;等級線性回歸的貝葉斯實現&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;等級線性回歸的貝葉斯實現&lt;/h2&gt;
&lt;div id=&#34;模型機制-mechanism&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;模型機制 mechanism&lt;/h3&gt;
&lt;p&gt;如果我們認爲每家公司的起點薪水 &lt;span class=&#34;math inline&#34;&gt;\(a[k]\)&lt;/span&gt; 服從正態分佈，且該正態分佈的平均值是全體公司的起點薪水的均值 &lt;span class=&#34;math inline&#34;&gt;\(a_\mu\)&lt;/span&gt;，方差是 &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_a\)&lt;/span&gt;。類似地，假設每家公司內隨着年齡增長而增加薪水的幅度 &lt;span class=&#34;math inline&#34;&gt;\(b[k]\)&lt;/span&gt; 也服從某個正態分佈，均值和方差分別是 &lt;span class=&#34;math inline&#34;&gt;\(b_\mu, \sigma^2_b\)&lt;/span&gt;。這樣我們就不僅僅是允許了各家公司的薪水年齡回歸直線擁有不同的斜率和截距，還對這些隨機斜率和截距的前概率分佈進行了設定。&lt;/p&gt;
&lt;p&gt;此時，隨機效應模型的數學表達式就可以寫成下面這樣:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
Y[n] &amp;amp;\sim \text{Normal}(a[\text{KID[n]}] + b[\text{KID}[n]] X[n], \sigma^2_Y) &amp;amp; n = 1, \dots, N \\
a[k] &amp;amp;= a_\mu + a_\varepsilon[k]   &amp;amp; k = 1, \dots, K \\
a_\varepsilon[k] &amp;amp; \sim \text{Normal}(0, \sigma^2_a) &amp;amp; k = 1, \dots, K \\
b[k] &amp;amp; = b_\mu + b_\varepsilon[k]  &amp;amp; k = 1, \dots, K \\
b_\varepsilon[k] &amp;amp;\sim \text{Normal}(0, \sigma^2_b) &amp;amp; k = 1, \dots, K
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;使用 &lt;code&gt;rstanarm&lt;/code&gt; 包可以使用下面的代碼實現&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rstanarm)
M1_stanlmer &amp;lt;- stan_lmer(formula = Y ~ X  + (X | KID), 
                            data = d,
                            seed = 1234)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &amp;#39;continuous&amp;#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 8.5e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.85 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 4.99284 seconds (Warm-up)
## Chain 1:                1.6147 seconds (Sampling)
## Chain 1:                6.60755 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &amp;#39;continuous&amp;#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 1.8e-05 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 6.82034 seconds (Warm-up)
## Chain 2:                2.69244 seconds (Sampling)
## Chain 2:                9.51278 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &amp;#39;continuous&amp;#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 1.7e-05 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.17 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 4.25133 seconds (Warm-up)
## Chain 3:                2.50171 seconds (Sampling)
## Chain 3:                6.75304 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &amp;#39;continuous&amp;#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 1.7e-05 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.17 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 5.0058 seconds (Warm-up)
## Chain 4:                2.08604 seconds (Sampling)
## Chain 4:                7.09184 seconds (Total)
## Chain 4:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(M1_stanlmer, digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## stan_lmer
##  family:       gaussian [identity]
##  formula:      Y ~ X + (X | KID)
##  observations: 40
## ------
##             Median MAD_SD
## (Intercept) 359.02  15.02
## X            12.80   2.98
## 
## Auxiliary parameter(s):
##       Median MAD_SD
## sigma 30.00   3.58 
## 
## Error terms:
##  Groups   Name        Std.Dev. Corr 
##  KID      (Intercept) 23.94         
##           X            8.76    -0.26
##  Residual             30.28         
## Num. levels: KID 4 
## 
## ------
## * For help interpreting the printed output see ?print.stanreg
## * For info on the priors used see ?prior_summary.stanreg&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(M1_stanlmer, 
        pars = c(&amp;quot;(Intercept)&amp;quot;, &amp;quot;X&amp;quot;,&amp;quot;sigma&amp;quot;, 
                 &amp;quot;Sigma[KID:(Intercept),(Intercept)]&amp;quot;,
                 &amp;quot;Sigma[KID:X,(Intercept)]&amp;quot;, &amp;quot;Sigma[KID:X,X]&amp;quot;),
        probs = c(0.025, 0.975),
        digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Model Info:
##  function:     stan_lmer
##  family:       gaussian [identity]
##  formula:      Y ~ X + (X | KID)
##  algorithm:    sampling
##  sample:       4000 (posterior sample size)
##  priors:       see help(&amp;#39;prior_summary&amp;#39;)
##  observations: 40
##  groups:       KID (4)
## 
## Estimates:
##                                      mean    sd      2.5%    97.5%
## (Intercept)                         358.72   18.63  322.31  394.35
## X                                    12.59    3.97    3.57   20.91
## sigma                                30.28    3.82   23.76   38.95
## Sigma[KID:(Intercept),(Intercept)]  572.98 1381.86    1.70 3803.32
## Sigma[KID:X,(Intercept)]            -54.05  190.09 -444.45  144.95
## Sigma[KID:X,X]                       76.74  130.06    6.91  412.48
## 
## MCMC diagnostics
##                                    mcse  Rhat  n_eff
## (Intercept)                         0.38  1.00 2464 
## X                                   0.14  1.00  771 
## sigma                               0.08  1.00 2032 
## Sigma[KID:(Intercept),(Intercept)] 33.93  1.01 1659 
## Sigma[KID:X,(Intercept)]            4.80  1.00 1569 
## Sigma[KID:X,X]                      4.34  1.01  898 
## 
## For each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;和非貝葉斯版本的概率論隨機效應線性回歸模型的結果相對比一下：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lme4)
M1 &amp;lt;- lmer(formula = Y ~ X  + (X | KID), 
           data = d, 
           REML = TRUE)
summary(M1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by REML [&amp;#39;lmerMod&amp;#39;]
## Formula: Y ~ X + (X | KID)
##    Data: d
## 
## REML criterion at convergence: 387.2
## 
## Scaled residuals: 
##      Min       1Q   Median       3Q      Max 
## -2.36969 -0.51837 -0.03545  0.76358  1.87881 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev. Corr 
##  KID      (Intercept) 503.78   22.445        
##           X            28.53    5.341   -1.00
##  Residual             833.95   28.878        
## Number of obs: 40, groups:  KID, 4
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)  358.207     15.383  23.286
## X             13.067      2.741   4.767
## 
## Correlation of Fixed Effects:
##   (Intr)
## X -0.848
## convergence code: 0
## boundary (singular) fit: see ?isSingular&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>泊松回歸模型的貝葉斯Stan實現</title>
      <link>https://wangcc.me/post/poisson-stan/</link>
      <pubDate>Thu, 11 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/poisson-stan/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#分析目的數據和選擇-poisson-回歸模型的原因&#34;&gt;分析目的，數據，和選擇 Poisson 回歸模型的原因&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#想象模型機制&#34;&gt;想象模型機制&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#寫下數學模型表達式&#34;&gt;寫下數學模型表達式&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#把數學模型翻譯成-stan-模型代碼&#34;&gt;把數學模型翻譯成 Stan 模型代碼&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#運行結果的解釋&#34;&gt;運行結果的解釋&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;分析目的數據和選擇-poisson-回歸模型的原因&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;分析目的，數據，和選擇 Poisson 回歸模型的原因&lt;/h1&gt;
&lt;p&gt;我們這裏使用&lt;a href=&#34;https://wangcc.me/post/logistic-rstan/&#34;&gt;之前擬合貝葉斯邏輯回歸模型&lt;/a&gt;時使用的相同的數據來展示如何跑貝葉斯泊松回歸模型。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d &amp;lt;- read.table(&amp;quot;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap05/input/data-attendance-2.txt&amp;quot;, sep = &amp;quot;,&amp;quot;, header = T)
head(d)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   PersonID A Score  M  Y
## 1        1 0    69 43 38
## 2        2 1   145 56 40
## 3        3 0   125 32 24
## 4        4 1    86 45 33
## 5        5 1   158 33 23
## 6        6 0   133 61 60&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;PersonID&lt;/code&gt;: 是學生的編號；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;A&lt;/code&gt;, &lt;code&gt;Score&lt;/code&gt;: 用來預測出勤率的兩個預測變量，分別是表示是否喜歡打工的 &lt;code&gt;A&lt;/code&gt;，和表示對學習本身是否喜歡的評分 (滿分200)；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;M&lt;/code&gt;: 過去三個月內，該名學生一共需要上課的總課時數；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Y&lt;/code&gt;: 過去三個月內，該名學生實際上出勤的課時數。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;這一次我們希望通過分析泊松回歸來回答「&lt;code&gt;A&lt;/code&gt; 和 &lt;code&gt;Score&lt;/code&gt; 對總課時數 &lt;code&gt;M&lt;/code&gt; 具體有多大的影響？」這個問題。&lt;a href=&#34;https://wangcc.me/post/logistic-rstan/&#34;&gt;之前擬合貝葉斯邏輯回歸模型&lt;/a&gt;時，使用的結果變量是 &lt;code&gt;Y&lt;/code&gt;，也就是實際出勤課時數。但是本小節我們用 &lt;code&gt;M&lt;/code&gt; 作爲結果變量。因爲總課時數是學生自己選課時的結果，也就是說學生本身的態度（是否喜歡打工，是否熱愛學習），可能本身左右了他/她到底會選多少課。背景知識假設是：喜歡多去打工的學生，選課可能態度消極，總課時數從開始可能就選的少。那麼像總選課時數這樣的非負（計數型）離散變量作爲結果變量的時候，&lt;strong&gt;泊松回歸模型是我們的第一選擇。&lt;/strong&gt;&lt;/p&gt;
&lt;div id=&#34;想象模型機制&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;想象模型機制&lt;/h2&gt;
&lt;p&gt;如果使用&lt;a href=&#34;https://wangcc.me/post/rstan-wonderful-r3/&#34;&gt;上上節介紹的多重線性回歸模型&lt;/a&gt;，那麼模型的預測變量的分佈便可能取到負數，這樣就不符合實際情況下“總選課時數”是非負（計數型）離散變量這一事實。這就需要把預測變量 &lt;code&gt;A&lt;/code&gt; 和 &lt;code&gt;Score&lt;/code&gt; 相加的線性模型 &lt;span class=&#34;math inline&#34;&gt;\((b_1 + b_2A + b_3Score)\)&lt;/span&gt;，通過數學轉換限制在非負數範圍。假設平均總課時數是 &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;，我們認爲它服從均值是 &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; 的泊松分佈。關於泊松分佈的詳細知識，期望值和方差的推導可以參考&lt;a href=&#34;https://wangcc.me/LSHTMlearningnote/poisson.html&#34;&gt;學習筆記&lt;/a&gt;。另外，非貝葉斯版本的一般性傳統泊松回歸模型可以參照學習筆記的&lt;a href=&#34;https://wangcc.me/LSHTMlearningnote/poisson-regression.html&#34;&gt;廣義線性回歸的泊松回歸模型章節&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;對泊松回歸模型略有瞭解的話應該很自然地想到，把結果變量限制在非負數範圍的標準鏈接方程是 &lt;span class=&#34;math inline&#34;&gt;\(\log(\lambda)\)&lt;/span&gt;，或者在 Stan 模型中，我們更自然地把線性模型部分寫在指數模型中: &lt;span class=&#34;math inline&#34;&gt;\(\exp(b_1 + b_2A + b_3Score)\)&lt;/span&gt;。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;寫下數學模型表達式&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;寫下數學模型表達式&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\lambda[n] &amp;amp; = \exp(b_1 + b_2A[n] + b_3Score[n]) &amp;amp; n = 1, \dots, N \\
M[n]       &amp;amp; \sim \text{Poisson}(\lambda[n])     &amp;amp; n = 1, \dots, N
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;，是該數據中學生的人數；&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;，是每名學生的標籤/編號（下標）；&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(b_1, b_2, b_3\)&lt;/span&gt; 是我們感興趣的參數。&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;把數學模型翻譯成-stan-模型代碼&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;把數學模型翻譯成 Stan 模型代碼&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;data {
  int N; 
  int&amp;lt;lower=0, upper=1&amp;gt; A[N]; 
  real&amp;lt;lower=0, upper=1&amp;gt; Score[N]; 
  int&amp;lt;lower=0&amp;gt; M[N];
}

parameters {
  real b[3]; 
}

transformed parameters {
  real lambda[N];
  for (n in 1:N) {
    lambda[n] = exp(b[1] + b[2]*A[n] + b[3]*Score[n]);
  }
}

model {
  for (n in 1:N) {
    M[n] ~ poisson(lambda[n]); 
  }
}

generated quantities {
  int m_pred[N]; 
  for (n in 1:N) {
    m_pred[n] = poisson_rng(M[n], q[n]);
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;值得一提的是，在 Stan 中，提供了 &lt;code&gt;poisson_log(x)&lt;/code&gt; 分佈函數，其實它等價於使用 &lt;code&gt;poisson(exp(x))&lt;/code&gt;。除了更加接近我們熟悉的泊松回歸模型的數學表達式，避免了 &lt;code&gt;exp&lt;/code&gt; 指數運算，計算結果穩定。於是我們還可以把上面的模型修改成：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data {
  int N; 
  int&amp;lt;lower=0, upper=1&amp;gt; A[N]; 
  real&amp;lt;lower=0, upper=1&amp;gt; Score[N]; 
  int&amp;lt;lower=0&amp;gt; M[N];
}

parameters {
  real b[3]; 
}

transformed parameters {
  real lambda[N];
  for (n in 1:N) {
    lambda[n] = b[1] + b[2]*A[n] + b[3]*Score[n]；
  }
}

model {
  for (n in 1:N) {
    M[n] ~ poisson_log(lambda[n]); 
  }
}

generated quantities {
  int m_pred[N]; 
  for (n in 1:N) {
    m_pred[n] = poisson_log_rng(M[n], q[n]);
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;運行它的代碼如下：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rstan)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: StanHeaders&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggplot2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## rstan (Version 2.19.2, GitRev: 2e1f913d3ca3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## For execution on a local, multicore CPU with excess RAM we recommend calling
## options(mc.cores = parallel::detectCores()).
## To avoid recompilation of unchanged Stan programs, we recommend calling
## rstan_options(auto_write = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data &amp;lt;- list(N=nrow(d), A=d$A, Score=d$Score/200, M=d$M)
# fit &amp;lt;- stan(file=&amp;#39;model/model5-6.stan&amp;#39;, data=data, seed=1234)
fit &amp;lt;- stan(file=&amp;#39;stanfiles/model5-6b.stan&amp;#39;, data=data, seed=1234, pars = c(&amp;quot;b&amp;quot;, &amp;quot;lambda&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &amp;#39;model5-6b&amp;#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 1.5e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.099699 seconds (Warm-up)
## Chain 1:                0.1112 seconds (Sampling)
## Chain 1:                0.210899 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &amp;#39;model5-6b&amp;#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 9e-06 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.102645 seconds (Warm-up)
## Chain 2:                0.113692 seconds (Sampling)
## Chain 2:                0.216337 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &amp;#39;model5-6b&amp;#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 7e-06 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.104213 seconds (Warm-up)
## Chain 3:                0.1045 seconds (Sampling)
## Chain 3:                0.208713 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &amp;#39;model5-6b&amp;#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 7e-06 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.099824 seconds (Warm-up)
## Chain 4:                0.109879 seconds (Sampling)
## Chain 4:                0.209703 seconds (Total)
## Chain 4:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Stan model: model5-6b.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##               mean se_mean   sd    2.5%     25%     50%     75%   97.5% n_eff
## b[1]          3.58    0.00 0.10    3.38    3.51    3.58    3.64    3.76  1187
## b[2]          0.26    0.00 0.04    0.18    0.24    0.26    0.29    0.35  1831
## b[3]          0.29    0.00 0.15    0.01    0.19    0.29    0.39    0.58  1227
## lambda[1]     3.68    0.00 0.05    3.58    3.65    3.68    3.71    3.77  1276
## lambda[2]     4.05    0.00 0.03    3.98    4.03    4.05    4.07    4.12  2189
## lambda[3]     3.76    0.00 0.03    3.70    3.74    3.76    3.78    3.81  2467
## lambda[4]     3.97    0.00 0.04    3.88    3.94    3.97    3.99    4.05  1735
## lambda[5]     4.07    0.00 0.04    3.99    4.04    4.07    4.10    4.15  1793
## lambda[6]     3.77    0.00 0.03    3.71    3.75    3.77    3.79    3.83  2509
## lambda[7]     3.74    0.00 0.03    3.68    3.72    3.74    3.76    3.79  1991
## lambda[8]     4.05    0.00 0.03    3.98    4.03    4.06    4.08    4.12  2102
## lambda[9]     3.79    0.00 0.03    3.72    3.77    3.79    3.81    3.85  2282
## lambda[10]    3.79    0.00 0.03    3.72    3.77    3.79    3.81    3.85  2305
## lambda[11]    4.05    0.00 0.03    3.98    4.02    4.05    4.07    4.11  2347
## lambda[12]    3.78    0.00 0.03    3.72    3.76    3.78    3.80    3.83  2465
## lambda[13]    4.01    0.00 0.03    3.95    3.99    4.01    4.03    4.07  2499
## lambda[14]    3.74    0.00 0.03    3.68    3.72    3.74    3.76    3.79  1991
## lambda[15]    3.73    0.00 0.03    3.68    3.72    3.74    3.76    3.79  1910
## lambda[16]    3.98    0.00 0.04    3.91    3.96    3.98    4.01    4.05  2088
## lambda[17]    3.74    0.00 0.03    3.68    3.72    3.74    3.76    3.80  2122
## lambda[18]    3.70    0.00 0.04    3.61    3.67    3.70    3.72    3.78  1360
## lambda[19]    3.85    0.00 0.05    3.74    3.81    3.85    3.88    3.95  1653
## lambda[20]    4.07    0.00 0.04    3.99    4.04    4.07    4.09    4.14  1835
## lambda[21]    3.97    0.00 0.04    3.88    3.94    3.97    3.99    4.05  1735
## lambda[22]    4.00    0.00 0.03    3.93    3.98    4.00    4.02    4.06  2298
## lambda[23]    3.99    0.00 0.03    3.93    3.97    3.99    4.02    4.06  2252
## lambda[24]    4.05    0.00 0.03    3.98    4.03    4.05    4.07    4.11  2283
## lambda[25]    4.01    0.00 0.03    3.95    3.99    4.01    4.03    4.07  2483
## lambda[26]    4.03    0.00 0.03    3.97    4.01    4.03    4.05    4.09  2548
## lambda[27]    3.75    0.00 0.03    3.69    3.73    3.75    3.77    3.80  2321
## lambda[28]    3.75    0.00 0.03    3.69    3.73    3.75    3.77    3.80  2321
## lambda[29]    3.81    0.00 0.04    3.73    3.78    3.81    3.84    3.89  1971
## lambda[30]    3.74    0.00 0.03    3.68    3.72    3.74    3.76    3.80  2077
## lambda[31]    4.08    0.00 0.04    4.00    4.05    4.08    4.11    4.17  1672
## lambda[32]    3.95    0.00 0.05    3.85    3.92    3.95    3.98    4.04  1571
## lambda[33]    4.04    0.00 0.03    3.98    4.02    4.04    4.06    4.10  2409
## lambda[34]    4.02    0.00 0.03    3.96    4.00    4.02    4.04    4.08  2554
## lambda[35]    3.76    0.00 0.03    3.70    3.74    3.76    3.78    3.81  2482
## lambda[36]    3.77    0.00 0.03    3.71    3.75    3.77    3.79    3.82  2516
## lambda[37]    3.99    0.00 0.03    3.93    3.97    3.99    4.02    4.06  2229
## lambda[38]    3.74    0.00 0.03    3.68    3.72    3.74    3.76    3.79  1950
## lambda[39]    3.71    0.00 0.04    3.63    3.68    3.71    3.73    3.78  1437
## lambda[40]    3.70    0.00 0.04    3.63    3.68    3.70    3.73    3.78  1424
## lambda[41]    3.76    0.00 0.03    3.71    3.75    3.76    3.78    3.82  2512
## lambda[42]    3.77    0.00 0.03    3.71    3.75    3.77    3.79    3.83  2509
## lambda[43]    3.75    0.00 0.03    3.70    3.74    3.75    3.77    3.81  2404
## lambda[44]    3.79    0.00 0.03    3.72    3.77    3.79    3.82    3.86  2236
## lambda[45]    3.84    0.00 0.05    3.74    3.81    3.84    3.88    3.95  1683
## lambda[46]    3.73    0.00 0.03    3.67    3.71    3.73    3.75    3.79  1772
## lambda[47]    3.65    0.00 0.06    3.53    3.61    3.65    3.69    3.77  1222
## lambda[48]    3.79    0.00 0.04    3.73    3.77    3.79    3.82    3.86  2191
## lambda[49]    3.72    0.00 0.03    3.65    3.70    3.72    3.74    3.79  1613
## lambda[50]    3.98    0.00 0.04    3.91    3.96    3.98    4.01    4.05  2088
## lp__       6896.53    0.03 1.28 6893.17 6895.97 6896.87 6897.45 6897.95  1452
##            Rhat
## b[1]          1
## b[2]          1
## b[3]          1
## lambda[1]     1
## lambda[2]     1
## lambda[3]     1
## lambda[4]     1
## lambda[5]     1
## lambda[6]     1
## lambda[7]     1
## lambda[8]     1
## lambda[9]     1
## lambda[10]    1
## lambda[11]    1
## lambda[12]    1
## lambda[13]    1
## lambda[14]    1
## lambda[15]    1
## lambda[16]    1
## lambda[17]    1
## lambda[18]    1
## lambda[19]    1
## lambda[20]    1
## lambda[21]    1
## lambda[22]    1
## lambda[23]    1
## lambda[24]    1
## lambda[25]    1
## lambda[26]    1
## lambda[27]    1
## lambda[28]    1
## lambda[29]    1
## lambda[30]    1
## lambda[31]    1
## lambda[32]    1
## lambda[33]    1
## lambda[34]    1
## lambda[35]    1
## lambda[36]    1
## lambda[37]    1
## lambda[38]    1
## lambda[39]    1
## lambda[40]    1
## lambda[41]    1
## lambda[42]    1
## lambda[43]    1
## lambda[44]    1
## lambda[45]    1
## lambda[46]    1
## lambda[47]    1
## lambda[48]    1
## lambda[49]    1
## lambda[50]    1
## lp__          1
## 
## Samples were drawn using NUTS(diag_e) at Tue Nov 26 14:59:39 2019.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;運行結果的解釋&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;運行結果的解釋&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;...{省略}...
              mean se_mean   sd    2.5%     25%     50%     75%   97.5% n_eff Rhat
b[1]          3.58    0.00 0.09    3.38    3.51    3.58    3.64    3.76  1373    1
b[2]          0.26    0.00 0.04    0.18    0.24    0.26    0.29    0.35  1797    1
b[3]          0.29    0.00 0.15    0.00    0.20    0.29    0.39    0.59  1422    1
lambda[1]     3.68    0.00 0.05    3.58    3.65    3.68    3.71    3.77  1510    1
...{省略}...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;我們把計算獲得的事後概率分佈均值放入前面寫下的數學表達式:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\lambda[n] &amp;amp; = \exp(3.58 + 0.26A[n] + 0.29Score[n]/200) &amp;amp; n = 1, \dots, N \\
M[n]       &amp;amp; \sim \text{Poisson}(\lambda[n])     &amp;amp; n = 1, \dots, N
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;例如說，&lt;code&gt;Score = 150&lt;/code&gt; 和 &lt;code&gt;Score = 50&lt;/code&gt; 的兩名學生，如果對打工喜好態度相同的話，他們之間選課的總課時數之比爲：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\frac{M_\text{Score = 150}}{M_\text{Score = 50}} &amp;amp; = \frac{\exp(3.58 + 0.26A + 0.29\times\frac{150}{200})}{\exp(3.58 + 0.26A + 0.29\times\frac{50}{200})} \\ 
&amp;amp; = \exp(0.29\times\frac{150-50}{200}) \approx 1.16
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;也就是熱愛學習分數 &lt;code&gt;Score&lt;/code&gt; 達到150的人和只有50的人相比，選課總課時數平均多 16%。相似地，喜歡打工 &lt;code&gt;A = 1&lt;/code&gt; 的學生和不喜歡打工 &lt;code&gt;A = 0&lt;/code&gt; 的學生選課總課時數之比爲 &lt;span class=&#34;math inline&#34;&gt;\(\exp(0.26)\approx1.30\)&lt;/span&gt;。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Rstan Wonderful R-(5)</title>
      <link>https://wangcc.me/post/logistic-rstan2/</link>
      <pubDate>Sun, 03 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/logistic-rstan2/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#另一種形式的貝葉斯邏輯回歸&#34;&gt;另一種形式的貝葉斯邏輯回歸&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#分析的目的&#34;&gt;分析的目的&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#確認數據分佈&#34;&gt;確認數據分佈&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#思考數據模型&#34;&gt;思考數據模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#寫下-stan-模型代碼&#34;&gt;寫下 Stan 模型代碼&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#檢查模型參數的收斂情況&#34;&gt;檢查模型參數的收斂情況&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#檢查模型的擬合情況&#34;&gt;檢查模型的擬合情況&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;另一種形式的貝葉斯邏輯回歸&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;另一種形式的貝葉斯邏輯回歸&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://wangcc.me/post/logistic-rstan/&#34;&gt;前面一節&lt;/a&gt;使用的數據是以學生爲單位，將每名學生的實際課時數和實際出勤數進行了彙總之後的總結性數據，本章我們來看看相同數據的另一種形式。由於分析中有人建議說，天氣狀況對出勤率也是有較大的影響的，所以希望在&lt;a href=&#34;https://wangcc.me/post/logistic-rstan/&#34;&gt;前一節&lt;/a&gt;已有的邏輯回歸模型中增加對天氣狀況的調整。那麼這時候需要使用的就是彙總之前的數據，也就是要是用實際記錄了每名學生每一次課時的出勤與否的原始數據。值得注意的是，這時候&lt;a href=&#34;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap05/input/data-attendance-3.txt&#34;&gt;原始數據&lt;/a&gt;中每名學生的記錄有許多行，因爲每行記錄的是該名學生每次上課時的天氣狀況和他/她是否出勤(0,1)的結果。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d &amp;lt;- read.table(&amp;quot;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap05/input/data-attendance-3.txt&amp;quot;, 
                     sep = &amp;quot;,&amp;quot;, header = T)
head(d, 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    PersonID A Score Weather Y
## 1         1 0    69       B 1
## 2         1 0    69       A 1
## 3         1 0    69       C 1
## 4         1 0    69       A 1
## 5         1 0    69       B 1
## 6         1 0    69       B 1
## 7         1 0    69       C 0
## 8         1 0    69       B 1
## 9         1 0    69       A 1
## 10        1 0    69       A 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Weather\)&lt;/span&gt;，天氣數據 (&lt;code&gt;A&lt;/code&gt; = 晴天，&lt;code&gt;B&lt;/code&gt; = 多雲，&lt;code&gt;C&lt;/code&gt; = 下雨)；&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;，該次課時學生是否出勤 (&lt;code&gt;0&lt;/code&gt; = 缺勤，&lt;code&gt;1&lt;/code&gt; = 出勤)。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其他的數據和前一節中使用的數據相同。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;分析的目的&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;分析的目的&lt;/h1&gt;
&lt;p&gt;本次數據分析的目的依然是瞭解幾個預測變量，天氣，是否喜歡打工，是否熱愛學習，對學生出勤率的影響。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;確認數據分佈&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;確認數據分佈&lt;/h1&gt;
&lt;p&gt;你可以用先進的 &lt;code&gt;tidyverse&lt;/code&gt; 進行簡單的數據彙總，看看天氣狀況不同時實際出勤率是否有差別:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
d %&amp;gt;% 
  group_by(Weather, Y) %&amp;gt;% 
  summarise (n= n()) %&amp;gt;%
  mutate(rel.freq = paste0(round(100 * n/sum(n), 2), &amp;quot;%&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 4
## # Groups:   Weather [3]
##   Weather     Y     n rel.freq
##   &amp;lt;fct&amp;gt;   &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;   
## 1 A           0   306 24.31%  
## 2 A           1   953 75.69%  
## 3 B           0   230 31.51%  
## 4 B           1   500 68.49%  
## 5 C           0   138 33.91%  
## 6 C           1   269 66.09%&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果你不想學習 &lt;code&gt;tidyverse&lt;/code&gt;，也可以用下面的方法獲得類似的效果，&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;aggregate(Y ~ Weather, data = d, FUN = table)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Weather Y.0 Y.1
## 1       A 306 953
## 2       B 230 500
## 3       C 138 269&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;無論是哪種方法，我們都能大概猜出，天氣是晴天的時候 (&lt;code&gt;Weather = A&lt;/code&gt;)，出勤率相對較高。&lt;/p&gt;
&lt;p&gt;在作者的原著中，&lt;a href=&#34;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap05/run-model5-5.R&#34;&gt;使用的是給分類型變量強制賦予連續值的方法&lt;/a&gt;，這點確實有點噁心，爲了正常的模型，我們需要把天氣轉換成爲更加常見的啞變量 (dummy variable) 如下:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d &amp;lt;- fastDummies::dummy_cols(d, select_columns = &amp;quot;Weather&amp;quot;)
head(d)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   PersonID A Score Weather Y Weather_A Weather_B Weather_C
## 1        1 0    69       B 1         0         1         0
## 2        1 0    69       A 1         1         0         0
## 3        1 0    69       C 1         0         0         1
## 4        1 0    69       A 1         1         0         0
## 5        1 0    69       B 1         0         1         0
## 6        1 0    69       B 1         0         1         0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;思考數據模型&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;思考數據模型&lt;/h1&gt;
&lt;p&gt;我們設想的數學模型應該是這樣子的:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\text{logit}(q[i]) &amp;amp; = b_{1} + b_{2}A_{i} + b_{3}\text{Score}_{i} + b_{4}\text{WeatherB} + b_{5}\text{WeatherC} \\ 
\text{where} &amp;amp; \\ 
&amp;amp; \text{ WeatherB} = 0, \text{ WeatherC} = 0 \text{ indicates weather = A} \\ 
&amp;amp; \text{ WeatherB} = 1, \text{ WeatherC} = 0 \text{ indicates weather = B} \\ 
&amp;amp; \text{ WeatherB} = 0, \text{ WeatherC} = 1 \text{ indicates weather = C} \\
Y[i] &amp;amp;\sim \text{Bernulli}(q[i])
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;寫下-stan-模型代碼&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;寫下 Stan 模型代碼&lt;/h1&gt;
&lt;p&gt;下面是相應的 Stan 模型:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data {
  int I;
  int&amp;lt;lower=0, upper=1&amp;gt; A[I];
  real&amp;lt;lower=0, upper=1&amp;gt; Score[I];
  int&amp;lt;lower=0, upper=1&amp;gt; W_B[I];
  int&amp;lt;lower=0, upper=1&amp;gt; W_C[I];
  int&amp;lt;lower=0, upper=1&amp;gt; Y[I];
}

// The parameters accepted by the model. 
parameters {
  real b[5];
}

// The model to be estimated. 
model {
   for (i in 1:I)
    Y[i] ~ bernoulli_logit(b[1] + b[2]*A[i] + b[3]*Score[i] + b[4]*W_B[i] + b[5]*W_C[i]);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;和跑它們的 R 代碼&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rstan)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: StanHeaders&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## rstan (Version 2.19.2, GitRev: 2e1f913d3ca3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## For execution on a local, multicore CPU with excess RAM we recommend calling
## options(mc.cores = parallel::detectCores()).
## To avoid recompilation of unchanged Stan programs, we recommend calling
## rstan_options(auto_write = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;rstan&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:tidyr&amp;#39;:
## 
##     extract&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data &amp;lt;- list(I=nrow(d), A=d$A, Score=d$Score/200, 
             W_A=d$Weather_A, W_B = d$Weather_B, W_C = d$Weather_C, 
             Y=d$Y)
fit1 &amp;lt;- stan(file=&amp;#39;stanfiles/myex4.stan&amp;#39;, data=data, pars=c(&amp;#39;b&amp;#39;, &amp;quot;OR1&amp;quot;, &amp;quot;OR2&amp;quot;, &amp;quot;OR3&amp;quot;, &amp;quot;OR4&amp;quot;, &amp;quot;q&amp;quot;), seed=1234)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &amp;#39;myex4&amp;#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0.000801 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 8.01 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 10.246 seconds (Warm-up)
## Chain 1:                10.2042 seconds (Sampling)
## Chain 1:                20.4502 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &amp;#39;myex4&amp;#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 0.000482 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 4.82 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 9.88978 seconds (Warm-up)
## Chain 2:                10.6792 seconds (Sampling)
## Chain 2:                20.569 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &amp;#39;myex4&amp;#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 0.000474 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 4.74 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 9.92416 seconds (Warm-up)
## Chain 3:                11.069 seconds (Sampling)
## Chain 3:                20.9931 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &amp;#39;myex4&amp;#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 0.000502 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 5.02 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 9.75029 seconds (Warm-up)
## Chain 4:                11.4305 seconds (Sampling)
## Chain 4:                21.1808 seconds (Total)
## Chain 4:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Stan model: myex4.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##             mean se_mean   sd     2.5%      25%      50%      75%    97.5%
## b[1]        0.27    0.00 0.23    -0.18     0.12     0.28     0.43     0.72
## b[2]       -0.63    0.00 0.09    -0.81    -0.69    -0.63    -0.57    -0.44
## b[3]        1.95    0.01 0.37     1.22     1.71     1.96     2.20     2.69
## b[4]       -0.38    0.00 0.11    -0.59    -0.45    -0.38    -0.31    -0.18
## b[5]       -0.49    0.00 0.12    -0.74    -0.58    -0.49    -0.41    -0.25
## OR1         0.54    0.00 0.05     0.44     0.50     0.53     0.57     0.64
## OR2         7.56    0.06 2.92     3.40     5.52     7.08     9.05    14.79
## OR3         0.69    0.00 0.07     0.55     0.64     0.68     0.74     0.84
## OR4         0.61    0.00 0.08     0.48     0.56     0.61     0.66     0.78
## q[1]        0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.70
## q[2]        0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[3]        0.61    0.00 0.04     0.54     0.59     0.61     0.64     0.68
## q[4]        0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[5]        0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.70
## q[6]        0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.70
## q[7]        0.61    0.00 0.04     0.54     0.59     0.61     0.64     0.68
## q[8]        0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.70
## q[9]        0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[10]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[11]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[12]       0.61    0.00 0.04     0.54     0.59     0.61     0.64     0.68
## q[13]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[14]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[15]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[16]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[17]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[18]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.70
## q[19]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[20]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[21]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.70
## q[22]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.70
## q[23]       0.61    0.00 0.04     0.54     0.59     0.61     0.64     0.68
## q[24]       0.61    0.00 0.04     0.54     0.59     0.61     0.64     0.68
## q[25]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[26]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[27]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.70
## q[28]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[29]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.70
## q[30]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[31]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.70
## q[32]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[33]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.70
## q[34]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[35]       0.61    0.00 0.04     0.54     0.59     0.61     0.64     0.68
## q[36]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[37]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[38]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[39]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.70
## q[40]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[41]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[42]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.70
## q[43]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.70
## q[44]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[45]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[46]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[47]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[48]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[49]       0.66    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[50]       0.66    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[51]       0.66    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[52]       0.66    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[53]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[54]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[55]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[56]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[57]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[58]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[59]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[60]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[61]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[62]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[63]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[64]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[65]       0.66    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[66]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[67]       0.66    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[68]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[69]       0.66    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[70]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[71]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[72]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[73]       0.66    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[74]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[75]       0.66    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[76]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[77]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[78]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[79]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[80]       0.66    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[81]       0.66    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[82]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[83]       0.66    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[84]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[85]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[86]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[87]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[88]       0.66    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[89]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[90]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[91]       0.66    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[92]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[93]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[94]       0.66    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[95]       0.66    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[96]       0.66    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[97]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[98]       0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[99]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[100]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[101]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[102]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[103]      0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[104]      0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[105]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[106]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[107]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[108]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[109]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[110]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[111]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[112]      0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[113]      0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[114]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[115]      0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[116]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[117]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[118]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[119]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[120]      0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[121]      0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[122]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[123]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[124]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[125]      0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[126]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[127]      0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[128]      0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[129]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[130]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[131]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[132]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[133]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[134]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[135]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[136]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[137]      0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[138]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[139]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[140]      0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[141]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[142]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[143]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[144]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[145]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[146]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[147]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[148]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[149]      0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[150]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[151]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[152]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[153]      0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[154]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[155]      0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[156]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[157]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[158]      0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[159]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[160]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[161]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[162]      0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[163]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[164]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[165]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[166]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[167]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[168]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[169]      0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[170]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[171]      0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[172]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[173]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[174]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[175]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[176]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[177]      0.67    0.00 0.03     0.61     0.65     0.67     0.69     0.72
## q[178]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[179]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[180]      0.67    0.00 0.03     0.61     0.65     0.67     0.69     0.72
## q[181]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[182]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[183]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[184]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[185]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[186]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[187]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[188]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[189]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[190]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[191]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[192]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[193]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[194]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[195]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[196]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[197]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[198]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[199]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[200]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[201]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[202]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[203]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[204]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[205]      0.67    0.00 0.03     0.61     0.65     0.67     0.69     0.72
## q[206]      0.67    0.00 0.03     0.61     0.65     0.67     0.69     0.72
## q[207]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[208]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[209]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[210]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[211]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[212]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[213]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[214]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[215]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[216]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[217]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[218]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[219]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[220]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[221]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[222]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[223]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[224]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[225]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[226]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[227]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[228]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[229]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[230]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[231]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[232]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[233]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[234]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[235]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[236]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[237]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[238]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[239]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[240]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[241]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[242]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[243]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[244]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[245]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[246]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[247]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[248]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[249]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[250]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[251]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[252]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[253]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[254]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[255]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[256]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[257]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[258]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[259]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[260]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[261]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[262]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[263]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[264]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[265]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[266]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[267]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[268]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[269]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[270]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[271]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[272]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[273]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[274]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[275]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[276]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[277]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[278]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[279]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[280]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[281]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[282]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[283]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[284]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[285]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[286]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[287]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[288]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[289]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[290]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[291]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[292]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[293]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[294]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[295]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[296]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[297]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[298]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[299]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[300]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[301]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[302]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[303]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[304]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[305]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[306]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[307]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[308]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[309]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[310]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[311]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[312]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[313]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[314]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[315]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[316]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[317]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[318]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[319]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[320]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[321]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[322]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[323]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[324]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[325]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[326]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[327]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[328]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[329]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[330]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[331]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[332]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[333]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[334]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[335]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[336]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[337]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[338]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[339]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[340]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[341]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[342]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[343]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[344]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[345]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[346]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[347]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[348]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[349]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[350]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[351]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[352]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[353]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[354]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[355]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[356]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[357]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[358]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[359]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[360]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[361]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[362]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[363]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[364]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[365]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[366]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[367]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[368]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[369]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[370]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[371]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[372]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[373]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[374]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[375]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[376]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[377]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[378]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[379]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[380]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[381]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[382]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[383]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[384]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[385]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[386]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[387]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[388]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[389]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[390]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.71
## q[391]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[392]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[393]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[394]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[395]      0.75    0.00 0.02     0.71     0.73     0.75     0.76     0.78
## q[396]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[397]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[398]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[399]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.81
## q[400]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[401]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[402]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[403]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[404]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[405]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[406]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[407]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.81
## q[408]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[409]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.81
## q[410]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[411]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[412]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[413]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[414]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[415]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[416]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[417]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.81
## q[418]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[419]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[420]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[421]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[422]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[423]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[424]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[425]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[426]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[427]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[428]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[429]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[430]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[431]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[432]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[433]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[434]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[435]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.81
## q[436]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[437]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[438]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[439]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[440]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[441]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[442]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[443]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[444]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[445]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[446]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[447]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[448]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[449]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[450]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[451]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[452]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[453]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[454]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[455]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[456]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[457]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[458]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[459]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.81
## q[460]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[461]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.81
## q[462]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[463]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[464]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[465]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.81
## q[466]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[467]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[468]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[469]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[470]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[471]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[472]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.81
## q[473]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[474]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[475]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[476]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[477]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[478]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[479]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[480]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[481]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[482]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[483]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[484]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[485]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[486]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[487]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[488]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[489]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[490]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[491]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[492]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[493]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[494]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[495]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[496]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[497]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[498]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[499]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[500]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[501]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[502]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[503]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[504]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[505]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[506]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[507]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[508]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[509]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[510]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[511]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[512]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[513]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[514]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[515]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[516]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[517]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[518]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[519]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[520]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[521]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[522]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[523]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[524]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[525]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[526]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[527]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[528]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[529]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[530]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[531]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[532]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[533]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[534]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[535]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[536]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[537]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[538]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[539]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[540]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[541]      0.66    0.00 0.02     0.61     0.64     0.66     0.67     0.70
## q[542]      0.66    0.00 0.02     0.61     0.64     0.66     0.67     0.70
## q[543]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[544]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[545]      0.63    0.00 0.03     0.57     0.61     0.63     0.65     0.68
## q[546]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[547]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[548]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[549]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[550]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[551]      0.66    0.00 0.02     0.61     0.64     0.66     0.67     0.70
## q[552]      0.63    0.00 0.03     0.57     0.61     0.63     0.65     0.68
## q[553]      0.66    0.00 0.02     0.61     0.64     0.66     0.67     0.70
## q[554]      0.66    0.00 0.02     0.61     0.64     0.66     0.67     0.70
## q[555]      0.63    0.00 0.03     0.57     0.61     0.63     0.65     0.68
## q[556]      0.63    0.00 0.03     0.57     0.61     0.63     0.65     0.68
## q[557]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[558]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[559]      0.63    0.00 0.03     0.57     0.61     0.63     0.65     0.68
## q[560]      0.66    0.00 0.02     0.61     0.64     0.66     0.67     0.70
## q[561]      0.63    0.00 0.03     0.57     0.61     0.63     0.65     0.68
## q[562]      0.66    0.00 0.02     0.61     0.64     0.66     0.67     0.70
## q[563]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[564]      0.66    0.00 0.02     0.61     0.64     0.66     0.67     0.70
## q[565]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[566]      0.63    0.00 0.03     0.57     0.61     0.63     0.65     0.68
## q[567]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[568]      0.66    0.00 0.02     0.61     0.64     0.66     0.67     0.70
## q[569]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[570]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[571]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[572]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[573]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[574]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[575]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[576]      0.75    0.00 0.02     0.71     0.74     0.75     0.77     0.80
## q[577]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[578]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[579]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[580]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[581]      0.75    0.00 0.02     0.71     0.74     0.75     0.77     0.80
## q[582]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[583]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[584]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[585]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[586]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[587]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[588]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[589]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[590]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[591]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[592]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[593]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[594]      0.75    0.00 0.02     0.71     0.74     0.75     0.77     0.80
## q[595]      0.75    0.00 0.02     0.71     0.74     0.75     0.77     0.80
## q[596]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[597]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[598]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[599]      0.75    0.00 0.02     0.71     0.74     0.75     0.77     0.80
## q[600]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[601]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[602]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[603]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[604]      0.75    0.00 0.02     0.71     0.74     0.75     0.77     0.80
## q[605]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[606]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[607]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[608]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[609]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[610]      0.75    0.00 0.02     0.71     0.74     0.75     0.77     0.80
## q[611]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[612]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[613]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[614]      0.75    0.00 0.02     0.71     0.74     0.75     0.77     0.80
## q[615]      0.75    0.00 0.02     0.71     0.74     0.75     0.77     0.80
## q[616]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[617]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[618]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[619]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[620]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[621]      0.58    0.00 0.03     0.52     0.56     0.58     0.59     0.63
## q[622]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[623]      0.58    0.00 0.03     0.52     0.56     0.58     0.59     0.63
## q[624]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[625]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[626]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[627]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[628]      0.58    0.00 0.03     0.52     0.56     0.58     0.59     0.63
## q[629]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[630]      0.58    0.00 0.03     0.52     0.56     0.58     0.59     0.63
## q[631]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[632]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[633]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[634]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[635]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[636]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[637]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[638]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[639]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[640]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[641]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[642]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[643]      0.58    0.00 0.03     0.52     0.56     0.58     0.59     0.63
## q[644]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[645]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[646]      0.58    0.00 0.03     0.52     0.56     0.58     0.59     0.63
## q[647]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[648]      0.58    0.00 0.03     0.52     0.56     0.58     0.59     0.63
## q[649]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[650]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[651]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[652]      0.58    0.00 0.03     0.52     0.56     0.58     0.59     0.63
## q[653]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[654]      0.58    0.00 0.03     0.52     0.56     0.58     0.59     0.63
## q[655]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[656]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[657]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[658]      0.58    0.00 0.03     0.52     0.56     0.58     0.59     0.63
## q[659]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[660]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[661]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[662]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[663]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[664]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[665]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[666]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[667]      0.58    0.00 0.03     0.52     0.56     0.58     0.59     0.63
## q[668]      0.58    0.00 0.03     0.52     0.56     0.58     0.59     0.63
## q[669]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[670]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[671]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[672]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[673]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[674]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[675]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[676]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[677]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[678]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[679]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[680]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[681]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[682]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[683]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[684]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[685]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[686]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[687]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[688]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[689]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[690]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[691]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[692]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[693]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[694]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[695]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[696]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[697]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[698]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[699]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[700]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[701]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[702]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[703]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[704]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[705]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[706]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[707]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[708]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[709]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[710]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[711]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[712]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[713]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[714]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[715]      0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.75
## q[716]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[717]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[718]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[719]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[720]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[721]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[722]      0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.75
## q[723]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[724]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[725]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[726]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[727]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[728]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[729]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[730]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[731]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[732]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[733]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[734]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[735]      0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.75
## q[736]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[737]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[738]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[739]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[740]      0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.75
## q[741]      0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.75
## q[742]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[743]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[744]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[745]      0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.75
## q[746]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[747]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[748]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[749]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[750]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[751]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[752]      0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.75
## q[753]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[754]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[755]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[756]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[757]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[758]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[759]      0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.75
## q[760]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[761]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[762]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[763]      0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.75
## q[764]      0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.75
## q[765]      0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[766]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[767]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[768]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[769]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[770]      0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[771]      0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[772]      0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[773]      0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[774]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[775]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[776]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[777]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[778]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[779]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[780]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[781]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[782]      0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[783]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[784]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[785]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[786]      0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[787]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[788]      0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[789]      0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[790]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[791]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[792]      0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[793]      0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[794]      0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[795]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[796]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[797]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[798]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[799]      0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[800]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[801]      0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[802]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[803]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[804]      0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[805]      0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[806]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[807]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[808]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[809]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[810]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[811]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[812]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[813]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[814]      0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[815]      0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[816]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[817]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[818]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[819]      0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[820]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[821]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[822]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[823]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[824]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[825]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[826]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[827]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[828]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[829]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[830]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[831]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[832]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[833]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[834]      0.71    0.00 0.02     0.66     0.69     0.71     0.73     0.75
## q[835]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[836]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[837]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[838]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[839]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[840]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[841]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[842]      0.71    0.00 0.02     0.66     0.69     0.71     0.73     0.75
## q[843]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[844]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[845]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[846]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[847]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[848]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[849]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[850]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[851]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[852]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[853]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[854]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[855]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[856]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[857]      0.71    0.00 0.02     0.66     0.69     0.71     0.73     0.75
## q[858]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[859]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[860]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[861]      0.71    0.00 0.02     0.66     0.69     0.71     0.73     0.75
## q[862]      0.67    0.00 0.03     0.61     0.65     0.67     0.69     0.72
## q[863]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[864]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[865]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[866]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[867]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[868]      0.67    0.00 0.03     0.61     0.65     0.67     0.69     0.72
## q[869]      0.67    0.00 0.03     0.61     0.65     0.67     0.69     0.72
## q[870]      0.67    0.00 0.03     0.61     0.65     0.67     0.69     0.72
## q[871]      0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.70
## q[872]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[873]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[874]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[875]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[876]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[877]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[878]      0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.70
## q[879]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[880]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[881]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[882]      0.67    0.00 0.03     0.61     0.65     0.67     0.69     0.72
## q[883]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[884]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[885]      0.67    0.00 0.03     0.61     0.65     0.67     0.69     0.72
## q[886]      0.67    0.00 0.03     0.61     0.65     0.67     0.69     0.72
## q[887]      0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.70
## q[888]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[889]      0.67    0.00 0.03     0.61     0.65     0.67     0.69     0.72
## q[890]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[891]      0.67    0.00 0.03     0.61     0.65     0.67     0.69     0.72
## q[892]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[893]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[894]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[895]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[896]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[897]      0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.70
## q[898]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[899]      0.67    0.00 0.03     0.61     0.65     0.67     0.69     0.72
## q[900]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[901]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[902]      0.67    0.00 0.03     0.61     0.65     0.67     0.69     0.72
## q[903]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[904]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[905]      0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.70
## q[906]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[907]      0.89    0.00 0.02     0.86     0.88     0.89     0.90     0.92
## q[908]      0.83    0.00 0.02     0.78     0.81     0.83     0.85     0.87
## q[909]      0.84    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[910]      0.84    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[911]      0.83    0.00 0.02     0.78     0.81     0.83     0.85     0.87
## q[912]      0.89    0.00 0.02     0.86     0.88     0.89     0.90     0.92
## q[913]      0.84    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[914]      0.89    0.00 0.02     0.86     0.88     0.89     0.90     0.92
## q[915]      0.89    0.00 0.02     0.86     0.88     0.89     0.90     0.92
## q[916]      0.84    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[917]      0.89    0.00 0.02     0.86     0.88     0.89     0.90     0.92
## q[918]      0.84    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[919]      0.89    0.00 0.02     0.86     0.88     0.89     0.90     0.92
## q[920]      0.89    0.00 0.02     0.86     0.88     0.89     0.90     0.92
## q[921]      0.84    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[922]      0.84    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[923]      0.83    0.00 0.02     0.78     0.81     0.83     0.85     0.87
## q[924]      0.84    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[925]      0.89    0.00 0.02     0.86     0.88     0.89     0.90     0.92
## q[926]      0.84    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[927]      0.84    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[928]      0.89    0.00 0.02     0.86     0.88     0.89     0.90     0.92
## q[929]      0.84    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[930]      0.83    0.00 0.02     0.78     0.81     0.83     0.85     0.87
## q[931]      0.89    0.00 0.02     0.86     0.88     0.89     0.90     0.92
## q[932]      0.83    0.00 0.02     0.78     0.81     0.83     0.85     0.87
## q[933]      0.84    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[934]      0.89    0.00 0.02     0.86     0.88     0.89     0.90     0.92
## q[935]      0.89    0.00 0.02     0.86     0.88     0.89     0.90     0.92
## q[936]      0.84    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[937]      0.84    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[938]      0.89    0.00 0.02     0.86     0.88     0.89     0.90     0.92
## q[939]      0.89    0.00 0.02     0.86     0.88     0.89     0.90     0.92
## q[940]      0.89    0.00 0.02     0.86     0.88     0.89     0.90     0.92
## q[941]      0.89    0.00 0.02     0.86     0.88     0.89     0.90     0.92
## q[942]      0.89    0.00 0.02     0.86     0.88     0.89     0.90     0.92
## q[943]      0.84    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[944]      0.89    0.00 0.02     0.86     0.88     0.89     0.90     0.92
## q[945]      0.89    0.00 0.02     0.86     0.88     0.89     0.90     0.92
## q[946]      0.83    0.00 0.02     0.78     0.81     0.83     0.85     0.87
## q[947]      0.83    0.00 0.02     0.78     0.81     0.83     0.85     0.87
## q[948]      0.89    0.00 0.02     0.86     0.88     0.89     0.90     0.92
## q[949]      0.84    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[950]      0.89    0.00 0.02     0.86     0.88     0.89     0.90     0.92
## q[951]      0.89    0.00 0.02     0.86     0.88     0.89     0.90     0.92
## q[952]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[953]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[954]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[955]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[956]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[957]      0.69    0.00 0.02     0.64     0.67     0.69     0.70     0.73
## q[958]      0.69    0.00 0.02     0.64     0.67     0.69     0.70     0.73
## q[959]      0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[960]      0.69    0.00 0.02     0.64     0.67     0.69     0.70     0.73
## q[961]      0.69    0.00 0.02     0.64     0.67     0.69     0.70     0.73
## q[962]      0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[963]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[964]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[965]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[966]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[967]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[968]      0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[969]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[970]      0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[971]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[972]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[973]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[974]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[975]      0.69    0.00 0.02     0.64     0.67     0.69     0.70     0.73
## q[976]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[977]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[978]      0.69    0.00 0.02     0.64     0.67     0.69     0.70     0.73
## q[979]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[980]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[981]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[982]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[983]      0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[984]      0.69    0.00 0.02     0.64     0.67     0.69     0.70     0.73
## q[985]      0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[986]      0.69    0.00 0.02     0.64     0.67     0.69     0.70     0.73
## q[987]      0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[988]      0.69    0.00 0.02     0.64     0.67     0.69     0.70     0.73
## q[989]      0.69    0.00 0.02     0.64     0.67     0.69     0.70     0.73
## q[990]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[991]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[992]      0.69    0.00 0.02     0.64     0.67     0.69     0.70     0.73
## q[993]      0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[994]      0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[995]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[996]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[997]      0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[998]      0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[999]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1000]     0.69    0.00 0.02     0.64     0.67     0.69     0.70     0.73
## q[1001]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1002]     0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[1003]     0.69    0.00 0.02     0.64     0.67     0.69     0.70     0.73
## q[1004]     0.69    0.00 0.02     0.64     0.67     0.69     0.70     0.73
## q[1005]     0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[1006]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1007]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1008]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1009]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1010]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1011]     0.69    0.00 0.02     0.64     0.67     0.69     0.70     0.73
## q[1012]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1013]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1014]     0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[1015]     0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[1016]     0.69    0.00 0.02     0.64     0.67     0.69     0.70     0.73
## q[1017]     0.69    0.00 0.02     0.64     0.67     0.69     0.70     0.73
## q[1018]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1019]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1020]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1021]     0.69    0.00 0.02     0.64     0.67     0.69     0.70     0.73
## q[1022]     0.69    0.00 0.02     0.64     0.67     0.69     0.70     0.73
## q[1023]     0.69    0.00 0.02     0.64     0.67     0.69     0.70     0.73
## q[1024]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1025]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1026]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1027]     0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[1028]     0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[1029]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1030]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1031]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1032]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1033]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1034]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1035]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1036]     0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[1037]     0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[1038]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1039]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1040]     0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[1041]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1042]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1043]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1044]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1045]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1046]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1047]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1048]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1049]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1050]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1051]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1052]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1053]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1054]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1055]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1056]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1057]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1058]     0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[1059]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1060]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1061]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1062]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1063]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1064]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1065]     0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[1066]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1067]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1068]     0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[1069]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1070]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1071]     0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[1072]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1073]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1074]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1075]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1076]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1077]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1078]     0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[1079]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1080]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1081]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1082]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1083]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1084]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1085]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1086]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1087]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1088]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1089]     0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[1090]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1091]     0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[1092]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1093]     0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[1094]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1095]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1096]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1097]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1098]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.66
## q[1099]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1100]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1101]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1102]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1103]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1104]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1105]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1106]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1107]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1108]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1109]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1110]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1111]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1112]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1113]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1114]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1115]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1116]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1117]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1118]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1119]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1120]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1121]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1122]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1123]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1124]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1125]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1126]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1127]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1128]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1129]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1130]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1131]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1132]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1133]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1134]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1135]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1136]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1137]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1138]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1139]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1140]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1141]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1142]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1143]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1144]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1145]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1146]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1147]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1148]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1149]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1150]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1151]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1152]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1153]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1154]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1155]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1156]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1157]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1158]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1159]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1160]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1161]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1162]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1163]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1164]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1165]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1166]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1167]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1168]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1169]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1170]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1171]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1172]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1173]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1174]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1175]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1176]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1177]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1178]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1179]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.60
## q[1180]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1181]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.60
## q[1182]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1183]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.60
## q[1184]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1185]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1186]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1187]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1188]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1189]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1190]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1191]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1192]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1193]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1194]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1195]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1196]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1197]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1198]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1199]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1200]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1201]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1202]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1203]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1204]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.60
## q[1205]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1206]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1207]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1208]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1209]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1210]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1211]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1212]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.60
## q[1213]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.60
## q[1214]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1215]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.60
## q[1216]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1217]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.60
## q[1218]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1219]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1220]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1221]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1222]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1223]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1224]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.60
## q[1225]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1226]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1227]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1228]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1229]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1230]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.60
## q[1231]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1232]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1233]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1234]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.60
## q[1235]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1236]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1237]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1238]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.60
## q[1239]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1240]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1241]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1242]     0.63    0.00 0.03     0.58     0.61     0.63     0.65     0.69
## q[1243]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1244]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1245]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1246]     0.66    0.00 0.02     0.61     0.64     0.66     0.68     0.70
## q[1247]     0.66    0.00 0.02     0.61     0.64     0.66     0.68     0.70
## q[1248]     0.66    0.00 0.02     0.61     0.64     0.66     0.68     0.70
## q[1249]     0.66    0.00 0.02     0.61     0.64     0.66     0.68     0.70
## q[1250]     0.66    0.00 0.02     0.61     0.64     0.66     0.68     0.70
## q[1251]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1252]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1253]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1254]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1255]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1256]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1257]     0.66    0.00 0.02     0.61     0.64     0.66     0.68     0.70
## q[1258]     0.63    0.00 0.03     0.58     0.61     0.63     0.65     0.69
## q[1259]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1260]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1261]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1262]     0.66    0.00 0.02     0.61     0.64     0.66     0.68     0.70
## q[1263]     0.66    0.00 0.02     0.61     0.64     0.66     0.68     0.70
## q[1264]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1265]     0.66    0.00 0.02     0.61     0.64     0.66     0.68     0.70
## q[1266]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1267]     0.66    0.00 0.02     0.61     0.64     0.66     0.68     0.70
## q[1268]     0.63    0.00 0.03     0.58     0.61     0.63     0.65     0.69
## q[1269]     0.66    0.00 0.02     0.61     0.64     0.66     0.68     0.70
## q[1270]     0.66    0.00 0.02     0.61     0.64     0.66     0.68     0.70
## q[1271]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1272]     0.66    0.00 0.02     0.61     0.64     0.66     0.68     0.70
## q[1273]     0.63    0.00 0.03     0.58     0.61     0.63     0.65     0.69
## q[1274]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1275]     0.66    0.00 0.02     0.61     0.64     0.66     0.68     0.70
## q[1276]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1277]     0.66    0.00 0.02     0.61     0.64     0.66     0.68     0.70
## q[1278]     0.63    0.00 0.03     0.58     0.61     0.63     0.65     0.69
## q[1279]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1280]     0.63    0.00 0.03     0.58     0.61     0.63     0.65     0.69
## q[1281]     0.66    0.00 0.02     0.61     0.64     0.66     0.68     0.70
## q[1282]     0.66    0.00 0.02     0.61     0.64     0.66     0.68     0.70
## q[1283]     0.66    0.00 0.02     0.61     0.64     0.66     0.68     0.70
## q[1284]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1285]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1286]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1287]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1288]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1289]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1290]     0.66    0.00 0.02     0.61     0.64     0.66     0.68     0.70
## q[1291]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1292]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1293]     0.63    0.00 0.03     0.58     0.61     0.63     0.65     0.69
## q[1294]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1295]     0.66    0.00 0.02     0.61     0.64     0.66     0.68     0.70
## q[1296]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1297]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1298]     0.63    0.00 0.03     0.58     0.61     0.63     0.65     0.69
## q[1299]     0.66    0.00 0.02     0.61     0.64     0.66     0.68     0.70
## q[1300]     0.66    0.00 0.02     0.61     0.64     0.66     0.68     0.70
## q[1301]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1302]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1303]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1304]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1305]     0.63    0.00 0.03     0.58     0.61     0.63     0.65     0.69
## q[1306]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1307]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1308]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1309]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1310]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1311]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1312]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1313]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1314]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1315]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1316]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1317]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1318]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1319]     0.57    0.00 0.03     0.52     0.55     0.57     0.59     0.63
## q[1320]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1321]     0.57    0.00 0.03     0.52     0.55     0.57     0.59     0.63
## q[1322]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1323]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1324]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1325]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1326]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1327]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1328]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1329]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1330]     0.57    0.00 0.03     0.52     0.55     0.57     0.59     0.63
## q[1331]     0.57    0.00 0.03     0.52     0.55     0.57     0.59     0.63
## q[1332]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1333]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1334]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1335]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1336]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1337]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1338]     0.57    0.00 0.03     0.52     0.55     0.57     0.59     0.63
## q[1339]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1340]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1341]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1342]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1343]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1344]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1345]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1346]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1347]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1348]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1349]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1350]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1351]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1352]     0.57    0.00 0.03     0.52     0.55     0.57     0.59     0.63
## q[1353]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1354]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1355]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1356]     0.57    0.00 0.03     0.52     0.55     0.57     0.59     0.63
## q[1357]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1358]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1359]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1360]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1361]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1362]     0.63    0.00 0.02     0.58     0.61     0.63     0.64     0.67
## q[1363]     0.60    0.00 0.03     0.55     0.58     0.60     0.62     0.65
## q[1364]     0.63    0.00 0.02     0.58     0.61     0.63     0.64     0.67
## q[1365]     0.63    0.00 0.02     0.58     0.61     0.63     0.64     0.67
## q[1366]     0.60    0.00 0.03     0.55     0.58     0.60     0.62     0.65
## q[1367]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1368]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1369]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1370]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1371]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1372]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1373]     0.63    0.00 0.02     0.58     0.61     0.63     0.64     0.67
## q[1374]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1375]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1376]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1377]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1378]     0.63    0.00 0.02     0.58     0.61     0.63     0.64     0.67
## q[1379]     0.60    0.00 0.03     0.55     0.58     0.60     0.62     0.65
## q[1380]     0.63    0.00 0.02     0.58     0.61     0.63     0.64     0.67
## q[1381]     0.63    0.00 0.02     0.58     0.61     0.63     0.64     0.67
## q[1382]     0.63    0.00 0.02     0.58     0.61     0.63     0.64     0.67
## q[1383]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1384]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1385]     0.63    0.00 0.02     0.58     0.61     0.63     0.64     0.67
## q[1386]     0.60    0.00 0.03     0.55     0.58     0.60     0.62     0.65
## q[1387]     0.60    0.00 0.03     0.55     0.58     0.60     0.62     0.65
## q[1388]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1389]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1390]     0.63    0.00 0.02     0.58     0.61     0.63     0.64     0.67
## q[1391]     0.60    0.00 0.03     0.55     0.58     0.60     0.62     0.65
## q[1392]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1393]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1394]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1395]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1396]     0.60    0.00 0.03     0.55     0.58     0.60     0.62     0.65
## q[1397]     0.60    0.00 0.03     0.55     0.58     0.60     0.62     0.65
## q[1398]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1399]     0.60    0.00 0.03     0.55     0.58     0.60     0.62     0.65
## q[1400]     0.63    0.00 0.02     0.58     0.61     0.63     0.64     0.67
## q[1401]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1402]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1403]     0.63    0.00 0.02     0.58     0.61     0.63     0.64     0.67
## q[1404]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1405]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1406]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1407]     0.60    0.00 0.03     0.55     0.58     0.60     0.62     0.65
## q[1408]     0.72    0.00 0.02     0.67     0.70     0.72     0.73     0.76
## q[1409]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1410]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1411]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.78
## q[1412]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.78
## q[1413]     0.72    0.00 0.02     0.67     0.70     0.72     0.73     0.76
## q[1414]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1415]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1416]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1417]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1418]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1419]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.78
## q[1420]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.78
## q[1421]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.78
## q[1422]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1423]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.78
## q[1424]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.78
## q[1425]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1426]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1427]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1428]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1429]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1430]     0.72    0.00 0.02     0.67     0.70     0.72     0.73     0.76
## q[1431]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1432]     0.72    0.00 0.02     0.67     0.70     0.72     0.73     0.76
## q[1433]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1434]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.78
## q[1435]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1436]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1437]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1438]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.78
## q[1439]     0.72    0.00 0.02     0.67     0.70     0.72     0.73     0.76
## q[1440]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1441]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.78
## q[1442]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1443]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.78
## q[1444]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1445]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.78
## q[1446]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1447]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1448]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.78
## q[1449]     0.72    0.00 0.02     0.67     0.70     0.72     0.73     0.76
## q[1450]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.78
## q[1451]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1452]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.78
## q[1453]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.78
## q[1454]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.78
## q[1455]     0.72    0.00 0.02     0.67     0.70     0.72     0.73     0.76
## q[1456]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1457]     0.72    0.00 0.02     0.67     0.70     0.72     0.73     0.76
## q[1458]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1459]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1460]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.78
## q[1461]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.78
## q[1462]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1463]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1464]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1465]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1466]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1467]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.78
## q[1468]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1469]     0.72    0.00 0.02     0.67     0.70     0.72     0.73     0.76
## q[1470]     0.72    0.00 0.02     0.67     0.70     0.72     0.73     0.76
## q[1471]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1472]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1473]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1474]     0.86    0.00 0.01     0.84     0.85     0.86     0.87     0.89
## q[1475]     0.81    0.00 0.02     0.78     0.80     0.81     0.83     0.85
## q[1476]     0.79    0.00 0.02     0.75     0.78     0.79     0.81     0.84
## q[1477]     0.86    0.00 0.01     0.84     0.85     0.86     0.87     0.89
## q[1478]     0.86    0.00 0.01     0.84     0.85     0.86     0.87     0.89
## q[1479]     0.81    0.00 0.02     0.78     0.80     0.81     0.83     0.85
## q[1480]     0.86    0.00 0.01     0.84     0.85     0.86     0.87     0.89
## q[1481]     0.86    0.00 0.01     0.84     0.85     0.86     0.87     0.89
## q[1482]     0.86    0.00 0.01     0.84     0.85     0.86     0.87     0.89
## q[1483]     0.79    0.00 0.02     0.75     0.78     0.79     0.81     0.84
## q[1484]     0.79    0.00 0.02     0.75     0.78     0.79     0.81     0.84
## q[1485]     0.81    0.00 0.02     0.78     0.80     0.81     0.83     0.85
## q[1486]     0.79    0.00 0.02     0.75     0.78     0.79     0.81     0.84
## q[1487]     0.79    0.00 0.02     0.75     0.78     0.79     0.81     0.84
## q[1488]     0.81    0.00 0.02     0.78     0.80     0.81     0.83     0.85
## q[1489]     0.86    0.00 0.01     0.84     0.85     0.86     0.87     0.89
## q[1490]     0.81    0.00 0.02     0.78     0.80     0.81     0.83     0.85
## q[1491]     0.79    0.00 0.02     0.75     0.78     0.79     0.81     0.84
## q[1492]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1493]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1494]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1495]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1496]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1497]     0.71    0.00 0.02     0.66     0.69     0.71     0.72     0.75
## q[1498]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1499]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1500]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1501]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1502]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1503]     0.71    0.00 0.02     0.66     0.69     0.71     0.72     0.75
## q[1504]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1505]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1506]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1507]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1508]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1509]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1510]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1511]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1512]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1513]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1514]     0.71    0.00 0.02     0.66     0.69     0.71     0.72     0.75
## q[1515]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1516]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1517]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1518]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1519]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1520]     0.71    0.00 0.02     0.66     0.69     0.71     0.72     0.75
## q[1521]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1522]     0.71    0.00 0.02     0.66     0.69     0.71     0.72     0.75
## q[1523]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1524]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1525]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1526]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1527]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1528]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1529]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1530]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1531]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1532]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1533]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1534]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1535]     0.71    0.00 0.02     0.66     0.69     0.71     0.72     0.75
## q[1536]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1537]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1538]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1539]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[1540]     0.71    0.00 0.02     0.66     0.69     0.71     0.72     0.75
## q[1541]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1542]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1543]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1544]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1545]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.75
## q[1546]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.75
## q[1547]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1548]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.75
## q[1549]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.75
## q[1550]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1551]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1552]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1553]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1554]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.75
## q[1555]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1556]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1557]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1558]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1559]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1560]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.75
## q[1561]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1562]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.75
## q[1563]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1564]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.75
## q[1565]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1566]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1567]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.75
## q[1568]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1569]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.75
## q[1570]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1571]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1572]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.75
## q[1573]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1574]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.75
## q[1575]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.75
## q[1576]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1577]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.75
## q[1578]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1579]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1580]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1581]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1582]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1583]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.75
## q[1584]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1585]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1586]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1587]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.75
## q[1588]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.75
## q[1589]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.75
## q[1590]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1591]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1592]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1593]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1594]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1595]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1596]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1597]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.75
## q[1598]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1599]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1600]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1601]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1602]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.75
## q[1603]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.75
## q[1604]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1605]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1606]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.75
## q[1607]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1608]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.75
## q[1609]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1610]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.75
## q[1611]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1612]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1613]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.82
## q[1614]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1615]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1616]     0.59    0.00 0.03     0.54     0.57     0.59     0.61     0.64
## q[1617]     0.59    0.00 0.03     0.54     0.57     0.59     0.61     0.64
## q[1618]     0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[1619]     0.59    0.00 0.03     0.54     0.57     0.59     0.61     0.64
## q[1620]     0.59    0.00 0.03     0.54     0.57     0.59     0.61     0.64
## q[1621]     0.47    0.00 0.04     0.40     0.44     0.47     0.49     0.54
## q[1622]     0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[1623]     0.59    0.00 0.03     0.54     0.57     0.59     0.61     0.64
## q[1624]     0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[1625]     0.59    0.00 0.03     0.54     0.57     0.59     0.61     0.64
## q[1626]     0.59    0.00 0.03     0.54     0.57     0.59     0.61     0.64
## q[1627]     0.59    0.00 0.03     0.54     0.57     0.59     0.61     0.64
## q[1628]     0.65    0.00 0.02     0.60     0.63     0.65     0.66     0.69
## q[1629]     0.65    0.00 0.02     0.60     0.63     0.65     0.66     0.69
## q[1630]     0.62    0.00 0.03     0.57     0.60     0.62     0.64     0.68
## q[1631]     0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.76
## q[1632]     0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.76
## q[1633]     0.62    0.00 0.03     0.57     0.60     0.62     0.64     0.68
## q[1634]     0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.76
## q[1635]     0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.76
## q[1636]     0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.76
## q[1637]     0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.76
## q[1638]     0.65    0.00 0.02     0.60     0.63     0.65     0.66     0.69
## q[1639]     0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.76
## q[1640]     0.65    0.00 0.02     0.60     0.63     0.65     0.66     0.69
## q[1641]     0.62    0.00 0.03     0.57     0.60     0.62     0.64     0.68
## q[1642]     0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.76
## q[1643]     0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.76
## q[1644]     0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.76
## q[1645]     0.65    0.00 0.02     0.60     0.63     0.65     0.66     0.69
## q[1646]     0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.76
## q[1647]     0.62    0.00 0.03     0.57     0.60     0.62     0.64     0.68
## q[1648]     0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.76
## q[1649]     0.65    0.00 0.02     0.60     0.63     0.65     0.66     0.69
## q[1650]     0.65    0.00 0.02     0.60     0.63     0.65     0.66     0.69
## q[1651]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1652]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1653]     0.59    0.00 0.03     0.54     0.57     0.59     0.61     0.64
## q[1654]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1655]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1656]     0.59    0.00 0.03     0.54     0.57     0.59     0.61     0.64
## q[1657]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1658]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1659]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1660]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1661]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1662]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1663]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1664]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1665]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1666]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1667]     0.59    0.00 0.03     0.54     0.57     0.59     0.61     0.64
## q[1668]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1669]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1670]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1671]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1672]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1673]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1674]     0.59    0.00 0.03     0.54     0.57     0.59     0.61     0.64
## q[1675]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1676]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1677]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1678]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1679]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1680]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1681]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1682]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1683]     0.59    0.00 0.03     0.54     0.57     0.59     0.61     0.64
## q[1684]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1685]     0.59    0.00 0.03     0.54     0.57     0.59     0.61     0.64
## q[1686]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1687]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1688]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1689]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1690]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1691]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1692]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1693]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1694]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1695]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1696]     0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.78
## q[1697]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1698]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1699]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1700]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1701]     0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.78
## q[1702]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1703]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1704]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1705]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1706]     0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.78
## q[1707]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1708]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1709]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1710]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1711]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1712]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1713]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1714]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1715]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1716]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1717]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1718]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1719]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1720]     0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.78
## q[1721]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1722]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1723]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1724]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1725]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1726]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1727]     0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.78
## q[1728]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1729]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1730]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1731]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1732]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1733]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1734]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1735]     0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.78
## q[1736]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1737]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1738]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1739]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1740]     0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.78
## q[1741]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1742]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1743]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1744]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1745]     0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.78
## q[1746]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1747]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1748]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1749]     0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.79
## q[1750]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1751]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1752]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1753]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1754]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1755]     0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.79
## q[1756]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1757]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1758]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1759]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1760]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1761]     0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.79
## q[1762]     0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.79
## q[1763]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1764]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1765]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1766]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1767]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1768]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1769]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1770]     0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.79
## q[1771]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1772]     0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.79
## q[1773]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1774]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1775]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1776]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1777]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1778]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1779]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1780]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1781]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1782]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1783]     0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.79
## q[1784]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1785]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1786]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1787]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1788]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1789]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1790]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1791]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1792]     0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.79
## q[1793]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1794]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1795]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1796]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1797]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1798]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1799]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1800]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1801]     0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.79
## q[1802]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1803]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1804]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1805]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1806]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1807]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1808]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1809]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1810]     0.54    0.00 0.03     0.49     0.52     0.54     0.56     0.60
## q[1811]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1812]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1813]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1814]     0.54    0.00 0.03     0.49     0.52     0.54     0.56     0.60
## q[1815]     0.54    0.00 0.03     0.49     0.52     0.54     0.56     0.60
## q[1816]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1817]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1818]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1819]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1820]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1821]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1822]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1823]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1824]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1825]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1826]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1827]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1828]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1829]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1830]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1831]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1832]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1833]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1834]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1835]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1836]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1837]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1838]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1839]     0.54    0.00 0.03     0.49     0.52     0.54     0.56     0.60
## q[1840]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1841]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1842]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1843]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1844]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1845]     0.54    0.00 0.03     0.49     0.52     0.54     0.56     0.60
## q[1846]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1847]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1848]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1849]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1850]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1851]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1852]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1853]     0.54    0.00 0.03     0.49     0.52     0.54     0.56     0.60
## q[1854]     0.54    0.00 0.03     0.49     0.52     0.54     0.56     0.60
## q[1855]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1856]     0.54    0.00 0.03     0.49     0.52     0.54     0.56     0.60
## q[1857]     0.54    0.00 0.03     0.49     0.52     0.54     0.56     0.60
## q[1858]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1859]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1860]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1861]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1862]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1863]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1864]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1865]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1866]     0.54    0.00 0.03     0.49     0.52     0.54     0.56     0.60
## q[1867]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1868]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1869]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1870]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1871]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1872]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1873]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1874]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1875]     0.54    0.00 0.03     0.49     0.52     0.54     0.56     0.60
## q[1876]     0.54    0.00 0.03     0.49     0.52     0.54     0.56     0.60
## q[1877]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1878]     0.54    0.00 0.03     0.49     0.52     0.54     0.56     0.60
## q[1879]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1880]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1881]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1882]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1883]     0.54    0.00 0.03     0.49     0.52     0.54     0.56     0.60
## q[1884]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1885]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1886]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1887]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1888]     0.66    0.00 0.02     0.63     0.65     0.66     0.67     0.70
## q[1889]     0.54    0.00 0.03     0.49     0.52     0.54     0.56     0.60
## q[1890]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1891]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1892]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1893]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1894]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1895]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1896]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1897]     0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.75
## q[1898]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1899]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1900]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1901]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1902]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1903]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1904]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1905]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1906]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1907]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1908]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1909]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1910]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1911]     0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.75
## q[1912]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1913]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1914]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1915]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1916]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1917]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1918]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1919]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1920]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1921]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1922]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1923]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1924]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1925]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1926]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1927]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1928]     0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.75
## q[1929]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1930]     0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.75
## q[1931]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1932]     0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.75
## q[1933]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1934]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1935]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1936]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1937]     0.68    0.00 0.02     0.63     0.67     0.68     0.70     0.73
## q[1938]     0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.71
## q[1939]     0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.71
## q[1940]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1941]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1942]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1943]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1944]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1945]     0.68    0.00 0.02     0.63     0.67     0.68     0.70     0.73
## q[1946]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1947]     0.68    0.00 0.02     0.63     0.67     0.68     0.70     0.73
## q[1948]     0.68    0.00 0.02     0.63     0.67     0.68     0.70     0.73
## q[1949]     0.68    0.00 0.02     0.63     0.67     0.68     0.70     0.73
## q[1950]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1951]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1952]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1953]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1954]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1955]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1956]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1957]     0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.71
## q[1958]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1959]     0.68    0.00 0.02     0.63     0.66     0.68     0.70     0.72
## q[1960]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1961]     0.65    0.00 0.03     0.60     0.63     0.65     0.67     0.71
## q[1962]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1963]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1964]     0.68    0.00 0.02     0.63     0.66     0.68     0.70     0.72
## q[1965]     0.65    0.00 0.03     0.60     0.63     0.65     0.67     0.71
## q[1966]     0.68    0.00 0.02     0.63     0.66     0.68     0.70     0.72
## q[1967]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1968]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1969]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1970]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1971]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1972]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1973]     0.68    0.00 0.02     0.63     0.66     0.68     0.70     0.72
## q[1974]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1975]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1976]     0.68    0.00 0.02     0.63     0.66     0.68     0.70     0.72
## q[1977]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1978]     0.68    0.00 0.02     0.63     0.66     0.68     0.70     0.72
## q[1979]     0.65    0.00 0.03     0.60     0.63     0.65     0.67     0.71
## q[1980]     0.68    0.00 0.02     0.63     0.66     0.68     0.70     0.72
## q[1981]     0.68    0.00 0.02     0.63     0.66     0.68     0.70     0.72
## q[1982]     0.68    0.00 0.02     0.63     0.66     0.68     0.70     0.72
## q[1983]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1984]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1985]     0.65    0.00 0.03     0.60     0.63     0.65     0.67     0.71
## q[1986]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1987]     0.65    0.00 0.03     0.60     0.63     0.65     0.67     0.71
## q[1988]     0.68    0.00 0.02     0.63     0.66     0.68     0.70     0.72
## q[1989]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1990]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1991]     0.68    0.00 0.02     0.63     0.66     0.68     0.70     0.72
## q[1992]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1993]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1994]     0.65    0.00 0.03     0.60     0.63     0.65     0.67     0.71
## q[1995]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1996]     0.68    0.00 0.02     0.63     0.66     0.68     0.70     0.72
## q[1997]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1998]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1999]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[2000]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2001]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2002]     0.74    0.00 0.02     0.69     0.72     0.74     0.75     0.78
## q[2003]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2004]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2005]     0.74    0.00 0.02     0.69     0.72     0.74     0.75     0.78
## q[2006]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2007]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2008]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2009]     0.74    0.00 0.02     0.69     0.72     0.74     0.75     0.78
## q[2010]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2011]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2012]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2013]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2014]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2015]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2016]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2017]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2018]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2019]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2020]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2021]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2022]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2023]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2024]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2025]     0.74    0.00 0.02     0.69     0.72     0.74     0.75     0.78
## q[2026]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2027]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2028]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2029]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2030]     0.74    0.00 0.02     0.69     0.72     0.74     0.75     0.78
## q[2031]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2032]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2033]     0.74    0.00 0.02     0.69     0.72     0.74     0.75     0.78
## q[2034]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2035]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2036]     0.74    0.00 0.02     0.69     0.72     0.74     0.75     0.78
## q[2037]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2038]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2039]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2040]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2041]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2042]     0.74    0.00 0.02     0.69     0.72     0.74     0.75     0.78
## q[2043]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2044]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2045]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2046]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2047]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2048]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2049]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2050]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2051]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2052]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2053]     0.74    0.00 0.02     0.69     0.72     0.74     0.75     0.78
## q[2054]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2055]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2056]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2057]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2058]     0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[2059]     0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[2060]     0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[2061]     0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[2062]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2063]     0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[2064]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2065]     0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[2066]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2067]     0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[2068]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2069]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2070]     0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[2071]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2072]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2073]     0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[2074]     0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[2075]     0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[2076]     0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[2077]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2078]     0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[2079]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2080]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2081]     0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[2082]     0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[2083]     0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[2084]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2085]     0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[2086]     0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[2087]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2088]     0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[2089]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2090]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2091]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2092]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2093]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2094]     0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.77
## q[2095]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2096]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2097]     0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.77
## q[2098]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2099]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2100]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2101]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2102]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2103]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2104]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2105]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2106]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2107]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2108]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2109]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2110]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2111]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2112]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2113]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2114]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2115]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2116]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2117]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2118]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2119]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2120]     0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.77
## q[2121]     0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.77
## q[2122]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2123]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2124]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2125]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2126]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2127]     0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.77
## q[2128]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2129]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2130]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2131]     0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.77
## q[2132]     0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.77
## q[2133]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2134]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2135]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2136]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2137]     0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.77
## q[2138]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2139]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2140]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2141]     0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.77
## q[2142]     0.79    0.00 0.02     0.76     0.78     0.79     0.80     0.82
## q[2143]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2144]     0.77    0.00 0.02     0.73     0.76     0.77     0.79     0.81
## q[2145]     0.77    0.00 0.02     0.73     0.76     0.77     0.79     0.81
## q[2146]     0.79    0.00 0.02     0.76     0.78     0.79     0.80     0.82
## q[2147]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2148]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2149]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2150]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2151]     0.79    0.00 0.02     0.76     0.78     0.79     0.80     0.82
## q[2152]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2153]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2154]     0.79    0.00 0.02     0.76     0.78     0.79     0.80     0.82
## q[2155]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2156]     0.79    0.00 0.02     0.76     0.78     0.79     0.80     0.82
## q[2157]     0.77    0.00 0.02     0.73     0.76     0.77     0.79     0.81
## q[2158]     0.79    0.00 0.02     0.76     0.78     0.79     0.80     0.82
## q[2159]     0.79    0.00 0.02     0.76     0.78     0.79     0.80     0.82
## q[2160]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2161]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2162]     0.77    0.00 0.02     0.73     0.76     0.77     0.79     0.81
## q[2163]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2164]     0.77    0.00 0.02     0.73     0.76     0.77     0.79     0.81
## q[2165]     0.79    0.00 0.02     0.76     0.78     0.79     0.80     0.82
## q[2166]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2167]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2168]     0.79    0.00 0.02     0.76     0.78     0.79     0.80     0.82
## q[2169]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2170]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2171]     0.79    0.00 0.02     0.76     0.78     0.79     0.80     0.82
## q[2172]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2173]     0.89    0.00 0.01     0.85     0.88     0.89     0.90     0.91
## q[2174]     0.89    0.00 0.01     0.85     0.88     0.89     0.90     0.91
## q[2175]     0.84    0.00 0.02     0.80     0.83     0.84     0.86     0.88
## q[2176]     0.82    0.00 0.02     0.78     0.81     0.83     0.84     0.87
## q[2177]     0.89    0.00 0.01     0.85     0.88     0.89     0.90     0.91
## q[2178]     0.89    0.00 0.01     0.85     0.88     0.89     0.90     0.91
## q[2179]     0.89    0.00 0.01     0.85     0.88     0.89     0.90     0.91
## q[2180]     0.89    0.00 0.01     0.85     0.88     0.89     0.90     0.91
## q[2181]     0.82    0.00 0.02     0.78     0.81     0.83     0.84     0.87
## q[2182]     0.84    0.00 0.02     0.80     0.83     0.84     0.86     0.88
## q[2183]     0.84    0.00 0.02     0.80     0.83     0.84     0.86     0.88
## q[2184]     0.84    0.00 0.02     0.80     0.83     0.84     0.86     0.88
## q[2185]     0.89    0.00 0.01     0.85     0.88     0.89     0.90     0.91
## q[2186]     0.84    0.00 0.02     0.80     0.83     0.84     0.86     0.88
## q[2187]     0.89    0.00 0.01     0.85     0.88     0.89     0.90     0.91
## q[2188]     0.89    0.00 0.01     0.85     0.88     0.89     0.90     0.91
## q[2189]     0.89    0.00 0.01     0.85     0.88     0.89     0.90     0.91
## q[2190]     0.89    0.00 0.01     0.85     0.88     0.89     0.90     0.91
## q[2191]     0.89    0.00 0.01     0.85     0.88     0.89     0.90     0.91
## q[2192]     0.89    0.00 0.01     0.85     0.88     0.89     0.90     0.91
## q[2193]     0.82    0.00 0.02     0.78     0.81     0.83     0.84     0.87
## q[2194]     0.89    0.00 0.01     0.85     0.88     0.89     0.90     0.91
## q[2195]     0.89    0.00 0.01     0.85     0.88     0.89     0.90     0.91
## q[2196]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2197]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2198]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2199]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2200]     0.71    0.00 0.02     0.67     0.70     0.72     0.73     0.75
## q[2201]     0.69    0.00 0.03     0.64     0.67     0.69     0.71     0.74
## q[2202]     0.71    0.00 0.02     0.67     0.70     0.72     0.73     0.75
## q[2203]     0.69    0.00 0.03     0.64     0.67     0.69     0.71     0.74
## q[2204]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2205]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2206]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2207]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2208]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2209]     0.71    0.00 0.02     0.67     0.70     0.72     0.73     0.75
## q[2210]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2211]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2212]     0.71    0.00 0.02     0.67     0.70     0.72     0.73     0.75
## q[2213]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2214]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2215]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2216]     0.69    0.00 0.03     0.64     0.67     0.69     0.71     0.74
## q[2217]     0.71    0.00 0.02     0.67     0.70     0.72     0.73     0.75
## q[2218]     0.71    0.00 0.02     0.67     0.70     0.72     0.73     0.75
## q[2219]     0.71    0.00 0.02     0.67     0.70     0.72     0.73     0.75
## q[2220]     0.71    0.00 0.02     0.67     0.70     0.72     0.73     0.75
## q[2221]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2222]     0.69    0.00 0.03     0.64     0.67     0.69     0.71     0.74
## q[2223]     0.69    0.00 0.03     0.64     0.67     0.69     0.71     0.74
## q[2224]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2225]     0.69    0.00 0.03     0.64     0.67     0.69     0.71     0.74
## q[2226]     0.69    0.00 0.03     0.64     0.67     0.69     0.71     0.74
## q[2227]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2228]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2229]     0.71    0.00 0.02     0.67     0.70     0.72     0.73     0.75
## q[2230]     0.71    0.00 0.02     0.67     0.70     0.72     0.73     0.75
## q[2231]     0.69    0.00 0.03     0.64     0.67     0.69     0.71     0.74
## q[2232]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2233]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2234]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2235]     0.69    0.00 0.03     0.64     0.67     0.69     0.71     0.74
## q[2236]     0.69    0.00 0.03     0.64     0.67     0.69     0.71     0.74
## q[2237]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2238]     0.69    0.00 0.03     0.64     0.67     0.69     0.71     0.74
## q[2239]     0.71    0.00 0.02     0.67     0.70     0.72     0.73     0.75
## q[2240]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2241]     0.71    0.00 0.02     0.67     0.70     0.72     0.73     0.75
## q[2242]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2243]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2244]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2245]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2246]     0.69    0.00 0.03     0.64     0.67     0.69     0.71     0.74
## q[2247]     0.57    0.00 0.04     0.49     0.54     0.57     0.60     0.65
## q[2248]     0.60    0.00 0.04     0.52     0.57     0.60     0.62     0.67
## q[2249]     0.60    0.00 0.04     0.52     0.57     0.60     0.62     0.67
## q[2250]     0.60    0.00 0.04     0.52     0.57     0.60     0.62     0.67
## q[2251]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[2252]     0.60    0.00 0.04     0.52     0.57     0.60     0.62     0.67
## q[2253]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[2254]     0.60    0.00 0.04     0.52     0.57     0.60     0.62     0.67
## q[2255]     0.60    0.00 0.04     0.52     0.57     0.60     0.62     0.67
## q[2256]     0.60    0.00 0.04     0.52     0.57     0.60     0.62     0.67
## q[2257]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[2258]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[2259]     0.60    0.00 0.04     0.52     0.57     0.60     0.62     0.67
## q[2260]     0.60    0.00 0.04     0.52     0.57     0.60     0.62     0.67
## q[2261]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[2262]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[2263]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[2264]     0.60    0.00 0.04     0.52     0.57     0.60     0.62     0.67
## q[2265]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[2266]     0.57    0.00 0.04     0.49     0.54     0.57     0.60     0.65
## q[2267]     0.60    0.00 0.04     0.52     0.57     0.60     0.62     0.67
## q[2268]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[2269]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2270]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2271]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2272]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2273]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2274]     0.78    0.00 0.02     0.73     0.76     0.78     0.79     0.82
## q[2275]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2276]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2277]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2278]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2279]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2280]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2281]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2282]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2283]     0.78    0.00 0.02     0.73     0.76     0.78     0.79     0.82
## q[2284]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2285]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2286]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2287]     0.78    0.00 0.02     0.73     0.76     0.78     0.79     0.82
## q[2288]     0.78    0.00 0.02     0.73     0.76     0.78     0.79     0.82
## q[2289]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2290]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2291]     0.78    0.00 0.02     0.73     0.76     0.78     0.79     0.82
## q[2292]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2293]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2294]     0.78    0.00 0.02     0.73     0.76     0.78     0.79     0.82
## q[2295]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2296]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2297]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2298]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2299]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2300]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2301]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2302]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2303]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2304]     0.78    0.00 0.02     0.73     0.76     0.78     0.79     0.82
## q[2305]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2306]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2307]     0.70    0.00 0.02     0.66     0.69     0.70     0.72     0.74
## q[2308]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.73
## q[2309]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.81
## q[2310]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.81
## q[2311]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.73
## q[2312]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.81
## q[2313]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.81
## q[2314]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.81
## q[2315]     0.70    0.00 0.02     0.66     0.69     0.70     0.72     0.74
## q[2316]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.73
## q[2317]     0.70    0.00 0.02     0.66     0.69     0.70     0.72     0.74
## q[2318]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.81
## q[2319]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.81
## q[2320]     0.70    0.00 0.02     0.66     0.69     0.70     0.72     0.74
## q[2321]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.81
## q[2322]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.73
## q[2323]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.81
## q[2324]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.81
## q[2325]     0.70    0.00 0.02     0.66     0.69     0.70     0.72     0.74
## q[2326]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.81
## q[2327]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.73
## q[2328]     0.70    0.00 0.02     0.66     0.69     0.70     0.72     0.74
## q[2329]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.81
## q[2330]     0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[2331]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2332]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2333]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2334]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2335]     0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[2336]     0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[2337]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[2338]     0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[2339]     0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[2340]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[2341]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2342]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2343]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2344]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2345]     0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[2346]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[2347]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2348]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2349]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2350]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2351]     0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[2352]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2353]     0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[2354]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2355]     0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[2356]     0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[2357]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2358]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2359]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2360]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2361]     0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[2362]     0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[2363]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2364]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[2365]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2366]     0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[2367]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2368]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2369]     0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[2370]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[2371]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[2372]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2373]     0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[2374]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2375]     0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[2376]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2377]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[2378]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2379]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2380]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2381]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2382]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2383]     0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[2384]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[2385]     0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[2386]     0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[2387]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2388]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2389]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2390]     0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[2391]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[2392]     0.56    0.00 0.03     0.51     0.54     0.56     0.58     0.61
## q[2393]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2394]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2395]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[2396]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## lp__    -1379.64    0.04 1.55 -1383.58 -1380.50 -1379.29 -1378.49 -1377.56
##         n_eff Rhat
## b[1]     2165    1
## b[2]     2912    1
## b[3]     2184    1
## b[4]     3323    1
## b[5]     2891    1
## OR1      2931    1
## OR2      2256    1
## OR3      3316    1
## OR4      2857    1
## q[1]     2525    1
## q[2]     2329    1
## q[3]     2636    1
## q[4]     2329    1
## q[5]     2525    1
## q[6]     2525    1
## q[7]     2636    1
## q[8]     2525    1
## q[9]     2329    1
## q[10]    2329    1
## q[11]    2329    1
## q[12]    2636    1
## q[13]    2329    1
## q[14]    2329    1
## q[15]    2329    1
## q[16]    2329    1
## q[17]    2329    1
## q[18]    2525    1
## q[19]    2329    1
## q[20]    2329    1
## q[21]    2525    1
## q[22]    2525    1
## q[23]    2636    1
## q[24]    2636    1
## q[25]    2329    1
## q[26]    2329    1
## q[27]    2525    1
## q[28]    2329    1
## q[29]    2525    1
## q[30]    2329    1
## q[31]    2525    1
## q[32]    2329    1
## q[33]    2525    1
## q[34]    2329    1
## q[35]    2636    1
## q[36]    2329    1
## q[37]    2329    1
## q[38]    2329    1
## q[39]    2525    1
## q[40]    2329    1
## q[41]    2329    1
## q[42]    2525    1
## q[43]    2525    1
## q[44]    2894    1
## q[45]    2894    1
## q[46]    2894    1
## q[47]    3001    1
## q[48]    2894    1
## q[49]    3490    1
## q[50]    3490    1
## q[51]    3490    1
## q[52]    3490    1
## q[53]    2894    1
## q[54]    2894    1
## q[55]    2894    1
## q[56]    2894    1
## q[57]    2894    1
## q[58]    3001    1
## q[59]    2894    1
## q[60]    3001    1
## q[61]    2894    1
## q[62]    2894    1
## q[63]    2894    1
## q[64]    2894    1
## q[65]    3490    1
## q[66]    2894    1
## q[67]    3490    1
## q[68]    3001    1
## q[69]    3490    1
## q[70]    3001    1
## q[71]    3001    1
## q[72]    2894    1
## q[73]    3490    1
## q[74]    2894    1
## q[75]    3490    1
## q[76]    3001    1
## q[77]    2894    1
## q[78]    2894    1
## q[79]    3001    1
## q[80]    3490    1
## q[81]    3490    1
## q[82]    3001    1
## q[83]    3490    1
## q[84]    2894    1
## q[85]    2894    1
## q[86]    2894    1
## q[87]    2894    1
## q[88]    3490    1
## q[89]    2894    1
## q[90]    2894    1
## q[91]    3490    1
## q[92]    2894    1
## q[93]    2894    1
## q[94]    3490    1
## q[95]    3490    1
## q[96]    3490    1
## q[97]    2894    1
## q[98]    2894    1
## q[99]    3001    1
## q[100]   3050    1
## q[101]   3050    1
## q[102]   3327    1
## q[103]   3946    1
## q[104]   3946    1
## q[105]   3050    1
## q[106]   3050    1
## q[107]   3050    1
## q[108]   3327    1
## q[109]   3050    1
## q[110]   3050    1
## q[111]   3050    1
## q[112]   3946    1
## q[113]   3946    1
## q[114]   3327    1
## q[115]   3946    1
## q[116]   3050    1
## q[117]   3327    1
## q[118]   3050    1
## q[119]   3327    1
## q[120]   3946    1
## q[121]   3946    1
## q[122]   3050    1
## q[123]   3050    1
## q[124]   3050    1
## q[125]   3946    1
## q[126]   3050    1
## q[127]   3946    1
## q[128]   3946    1
## q[129]   3050    1
## q[130]   3050    1
## q[131]   3327    1
## q[132]   2890    1
## q[133]   2890    1
## q[134]   2890    1
## q[135]   3152    1
## q[136]   3152    1
## q[137]   2945    1
## q[138]   2890    1
## q[139]   2890    1
## q[140]   2945    1
## q[141]   2890    1
## q[142]   2890    1
## q[143]   2890    1
## q[144]   2890    1
## q[145]   2890    1
## q[146]   3152    1
## q[147]   2890    1
## q[148]   2890    1
## q[149]   2945    1
## q[150]   3152    1
## q[151]   3152    1
## q[152]   3152    1
## q[153]   2945    1
## q[154]   3152    1
## q[155]   2945    1
## q[156]   2890    1
## q[157]   2890    1
## q[158]   2945    1
## q[159]   2890    1
## q[160]   3152    1
## q[161]   2890    1
## q[162]   2945    1
## q[163]   3152    1
## q[164]   2890    1
## q[165]   2890    1
## q[166]   3152    1
## q[167]   2890    1
## q[168]   2890    1
## q[169]   2945    1
## q[170]   2890    1
## q[171]   2945    1
## q[172]   3152    1
## q[173]   2890    1
## q[174]   2890    1
## q[175]   2890    1
## q[176]   2890    1
## q[177]   2823    1
## q[178]   3152    1
## q[179]   3152    1
## q[180]   2823    1
## q[181]   3152    1
## q[182]   2650    1
## q[183]   2650    1
## q[184]   3152    1
## q[185]   2650    1
## q[186]   2650    1
## q[187]   3152    1
## q[188]   3152    1
## q[189]   3152    1
## q[190]   2650    1
## q[191]   3152    1
## q[192]   2650    1
## q[193]   3152    1
## q[194]   3152    1
## q[195]   2650    1
## q[196]   2650    1
## q[197]   3152    1
## q[198]   2650    1
## q[199]   2650    1
## q[200]   2650    1
## q[201]   2650    1
## q[202]   2650    1
## q[203]   3152    1
## q[204]   2650    1
## q[205]   2823    1
## q[206]   2823    1
## q[207]   2650    1
## q[208]   3152    1
## q[209]   2650    1
## q[210]   2998    1
## q[211]   2998    1
## q[212]   2998    1
## q[213]   3292    1
## q[214]   2998    1
## q[215]   4018    1
## q[216]   4018    1
## q[217]   3292    1
## q[218]   4018    1
## q[219]   4018    1
## q[220]   3292    1
## q[221]   2998    1
## q[222]   2998    1
## q[223]   2998    1
## q[224]   3292    1
## q[225]   2998    1
## q[226]   2998    1
## q[227]   2998    1
## q[228]   2998    1
## q[229]   2998    1
## q[230]   4018    1
## q[231]   2998    1
## q[232]   2998    1
## q[233]   2998    1
## q[234]   4018    1
## q[235]   2998    1
## q[236]   3292    1
## q[237]   4018    1
## q[238]   2998    1
## q[239]   4018    1
## q[240]   4018    1
## q[241]   2998    1
## q[242]   2998    1
## q[243]   4018    1
## q[244]   3292    1
## q[245]   4018    1
## q[246]   3292    1
## q[247]   2998    1
## q[248]   2998    1
## q[249]   4018    1
## q[250]   3292    1
## q[251]   2998    1
## q[252]   2998    1
## q[253]   2998    1
## q[254]   2998    1
## q[255]   2998    1
## q[256]   2998    1
## q[257]   3292    1
## q[258]   3292    1
## q[259]   4018    1
## q[260]   4018    1
## q[261]   2998    1
## q[262]   2998    1
## q[263]   2998    1
## q[264]   4018    1
## q[265]   4018    1
## q[266]   2998    1
## q[267]   4018    1
## q[268]   2998    1
## q[269]   3292    1
## q[270]   3292    1
## q[271]   2933    1
## q[272]   3465    1
## q[273]   3465    1
## q[274]   3257    1
## q[275]   2933    1
## q[276]   2933    1
## q[277]   3257    1
## q[278]   2933    1
## q[279]   3465    1
## q[280]   2933    1
## q[281]   2933    1
## q[282]   3465    1
## q[283]   2933    1
## q[284]   3465    1
## q[285]   2933    1
## q[286]   2933    1
## q[287]   3465    1
## q[288]   3257    1
## q[289]   3465    1
## q[290]   2933    1
## q[291]   3465    1
## q[292]   3465    1
## q[293]   3257    1
## q[294]   3465    1
## q[295]   3257    1
## q[296]   2933    1
## q[297]   2933    1
## q[298]   3257    1
## q[299]   2933    1
## q[300]   3465    1
## q[301]   2933    1
## q[302]   3465    1
## q[303]   3257    1
## q[304]   3465    1
## q[305]   2933    1
## q[306]   2933    1
## q[307]   2933    1
## q[308]   2933    1
## q[309]   2933    1
## q[310]   3465    1
## q[311]   2933    1
## q[312]   3465    1
## q[313]   2933    1
## q[314]   3257    1
## q[315]   3465    1
## q[316]   3257    1
## q[317]   2933    1
## q[318]   2933    1
## q[319]   2933    1
## q[320]   3447    1
## q[321]   2850    1
## q[322]   2850    1
## q[323]   2976    1
## q[324]   2850    1
## q[325]   2850    1
## q[326]   3447    1
## q[327]   3447    1
## q[328]   3447    1
## q[329]   2976    1
## q[330]   3447    1
## q[331]   2976    1
## q[332]   2850    1
## q[333]   2850    1
## q[334]   2850    1
## q[335]   2850    1
## q[336]   2976    1
## q[337]   2850    1
## q[338]   2850    1
## q[339]   2850    1
## q[340]   2850    1
## q[341]   2976    1
## q[342]   2850    1
## q[343]   2850    1
## q[344]   2850    1
## q[345]   2850    1
## q[346]   3447    1
## q[347]   2850    1
## q[348]   2850    1
## q[349]   2850    1
## q[350]   2850    1
## q[351]   3447    1
## q[352]   2850    1
## q[353]   3447    1
## q[354]   2976    1
## q[355]   3447    1
## q[356]   3447    1
## q[357]   3447    1
## q[358]   3447    1
## q[359]   2976    1
## q[360]   2850    1
## q[361]   3447    1
## q[362]   2850    1
## q[363]   2976    1
## q[364]   2850    1
## q[365]   2850    1
## q[366]   2976    1
## q[367]   3447    1
## q[368]   2850    1
## q[369]   2850    1
## q[370]   3447    1
## q[371]   2850    1
## q[372]   2850    1
## q[373]   2976    1
## q[374]   3447    1
## q[375]   2850    1
## q[376]   2850    1
## q[377]   2850    1
## q[378]   3447    1
## q[379]   2850    1
## q[380]   2850    1
## q[381]   2850    1
## q[382]   2976    1
## q[383]   2850    1
## q[384]   2976    1
## q[385]   3447    1
## q[386]   2850    1
## q[387]   3447    1
## q[388]   3447    1
## q[389]   2850    1
## q[390]   3447    1
## q[391]   2850    1
## q[392]   2850    1
## q[393]   2850    1
## q[394]   2976    1
## q[395]   2850    1
## q[396]   3767    1
## q[397]   2879    1
## q[398]   2879    1
## q[399]   3204    1
## q[400]   2879    1
## q[401]   2879    1
## q[402]   2879    1
## q[403]   3767    1
## q[404]   3767    1
## q[405]   3767    1
## q[406]   3767    1
## q[407]   3204    1
## q[408]   3767    1
## q[409]   3204    1
## q[410]   2879    1
## q[411]   2879    1
## q[412]   2879    1
## q[413]   2879    1
## q[414]   2879    1
## q[415]   2879    1
## q[416]   3767    1
## q[417]   3204    1
## q[418]   2879    1
## q[419]   2879    1
## q[420]   2879    1
## q[421]   2879    1
## q[422]   2879    1
## q[423]   3767    1
## q[424]   2879    1
## q[425]   2879    1
## q[426]   3767    1
## q[427]   3767    1
## q[428]   2879    1
## q[429]   2879    1
## q[430]   3767    1
## q[431]   3767    1
## q[432]   3767    1
## q[433]   3767    1
## q[434]   2879    1
## q[435]   3204    1
## q[436]   3767    1
## q[437]   2879    1
## q[438]   3767    1
## q[439]   2879    1
## q[440]   3767    1
## q[441]   2879    1
## q[442]   3767    1
## q[443]   3767    1
## q[444]   2879    1
## q[445]   2879    1
## q[446]   2879    1
## q[447]   3767    1
## q[448]   2879    1
## q[449]   2879    1
## q[450]   2879    1
## q[451]   2879    1
## q[452]   2879    1
## q[453]   2879    1
## q[454]   2879    1
## q[455]   2879    1
## q[456]   2879    1
## q[457]   3767    1
## q[458]   2879    1
## q[459]   3204    1
## q[460]   2879    1
## q[461]   3204    1
## q[462]   3767    1
## q[463]   2879    1
## q[464]   2879    1
## q[465]   3204    1
## q[466]   3767    1
## q[467]   2879    1
## q[468]   3767    1
## q[469]   2879    1
## q[470]   2879    1
## q[471]   2879    1
## q[472]   3204    1
## q[473]   2879    1
## q[474]   3790    1
## q[475]   2886    1
## q[476]   2886    1
## q[477]   2886    1
## q[478]   3790    1
## q[479]   3790    1
## q[480]   3209    1
## q[481]   3209    1
## q[482]   3209    1
## q[483]   2886    1
## q[484]   2886    1
## q[485]   2886    1
## q[486]   2886    1
## q[487]   3209    1
## q[488]   2886    1
## q[489]   2886    1
## q[490]   2886    1
## q[491]   3790    1
## q[492]   2886    1
## q[493]   2886    1
## q[494]   2886    1
## q[495]   2886    1
## q[496]   3790    1
## q[497]   2886    1
## q[498]   3790    1
## q[499]   2886    1
## q[500]   2886    1
## q[501]   2886    1
## q[502]   3209    1
## q[503]   3790    1
## q[504]   3790    1
## q[505]   3790    1
## q[506]   3790    1
## q[507]   2886    1
## q[508]   2886    1
## q[509]   3209    1
## q[510]   3209    1
## q[511]   2886    1
## q[512]   3209    1
## q[513]   3209    1
## q[514]   2886    1
## q[515]   2886    1
## q[516]   2886    1
## q[517]   2886    1
## q[518]   3790    1
## q[519]   3790    1
## q[520]   3209    1
## q[521]   2886    1
## q[522]   2886    1
## q[523]   2886    1
## q[524]   2886    1
## q[525]   3209    1
## q[526]   3209    1
## q[527]   3790    1
## q[528]   3790    1
## q[529]   2886    1
## q[530]   2886    1
## q[531]   3790    1
## q[532]   3790    1
## q[533]   2886    1
## q[534]   2886    1
## q[535]   2886    1
## q[536]   3209    1
## q[537]   3209    1
## q[538]   2987    1
## q[539]   2987    1
## q[540]   2987    1
## q[541]   3577    1
## q[542]   3577    1
## q[543]   2987    1
## q[544]   2987    1
## q[545]   3049    1
## q[546]   2987    1
## q[547]   2987    1
## q[548]   2987    1
## q[549]   2987    1
## q[550]   2987    1
## q[551]   3577    1
## q[552]   3049    1
## q[553]   3577    1
## q[554]   3577    1
## q[555]   3049    1
## q[556]   3049    1
## q[557]   2987    1
## q[558]   2987    1
## q[559]   3049    1
## q[560]   3577    1
## q[561]   3049    1
## q[562]   3577    1
## q[563]   2987    1
## q[564]   3577    1
## q[565]   2987    1
## q[566]   3049    1
## q[567]   2987    1
## q[568]   3577    1
## q[569]   2987    1
## q[570]   2987    1
## q[571]   2987    1
## q[572]   2949    1
## q[573]   2949    1
## q[574]   3955    1
## q[575]   3955    1
## q[576]   3258    1
## q[577]   3955    1
## q[578]   2949    1
## q[579]   2949    1
## q[580]   3955    1
## q[581]   3258    1
## q[582]   2949    1
## q[583]   2949    1
## q[584]   3955    1
## q[585]   2949    1
## q[586]   3955    1
## q[587]   2949    1
## q[588]   3955    1
## q[589]   2949    1
## q[590]   2949    1
## q[591]   3955    1
## q[592]   2949    1
## q[593]   2949    1
## q[594]   3258    1
## q[595]   3258    1
## q[596]   3955    1
## q[597]   3955    1
## q[598]   2949    1
## q[599]   3258    1
## q[600]   2949    1
## q[601]   2949    1
## q[602]   2949    1
## q[603]   3955    1
## q[604]   3258    1
## q[605]   3955    1
## q[606]   3955    1
## q[607]   2949    1
## q[608]   2949    1
## q[609]   3955    1
## q[610]   3258    1
## q[611]   3955    1
## q[612]   2949    1
## q[613]   2949    1
## q[614]   3258    1
## q[615]   3258    1
## q[616]   3844    1
## q[617]   3416    1
## q[618]   3416    1
## q[619]   3844    1
## q[620]   3844    1
## q[621]   3199    1
## q[622]   3844    1
## q[623]   3199    1
## q[624]   3416    1
## q[625]   3416    1
## q[626]   3416    1
## q[627]   3416    1
## q[628]   3199    1
## q[629]   3416    1
## q[630]   3199    1
## q[631]   3416    1
## q[632]   3416    1
## q[633]   3416    1
## q[634]   3416    1
## q[635]   3416    1
## q[636]   3416    1
## q[637]   3844    1
## q[638]   3844    1
## q[639]   3416    1
## q[640]   3416    1
## q[641]   3416    1
## q[642]   3844    1
## q[643]   3199    1
## q[644]   3416    1
## q[645]   3416    1
## q[646]   3199    1
## q[647]   3416    1
## q[648]   3199    1
## q[649]   3416    1
## q[650]   3844    1
## q[651]   3416    1
## q[652]   3199    1
## q[653]   3844    1
## q[654]   3199    1
## q[655]   3416    1
## q[656]   3416    1
## q[657]   3844    1
## q[658]   3199    1
## q[659]   3844    1
## q[660]   3844    1
## q[661]   3416    1
## q[662]   3416    1
## q[663]   3844    1
## q[664]   3844    1
## q[665]   3844    1
## q[666]   3416    1
## q[667]   3199    1
## q[668]   3199    1
## q[669]   3465    1
## q[670]   2933    1
## q[671]   3465    1
## q[672]   3257    1
## q[673]   3257    1
## q[674]   3257    1
## q[675]   2933    1
## q[676]   2933    1
## q[677]   2933    1
## q[678]   2933    1
## q[679]   2933    1
## q[680]   2933    1
## q[681]   3465    1
## q[682]   2933    1
## q[683]   3465    1
## q[684]   2933    1
## q[685]   2933    1
## q[686]   2933    1
## q[687]   3465    1
## q[688]   3465    1
## q[689]   3465    1
## q[690]   2933    1
## q[691]   2933    1
## q[692]   3257    1
## q[693]   3257    1
## q[694]   2933    1
## q[695]   2933    1
## q[696]   2933    1
## q[697]   2933    1
## q[698]   3465    1
## q[699]   3257    1
## q[700]   2933    1
## q[701]   2933    1
## q[702]   3257    1
## q[703]   3257    1
## q[704]   3465    1
## q[705]   2933    1
## q[706]   3465    1
## q[707]   2933    1
## q[708]   3257    1
## q[709]   2891    1
## q[710]   2891    1
## q[711]   2891    1
## q[712]   2891    1
## q[713]   3413    1
## q[714]   3413    1
## q[715]   3235    1
## q[716]   3413    1
## q[717]   3413    1
## q[718]   2891    1
## q[719]   2891    1
## q[720]   2891    1
## q[721]   3413    1
## q[722]   3235    1
## q[723]   2891    1
## q[724]   2891    1
## q[725]   3413    1
## q[726]   2891    1
## q[727]   3413    1
## q[728]   2891    1
## q[729]   3413    1
## q[730]   2891    1
## q[731]   2891    1
## q[732]   2891    1
## q[733]   3413    1
## q[734]   2891    1
## q[735]   3235    1
## q[736]   3413    1
## q[737]   2891    1
## q[738]   2891    1
## q[739]   3413    1
## q[740]   3235    1
## q[741]   3235    1
## q[742]   3413    1
## q[743]   3413    1
## q[744]   2891    1
## q[745]   3235    1
## q[746]   2891    1
## q[747]   2891    1
## q[748]   2891    1
## q[749]   2891    1
## q[750]   2891    1
## q[751]   3413    1
## q[752]   3235    1
## q[753]   3413    1
## q[754]   3413    1
## q[755]   2891    1
## q[756]   2891    1
## q[757]   2891    1
## q[758]   3413    1
## q[759]   3235    1
## q[760]   3413    1
## q[761]   2891    1
## q[762]   2891    1
## q[763]   3235    1
## q[764]   3235    1
## q[765]   3569    1
## q[766]   3165    1
## q[767]   3165    1
## q[768]   3165    1
## q[769]   3165    1
## q[770]   3569    1
## q[771]   3569    1
## q[772]   3569    1
## q[773]   3569    1
## q[774]   3086    1
## q[775]   3165    1
## q[776]   3165    1
## q[777]   3165    1
## q[778]   3086    1
## q[779]   3165    1
## q[780]   3165    1
## q[781]   3165    1
## q[782]   3569    1
## q[783]   3165    1
## q[784]   3165    1
## q[785]   3165    1
## q[786]   3569    1
## q[787]   3165    1
## q[788]   3569    1
## q[789]   3569    1
## q[790]   3165    1
## q[791]   3165    1
## q[792]   3569    1
## q[793]   3569    1
## q[794]   3569    1
## q[795]   3165    1
## q[796]   3086    1
## q[797]   3086    1
## q[798]   3165    1
## q[799]   3569    1
## q[800]   3165    1
## q[801]   3569    1
## q[802]   3165    1
## q[803]   3165    1
## q[804]   3569    1
## q[805]   3569    1
## q[806]   3165    1
## q[807]   3086    1
## q[808]   3165    1
## q[809]   3165    1
## q[810]   3165    1
## q[811]   3165    1
## q[812]   3165    1
## q[813]   3165    1
## q[814]   3569    1
## q[815]   3569    1
## q[816]   3086    1
## q[817]   3165    1
## q[818]   3165    1
## q[819]   3569    1
## q[820]   3086    1
## q[821]   3165    1
## q[822]   3165    1
## q[823]   3165    1
## q[824]   2994    1
## q[825]   2994    1
## q[826]   2994    1
## q[827]   3551    1
## q[828]   3551    1
## q[829]   3551    1
## q[830]   2994    1
## q[831]   2994    1
## q[832]   2994    1
## q[833]   3551    1
## q[834]   3285    1
## q[835]   2994    1
## q[836]   3551    1
## q[837]   2994    1
## q[838]   3551    1
## q[839]   2994    1
## q[840]   3551    1
## q[841]   2994    1
## q[842]   3285    1
## q[843]   3551    1
## q[844]   2994    1
## q[845]   3551    1
## q[846]   3551    1
## q[847]   2994    1
## q[848]   2994    1
## q[849]   2994    1
## q[850]   2994    1
## q[851]   2994    1
## q[852]   2994    1
## q[853]   3551    1
## q[854]   3551    1
## q[855]   2994    1
## q[856]   2994    1
## q[857]   3285    1
## q[858]   3551    1
## q[859]   2994    1
## q[860]   2994    1
## q[861]   3285    1
## q[862]   2746    1
## q[863]   2434    1
## q[864]   2434    1
## q[865]   2434    1
## q[866]   2434    1
## q[867]   2434    1
## q[868]   2746    1
## q[869]   2746    1
## q[870]   2746    1
## q[871]   2797    1
## q[872]   2434    1
## q[873]   2434    1
## q[874]   2434    1
## q[875]   2434    1
## q[876]   2434    1
## q[877]   2434    1
## q[878]   2797    1
## q[879]   2434    1
## q[880]   2434    1
## q[881]   2434    1
## q[882]   2746    1
## q[883]   2434    1
## q[884]   2434    1
## q[885]   2746    1
## q[886]   2746    1
## q[887]   2797    1
## q[888]   2434    1
## q[889]   2746    1
## q[890]   2434    1
## q[891]   2746    1
## q[892]   2434    1
## q[893]   2434    1
## q[894]   2434    1
## q[895]   2434    1
## q[896]   2434    1
## q[897]   2797    1
## q[898]   2434    1
## q[899]   2746    1
## q[900]   2434    1
## q[901]   2434    1
## q[902]   2746    1
## q[903]   2434    1
## q[904]   2434    1
## q[905]   2797    1
## q[906]   2434    1
## q[907]   2523    1
## q[908]   2798    1
## q[909]   2975    1
## q[910]   2975    1
## q[911]   2798    1
## q[912]   2523    1
## q[913]   2975    1
## q[914]   2523    1
## q[915]   2523    1
## q[916]   2975    1
## q[917]   2523    1
## q[918]   2975    1
## q[919]   2523    1
## q[920]   2523    1
## q[921]   2975    1
## q[922]   2975    1
## q[923]   2798    1
## q[924]   2975    1
## q[925]   2523    1
## q[926]   2975    1
## q[927]   2975    1
## q[928]   2523    1
## q[929]   2975    1
## q[930]   2798    1
## q[931]   2523    1
## q[932]   2798    1
## q[933]   2975    1
## q[934]   2523    1
## q[935]   2523    1
## q[936]   2975    1
## q[937]   2975    1
## q[938]   2523    1
## q[939]   2523    1
## q[940]   2523    1
## q[941]   2523    1
## q[942]   2523    1
## q[943]   2975    1
## q[944]   2523    1
## q[945]   2523    1
## q[946]   2798    1
## q[947]   2798    1
## q[948]   2523    1
## q[949]   2975    1
## q[950]   2523    1
## q[951]   2523    1
## q[952]   2681    1
## q[953]   2681    1
## q[954]   2681    1
## q[955]   2681    1
## q[956]   2681    1
## q[957]   3216    1
## q[958]   3216    1
## q[959]   2854    1
## q[960]   3216    1
## q[961]   3216    1
## q[962]   2854    1
## q[963]   2681    1
## q[964]   2681    1
## q[965]   2681    1
## q[966]   2681    1
## q[967]   2681    1
## q[968]   2854    1
## q[969]   2681    1
## q[970]   2854    1
## q[971]   2681    1
## q[972]   2681    1
## q[973]   2681    1
## q[974]   2681    1
## q[975]   3216    1
## q[976]   2681    1
## q[977]   2681    1
## q[978]   3216    1
## q[979]   2681    1
## q[980]   2681    1
## q[981]   2681    1
## q[982]   2681    1
## q[983]   2854    1
## q[984]   3216    1
## q[985]   2854    1
## q[986]   3216    1
## q[987]   2854    1
## q[988]   3216    1
## q[989]   3216    1
## q[990]   2681    1
## q[991]   2681    1
## q[992]   3216    1
## q[993]   2854    1
## q[994]   2854    1
## q[995]   2681    1
## q[996]   2681    1
## q[997]   2854    1
## q[998]   2854    1
## q[999]   2681    1
## q[1000]  3216    1
## q[1001]  2681    1
## q[1002]  2854    1
## q[1003]  3216    1
## q[1004]  3216    1
## q[1005]  2854    1
## q[1006]  2681    1
## q[1007]  2681    1
## q[1008]  2681    1
## q[1009]  2681    1
## q[1010]  2681    1
## q[1011]  3216    1
## q[1012]  2681    1
## q[1013]  2681    1
## q[1014]  2854    1
## q[1015]  2854    1
## q[1016]  3216    1
## q[1017]  3216    1
## q[1018]  2681    1
## q[1019]  2681    1
## q[1020]  2681    1
## q[1021]  3216    1
## q[1022]  3216    1
## q[1023]  3216    1
## q[1024]  2681    1
## q[1025]  2681    1
## q[1026]  2681    1
## q[1027]  2854    1
## q[1028]  2854    1
## q[1029]  3152    1
## q[1030]  2890    1
## q[1031]  2890    1
## q[1032]  2890    1
## q[1033]  3152    1
## q[1034]  3152    1
## q[1035]  3152    1
## q[1036]  2945    1
## q[1037]  2945    1
## q[1038]  2890    1
## q[1039]  2890    1
## q[1040]  2945    1
## q[1041]  2890    1
## q[1042]  2890    1
## q[1043]  2890    1
## q[1044]  2890    1
## q[1045]  3152    1
## q[1046]  2890    1
## q[1047]  2890    1
## q[1048]  3152    1
## q[1049]  2890    1
## q[1050]  2890    1
## q[1051]  2890    1
## q[1052]  3152    1
## q[1053]  2890    1
## q[1054]  2890    1
## q[1055]  3152    1
## q[1056]  3152    1
## q[1057]  2890    1
## q[1058]  2945    1
## q[1059]  3152    1
## q[1060]  3152    1
## q[1061]  3152    1
## q[1062]  2890    1
## q[1063]  3152    1
## q[1064]  3152    1
## q[1065]  2945    1
## q[1066]  3152    1
## q[1067]  2890    1
## q[1068]  2945    1
## q[1069]  2890    1
## q[1070]  2890    1
## q[1071]  2945    1
## q[1072]  2890    1
## q[1073]  2890    1
## q[1074]  3152    1
## q[1075]  2890    1
## q[1076]  3152    1
## q[1077]  2890    1
## q[1078]  2945    1
## q[1079]  3152    1
## q[1080]  2890    1
## q[1081]  2890    1
## q[1082]  2890    1
## q[1083]  2890    1
## q[1084]  2890    1
## q[1085]  3152    1
## q[1086]  2890    1
## q[1087]  3152    1
## q[1088]  2890    1
## q[1089]  2945    1
## q[1090]  2890    1
## q[1091]  2945    1
## q[1092]  3152    1
## q[1093]  2945    1
## q[1094]  2890    1
## q[1095]  2890    1
## q[1096]  2890    1
## q[1097]  2890    1
## q[1098]  2890    1
## q[1099]  3343    1
## q[1100]  3343    1
## q[1101]  3343    1
## q[1102]  3343    1
## q[1103]  3343    1
## q[1104]  3737    1
## q[1105]  3162    1
## q[1106]  3737    1
## q[1107]  3737    1
## q[1108]  3737    1
## q[1109]  3737    1
## q[1110]  3162    1
## q[1111]  3343    1
## q[1112]  3343    1
## q[1113]  3343    1
## q[1114]  3162    1
## q[1115]  3343    1
## q[1116]  3343    1
## q[1117]  3737    1
## q[1118]  3343    1
## q[1119]  3343    1
## q[1120]  3343    1
## q[1121]  3737    1
## q[1122]  3343    1
## q[1123]  3737    1
## q[1124]  3343    1
## q[1125]  3343    1
## q[1126]  3737    1
## q[1127]  3343    1
## q[1128]  3343    1
## q[1129]  3343    1
## q[1130]  3737    1
## q[1131]  3737    1
## q[1132]  3343    1
## q[1133]  3162    1
## q[1134]  3737    1
## q[1135]  3162    1
## q[1136]  3737    1
## q[1137]  3737    1
## q[1138]  3343    1
## q[1139]  3343    1
## q[1140]  3737    1
## q[1141]  3162    1
## q[1142]  3343    1
## q[1143]  3162    1
## q[1144]  3343    1
## q[1145]  3737    1
## q[1146]  3343    1
## q[1147]  3737    1
## q[1148]  3162    1
## q[1149]  3737    1
## q[1150]  3343    1
## q[1151]  3162    1
## q[1152]  3343    1
## q[1153]  3343    1
## q[1154]  3343    1
## q[1155]  3343    1
## q[1156]  3343    1
## q[1157]  3343    1
## q[1158]  3737    1
## q[1159]  3343    1
## q[1160]  3737    1
## q[1161]  3162    1
## q[1162]  3162    1
## q[1163]  3343    1
## q[1164]  3162    1
## q[1165]  3737    1
## q[1166]  3343    1
## q[1167]  3343    1
## q[1168]  3737    1
## q[1169]  3737    1
## q[1170]  3162    1
## q[1171]  3343    1
## q[1172]  3343    1
## q[1173]  3343    1
## q[1174]  3343    1
## q[1175]  3162    1
## q[1176]  3703    1
## q[1177]  3309    1
## q[1178]  3309    1
## q[1179]  3148    1
## q[1180]  3703    1
## q[1181]  3148    1
## q[1182]  3703    1
## q[1183]  3148    1
## q[1184]  3703    1
## q[1185]  3309    1
## q[1186]  3309    1
## q[1187]  3703    1
## q[1188]  3309    1
## q[1189]  3309    1
## q[1190]  3309    1
## q[1191]  3703    1
## q[1192]  3309    1
## q[1193]  3309    1
## q[1194]  3703    1
## q[1195]  3309    1
## q[1196]  3703    1
## q[1197]  3309    1
## q[1198]  3703    1
## q[1199]  3703    1
## q[1200]  3309    1
## q[1201]  3309    1
## q[1202]  3703    1
## q[1203]  3309    1
## q[1204]  3148    1
## q[1205]  3703    1
## q[1206]  3703    1
## q[1207]  3309    1
## q[1208]  3703    1
## q[1209]  3309    1
## q[1210]  3309    1
## q[1211]  3309    1
## q[1212]  3148    1
## q[1213]  3148    1
## q[1214]  3309    1
## q[1215]  3148    1
## q[1216]  3703    1
## q[1217]  3148    1
## q[1218]  3309    1
## q[1219]  3703    1
## q[1220]  3309    1
## q[1221]  3703    1
## q[1222]  3703    1
## q[1223]  3309    1
## q[1224]  3148    1
## q[1225]  3309    1
## q[1226]  3309    1
## q[1227]  3703    1
## q[1228]  3309    1
## q[1229]  3309    1
## q[1230]  3148    1
## q[1231]  3703    1
## q[1232]  3309    1
## q[1233]  3703    1
## q[1234]  3148    1
## q[1235]  3703    1
## q[1236]  3309    1
## q[1237]  3309    1
## q[1238]  3148    1
## q[1239]  2939    1
## q[1240]  2939    1
## q[1241]  2939    1
## q[1242]  3025    1
## q[1243]  2939    1
## q[1244]  2939    1
## q[1245]  2939    1
## q[1246]  3534    1
## q[1247]  3534    1
## q[1248]  3534    1
## q[1249]  3534    1
## q[1250]  3534    1
## q[1251]  2939    1
## q[1252]  2939    1
## q[1253]  2939    1
## q[1254]  2939    1
## q[1255]  2939    1
## q[1256]  2939    1
## q[1257]  3534    1
## q[1258]  3025    1
## q[1259]  2939    1
## q[1260]  2939    1
## q[1261]  2939    1
## q[1262]  3534    1
## q[1263]  3534    1
## q[1264]  2939    1
## q[1265]  3534    1
## q[1266]  2939    1
## q[1267]  3534    1
## q[1268]  3025    1
## q[1269]  3534    1
## q[1270]  3534    1
## q[1271]  2939    1
## q[1272]  3534    1
## q[1273]  3025    1
## q[1274]  2939    1
## q[1275]  3534    1
## q[1276]  2939    1
## q[1277]  3534    1
## q[1278]  3025    1
## q[1279]  2939    1
## q[1280]  3025    1
## q[1281]  3534    1
## q[1282]  3534    1
## q[1283]  3534    1
## q[1284]  2939    1
## q[1285]  2939    1
## q[1286]  2939    1
## q[1287]  2939    1
## q[1288]  2939    1
## q[1289]  2939    1
## q[1290]  3534    1
## q[1291]  2939    1
## q[1292]  2939    1
## q[1293]  3025    1
## q[1294]  2939    1
## q[1295]  3534    1
## q[1296]  2939    1
## q[1297]  2939    1
## q[1298]  3025    1
## q[1299]  3534    1
## q[1300]  3534    1
## q[1301]  2939    1
## q[1302]  2939    1
## q[1303]  2939    1
## q[1304]  2939    1
## q[1305]  3025    1
## q[1306]  2939    1
## q[1307]  3417    1
## q[1308]  3417    1
## q[1309]  3417    1
## q[1310]  3839    1
## q[1311]  3839    1
## q[1312]  3839    1
## q[1313]  3839    1
## q[1314]  3417    1
## q[1315]  3417    1
## q[1316]  3417    1
## q[1317]  3417    1
## q[1318]  3417    1
## q[1319]  3198    1
## q[1320]  3839    1
## q[1321]  3198    1
## q[1322]  3417    1
## q[1323]  3417    1
## q[1324]  3839    1
## q[1325]  3417    1
## q[1326]  3839    1
## q[1327]  3417    1
## q[1328]  3839    1
## q[1329]  3417    1
## q[1330]  3198    1
## q[1331]  3198    1
## q[1332]  3839    1
## q[1333]  3417    1
## q[1334]  3839    1
## q[1335]  3417    1
## q[1336]  3839    1
## q[1337]  3839    1
## q[1338]  3198    1
## q[1339]  3417    1
## q[1340]  3417    1
## q[1341]  3417    1
## q[1342]  3417    1
## q[1343]  3417    1
## q[1344]  3417    1
## q[1345]  3417    1
## q[1346]  3839    1
## q[1347]  3839    1
## q[1348]  3839    1
## q[1349]  3417    1
## q[1350]  3417    1
## q[1351]  3839    1
## q[1352]  3198    1
## q[1353]  3839    1
## q[1354]  3417    1
## q[1355]  3417    1
## q[1356]  3198    1
## q[1357]  3294    1
## q[1358]  3294    1
## q[1359]  3294    1
## q[1360]  3294    1
## q[1361]  3294    1
## q[1362]  3805    1
## q[1363]  3170    1
## q[1364]  3805    1
## q[1365]  3805    1
## q[1366]  3170    1
## q[1367]  3294    1
## q[1368]  3294    1
## q[1369]  3294    1
## q[1370]  3294    1
## q[1371]  3294    1
## q[1372]  3294    1
## q[1373]  3805    1
## q[1374]  3294    1
## q[1375]  3294    1
## q[1376]  3294    1
## q[1377]  3294    1
## q[1378]  3805    1
## q[1379]  3170    1
## q[1380]  3805    1
## q[1381]  3805    1
## q[1382]  3805    1
## q[1383]  3294    1
## q[1384]  3294    1
## q[1385]  3805    1
## q[1386]  3170    1
## q[1387]  3170    1
## q[1388]  3294    1
## q[1389]  3294    1
## q[1390]  3805    1
## q[1391]  3170    1
## q[1392]  3294    1
## q[1393]  3294    1
## q[1394]  3294    1
## q[1395]  3294    1
## q[1396]  3170    1
## q[1397]  3170    1
## q[1398]  3294    1
## q[1399]  3170    1
## q[1400]  3805    1
## q[1401]  3294    1
## q[1402]  3294    1
## q[1403]  3805    1
## q[1404]  3294    1
## q[1405]  3294    1
## q[1406]  3294    1
## q[1407]  3170    1
## q[1408]  3317    1
## q[1409]  3055    1
## q[1410]  3055    1
## q[1411]  3753    1
## q[1412]  3753    1
## q[1413]  3317    1
## q[1414]  3055    1
## q[1415]  3055    1
## q[1416]  3055    1
## q[1417]  3055    1
## q[1418]  3055    1
## q[1419]  3753    1
## q[1420]  3753    1
## q[1421]  3753    1
## q[1422]  3055    1
## q[1423]  3753    1
## q[1424]  3753    1
## q[1425]  3055    1
## q[1426]  3055    1
## q[1427]  3055    1
## q[1428]  3055    1
## q[1429]  3055    1
## q[1430]  3317    1
## q[1431]  3055    1
## q[1432]  3317    1
## q[1433]  3055    1
## q[1434]  3753    1
## q[1435]  3055    1
## q[1436]  3055    1
## q[1437]  3055    1
## q[1438]  3753    1
## q[1439]  3317    1
## q[1440]  3055    1
## q[1441]  3753    1
## q[1442]  3055    1
## q[1443]  3753    1
## q[1444]  3055    1
## q[1445]  3753    1
## q[1446]  3055    1
## q[1447]  3055    1
## q[1448]  3753    1
## q[1449]  3317    1
## q[1450]  3753    1
## q[1451]  3055    1
## q[1452]  3753    1
## q[1453]  3753    1
## q[1454]  3753    1
## q[1455]  3317    1
## q[1456]  3055    1
## q[1457]  3317    1
## q[1458]  3055    1
## q[1459]  3055    1
## q[1460]  3753    1
## q[1461]  3753    1
## q[1462]  3055    1
## q[1463]  3055    1
## q[1464]  3055    1
## q[1465]  3055    1
## q[1466]  3055    1
## q[1467]  3753    1
## q[1468]  3055    1
## q[1469]  3317    1
## q[1470]  3317    1
## q[1471]  3055    1
## q[1472]  3055    1
## q[1473]  3055    1
## q[1474]  2739    1
## q[1475]  3439    1
## q[1476]  3066    1
## q[1477]  2739    1
## q[1478]  2739    1
## q[1479]  3439    1
## q[1480]  2739    1
## q[1481]  2739    1
## q[1482]  2739    1
## q[1483]  3066    1
## q[1484]  3066    1
## q[1485]  3439    1
## q[1486]  3066    1
## q[1487]  3066    1
## q[1488]  3439    1
## q[1489]  2739    1
## q[1490]  3439    1
## q[1491]  3066    1
## q[1492]  3515    1
## q[1493]  2974    1
## q[1494]  2974    1
## q[1495]  3515    1
## q[1496]  3515    1
## q[1497]  3277    1
## q[1498]  2974    1
## q[1499]  2974    1
## q[1500]  2974    1
## q[1501]  2974    1
## q[1502]  3515    1
## q[1503]  3277    1
## q[1504]  2974    1
## q[1505]  2974    1
## q[1506]  3515    1
## q[1507]  2974    1
## q[1508]  2974    1
## q[1509]  3515    1
## q[1510]  2974    1
## q[1511]  3515    1
## q[1512]  3515    1
## q[1513]  2974    1
## q[1514]  3277    1
## q[1515]  3515    1
## q[1516]  3515    1
## q[1517]  2974    1
## q[1518]  3515    1
## q[1519]  2974    1
## q[1520]  3277    1
## q[1521]  2974    1
## q[1522]  3277    1
## q[1523]  2974    1
## q[1524]  3515    1
## q[1525]  2974    1
## q[1526]  3515    1
## q[1527]  2974    1
## q[1528]  2974    1
## q[1529]  2974    1
## q[1530]  2974    1
## q[1531]  3515    1
## q[1532]  2974    1
## q[1533]  3515    1
## q[1534]  2974    1
## q[1535]  3277    1
## q[1536]  3515    1
## q[1537]  2974    1
## q[1538]  2974    1
## q[1539]  2974    1
## q[1540]  3277    1
## q[1541]  2555    1
## q[1542]  2555    1
## q[1543]  2555    1
## q[1544]  2724    1
## q[1545]  2955    1
## q[1546]  2955    1
## q[1547]  2724    1
## q[1548]  2955    1
## q[1549]  2955    1
## q[1550]  2724    1
## q[1551]  2555    1
## q[1552]  2555    1
## q[1553]  2555    1
## q[1554]  2955    1
## q[1555]  2724    1
## q[1556]  2555    1
## q[1557]  2555    1
## q[1558]  2555    1
## q[1559]  2555    1
## q[1560]  2955    1
## q[1561]  2555    1
## q[1562]  2955    1
## q[1563]  2555    1
## q[1564]  2955    1
## q[1565]  2555    1
## q[1566]  2555    1
## q[1567]  2955    1
## q[1568]  2555    1
## q[1569]  2955    1
## q[1570]  2555    1
## q[1571]  2724    1
## q[1572]  2955    1
## q[1573]  2555    1
## q[1574]  2955    1
## q[1575]  2955    1
## q[1576]  2555    1
## q[1577]  2955    1
## q[1578]  2555    1
## q[1579]  2724    1
## q[1580]  2724    1
## q[1581]  2555    1
## q[1582]  2724    1
## q[1583]  2955    1
## q[1584]  2724    1
## q[1585]  2555    1
## q[1586]  2555    1
## q[1587]  2955    1
## q[1588]  2955    1
## q[1589]  2955    1
## q[1590]  2555    1
## q[1591]  2724    1
## q[1592]  2555    1
## q[1593]  2555    1
## q[1594]  2555    1
## q[1595]  2555    1
## q[1596]  2555    1
## q[1597]  2955    1
## q[1598]  2555    1
## q[1599]  2555    1
## q[1600]  2724    1
## q[1601]  2724    1
## q[1602]  2955    1
## q[1603]  2955    1
## q[1604]  2555    1
## q[1605]  2555    1
## q[1606]  2955    1
## q[1607]  2724    1
## q[1608]  2955    1
## q[1609]  2555    1
## q[1610]  2955    1
## q[1611]  2555    1
## q[1612]  2555    1
## q[1613]  2555    1
## q[1614]  2724    1
## q[1615]  2724    1
## q[1616]  2699    1
## q[1617]  2699    1
## q[1618]  2804    1
## q[1619]  2699    1
## q[1620]  2699    1
## q[1621]  2818    1
## q[1622]  2804    1
## q[1623]  2699    1
## q[1624]  2804    1
## q[1625]  2699    1
## q[1626]  2699    1
## q[1627]  2699    1
## q[1628]  3640    1
## q[1629]  3640    1
## q[1630]  3082    1
## q[1631]  3060    1
## q[1632]  3060    1
## q[1633]  3082    1
## q[1634]  3060    1
## q[1635]  3060    1
## q[1636]  3060    1
## q[1637]  3060    1
## q[1638]  3640    1
## q[1639]  3060    1
## q[1640]  3640    1
## q[1641]  3082    1
## q[1642]  3060    1
## q[1643]  3060    1
## q[1644]  3060    1
## q[1645]  3640    1
## q[1646]  3060    1
## q[1647]  3082    1
## q[1648]  3060    1
## q[1649]  3640    1
## q[1650]  3640    1
## q[1651]  3839    1
## q[1652]  3839    1
## q[1653]  3190    1
## q[1654]  3364    1
## q[1655]  3364    1
## q[1656]  3190    1
## q[1657]  3839    1
## q[1658]  3364    1
## q[1659]  3364    1
## q[1660]  3839    1
## q[1661]  3364    1
## q[1662]  3364    1
## q[1663]  3839    1
## q[1664]  3839    1
## q[1665]  3364    1
## q[1666]  3839    1
## q[1667]  3190    1
## q[1668]  3839    1
## q[1669]  3364    1
## q[1670]  3364    1
## q[1671]  3839    1
## q[1672]  3364    1
## q[1673]  3839    1
## q[1674]  3190    1
## q[1675]  3364    1
## q[1676]  3364    1
## q[1677]  3364    1
## q[1678]  3364    1
## q[1679]  3364    1
## q[1680]  3839    1
## q[1681]  3364    1
## q[1682]  3839    1
## q[1683]  3190    1
## q[1684]  3839    1
## q[1685]  3190    1
## q[1686]  3364    1
## q[1687]  3364    1
## q[1688]  3970    1
## q[1689]  3051    1
## q[1690]  3051    1
## q[1691]  3051    1
## q[1692]  3970    1
## q[1693]  3970    1
## q[1694]  3970    1
## q[1695]  3970    1
## q[1696]  3325    1
## q[1697]  3051    1
## q[1698]  3051    1
## q[1699]  3051    1
## q[1700]  3051    1
## q[1701]  3325    1
## q[1702]  3051    1
## q[1703]  3051    1
## q[1704]  3051    1
## q[1705]  3970    1
## q[1706]  3325    1
## q[1707]  3051    1
## q[1708]  3051    1
## q[1709]  3051    1
## q[1710]  3051    1
## q[1711]  3970    1
## q[1712]  3051    1
## q[1713]  3970    1
## q[1714]  3970    1
## q[1715]  3051    1
## q[1716]  3970    1
## q[1717]  3970    1
## q[1718]  3970    1
## q[1719]  3051    1
## q[1720]  3325    1
## q[1721]  3051    1
## q[1722]  3051    1
## q[1723]  3051    1
## q[1724]  3970    1
## q[1725]  3970    1
## q[1726]  3051    1
## q[1727]  3325    1
## q[1728]  3051    1
## q[1729]  3051    1
## q[1730]  3051    1
## q[1731]  3051    1
## q[1732]  3051    1
## q[1733]  3970    1
## q[1734]  3970    1
## q[1735]  3325    1
## q[1736]  3051    1
## q[1737]  3970    1
## q[1738]  3051    1
## q[1739]  3970    1
## q[1740]  3325    1
## q[1741]  3970    1
## q[1742]  3051    1
## q[1743]  3051    1
## q[1744]  3051    1
## q[1745]  3325    1
## q[1746]  3051    1
## q[1747]  3020    1
## q[1748]  3020    1
## q[1749]  3305    1
## q[1750]  3020    1
## q[1751]  4041    1
## q[1752]  4041    1
## q[1753]  4041    1
## q[1754]  4041    1
## q[1755]  3305    1
## q[1756]  3020    1
## q[1757]  3020    1
## q[1758]  3020    1
## q[1759]  3020    1
## q[1760]  3020    1
## q[1761]  3305    1
## q[1762]  3305    1
## q[1763]  3020    1
## q[1764]  3020    1
## q[1765]  3020    1
## q[1766]  3020    1
## q[1767]  3020    1
## q[1768]  3020    1
## q[1769]  4041    1
## q[1770]  3305    1
## q[1771]  4041    1
## q[1772]  3305    1
## q[1773]  3020    1
## q[1774]  4041    1
## q[1775]  4041    1
## q[1776]  3020    1
## q[1777]  4041    1
## q[1778]  3020    1
## q[1779]  4041    1
## q[1780]  3020    1
## q[1781]  4041    1
## q[1782]  3020    1
## q[1783]  3305    1
## q[1784]  3020    1
## q[1785]  3020    1
## q[1786]  3020    1
## q[1787]  3020    1
## q[1788]  3020    1
## q[1789]  4041    1
## q[1790]  3020    1
## q[1791]  3020    1
## q[1792]  3305    1
## q[1793]  4041    1
## q[1794]  3020    1
## q[1795]  3020    1
## q[1796]  4041    1
## q[1797]  4041    1
## q[1798]  3020    1
## q[1799]  4041    1
## q[1800]  3020    1
## q[1801]  3305    1
## q[1802]  3685    1
## q[1803]  3290    1
## q[1804]  3290    1
## q[1805]  3290    1
## q[1806]  3290    1
## q[1807]  3290    1
## q[1808]  3290    1
## q[1809]  3685    1
## q[1810]  3140    1
## q[1811]  3685    1
## q[1812]  3685    1
## q[1813]  3685    1
## q[1814]  3140    1
## q[1815]  3140    1
## q[1816]  3290    1
## q[1817]  3290    1
## q[1818]  3290    1
## q[1819]  3290    1
## q[1820]  3290    1
## q[1821]  3685    1
## q[1822]  3290    1
## q[1823]  3290    1
## q[1824]  3685    1
## q[1825]  3290    1
## q[1826]  3290    1
## q[1827]  3290    1
## q[1828]  3685    1
## q[1829]  3290    1
## q[1830]  3685    1
## q[1831]  3290    1
## q[1832]  3290    1
## q[1833]  3685    1
## q[1834]  3685    1
## q[1835]  3290    1
## q[1836]  3290    1
## q[1837]  3290    1
## q[1838]  3290    1
## q[1839]  3140    1
## q[1840]  3685    1
## q[1841]  3685    1
## q[1842]  3685    1
## q[1843]  3290    1
## q[1844]  3685    1
## q[1845]  3140    1
## q[1846]  3685    1
## q[1847]  3685    1
## q[1848]  3290    1
## q[1849]  3685    1
## q[1850]  3290    1
## q[1851]  3290    1
## q[1852]  3685    1
## q[1853]  3140    1
## q[1854]  3140    1
## q[1855]  3290    1
## q[1856]  3140    1
## q[1857]  3140    1
## q[1858]  3290    1
## q[1859]  3290    1
## q[1860]  3290    1
## q[1861]  3685    1
## q[1862]  3290    1
## q[1863]  3685    1
## q[1864]  3685    1
## q[1865]  3290    1
## q[1866]  3140    1
## q[1867]  3290    1
## q[1868]  3290    1
## q[1869]  3290    1
## q[1870]  3290    1
## q[1871]  3290    1
## q[1872]  3290    1
## q[1873]  3685    1
## q[1874]  3290    1
## q[1875]  3140    1
## q[1876]  3140    1
## q[1877]  3290    1
## q[1878]  3140    1
## q[1879]  3685    1
## q[1880]  3290    1
## q[1881]  3290    1
## q[1882]  3685    1
## q[1883]  3140    1
## q[1884]  3290    1
## q[1885]  3290    1
## q[1886]  3290    1
## q[1887]  3290    1
## q[1888]  3290    1
## q[1889]  3140    1
## q[1890]  2912    1
## q[1891]  2912    1
## q[1892]  2912    1
## q[1893]  2912    1
## q[1894]  3439    1
## q[1895]  3439    1
## q[1896]  3439    1
## q[1897]  3247    1
## q[1898]  2912    1
## q[1899]  2912    1
## q[1900]  2912    1
## q[1901]  3439    1
## q[1902]  2912    1
## q[1903]  3439    1
## q[1904]  2912    1
## q[1905]  2912    1
## q[1906]  3439    1
## q[1907]  2912    1
## q[1908]  3439    1
## q[1909]  3439    1
## q[1910]  2912    1
## q[1911]  3247    1
## q[1912]  3439    1
## q[1913]  3439    1
## q[1914]  3439    1
## q[1915]  2912    1
## q[1916]  3439    1
## q[1917]  2912    1
## q[1918]  2912    1
## q[1919]  3439    1
## q[1920]  2912    1
## q[1921]  2912    1
## q[1922]  2912    1
## q[1923]  2912    1
## q[1924]  2912    1
## q[1925]  2912    1
## q[1926]  2912    1
## q[1927]  3439    1
## q[1928]  3247    1
## q[1929]  2912    1
## q[1930]  3247    1
## q[1931]  2912    1
## q[1932]  3247    1
## q[1933]  2912    1
## q[1934]  2912    1
## q[1935]  2912    1
## q[1936]  2912    1
## q[1937]  2891    1
## q[1938]  2906    1
## q[1939]  2906    1
## q[1940]  2519    1
## q[1941]  2519    1
## q[1942]  2519    1
## q[1943]  2519    1
## q[1944]  2519    1
## q[1945]  2891    1
## q[1946]  2519    1
## q[1947]  2891    1
## q[1948]  2891    1
## q[1949]  2891    1
## q[1950]  2519    1
## q[1951]  2519    1
## q[1952]  2519    1
## q[1953]  2519    1
## q[1954]  2519    1
## q[1955]  2519    1
## q[1956]  2519    1
## q[1957]  2906    1
## q[1958]  2519    1
## q[1959]  2870    1
## q[1960]  2506    1
## q[1961]  2889    1
## q[1962]  2506    1
## q[1963]  2506    1
## q[1964]  2870    1
## q[1965]  2889    1
## q[1966]  2870    1
## q[1967]  2506    1
## q[1968]  2506    1
## q[1969]  2506    1
## q[1970]  2506    1
## q[1971]  2506    1
## q[1972]  2506    1
## q[1973]  2870    1
## q[1974]  2506    1
## q[1975]  2506    1
## q[1976]  2870    1
## q[1977]  2506    1
## q[1978]  2870    1
## q[1979]  2889    1
## q[1980]  2870    1
## q[1981]  2870    1
## q[1982]  2870    1
## q[1983]  2506    1
## q[1984]  2506    1
## q[1985]  2889    1
## q[1986]  2506    1
## q[1987]  2889    1
## q[1988]  2870    1
## q[1989]  2506    1
## q[1990]  2506    1
## q[1991]  2870    1
## q[1992]  2506    1
## q[1993]  2506    1
## q[1994]  2889    1
## q[1995]  2506    1
## q[1996]  2870    1
## q[1997]  2506    1
## q[1998]  2506    1
## q[1999]  2506    1
## q[2000]  4027    1
## q[2001]  3041    1
## q[2002]  3316    1
## q[2003]  4027    1
## q[2004]  4027    1
## q[2005]  3316    1
## q[2006]  4027    1
## q[2007]  3041    1
## q[2008]  3041    1
## q[2009]  3316    1
## q[2010]  3041    1
## q[2011]  3041    1
## q[2012]  4027    1
## q[2013]  3041    1
## q[2014]  3041    1
## q[2015]  3041    1
## q[2016]  4027    1
## q[2017]  3041    1
## q[2018]  3041    1
## q[2019]  4027    1
## q[2020]  3041    1
## q[2021]  4027    1
## q[2022]  4027    1
## q[2023]  3041    1
## q[2024]  4027    1
## q[2025]  3316    1
## q[2026]  4027    1
## q[2027]  4027    1
## q[2028]  3041    1
## q[2029]  4027    1
## q[2030]  3316    1
## q[2031]  3041    1
## q[2032]  3041    1
## q[2033]  3316    1
## q[2034]  3041    1
## q[2035]  3041    1
## q[2036]  3316    1
## q[2037]  4027    1
## q[2038]  3041    1
## q[2039]  4027    1
## q[2040]  4027    1
## q[2041]  3041    1
## q[2042]  3316    1
## q[2043]  4027    1
## q[2044]  3041    1
## q[2045]  3041    1
## q[2046]  3041    1
## q[2047]  3041    1
## q[2048]  4027    1
## q[2049]  4027    1
## q[2050]  3041    1
## q[2051]  3041    1
## q[2052]  4027    1
## q[2053]  3316    1
## q[2054]  4027    1
## q[2055]  3041    1
## q[2056]  3041    1
## q[2057]  2998    1
## q[2058]  4018    1
## q[2059]  3292    1
## q[2060]  4018    1
## q[2061]  4018    1
## q[2062]  2998    1
## q[2063]  4018    1
## q[2064]  2998    1
## q[2065]  4018    1
## q[2066]  2998    1
## q[2067]  4018    1
## q[2068]  2998    1
## q[2069]  2998    1
## q[2070]  4018    1
## q[2071]  2998    1
## q[2072]  2998    1
## q[2073]  3292    1
## q[2074]  3292    1
## q[2075]  4018    1
## q[2076]  4018    1
## q[2077]  2998    1
## q[2078]  3292    1
## q[2079]  2998    1
## q[2080]  2998    1
## q[2081]  4018    1
## q[2082]  3292    1
## q[2083]  4018    1
## q[2084]  2998    1
## q[2085]  4018    1
## q[2086]  3292    1
## q[2087]  2998    1
## q[2088]  3292    1
## q[2089]  3858    1
## q[2090]  3058    1
## q[2091]  3058    1
## q[2092]  3058    1
## q[2093]  3858    1
## q[2094]  3326    1
## q[2095]  3858    1
## q[2096]  3858    1
## q[2097]  3326    1
## q[2098]  3058    1
## q[2099]  3058    1
## q[2100]  3058    1
## q[2101]  3858    1
## q[2102]  3058    1
## q[2103]  3058    1
## q[2104]  3058    1
## q[2105]  3858    1
## q[2106]  3058    1
## q[2107]  3858    1
## q[2108]  3058    1
## q[2109]  3858    1
## q[2110]  3858    1
## q[2111]  3058    1
## q[2112]  3058    1
## q[2113]  3058    1
## q[2114]  3858    1
## q[2115]  3858    1
## q[2116]  3858    1
## q[2117]  3058    1
## q[2118]  3058    1
## q[2119]  3058    1
## q[2120]  3326    1
## q[2121]  3326    1
## q[2122]  3058    1
## q[2123]  3858    1
## q[2124]  3058    1
## q[2125]  3858    1
## q[2126]  3058    1
## q[2127]  3326    1
## q[2128]  3058    1
## q[2129]  3058    1
## q[2130]  3858    1
## q[2131]  3326    1
## q[2132]  3326    1
## q[2133]  3058    1
## q[2134]  3858    1
## q[2135]  3058    1
## q[2136]  3858    1
## q[2137]  3326    1
## q[2138]  3058    1
## q[2139]  3058    1
## q[2140]  3058    1
## q[2141]  3326    1
## q[2142]  3722    1
## q[2143]  2864    1
## q[2144]  3191    1
## q[2145]  3191    1
## q[2146]  3722    1
## q[2147]  2864    1
## q[2148]  2864    1
## q[2149]  2864    1
## q[2150]  2864    1
## q[2151]  3722    1
## q[2152]  2864    1
## q[2153]  2864    1
## q[2154]  3722    1
## q[2155]  2864    1
## q[2156]  3722    1
## q[2157]  3191    1
## q[2158]  3722    1
## q[2159]  3722    1
## q[2160]  2864    1
## q[2161]  2864    1
## q[2162]  3191    1
## q[2163]  2864    1
## q[2164]  3191    1
## q[2165]  3722    1
## q[2166]  2864    1
## q[2167]  2864    1
## q[2168]  3722    1
## q[2169]  2864    1
## q[2170]  2864    1
## q[2171]  3722    1
## q[2172]  2864    1
## q[2173]  2546    1
## q[2174]  2546    1
## q[2175]  3027    1
## q[2176]  2829    1
## q[2177]  2546    1
## q[2178]  2546    1
## q[2179]  2546    1
## q[2180]  2546    1
## q[2181]  2829    1
## q[2182]  3027    1
## q[2183]  3027    1
## q[2184]  3027    1
## q[2185]  2546    1
## q[2186]  3027    1
## q[2187]  2546    1
## q[2188]  2546    1
## q[2189]  2546    1
## q[2190]  2546    1
## q[2191]  2546    1
## q[2192]  2546    1
## q[2193]  2829    1
## q[2194]  2546    1
## q[2195]  2546    1
## q[2196]  2806    1
## q[2197]  2806    1
## q[2198]  2806    1
## q[2199]  2806    1
## q[2200]  3303    1
## q[2201]  3185    1
## q[2202]  3303    1
## q[2203]  3185    1
## q[2204]  2806    1
## q[2205]  2806    1
## q[2206]  2806    1
## q[2207]  2806    1
## q[2208]  2806    1
## q[2209]  3303    1
## q[2210]  2806    1
## q[2211]  2806    1
## q[2212]  3303    1
## q[2213]  2806    1
## q[2214]  2806    1
## q[2215]  2806    1
## q[2216]  3185    1
## q[2217]  3303    1
## q[2218]  3303    1
## q[2219]  3303    1
## q[2220]  3303    1
## q[2221]  2806    1
## q[2222]  3185    1
## q[2223]  3185    1
## q[2224]  2806    1
## q[2225]  3185    1
## q[2226]  3185    1
## q[2227]  2806    1
## q[2228]  2806    1
## q[2229]  3303    1
## q[2230]  3303    1
## q[2231]  3185    1
## q[2232]  2806    1
## q[2233]  2806    1
## q[2234]  2806    1
## q[2235]  3185    1
## q[2236]  3185    1
## q[2237]  2806    1
## q[2238]  3185    1
## q[2239]  3303    1
## q[2240]  2806    1
## q[2241]  3303    1
## q[2242]  2806    1
## q[2243]  2806    1
## q[2244]  2806    1
## q[2245]  2806    1
## q[2246]  3185    1
## q[2247]  2486    1
## q[2248]  2365    1
## q[2249]  2365    1
## q[2250]  2365    1
## q[2251]  2257    1
## q[2252]  2365    1
## q[2253]  2257    1
## q[2254]  2365    1
## q[2255]  2365    1
## q[2256]  2365    1
## q[2257]  2257    1
## q[2258]  2257    1
## q[2259]  2365    1
## q[2260]  2365    1
## q[2261]  2257    1
## q[2262]  2257    1
## q[2263]  2257    1
## q[2264]  2365    1
## q[2265]  2257    1
## q[2266]  2486    1
## q[2267]  2365    1
## q[2268]  2257    1
## q[2269]  2847    1
## q[2270]  3676    1
## q[2271]  3676    1
## q[2272]  2847    1
## q[2273]  2847    1
## q[2274]  3176    1
## q[2275]  2847    1
## q[2276]  3676    1
## q[2277]  2847    1
## q[2278]  2847    1
## q[2279]  3676    1
## q[2280]  3676    1
## q[2281]  2847    1
## q[2282]  3676    1
## q[2283]  3176    1
## q[2284]  3676    1
## q[2285]  2847    1
## q[2286]  3676    1
## q[2287]  3176    1
## q[2288]  3176    1
## q[2289]  2847    1
## q[2290]  2847    1
## q[2291]  3176    1
## q[2292]  3676    1
## q[2293]  3676    1
## q[2294]  3176    1
## q[2295]  3676    1
## q[2296]  2847    1
## q[2297]  2847    1
## q[2298]  2847    1
## q[2299]  2847    1
## q[2300]  3676    1
## q[2301]  3676    1
## q[2302]  2847    1
## q[2303]  3676    1
## q[2304]  3176    1
## q[2305]  2847    1
## q[2306]  2847    1
## q[2307]  3137    1
## q[2308]  3082    1
## q[2309]  2685    1
## q[2310]  2685    1
## q[2311]  3082    1
## q[2312]  2685    1
## q[2313]  2685    1
## q[2314]  2685    1
## q[2315]  3137    1
## q[2316]  3082    1
## q[2317]  3137    1
## q[2318]  2685    1
## q[2319]  2685    1
## q[2320]  3137    1
## q[2321]  2685    1
## q[2322]  3082    1
## q[2323]  2685    1
## q[2324]  2685    1
## q[2325]  3137    1
## q[2326]  2685    1
## q[2327]  3082    1
## q[2328]  3137    1
## q[2329]  2685    1
## q[2330]  3569    1
## q[2331]  3165    1
## q[2332]  3165    1
## q[2333]  3165    1
## q[2334]  3165    1
## q[2335]  3569    1
## q[2336]  3569    1
## q[2337]  3086    1
## q[2338]  3569    1
## q[2339]  3569    1
## q[2340]  3086    1
## q[2341]  3165    1
## q[2342]  3165    1
## q[2343]  3165    1
## q[2344]  3165    1
## q[2345]  3569    1
## q[2346]  3086    1
## q[2347]  3165    1
## q[2348]  3165    1
## q[2349]  3165    1
## q[2350]  3165    1
## q[2351]  3569    1
## q[2352]  3165    1
## q[2353]  3569    1
## q[2354]  3165    1
## q[2355]  3569    1
## q[2356]  3569    1
## q[2357]  3165    1
## q[2358]  3165    1
## q[2359]  3165    1
## q[2360]  3165    1
## q[2361]  3569    1
## q[2362]  3569    1
## q[2363]  3165    1
## q[2364]  3086    1
## q[2365]  3165    1
## q[2366]  3569    1
## q[2367]  3165    1
## q[2368]  3165    1
## q[2369]  3569    1
## q[2370]  3086    1
## q[2371]  3086    1
## q[2372]  3165    1
## q[2373]  3569    1
## q[2374]  3165    1
## q[2375]  3569    1
## q[2376]  3165    1
## q[2377]  3086    1
## q[2378]  3165    1
## q[2379]  3165    1
## q[2380]  3165    1
## q[2381]  3165    1
## q[2382]  3165    1
## q[2383]  3569    1
## q[2384]  3086    1
## q[2385]  3569    1
## q[2386]  3569    1
## q[2387]  3165    1
## q[2388]  3165    1
## q[2389]  3165    1
## q[2390]  3569    1
## q[2391]  3086    1
## q[2392]  3569    1
## q[2393]  3165    1
## q[2394]  3165    1
## q[2395]  3086    1
## q[2396]  3086    1
## lp__     1673    1
## 
## Samples were drawn using NUTS(diag_e) at Tue Nov 26 15:32:51 2019.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;檢查模型參數的收斂情況&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;檢查模型參數的收斂情況&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(bayesplot)

color_scheme_set(&amp;quot;mix-brightblue-gray&amp;quot;)

posterior5.5 &amp;lt;- rstan::extract(fit1, inc_warmup = TRUE, permuted = FALSE)

p &amp;lt;- mcmc_trace(posterior5.5, n_warmup = 0, pars = c(&amp;quot;b[1]&amp;quot;, &amp;quot;b[2]&amp;quot;, &amp;quot;b[3]&amp;quot;,   &amp;quot;lp__&amp;quot;), facet_args = list(nrow = 2, labeller = label_parsed))

p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:chapter5-5&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-27-logistic2-rstan_files/figure-html/chapter5-5-1.png&#34; alt=&#34;用 bayesplot包數繪製的模型5-5的MCMC鏈式軌跡圖 (trace plot)。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: 用 bayesplot包數繪製的模型5-5的MCMC鏈式軌跡圖 (trace plot)。
&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- mcmc_acf_bar(posterior5.5, pars = c(&amp;quot;b[1]&amp;quot;, &amp;quot;b[2]&amp;quot;, &amp;quot;b[3]&amp;quot;,   &amp;quot;lp__&amp;quot;))
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:chapter5-5-acf&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-27-logistic2-rstan_files/figure-html/chapter5-5-acf-1.png&#34; alt=&#34;用 bayesplot包數繪製的事後樣本自相關圖(autocorrelation)。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: 用 bayesplot包數繪製的事後樣本自相關圖(autocorrelation)。
&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- mcmc_dens_overlay(posterior5.5, pars = c(&amp;quot;b[1]&amp;quot;, &amp;quot;b[2]&amp;quot;, &amp;quot;b[3]&amp;quot;, &amp;quot;OR1&amp;quot;,  &amp;quot;OR2&amp;quot;, &amp;quot;lp__&amp;quot;), color_chains = T)
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step5-5-density&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-27-logistic2-rstan_files/figure-html/step5-5-density-1.png&#34; alt=&#34;用 bayesplot包數繪製的事後樣本密度分佈圖。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: 用 bayesplot包數繪製的事後樣本密度分佈圖。
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;檢查模型的擬合情況&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;檢查模型的擬合情況&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ms &amp;lt;- rstan::extract(fit1)
set.seed(123)
logistic &amp;lt;- function(x) 1/(1+exp(-x))
X &amp;lt;- 30:200
q_qua &amp;lt;- logistic(t(sapply(1:length(X), function(i) {
  q_mcmc &amp;lt;- ms$b[,1] + ms$b[,3]*X[i]/200
  quantile(q_mcmc, probs=c(0.1, 0.5, 0.9))
})))
d_est &amp;lt;- data.frame(X, q_qua)
colnames(d_est) &amp;lt;- c(&amp;#39;X&amp;#39;, &amp;#39;p10&amp;#39;, &amp;#39;p50&amp;#39;, &amp;#39;p90&amp;#39;)
d$A &amp;lt;- as.factor(d$A)

p &amp;lt;- ggplot(d_est, aes(x=X, y=p50))
p &amp;lt;- p + theme_bw(base_size=18)
p &amp;lt;- p + geom_ribbon(aes(ymin=p10, ymax=p90), fill=&amp;#39;black&amp;#39;, alpha=2/6)
p &amp;lt;- p + geom_line(size=1)
p &amp;lt;- p + geom_point(data=subset(d, A==0 &amp;amp; Weather==&amp;#39;A&amp;#39;), aes(x=Score, y=Y, color=A),
  position=position_jitter(w=0, h=0.1), size=1)
p &amp;lt;- p + labs(x=&amp;#39;Score&amp;#39;, y=&amp;#39;q&amp;#39;)
p &amp;lt;- p + scale_color_manual(values=c(&amp;#39;black&amp;#39;))
p &amp;lt;- p + scale_y_continuous(breaks=seq(0, 1, 0.2))
p &amp;lt;- p + xlim(30, 200)
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:validity-of-model&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-27-logistic2-rstan_files/figure-html/validity-of-model-1.png&#34; alt=&#34;不喜歡打工，且天氣晴天的情況下，分數在 30-200 點之間的學生的出勤概率 q 和它的 80% 可信區間範圍。圖中黑色實線是預測概率的事後分佈的中央值，灰色帶是可信區間範圍。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4: 不喜歡打工，且天氣晴天的情況下，分數在 30-200 點之間的學生的出勤概率 q 和它的 80% 可信區間範圍。圖中黑色實線是預測概率的事後分佈的中央值，灰色帶是可信區間範圍。
&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# ggsave(file=&amp;#39;output/fig5-9.png&amp;#39;, plot=p, dpi=300, w=4.5, h=3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;圖&lt;a href=&#34;#fig:validity-of-model&#34;&gt;4&lt;/a&gt;試圖把分數範圍在 30-200 之間的學生中，通過模型計算獲得的，在天氣晴朗，且不愛打工的孩子們的事後出勤概率的預測值(黑色實線)，和它的事後概率80%可信區間，以及對應的實際觀測值的結果(黑點)。但是，當預測變量越來越多，模型結果的可視化變得越來越困難。下面我們介紹兩種常見的評價邏輯回歸擬合結果的可視化圖。&lt;/p&gt;
&lt;p&gt;首先是圖 &lt;a href=&#34;#fig:validity-of-model1&#34;&gt;5&lt;/a&gt; 顯示的事後出勤概率，和實際觀察出勤結果之間的關係圖。在這個圖中，橫軸是 &lt;span class=&#34;math inline&#34;&gt;\(q[i]\)&lt;/span&gt; 的事後分佈的中央值(每名學生都有自己的事後出勤概率預測，它的中央值)，縱軸是該名學生實際是否在該次課上出勤的觀察結果。如果模型擬合的理想的話，那麼在 &lt;span class=&#34;math inline&#34;&gt;\(Y=0\)&lt;/span&gt;，也就是圖中的下半部分，大多數的預測點應該靠近概率較低的部分(也就是靠近左側)，同時，&lt;span class=&#34;math inline&#34;&gt;\(y = 1\)&lt;/span&gt; 的部分預測概率應該大多數在靠近左側的部分。此圖其實提示我們該模型的擬合效果不理想。不能明顯地將出勤與不出勤較爲準確地區分開來。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)
ms &amp;lt;- rstan::extract(fit1)
d_qua &amp;lt;- t(apply(ms$q, 2, quantile, prob=c(0.1, 0.5, 0.9)))
colnames(d_qua) &amp;lt;- c(&amp;#39;p10&amp;#39;, &amp;#39;p50&amp;#39;, &amp;#39;p90&amp;#39;)
d_qua &amp;lt;- data.frame(d, d_qua)
d_qua$Y &amp;lt;- as.factor(d_qua$Y)
d_qua$A &amp;lt;- as.factor(d_qua$A)

p &amp;lt;- ggplot(data=d_qua, aes(x=Y, y=p50))
p &amp;lt;- p + theme_bw(base_size=18)
p &amp;lt;- p + coord_flip()
p &amp;lt;- p + geom_violin(trim=FALSE, size=1.5, color=&amp;#39;grey80&amp;#39;)
p &amp;lt;- p + geom_point(aes(color=A), position=position_jitter(w=0.4, h=0), size=1)
p &amp;lt;- p + scale_color_manual(values=c(&amp;#39;grey5&amp;#39;, &amp;#39;grey50&amp;#39;))
p &amp;lt;- p + labs(x=&amp;#39;Y&amp;#39;, y=&amp;#39;q&amp;#39;)
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:validity-of-model1&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-27-logistic2-rstan_files/figure-html/validity-of-model1-1.png&#34; alt=&#34;把計算獲得的事後概率的中央值作爲每名學生是否出勤的概率預測(x 軸)，和實際觀察的出勤結果(y 軸)，繪製的散點圖。其中，灰色的小提琴圖其實是根據獲得的事後概率的中央值的概率密度曲線，繪製的上下對稱的圖形，形似小提琴。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 5: 把計算獲得的事後概率的中央值作爲每名學生是否出勤的概率預測(x 軸)，和實際觀察的出勤結果(y 軸)，繪製的散點圖。其中，灰色的小提琴圖其實是根據獲得的事後概率的中央值的概率密度曲線，繪製的上下對稱的圖形，形似小提琴。
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Rstan Wonderful R-(4)</title>
      <link>https://wangcc.me/post/logistic-rstan/</link>
      <pubDate>Sun, 27 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/logistic-rstan/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#邏輯回歸模型的-rstan-貝葉斯實現&#34;&gt;邏輯回歸模型的 Rstan 貝葉斯實現&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#確定分析目的&#34;&gt;確定分析目的&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#確認數據分佈&#34;&gt;確認數據分佈&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#寫下數學模型表達式&#34;&gt;寫下數學模型表達式&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#確認收斂效果&#34;&gt;確認收斂效果&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;邏輯回歸模型的-rstan-貝葉斯實現&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;邏輯回歸模型的 Rstan 貝葉斯實現&lt;/h1&gt;
&lt;p&gt;本小節使用的&lt;a href=&#34;https://raw.githubusercontent.com/MatsuuraKentaro/RStanBook/master/chap05/input/data-attendance-2.txt&#34;&gt;數據&lt;/a&gt;，和前一節的出勤率數據很類似:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d &amp;lt;- read.table(&amp;quot;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap05/input/data-attendance-2.txt&amp;quot;, 
                     sep = &amp;quot;,&amp;quot;, header = T)
head(d)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   PersonID A Score  M  Y
## 1        1 0    69 43 38
## 2        2 1   145 56 40
## 3        3 0   125 32 24
## 4        4 1    86 45 33
## 5        5 1   158 33 23
## 6        6 0   133 61 60&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;PersonID&lt;/code&gt;: 是學生的編號；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;A&lt;/code&gt;, &lt;code&gt;Score&lt;/code&gt;: 和之前一樣用來預測出勤率的兩個預測變量，分別是表示是否喜歡打工的 &lt;code&gt;A&lt;/code&gt;，和表示對學習本身是否喜歡的評分 (滿分200)；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;M&lt;/code&gt;: 過去三個月內，該名學生一共需要上課的總課時數；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Y&lt;/code&gt;: 過去三個月內，該名學生實際上出勤的課時數。&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;確定分析目的&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;確定分析目的&lt;/h1&gt;
&lt;p&gt;需要回答的問題依然是，&lt;span class=&#34;math inline&#34;&gt;\(A, Score\)&lt;/span&gt; 分別在多大程度上預測學生的出勤率？另外，我們希望知道的是，當需要修的課時數固定的事後，這兩個預測變量能準確提供 &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; 的多少信息？&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;確認數據分佈&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;確認數據分佈&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(GGally)

set.seed(1)
d &amp;lt;- d[, -1]
# d &amp;lt;- read.csv(file=&amp;#39;input/data-attendance-2.txt&amp;#39;)[,-1]
d$A &amp;lt;- as.factor(d$A)
d &amp;lt;- transform(d, ratio=Y/M)
N_col &amp;lt;- ncol(d)
ggp &amp;lt;- ggpairs(d, upper=&amp;#39;blank&amp;#39;, diag=&amp;#39;blank&amp;#39;, lower=&amp;#39;blank&amp;#39;)

for(i in 1:N_col) {
  x &amp;lt;- d[,i]
  p &amp;lt;- ggplot(data.frame(x, A=d$A), aes(x))
  p &amp;lt;- p + theme_bw(base_size=14)
  p &amp;lt;- p + theme(axis.text.x=element_text(angle=40, vjust=1, hjust=1))
  if (class(x) == &amp;#39;factor&amp;#39;) {
    p &amp;lt;- p + geom_bar(aes(fill=A), color=&amp;#39;grey20&amp;#39;)
  } else {
    bw &amp;lt;- (max(x)-min(x))/10
    p &amp;lt;- p + geom_histogram(aes(fill=A), color=&amp;#39;grey20&amp;#39;, binwidth=bw)
    p &amp;lt;- p + geom_line(eval(bquote(aes(y=..count..*.(bw)))), stat=&amp;#39;density&amp;#39;)
  }
  p &amp;lt;- p + geom_label(data=data.frame(x=-Inf, y=Inf, label=colnames(d)[i]), aes(x=x, y=y, label=label), hjust=0, vjust=1)
  p &amp;lt;- p + scale_fill_manual(values=alpha(c(&amp;#39;white&amp;#39;, &amp;#39;grey40&amp;#39;), 0.5))
  ggp &amp;lt;- putPlot(ggp, p, i, i)
}

zcolat &amp;lt;- seq(-1, 1, length=81)
zcolre &amp;lt;- c(zcolat[1:40]+1, rev(zcolat[41:81]))

for(i in 1:(N_col-1)) {
  for(j in (i+1):N_col) {
    x &amp;lt;- as.numeric(d[,i])
    y &amp;lt;- as.numeric(d[,j])
    r &amp;lt;- cor(x, y, method=&amp;#39;spearman&amp;#39;, use=&amp;#39;pairwise.complete.obs&amp;#39;)
    zcol &amp;lt;- lattice::level.colors(r, at=zcolat, col.regions=grey(zcolre))
    textcol &amp;lt;- ifelse(abs(r) &amp;lt; 0.4, &amp;#39;grey20&amp;#39;, &amp;#39;white&amp;#39;)
    ell &amp;lt;- ellipse::ellipse(r, level=0.95, type=&amp;#39;l&amp;#39;, npoints=50, scale=c(.2, .2), centre=c(.5, .5))
    p &amp;lt;- ggplot(data.frame(ell), aes(x=x, y=y))
    p &amp;lt;- p + theme_bw() + theme(
      plot.background=element_blank(),
      panel.grid.major=element_blank(), panel.grid.minor=element_blank(),
      panel.border=element_blank(), axis.ticks=element_blank()
    )
    p &amp;lt;- p + geom_polygon(fill=zcol, color=zcol)
    p &amp;lt;- p + geom_text(data=NULL, x=.5, y=.5, label=100*round(r, 2), size=6, col=textcol)
    ggp &amp;lt;- putPlot(ggp, p, i, j)
  }
}

for(j in 1:(N_col-1)) {
  for(i in (j+1):N_col) {
    x &amp;lt;- d[,j]
    y &amp;lt;- d[,i]
    p &amp;lt;- ggplot(data.frame(x, y, gr=d$A), aes(x=x, y=y, fill=gr, shape=gr))
    p &amp;lt;- p + theme_bw(base_size=14)
    p &amp;lt;- p + theme(axis.text.x=element_text(angle=40, vjust=1, hjust=1))
    if (class(x) == &amp;#39;factor&amp;#39;) {
      p &amp;lt;- p + geom_boxplot(aes(group=x), alpha=3/6, outlier.size=0, fill=&amp;#39;white&amp;#39;)
      p &amp;lt;- p + geom_point(position=position_jitter(w=0.4, h=0), size=2)
    } else {
      p &amp;lt;- p + geom_point(size=2)
    }
    p &amp;lt;- p + scale_shape_manual(values=c(21, 24))
    p &amp;lt;- p + scale_fill_manual(values=alpha(c(&amp;#39;white&amp;#39;, &amp;#39;grey40&amp;#39;), 0.5))
    ggp &amp;lt;- putPlot(ggp, p, i, j)
  }
}

ggp&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step1&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-27-logistic-rstan_files/figure-html/step1-1.png&#34; alt=&#34;三個變量的分佈觀察圖，相比之前增加了 $ratio = Y/M$ 列。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: 三個變量的分佈觀察圖，相比之前增加了 &lt;span class=&#34;math inline&#34;&gt;\(ratio = Y/M\)&lt;/span&gt; 列。
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;從圖 &lt;a href=&#34;#fig:step1&#34;&gt;1&lt;/a&gt; 還可以看出，由於總課時數越多，學生實際出勤的課時數也會越多所以 &lt;span class=&#34;math inline&#34;&gt;\(M, Y\)&lt;/span&gt; 兩者之間理應有很強的正相關。另外可能可以推測的是 &lt;span class=&#34;math inline&#34;&gt;\(Ratio\)&lt;/span&gt; 和是否愛學習的分數之間大概有可能有正相關，和是否喜歡打工之間大概可能有負相關。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;寫下數學模型表達式&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;寫下數學模型表達式&lt;/h1&gt;
&lt;p&gt;在 Stan 的語法中，使用的是反邏輯函數 (inverse logit): &lt;code&gt;inv_logit&lt;/code&gt; 來描述下面的邏輯回歸模型 5-4。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{array}{l}
q[n] = \text{inv_logit}(b_1 + b_2 A[n] + b_3Score[n]) &amp;amp; n = 1, 2, \dots, N \\
Y[n] \sim \text{Binomial}(M[n], q[n])                 &amp;amp; n = 1, 2, \dots, N \\
\end{array}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;上面的數學模型，可以被翻譯成下面的 Stan 語言:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data {
  int N; 
  int&amp;lt;lower=0, upper=1&amp;gt; A[N]; 
  real&amp;lt;lower=0, upper=1&amp;gt; Score[N]; 
  int&amp;lt;lower=0&amp;gt; M[N];
  int&amp;lt;lower=0&amp;gt; Y[N];
}

parameters {
  real b1; 
  real b2; 
  real b3;
}

transformed parameters {
  real q[N];
  for (n in 1:N) {
    q[n] = inv_logit(b1 + b2*A[n] + b3*Score[n]);
  }
}

model {
  for (n in 1:N) {
    Y[n] ~ binomial(M[n], q[n]); 
  }
}

generated quantities {
  real y_pred[N]; 
  for (n in 1:N) {
    y_pred[n] = binomial_rng(M[n], q[n]);
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;下面的 R 代碼用來實現對上面 Stan 模型的擬合:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rstan)
d &amp;lt;- read.csv(file=&amp;#39;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap05/input/data-attendance-2.txt&amp;#39;, header = T)
data &amp;lt;- list(N=nrow(d), A=d$A, Score=d$Score/200, M=d$M, Y=d$Y)
fit &amp;lt;- stan(file=&amp;#39;stanfiles/model5-4.stan&amp;#39;, data=data, seed=1234)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &amp;#39;model5-4&amp;#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 1.6e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.131543 seconds (Warm-up)
## Chain 1:                0.139239 seconds (Sampling)
## Chain 1:                0.270782 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &amp;#39;model5-4&amp;#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 8e-06 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.134734 seconds (Warm-up)
## Chain 2:                0.137725 seconds (Sampling)
## Chain 2:                0.272459 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &amp;#39;model5-4&amp;#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 8e-06 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.131595 seconds (Warm-up)
## Chain 3:                0.135589 seconds (Sampling)
## Chain 3:                0.267184 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &amp;#39;model5-4&amp;#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 8e-06 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.13241 seconds (Warm-up)
## Chain 4:                0.146656 seconds (Sampling)
## Chain 4:                0.279066 seconds (Total)
## Chain 4:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Stan model: model5-4.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##                mean se_mean   sd     2.5%      25%      50%      75%    97.5%
## b1             0.09    0.01 0.22    -0.37    -0.05     0.09     0.24     0.53
## b2            -0.62    0.00 0.09    -0.80    -0.68    -0.62    -0.56    -0.44
## b3             1.90    0.01 0.36     1.19     1.67     1.90     2.13     2.64
## q[1]           0.68    0.00 0.02     0.63     0.66     0.68     0.69     0.72
## q[2]           0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[3]           0.78    0.00 0.01     0.76     0.77     0.78     0.79     0.80
## q[4]           0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[5]           0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[6]           0.79    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[7]           0.76    0.00 0.01     0.73     0.75     0.76     0.77     0.78
## q[8]           0.70    0.00 0.02     0.67     0.69     0.70     0.72     0.74
## q[9]           0.81    0.00 0.01     0.79     0.81     0.81     0.82     0.84
## q[10]          0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[11]          0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[12]          0.80    0.00 0.01     0.78     0.79     0.80     0.81     0.82
## q[13]          0.64    0.00 0.01     0.62     0.63     0.64     0.65     0.67
## q[14]          0.76    0.00 0.01     0.73     0.75     0.76     0.77     0.78
## q[15]          0.76    0.00 0.01     0.73     0.75     0.76     0.76     0.78
## q[16]          0.60    0.00 0.02     0.57     0.59     0.60     0.61     0.64
## q[17]          0.76    0.00 0.01     0.74     0.76     0.76     0.77     0.79
## q[18]          0.70    0.00 0.02     0.66     0.69     0.71     0.72     0.74
## q[19]          0.86    0.00 0.02     0.83     0.85     0.86     0.87     0.89
## q[20]          0.72    0.00 0.02     0.69     0.71     0.72     0.73     0.76
## q[21]          0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[22]          0.62    0.00 0.02     0.59     0.61     0.62     0.63     0.65
## q[23]          0.62    0.00 0.02     0.59     0.61     0.62     0.63     0.65
## q[24]          0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[25]          0.64    0.00 0.01     0.61     0.63     0.64     0.65     0.67
## q[26]          0.67    0.00 0.01     0.64     0.66     0.67     0.68     0.69
## q[27]          0.77    0.00 0.01     0.75     0.76     0.77     0.78     0.79
## q[28]          0.77    0.00 0.01     0.75     0.76     0.77     0.78     0.79
## q[29]          0.83    0.00 0.01     0.81     0.83     0.84     0.84     0.86
## q[30]          0.76    0.00 0.01     0.74     0.75     0.76     0.77     0.79
## q[31]          0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.78
## q[32]          0.54    0.00 0.03     0.49     0.53     0.54     0.56     0.60
## q[33]          0.69    0.00 0.01     0.66     0.68     0.69     0.70     0.72
## q[34]          0.66    0.00 0.01     0.63     0.65     0.66     0.67     0.69
## q[35]          0.78    0.00 0.01     0.76     0.78     0.78     0.79     0.81
## q[36]          0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.81
## q[37]          0.62    0.00 0.02     0.58     0.60     0.62     0.63     0.65
## q[38]          0.76    0.00 0.01     0.73     0.75     0.76     0.77     0.78
## q[39]          0.72    0.00 0.02     0.68     0.71     0.72     0.73     0.75
## q[40]          0.72    0.00 0.02     0.68     0.70     0.72     0.73     0.75
## q[41]          0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[42]          0.79    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[43]          0.78    0.00 0.01     0.75     0.77     0.78     0.79     0.80
## q[44]          0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[45]          0.86    0.00 0.02     0.83     0.85     0.86     0.87     0.89
## q[46]          0.75    0.00 0.01     0.72     0.74     0.75     0.76     0.77
## q[47]          0.64    0.00 0.03     0.57     0.62     0.64     0.66     0.70
## q[48]          0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[49]          0.74    0.00 0.01     0.71     0.73     0.74     0.75     0.76
## q[50]          0.60    0.00 0.02     0.57     0.59     0.60     0.61     0.64
## y_pred[1]     29.19    0.05 3.21    23.00    27.00    29.00    31.00    35.00
## y_pred[2]     39.25    0.06 3.56    32.00    37.00    39.00    42.00    46.00
## y_pred[3]     25.07    0.04 2.38    20.00    24.00    25.00    27.00    29.00
## y_pred[4]     25.76    0.06 3.43    19.00    23.00    26.00    28.00    32.00
## y_pred[5]     23.89    0.04 2.62    19.00    22.00    24.00    26.00    29.00
## y_pred[6]     48.46    0.05 3.24    42.00    46.00    49.00    51.00    54.00
## y_pred[7]     37.21    0.05 3.10    31.00    35.00    37.00    39.00    43.00
## y_pred[8]     53.44    0.07 4.12    45.00    51.00    54.00    56.00    61.00
## y_pred[9]     63.52    0.06 3.54    56.00    61.00    64.00    66.00    70.00
## y_pred[10]    51.98    0.05 3.21    45.00    50.00    52.00    54.00    58.00
## y_pred[11]    23.50    0.05 2.73    18.00    22.00    24.00    25.00    29.00
## y_pred[12]    35.26    0.04 2.68    30.00    33.00    35.00    37.00    40.00
## y_pred[13]    34.17    0.06 3.53    27.00    32.00    34.00    37.00    41.00
## y_pred[14]    30.39    0.04 2.73    25.00    29.00    30.00    32.00    35.00
## y_pred[15]    42.23    0.05 3.23    36.00    40.00    42.00    44.00    48.00
## y_pred[16]    35.48    0.06 3.88    28.00    33.00    36.00    38.00    43.00
## y_pred[17]    28.97    0.04 2.69    23.00    27.00    29.00    31.00    34.00
## y_pred[18]    31.67    0.05 3.12    25.00    30.00    32.00    34.00    38.00
## y_pred[19]    38.81    0.04 2.39    34.00    37.00    39.00    40.00    43.00
## y_pred[20]    55.48    0.07 4.26    47.00    53.00    56.00    58.00    63.00
## y_pred[21]    40.16    0.07 4.39    32.00    37.00    40.00    43.00    49.00
## y_pred[22]    47.97    0.08 4.48    39.00    45.00    48.00    51.00    56.03
## y_pred[23]    38.94    0.06 3.89    32.00    36.00    39.00    42.00    46.00
## y_pred[24]    47.35    0.06 3.87    40.00    45.00    47.00    50.00    55.00
## y_pred[25]    32.10    0.06 3.42    25.00    30.00    32.00    34.00    39.00
## y_pred[26]    34.02    0.05 3.38    27.00    32.00    34.00    36.00    40.00
## y_pred[27]    22.42    0.04 2.29    18.00    21.00    23.00    24.00    27.00
## y_pred[28]    28.59    0.04 2.57    23.00    27.00    29.00    30.00    33.00
## y_pred[29]    15.08    0.02 1.59    12.00    14.00    15.00    16.00    18.00
## y_pred[30]    37.37    0.05 3.02    31.00    35.00    37.00    39.00    43.00
## y_pred[31]    55.42    0.07 4.05    47.00    53.00    56.00    58.00    63.00
## y_pred[32]     6.50    0.03 1.75     3.00     5.00     7.00     8.00    10.00
## y_pred[33]    15.82    0.04 2.24    11.00    14.00    16.00    17.00    20.00
## y_pred[34]    24.33    0.05 2.88    19.00    22.00    24.00    26.00    30.00
## y_pred[35]    46.26    0.05 3.27    39.00    44.00    46.00    49.00    52.00
## y_pred[36]    43.51    0.05 3.06    37.00    41.75    44.00    46.00    49.00
## y_pred[37]    54.28    0.08 4.82    45.00    51.00    54.00    58.00    63.00
## y_pred[38]    35.60    0.05 3.04    29.00    34.00    36.00    38.00    41.00
## y_pred[39]    15.84    0.04 2.19    11.00    14.00    16.00    17.00    20.00
## y_pred[40]    29.39    0.05 2.98    23.00    27.00    29.00    31.00    35.00
## y_pred[41]    45.05    0.05 3.14    39.00    43.00    45.00    47.00    51.00
## y_pred[42]    25.43    0.04 2.34    21.00    24.00    26.00    27.00    30.00
## y_pred[43]    41.18    0.05 3.07    35.00    39.00    41.00    43.00    47.00
## y_pred[44]    25.37    0.03 2.16    21.00    24.00    25.00    27.00    29.00
## y_pred[45]    19.76    0.03 1.74    16.00    19.00    20.00    21.00    23.00
## y_pred[46]    38.21    0.05 3.19    32.00    36.00    38.00    40.00    44.00
## y_pred[47]    14.09    0.04 2.37     9.00    12.00    14.00    16.00    18.00
## y_pred[48]    31.20    0.04 2.41    26.00    30.00    31.00    33.00    36.00
## y_pred[49]    16.90    0.03 2.12    12.00    16.00    17.00    18.00    21.00
## y_pred[50]    40.34    0.07 4.25    32.00    37.00    40.00    43.00    48.00
## lp__       -1389.32    0.03 1.20 -1392.42 -1389.91 -1389.01 -1388.42 -1387.95
##            n_eff Rhat
## b1          1443    1
## b2          2052    1
## b3          1519    1
## q[1]        1523    1
## q[2]        2412    1
## q[3]        2751    1
## q[4]        2023    1
## q[5]        2065    1
## q[6]        2718    1
## q[7]        2352    1
## q[8]        2345    1
## q[9]        2443    1
## q[10]       2467    1
## q[11]       2561    1
## q[12]       2648    1
## q[13]       3089    1
## q[14]       2352    1
## q[15]       2267    1
## q[16]       2372    1
## q[17]       2483    1
## q[18]       1617    1
## q[19]       1891    1
## q[20]       2106    1
## q[21]       2023    1
## q[22]       2727    1
## q[23]       2642    1
## q[24]       2484    1
## q[25]       3065    1
## q[26]       3059    1
## q[27]       2648    1
## q[28]       2648    1
## q[29]       2149    1
## q[30]       2439    1
## q[31]       1948    1
## q[32]       1839    1
## q[33]       2682    1
## q[34]       3131    1
## q[35]       2757    1
## q[36]       2743    1
## q[37]       2600    1
## q[38]       2309    1
## q[39]       1707    1
## q[40]       1691    1
## q[41]       2757    1
## q[42]       2718    1
## q[43]       2713    1
## q[44]       2398    1
## q[45]       1913    1
## q[46]       2111    1
## q[47]       1470    1
## q[48]       2354    1
## q[49]       1919    1
## q[50]       2372    1
## y_pred[1]   3738    1
## y_pred[2]   3669    1
## y_pred[3]   3582    1
## y_pred[4]   3362    1
## y_pred[5]   3706    1
## y_pred[6]   3914    1
## y_pred[7]   3706    1
## y_pred[8]   3447    1
## y_pred[9]   3934    1
## y_pred[10]  3907    1
## y_pred[11]  3488    1
## y_pred[12]  4354    1
## y_pred[13]  3685    1
## y_pred[14]  3854    1
## y_pred[15]  3715    1
## y_pred[16]  3841    1
## y_pred[17]  3972    1
## y_pred[18]  3780    1
## y_pred[19]  3683    1
## y_pred[20]  3505    1
## y_pred[21]  3841    1
## y_pred[22]  3432    1
## y_pred[23]  3919    1
## y_pred[24]  3814    1
## y_pred[25]  3828    1
## y_pred[26]  3793    1
## y_pred[27]  3778    1
## y_pred[28]  4052    1
## y_pred[29]  4096    1
## y_pred[30]  3820    1
## y_pred[31]  3352    1
## y_pred[32]  3583    1
## y_pred[33]  3970    1
## y_pred[34]  3628    1
## y_pred[35]  3789    1
## y_pred[36]  3968    1
## y_pred[37]  3355    1
## y_pred[38]  3092    1
## y_pred[39]  3815    1
## y_pred[40]  3738    1
## y_pred[41]  4172    1
## y_pred[42]  4290    1
## y_pred[43]  3624    1
## y_pred[44]  4023    1
## y_pred[45]  3531    1
## y_pred[46]  3976    1
## y_pred[47]  3218    1
## y_pred[48]  3811    1
## y_pred[49]  3951    1
## y_pred[50]  3826    1
## lp__        1381    1
## 
## Samples were drawn using NUTS(diag_e) at Tue Nov 26 15:30:31 2019.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;把獲得的參數事後樣本的均值代入上面的數學模型中可得:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{array}{l}
q[n] = \text{inv_logit}(0.09 - 0.62 A[n] + 1.90Score[n]) &amp;amp; n = 1, 2, \dots, N \\
Y[n] \sim \text{Binomial}(M[n], q[n])                 &amp;amp; n = 1, 2, \dots, N \\
\end{array}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;確認收斂效果&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;確認收斂效果&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(bayesplot)

color_scheme_set(&amp;quot;mix-brightblue-gray&amp;quot;)

posterior2 &amp;lt;- rstan::extract(fit, inc_warmup = TRUE, permuted = FALSE)

p &amp;lt;- mcmc_trace(posterior2, n_warmup = 0, pars = c(&amp;quot;b1&amp;quot;, &amp;quot;b2&amp;quot;, &amp;quot;b3&amp;quot;, &amp;quot;lp__&amp;quot;),
                facet_args = list(nrow = 2, labeller = label_parsed))
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step53&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-27-logistic-rstan_files/figure-html/step53-1.png&#34; alt=&#34;用 bayesplot包數繪製的模型5-3的MCMC鏈式軌跡圖 (trace plot)。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: 用 bayesplot包數繪製的模型5-3的MCMC鏈式軌跡圖 (trace plot)。
&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ms &amp;lt;- rstan::extract(fit)

d_qua &amp;lt;- t(apply(ms$y_pred, 2, quantile, prob=c(0.1, 0.5, 0.9)))
colnames(d_qua) &amp;lt;- c(&amp;#39;p10&amp;#39;, &amp;#39;p50&amp;#39;, &amp;#39;p90&amp;#39;)
d_qua &amp;lt;- data.frame(d, d_qua)
d_qua$A &amp;lt;- as.factor(d_qua$A)

p &amp;lt;- ggplot(data=d_qua, aes(x=Y, y=p50, ymin=p10, ymax=p90, shape=A, fill=A))
p &amp;lt;- p + theme_bw(base_size=18) + theme(legend.key.height=grid::unit(2.5,&amp;#39;line&amp;#39;))
p &amp;lt;- p + coord_fixed(ratio=1, xlim=c(5, 70), ylim=c(5, 70))
p &amp;lt;- p + geom_pointrange(size=0.8, color=&amp;#39;grey5&amp;#39;)
p &amp;lt;- p + geom_abline(aes(slope=1, intercept=0), color=&amp;#39;black&amp;#39;, alpha=3/5, linetype=&amp;#39;31&amp;#39;)
p &amp;lt;- p + scale_shape_manual(values=c(21, 24))
p &amp;lt;- p + scale_fill_manual(values=c(&amp;#39;white&amp;#39;, &amp;#39;grey70&amp;#39;))
p &amp;lt;- p + labs(x=&amp;#39;Observed&amp;#39;, y=&amp;#39;Predicted&amp;#39;)
p &amp;lt;- p + scale_x_continuous(breaks=seq(from=0, to=70, by=20))
p &amp;lt;- p + scale_y_continuous(breaks=seq(from=0, to=70, by=20))
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:fig58&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-27-logistic-rstan_files/figure-html/fig58-1.png&#34; alt=&#34;觀測值(x)，和預測值(y)的散點圖，以及預測值的80%預測區間。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: 觀測值(x)，和預測值(y)的散點圖，以及預測值的80%預測區間。
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Rstan Wonderful R-(3)</title>
      <link>https://wangcc.me/post/rstan-wonderful-r3/</link>
      <pubDate>Tue, 22 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/rstan-wonderful-r3/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#多重回歸-multiple-regression&#34;&gt;多重回歸 multiple regression&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#step-1.-確認數據分佈&#34;&gt;Step 1. 確認數據分佈&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-2.-寫下數學模型&#34;&gt;Step 2. 寫下數學模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-3.-看圖確認模型擬合狀況&#34;&gt;Step 3. 看圖確認模型擬合狀況&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-4.-mcmc-樣本的散點圖矩陣&#34;&gt;Step 4. MCMC 樣本的散點圖矩陣&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;多重回歸-multiple-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;多重回歸 multiple regression&lt;/h1&gt;
&lt;p&gt;本章使用的數據，大學生出勤記錄也是&lt;a href=&#34;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap05/input/data-attendance-1.txt&#34;&gt;架空的數據&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;有大學記錄了50名大學生的出勤狀況：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;A,Score,Y
0,69,0.286
1,145,0.196
0,125,0.261
1,86,0.109
1,158,0.23
0,133,0.35
0,111,0.33
1,147,0.194
0,146,0.413
0,145,0.36
1,141,0.225
0,137,0.423
1,118,0.186
0,111,0.287
...
0,99,0.268
1,99,0.234&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;: 是學生大學二年級時進行的問卷調查時回答是否喜歡打零工的結果（0:不喜歡打工；1:喜歡打工）&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Score\)&lt;/span&gt;: 是大學二年級時進行的問卷調查時計算的該學生對學習是否感興趣的數值評分(200分滿分，分數越高，該學生越熱愛學習)&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;: 是該學生一年內的出勤率&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在本次分析範例中，把&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;出勤率當作是連續型結果變量，我們來用Stan實施多重回歸分析，回答學生喜歡打零工與否，和學生對學習的熱情程度兩個變量能解釋多少出勤率。&lt;/p&gt;
&lt;div id=&#34;step-1.-確認數據分佈&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 1. 確認數據分佈&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# The following figure codes come from the authors website: 
# https://github.com/MatsuuraKentaro/RStanBook/blob/master/chap05/fig5-1.R
library(ggplot2)
library(GGally)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Registered S3 method overwritten by &amp;#39;GGally&amp;#39;:
##   method from   
##   +.gg   ggplot2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)
d &amp;lt;- read.csv(file=&amp;#39;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap05/input/data-attendance-1.txt&amp;#39;, header = T)
d$A &amp;lt;- as.factor(d$A)

N_col &amp;lt;- ncol(d)
ggp &amp;lt;- ggpairs(d, upper=&amp;#39;blank&amp;#39;, diag=&amp;#39;blank&amp;#39;, lower=&amp;#39;blank&amp;#39;)

for(i in 1:N_col) {
  x &amp;lt;- d[,i]
  p &amp;lt;- ggplot(data.frame(x, A=d$A), aes(x))
  p &amp;lt;- p + theme_bw(base_size=14)
  p &amp;lt;- p + theme(axis.text.x=element_text(angle=40, vjust=1, hjust=1))
  if (class(x) == &amp;#39;factor&amp;#39;) {
    p &amp;lt;- p + geom_bar(aes(fill=A), color=&amp;#39;grey5&amp;#39;)
  } else {
    bw &amp;lt;- (max(x)-min(x))/10
    p &amp;lt;- p + geom_histogram(binwidth=bw, aes(fill=A), color=&amp;#39;grey5&amp;#39;) #繪製柱狀圖
    p &amp;lt;- p + geom_line(eval(bquote(aes(y=..count..*.(bw)))), stat=&amp;#39;density&amp;#39;) #添加概率密度曲線
  }
  p &amp;lt;- p + geom_label(data=data.frame(x=-Inf, y=Inf, label=colnames(d)[i]), aes(x=x, y=y, label=label), hjust=0, vjust=1)
  p &amp;lt;- p + scale_fill_manual(values=alpha(c(&amp;#39;white&amp;#39;, &amp;#39;grey40&amp;#39;), 0.5))
  ggp &amp;lt;- putPlot(ggp, p, i, i)
}

zcolat &amp;lt;- seq(-1, 1, length=81)
zcolre &amp;lt;- c(zcolat[1:40]+1, rev(zcolat[41:81]))

for(i in 1:(N_col-1)) {
  for(j in (i+1):N_col) {
    x &amp;lt;- as.numeric(d[,i])
    y &amp;lt;- as.numeric(d[,j])
    r &amp;lt;- cor(x, y, method=&amp;#39;spearman&amp;#39;, use=&amp;#39;pairwise.complete.obs&amp;#39;)
    zcol &amp;lt;- lattice::level.colors(r, at=zcolat, col.regions=grey(zcolre))
    textcol &amp;lt;- ifelse(abs(r) &amp;lt; 0.4, &amp;#39;grey20&amp;#39;, &amp;#39;white&amp;#39;)
    ell &amp;lt;- ellipse::ellipse(r, level=0.95, type=&amp;#39;l&amp;#39;, npoints=50, scale=c(.2, .2), centre=c(.5, .5))
    p &amp;lt;- ggplot(data.frame(ell), aes(x=x, y=y))
    p &amp;lt;- p + theme_bw() + theme(
      plot.background=element_blank(),
      panel.grid.major=element_blank(), panel.grid.minor=element_blank(),
      panel.border=element_blank(), axis.ticks=element_blank()
    )
    p &amp;lt;- p + geom_polygon(fill=zcol, color=zcol)
    p &amp;lt;- p + geom_text(data=NULL, x=.5, y=.5, label=100*round(r, 2), size=6, col=textcol)
    ggp &amp;lt;- putPlot(ggp, p, i, j)
  }
}

for(j in 1:(N_col-1)) {
  for(i in (j+1):N_col) {
    x &amp;lt;- d[,j]
    y &amp;lt;- d[,i]
    p &amp;lt;- ggplot(data.frame(x, y, gr=d$A), aes(x=x, y=y, fill=gr, shape=gr))
    p &amp;lt;- p + theme_bw(base_size=14)
    p &amp;lt;- p + theme(axis.text.x=element_text(angle=40, vjust=1, hjust=1))
    if (class(x) == &amp;#39;factor&amp;#39;) {
      p &amp;lt;- p + geom_boxplot(aes(group=x), alpha=3/6, outlier.size=0, fill=&amp;#39;white&amp;#39;)
      p &amp;lt;- p + geom_point(position=position_jitter(w=0.4, h=0), size=2)
    } else {
      p &amp;lt;- p + geom_point(size=2)
    }
    p &amp;lt;- p + scale_shape_manual(values=c(21, 24))
    p &amp;lt;- p + scale_fill_manual(values=alpha(c(&amp;#39;white&amp;#39;, &amp;#39;grey40&amp;#39;), 0.5))
    ggp &amp;lt;- putPlot(ggp, p, i, j)
  }
}

ggp&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step1&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-22-rstan-wonderful-r-multi-lm_files/figure-html/step1-1.png&#34; alt=&#34;三個變量的分佈觀察圖，對角線上是三個變量各自的柱狀圖 (histogram) 和計算獲得的概率密度函數曲線；左下角三個圖是三個變量的箱式圖和散點圖；右上角三個圖是三個變量兩兩計算獲得的 Spearman 秩相關乘以100之後的數值。對角線上及左下角三個圖中數據點和形狀的不同分別表示學生喜歡(三角形)和不喜歡(圓形)打工。右上角表示秩相關的數值越接近0，顏色越白圖形越接近圓形，相關係數的絕對值越接近1，則顏色越深，橢圓越細長。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: 三個變量的分佈觀察圖，對角線上是三個變量各自的柱狀圖 (histogram) 和計算獲得的概率密度函數曲線；左下角三個圖是三個變量的箱式圖和散點圖；右上角三個圖是三個變量兩兩計算獲得的 Spearman 秩相關乘以100之後的數值。對角線上及左下角三個圖中數據點和形狀的不同分別表示學生喜歡(三角形)和不喜歡(圓形)打工。右上角表示秩相關的數值越接近0，顏色越白圖形越接近圓形，相關係數的絕對值越接近1，則顏色越深，橢圓越細長。
&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# png(file=&amp;#39;output/fig5-1.png&amp;#39;, w=1600, h=1600, res=300)
# print(ggp, left=0.3, bottom=0.3)
# dev.off()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;step-2.-寫下數學模型&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 2. 寫下數學模型&lt;/h2&gt;
&lt;p&gt;Model can be written as (Model5-1):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{array}{l}
Y[n]        = b_1 + b_2A[n] + b_3Sore[n] + \varepsilon [n]&amp;amp;  n = 1,2,\dots,N \\
\varepsilon[n] \sim \text{Normal}(0, \sigma) &amp;amp; n = 1,2,\dots,N \\ 
\end{array}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; 表示學生的人數，&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;則是學生編號的下標；&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(b_1\)&lt;/span&gt; 是回歸直線的截距；&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(b_2\)&lt;/span&gt; 是&lt;span class=&#34;math inline&#34;&gt;\(Score\)&lt;/span&gt;保持不變時，&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;從&lt;span class=&#34;math inline&#34;&gt;\(0\rightarrow 1\)&lt;/span&gt;時出勤率的變化(增加，或者減少)；&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(b_3\)&lt;/span&gt; 是&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;保持不變時，&lt;span class=&#34;math inline&#34;&gt;\(Score\)&lt;/span&gt;增加一個單位時出勤率的變化(增加，或者減少)。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Model can also be written as (Model5-2):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{array}{l}
Y[n]       \sim \text{Normal}(b_1 + b_2A[n] + b_3Score[n], \sigma) &amp;amp;  n = 1,2,\dots,N \\
\end{array}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;如果認爲&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(Score\)&lt;/span&gt;所能預測的出勤率有一個基礎的均值 &lt;span class=&#34;math inline&#34;&gt;\(\mu[n]\)&lt;/span&gt;，剩下的每名學生的出勤率服從這個均值和標準差爲 &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; 的正態分佈，那麼模型又可以繼續改寫成爲下面的 Model 5-3:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{array}{l}
\mu[n]        = b_1 + b_2A[n] + b_3Sore[n] &amp;amp;  n = 1,2,\dots,N \\
Y[n] \sim \text{Normal}(\mu[n], \sigma) &amp;amp; n = 1,2,\dots,N \\ 
\end{array}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;下面的 Stan 模型是按照 Model 5-3 寫的，它的模型參數有四個，&lt;span class=&#34;math inline&#34;&gt;\(b_1, b_2, b_3, \sigma\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(\mu[n]\)&lt;/span&gt;通過 &lt;code&gt;transformed parameter&lt;/code&gt; 計算獲得:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data {
  int N; 
  int&amp;lt;lower=0, upper=1&amp;gt; A[N];
  real&amp;lt;lower=0, upper=1&amp;gt; Score[N];
  real&amp;lt;lower=0, upper=1&amp;gt; Y[N];
}

parameters {
  real b1; 
  real b2;
  real b3;
  real&amp;lt;lower=0&amp;gt; sigma;
}

transformed parameters {
  real mu[N];
  for (n in 1:N) {
    mu[n] = b1 + b2*A[n] + b3*Score[n];
  }
}

model {
  for (n in 1:N) {
    Y[n] ~ normal(mu[n], sigma);
  }
}

generated quantities {
  real y_pred[N];
  for (n in 1:N) {
    y_pred[n] = normal_rng(mu[n], sigma);
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;下面的 R 代碼用來實現對上面 Stan 模型的擬合:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rstan)
d &amp;lt;- read.csv(file=&amp;#39;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap05/input/data-attendance-1.txt&amp;#39;, header = T)
data &amp;lt;- list(N=nrow(d), A=d$A, Score=d$Score/200, Y=d$Y)
fit &amp;lt;- stan(file=&amp;#39;stanfiles/model5-3.stan&amp;#39;, data=data, seed=1234)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &amp;#39;model5-3&amp;#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 1.1e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.104243 seconds (Warm-up)
## Chain 1:                0.106625 seconds (Sampling)
## Chain 1:                0.210868 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &amp;#39;model5-3&amp;#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 7e-06 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.102678 seconds (Warm-up)
## Chain 2:                0.109559 seconds (Sampling)
## Chain 2:                0.212237 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &amp;#39;model5-3&amp;#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 6e-06 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.105573 seconds (Warm-up)
## Chain 3:                0.105132 seconds (Sampling)
## Chain 3:                0.210705 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &amp;#39;model5-3&amp;#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 6e-06 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.100771 seconds (Warm-up)
## Chain 4:                0.094318 seconds (Sampling)
## Chain 4:                0.195089 seconds (Total)
## Chain 4:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Stan model: model5-3.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##              mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat
## b1           0.12    0.00 0.03   0.06   0.10   0.12   0.15   0.19  1844    1
## b2          -0.14    0.00 0.01  -0.17  -0.15  -0.14  -0.13  -0.11  2973    1
## b3           0.32    0.00 0.05   0.22   0.29   0.32   0.36   0.43  1880    1
## sigma        0.05    0.00 0.01   0.04   0.05   0.05   0.05   0.06  2478    1
## mu[1]        0.24    0.00 0.02   0.20   0.22   0.24   0.25   0.27  1996    1
## mu[2]        0.22    0.00 0.01   0.19   0.21   0.22   0.22   0.24  2809    1
## mu[3]        0.33    0.00 0.01   0.31   0.32   0.33   0.33   0.34  3531    1
## mu[4]        0.12    0.00 0.02   0.09   0.11   0.12   0.13   0.15  2784    1
## mu[5]        0.24    0.00 0.01   0.21   0.23   0.24   0.25   0.27  2468    1
## mu[6]        0.34    0.00 0.01   0.32   0.33   0.34   0.35   0.36  3463    1
## mu[7]        0.30    0.00 0.01   0.29   0.30   0.30   0.31   0.32  3039    1
## mu[8]        0.22    0.00 0.01   0.19   0.21   0.22   0.23   0.24  2747    1
## mu[9]        0.36    0.00 0.01   0.34   0.35   0.36   0.37   0.38  3046    1
## mu[10]       0.36    0.00 0.01   0.34   0.35   0.36   0.37   0.38  3080    1
## mu[11]       0.21    0.00 0.01   0.18   0.20   0.21   0.22   0.23  2942    1
## mu[12]       0.35    0.00 0.01   0.33   0.34   0.35   0.35   0.37  3351    1
## mu[13]       0.17    0.00 0.01   0.15   0.16   0.17   0.18   0.19  3511    1
## mu[14]       0.30    0.00 0.01   0.29   0.30   0.30   0.31   0.32  3039    1
## mu[15]       0.30    0.00 0.01   0.28   0.29   0.30   0.31   0.32  2943    1
## mu[16]       0.14    0.00 0.01   0.11   0.13   0.14   0.15   0.17  3138    1
## mu[17]       0.31    0.00 0.01   0.29   0.30   0.31   0.31   0.33  3183    1
## mu[18]       0.26    0.00 0.01   0.23   0.25   0.26   0.27   0.28  2136    1
## mu[19]       0.42    0.00 0.02   0.39   0.41   0.42   0.44   0.46  2308    1
## mu[20]       0.23    0.00 0.01   0.20   0.22   0.23   0.24   0.26  2511    1
## mu[21]       0.12    0.00 0.02   0.09   0.11   0.12   0.13   0.15  2784    1
## mu[22]       0.16    0.00 0.01   0.13   0.15   0.15   0.16   0.18  3387    1
## mu[23]       0.15    0.00 0.01   0.13   0.14   0.15   0.16   0.18  3337    1
## mu[24]       0.21    0.00 0.01   0.19   0.20   0.21   0.22   0.24  2874    1
## mu[25]       0.17    0.00 0.01   0.15   0.16   0.17   0.18   0.19  3508    1
## mu[26]       0.19    0.00 0.01   0.16   0.18   0.19   0.20   0.21  3373    1
## mu[27]       0.32    0.00 0.01   0.30   0.31   0.32   0.32   0.33  3393    1
## mu[28]       0.32    0.00 0.01   0.30   0.31   0.32   0.32   0.33  3393    1
## mu[29]       0.38    0.00 0.01   0.36   0.38   0.38   0.39   0.41  2639    1
## mu[30]       0.31    0.00 0.01   0.29   0.30   0.31   0.31   0.33  3135    1
## mu[31]       0.25    0.00 0.02   0.22   0.24   0.25   0.26   0.28  2340    1
## mu[32]       0.10    0.00 0.02   0.06   0.09   0.10   0.11   0.13  2546    1
## mu[33]       0.20    0.00 0.01   0.18   0.20   0.20   0.21   0.23  3046    1
## mu[34]       0.18    0.00 0.01   0.16   0.17   0.18   0.19   0.20  3463    1
## mu[35]       0.33    0.00 0.01   0.31   0.32   0.33   0.33   0.35  3538    1
## mu[36]       0.34    0.00 0.01   0.32   0.33   0.34   0.34   0.35  3504    1
## mu[37]       0.15    0.00 0.01   0.13   0.14   0.15   0.16   0.17  3310    1
## mu[38]       0.30    0.00 0.01   0.28   0.30   0.30   0.31   0.32  2991    1
## mu[39]       0.27    0.00 0.01   0.24   0.26   0.27   0.28   0.29  2259    1
## mu[40]       0.27    0.00 0.01   0.24   0.26   0.27   0.27   0.29  2239    1
## mu[41]       0.33    0.00 0.01   0.31   0.33   0.33   0.34   0.35  3530    1
## mu[42]       0.34    0.00 0.01   0.32   0.33   0.34   0.35   0.36  3463    1
## mu[43]       0.32    0.00 0.01   0.30   0.31   0.32   0.33   0.34  3482    1
## mu[44]       0.36    0.00 0.01   0.34   0.36   0.36   0.37   0.39  2981    1
## mu[45]       0.42    0.00 0.02   0.38   0.41   0.42   0.43   0.46  2336    1
## mu[46]       0.29    0.00 0.01   0.27   0.29   0.29   0.30   0.31  2760    1
## mu[47]       0.21    0.00 0.02   0.17   0.19   0.21   0.22   0.24  1907    1
## mu[48]       0.37    0.00 0.01   0.34   0.36   0.37   0.38   0.39  2919    1
## mu[49]       0.28    0.00 0.01   0.26   0.28   0.28   0.29   0.30  2527    1
## mu[50]       0.14    0.00 0.01   0.11   0.13   0.14   0.15   0.17  3138    1
## y_pred[1]    0.24    0.00 0.06   0.13   0.20   0.24   0.27   0.34  3503    1
## y_pred[2]    0.21    0.00 0.05   0.11   0.18   0.21   0.25   0.32  3779    1
## y_pred[3]    0.33    0.00 0.05   0.22   0.29   0.33   0.36   0.43  3944    1
## y_pred[4]    0.12    0.00 0.05   0.01   0.08   0.12   0.16   0.23  3762    1
## y_pred[5]    0.24    0.00 0.05   0.13   0.20   0.24   0.27   0.34  4038    1
## y_pred[6]    0.34    0.00 0.05   0.24   0.30   0.34   0.37   0.44  3457    1
## y_pred[7]    0.30    0.00 0.05   0.20   0.27   0.30   0.34   0.41  4030    1
## y_pred[8]    0.22    0.00 0.05   0.11   0.18   0.22   0.26   0.33  3217    1
## y_pred[9]    0.36    0.00 0.05   0.26   0.33   0.36   0.40   0.47  3869    1
## y_pred[10]   0.36    0.00 0.05   0.26   0.32   0.36   0.39   0.46  3884    1
## y_pred[11]   0.21    0.00 0.05   0.11   0.17   0.21   0.25   0.31  3871    1
## y_pred[12]   0.34    0.00 0.05   0.24   0.31   0.34   0.38   0.45  3866    1
## y_pred[13]   0.17    0.00 0.05   0.07   0.13   0.17   0.21   0.27  4064    1
## y_pred[14]   0.30    0.00 0.05   0.20   0.27   0.30   0.34   0.41  3869    1
## y_pred[15]   0.30    0.00 0.05   0.20   0.26   0.30   0.33   0.40  3929    1
## y_pred[16]   0.14    0.00 0.05   0.03   0.11   0.14   0.18   0.25  3900    1
## y_pred[17]   0.31    0.00 0.05   0.21   0.27   0.31   0.35   0.41  4048    1
## y_pred[18]   0.26    0.00 0.05   0.15   0.22   0.26   0.29   0.36  3705    1
## y_pred[19]   0.42    0.00 0.05   0.32   0.39   0.42   0.46   0.53  3474    1
## y_pred[20]   0.23    0.00 0.05   0.13   0.20   0.23   0.27   0.34  3792    1
## y_pred[21]   0.12    0.00 0.05   0.01   0.08   0.12   0.15   0.23  3581    1
## y_pred[22]   0.16    0.00 0.05   0.05   0.12   0.16   0.19   0.26  4075    1
## y_pred[23]   0.15    0.00 0.05   0.04   0.12   0.15   0.19   0.26  3975    1
## y_pred[24]   0.21    0.00 0.05   0.11   0.18   0.21   0.25   0.32  3489    1
## y_pred[25]   0.17    0.00 0.05   0.06   0.14   0.17   0.20   0.27  3938    1
## y_pred[26]   0.19    0.00 0.05   0.08   0.15   0.19   0.22   0.29  3955    1
## y_pred[27]   0.32    0.00 0.05   0.21   0.28   0.32   0.35   0.42  4075    1
## y_pred[28]   0.32    0.00 0.05   0.21   0.28   0.32   0.35   0.42  4110    1
## y_pred[29]   0.38    0.00 0.05   0.28   0.35   0.38   0.42   0.49  3631    1
## y_pred[30]   0.31    0.00 0.05   0.20   0.27   0.31   0.34   0.41  3865    1
## y_pred[31]   0.25    0.00 0.05   0.14   0.21   0.25   0.28   0.35  3818    1
## y_pred[32]   0.10    0.00 0.05  -0.01   0.06   0.10   0.14   0.20  3857    1
## y_pred[33]   0.20    0.00 0.05   0.10   0.17   0.20   0.24   0.31  3304    1
## y_pred[34]   0.18    0.00 0.05   0.08   0.14   0.18   0.22   0.28  3946    1
## y_pred[35]   0.33    0.00 0.05   0.22   0.29   0.33   0.36   0.43  4182    1
## y_pred[36]   0.34    0.00 0.05   0.23   0.30   0.34   0.37   0.44  4022    1
## y_pred[37]   0.15    0.00 0.05   0.05   0.12   0.15   0.19   0.25  3719    1
## y_pred[38]   0.30    0.00 0.05   0.20   0.27   0.30   0.34   0.41  3655    1
## y_pred[39]   0.27    0.00 0.05   0.16   0.23   0.27   0.30   0.37  3522    1
## y_pred[40]   0.27    0.00 0.05   0.16   0.23   0.27   0.30   0.37  3742    1
## y_pred[41]   0.33    0.00 0.05   0.23   0.30   0.33   0.37   0.44  3979    1
## y_pred[42]   0.34    0.00 0.05   0.24   0.30   0.34   0.37   0.44  3914    1
## y_pred[43]   0.32    0.00 0.05   0.22   0.29   0.32   0.36   0.43  4005    1
## y_pred[44]   0.36    0.00 0.05   0.26   0.33   0.36   0.40   0.47  3818    1
## y_pred[45]   0.42    0.00 0.05   0.31   0.38   0.42   0.45   0.53  3443    1
## y_pred[46]   0.29    0.00 0.05   0.19   0.26   0.29   0.33   0.40  3839    1
## y_pred[47]   0.21    0.00 0.06   0.09   0.17   0.21   0.24   0.31  3611    1
## y_pred[48]   0.37    0.00 0.05   0.26   0.33   0.37   0.40   0.47  3784    1
## y_pred[49]   0.28    0.00 0.05   0.18   0.25   0.28   0.32   0.39  3745    1
## y_pred[50]   0.14    0.00 0.05   0.03   0.10   0.14   0.17   0.24  3701    1
## lp__       120.94    0.04 1.39 117.48 120.23 121.24 121.96 122.70  1407    1
## 
## Samples were drawn using NUTS(diag_e) at Tue Nov 26 15:29:20 2019.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;上述代碼中值得注意的是我們對 &lt;span class=&#34;math inline&#34;&gt;\(Score\)&lt;/span&gt; 進行了全部除以 &lt;span class=&#34;math inline&#34;&gt;\(200\)&lt;/span&gt; 的數據縮放調整 (scaling)。這樣有助於我們的模型在進行 MCMC 計算時加速其達到收斂時所需要的時間。&lt;/p&gt;
&lt;p&gt;把計算獲得的事後模型參數平均值代入模型 Model 5-3:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{array}{l}
\mu[n]        = 0.12 - 0.14A[n] + 0.32Sore[n] &amp;amp;  n = 1,2,\dots,N \\
Y[n] \sim \text{Normal}(\mu[n], 0.05) &amp;amp; n = 1,2,\dots,N \\ 
\end{array}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;從輸出的結果報告來看，所有的 &lt;code&gt;Rhat&lt;/code&gt; 都小於1.1，可以認爲採樣已經達到收斂效果，再來確認一下軌跡圖：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(bayesplot)

color_scheme_set(&amp;quot;mix-brightblue-gray&amp;quot;)

posterior2 &amp;lt;- rstan::extract(fit, inc_warmup = TRUE, permuted = FALSE)

p &amp;lt;- mcmc_trace(posterior2, n_warmup = 0, pars = c(&amp;quot;b1&amp;quot;, &amp;quot;b2&amp;quot;, &amp;quot;b3&amp;quot;, &amp;quot;sigma&amp;quot;, &amp;quot;lp__&amp;quot;),
                facet_args = list(nrow = 2, labeller = label_parsed))
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step53&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-22-rstan-wonderful-r-multi-lm_files/figure-html/step53-1.png&#34; alt=&#34;用 bayesplot包數繪製的模型5-3的MCMC鏈式軌跡圖 (trace plot)。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: 用 bayesplot包數繪製的模型5-3的MCMC鏈式軌跡圖 (trace plot)。
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;收斂效果很不錯，下面來解釋回歸係數的事後均值的涵義：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;b3&lt;/code&gt;的事後均值是&lt;span class=&#34;math inline&#34;&gt;\(0.32\)&lt;/span&gt;，所以，&lt;span class=&#34;math inline&#34;&gt;\(Score=150\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(Score=50\)&lt;/span&gt;的兩名學生，當他們同時都是喜歡或者同時都不喜歡打工時，&lt;span class=&#34;math inline&#34;&gt;\(Score = 150\)&lt;/span&gt;的學生的出勤率平均比 &lt;span class=&#34;math inline&#34;&gt;\(Score = 50\)&lt;/span&gt; 的學生的出勤率高 &lt;span class=&#34;math inline&#34;&gt;\(0.32 \times (150-50)/200 = 0.16\)&lt;/span&gt;。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;b2&lt;/code&gt;的事後均值是&lt;span class=&#34;math inline&#34;&gt;\(-0.14\)&lt;/span&gt;，所以，同樣地，&lt;span class=&#34;math inline&#34;&gt;\(Score\)&lt;/span&gt;相同的兩名學生，喜歡打工的學生比不喜歡打工的學生出勤率平均要低 &lt;span class=&#34;math inline&#34;&gt;\(0.14\)&lt;/span&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;step-3.-看圖確認模型擬合狀況&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 3. 看圖確認模型擬合狀況&lt;/h2&gt;
&lt;p&gt;下圖繪製了上面貝葉斯多重線性回歸模型計算獲得的事後貝葉斯預測區間，和觀測值&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;出勤率之間的直觀關係：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;source(&amp;quot;commonRstan.R&amp;quot;)

ms &amp;lt;- rstan::extract(fit)

Score_new &amp;lt;- 50:200
N_X &amp;lt;- length(Score_new)
N_mcmc &amp;lt;- length(ms$lp__)

set.seed(1234)
y_base_mcmc &amp;lt;- as.data.frame(matrix(nrow=N_mcmc, ncol=N_X))
y_base_a0_mcmc &amp;lt;- as.data.frame(matrix(nrow = N_mcmc, ncol = N_X))
y_mcmc &amp;lt;- as.data.frame(matrix(nrow=N_mcmc, ncol=N_X))
y_a0_mcmc &amp;lt;- as.data.frame(matrix(nrow=N_mcmc, ncol=N_X))

for (i in 1:N_X) {
  y_base_mcmc[,i] &amp;lt;- ms$b1 + ms$b2 + ms$b3 * Score_new[i]/200
  y_base_a0_mcmc[] &amp;lt;- ms$b1 + ms$b2*0 + ms$b3 * Score_new[i]/200
  y_mcmc[,i] &amp;lt;- rnorm(n=N_mcmc, mean=y_base_mcmc[,i], sd=ms$sigma)
  y_a0_mcmc[,i] &amp;lt;- rnorm(n=N_mcmc, mean=y_base_a0_mcmc[,i], sd=ms$sigma)
}

customize.ggplot.axis &amp;lt;- function(p) {
  p &amp;lt;- p + labs(x=&amp;#39;Score&amp;#39;, y=&amp;#39;Y&amp;#39;)
  p &amp;lt;- p + scale_y_continuous(breaks=seq(from=-0.2, to=0.8, by=0.2))
  p &amp;lt;- p + coord_cartesian(xlim=c(50, 200), ylim=c(-0.2, 0.6))
  return(p)
}

d_est &amp;lt;- data.frame.quantile.mcmc(x=Score_new, y_mcmc=y_mcmc)
d_esta0 &amp;lt;- data.frame.quantile.mcmc(x=Score_new, y_mcmc=y_a0_mcmc)
# p &amp;lt;- ggplot.5quantile(data=d_est)
# p2 &amp;lt;- ggplot.5quantile(data = d_esta0)
# p &amp;lt;- p + geom_point(data=d[d$A==1, ], aes(x=Score, y=Y), shape=24, size=5)
# p2 &amp;lt;- p2 + geom_point(data=d[d$A==0, ], aes(x=Score, y=Y), shape=1, size=5)
# p &amp;lt;- customize.ggplot.axis(p)
# p2 &amp;lt;- customize.ggplot.axis(p2)

visuals = rbind(d_est,d_esta0)
visuals$A=c(rep(1,151),rep(0,151)) # 151 points of each flavour

qn &amp;lt;- colnames(visuals)[-1]
p &amp;lt;- ggplot(data=visuals, aes(x=X, y=p50, group = A))
p &amp;lt;- p + my_theme()
p &amp;lt;- p + geom_ribbon(aes_string(ymin=qn[1], ymax=qn[5]), fill=&amp;#39;black&amp;#39;, alpha=1/6)
p &amp;lt;- p + geom_ribbon(aes_string(ymin=qn[2], ymax=qn[4]), fill=&amp;#39;black&amp;#39;, alpha=2/6)
p &amp;lt;- p + geom_line(size=1)
p &amp;lt;- p + geom_point(data=d[d$A==1, ], aes(x=Score, y=Y), shape=24, size=5)
p &amp;lt;- p + geom_point(data=d[d$A==0, ], aes(x=Score, y=Y), shape=20, size=5)
p &amp;lt;- customize.ggplot.axis(p)
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:fig52&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-22-rstan-wonderful-r-multi-lm_files/figure-html/fig52-1.png&#34; alt=&#34;黑色原點(不喜歡打工)，和無色三角形(喜歡打工)的學生的出勤率，和模型計算獲得的貝葉斯事後預測區間。黑色線是中位數，灰色帶是50%預測區間和95%預測區間。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: 黑色原點(不喜歡打工)，和無色三角形(喜歡打工)的學生的出勤率，和模型計算獲得的貝葉斯事後預測區間。黑色線是中位數，灰色帶是50%預測區間和95%預測區間。
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;上述觀察預測值區間和實際觀測之間的關係的視覺化圖形，在多重線性回歸模型只有兩個預測變量的事後還較爲容易獲得，當模型中有三個或以上的預測變量時，可視化變得困難重重。&lt;/p&gt;
&lt;p&gt;此時我們推薦繪製“實際觀測值和預測值”，以及模型給出的每個預測值的隨機誤差&lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt;分佈範圍，相結合的圖形來判斷模型擬合程度。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d_qua &amp;lt;- t(apply(ms$y_pred, 2, quantile, prob=c(0.1, 0.5, 0.9)))
colnames(d_qua) &amp;lt;- c(&amp;#39;p10&amp;#39;, &amp;#39;p50&amp;#39;, &amp;#39;p90&amp;#39;)
d_qua &amp;lt;- data.frame(d, d_qua)
d_qua$A &amp;lt;- as.factor(d_qua$A)

p &amp;lt;- ggplot(data=d_qua, aes(x=Y, y=p50, ymin=p10, ymax=p90, shape=A, fill=A))
p &amp;lt;- p + theme_bw(base_size=18) + theme(legend.key.height=grid::unit(2.5,&amp;#39;line&amp;#39;))
p &amp;lt;- p + coord_fixed(ratio=1, xlim=c(0, 0.5), ylim=c(0, 0.5))
p &amp;lt;- p + geom_pointrange(size=0.8, color=&amp;#39;grey5&amp;#39;)
p &amp;lt;- p + geom_abline(aes(slope=1, intercept=0), color=&amp;#39;black&amp;#39;, alpha=3/5, linetype=&amp;#39;31&amp;#39;)
p &amp;lt;- p + scale_shape_manual(values=c(21, 24))
p &amp;lt;- p + scale_fill_manual(values=c(&amp;#39;white&amp;#39;, &amp;#39;grey70&amp;#39;))
p &amp;lt;- p + labs(x=&amp;#39;Observed&amp;#39;, y=&amp;#39;Predicted&amp;#39;)
p &amp;lt;- p + scale_x_continuous(breaks=seq(from=0, to=0.5, by=0.1))
p &amp;lt;- p + scale_y_continuous(breaks=seq(from=0, to=0.5, by=0.1))
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:fig53&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-22-rstan-wonderful-r-multi-lm_files/figure-html/fig53-1.png&#34; alt=&#34;觀測值(x)，和預測值(y)的散點圖，以及預測值的80%預測區間。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4: 觀測值(x)，和預測值(y)的散點圖，以及預測值的80%預測區間。
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;從上圖中可以看出，大多數的觀測點和預測點以及預測的80%區間基本都在 &lt;span class=&#34;math inline&#34;&gt;\(y = x\)&lt;/span&gt; 這條對角線上。大致可以認爲本次貝葉斯多重線性回歸擬合效果尚且能夠接受。&lt;/p&gt;
&lt;p&gt;隨機誤差 &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon[n]\)&lt;/span&gt; 被認爲服從 &lt;span class=&#34;math inline&#34;&gt;\(\text{Normal}(0, \sigma)\)&lt;/span&gt; 的正態分佈。從模型中可以計算獲得每個學生出勤率的預測值和實際觀測值之間的差，這就是隨機誤差。貝葉斯框架之下，我們實際獲得的會是每名學生隨機誤差的分佈：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;N_mcmc &amp;lt;- length(ms$lp__)

d_noise &amp;lt;- data.frame(t(-t(ms$mu) + d$Y))
colnames(d_noise) &amp;lt;- paste0(&amp;#39;noise&amp;#39;, 1:nrow(d))
d_est &amp;lt;- data.frame(mcmc=1:N_mcmc, d_noise)
d_melt &amp;lt;- reshape2::melt(d_est, id=c(&amp;#39;mcmc&amp;#39;), variable.name=&amp;#39;X&amp;#39;)

d_mode &amp;lt;- data.frame(t(apply(d_noise, 2, function(x) {
  dens &amp;lt;- density(x)
  mode_i &amp;lt;- which.max(dens$y)
  mode_x &amp;lt;- dens$x[mode_i]
  mode_y &amp;lt;- dens$y[mode_i]
  c(mode_x, mode_y)
})))
colnames(d_mode) &amp;lt;- c(&amp;#39;X&amp;#39;, &amp;#39;Y&amp;#39;)

p &amp;lt;- ggplot()
p &amp;lt;- p + theme_bw(base_size=18)
p &amp;lt;- p + geom_line(data=d_melt, aes(x=value, group=X), stat=&amp;#39;density&amp;#39;, color=&amp;#39;black&amp;#39;, alpha=0.4)
p &amp;lt;- p + geom_segment(data=d_mode, aes(x=X, xend=X, y=Y, yend=0), color=&amp;#39;black&amp;#39;, linetype=&amp;#39;dashed&amp;#39;, alpha=0.4)
p &amp;lt;- p + geom_rug(data=d_mode, aes(x=X), sides=&amp;#39;b&amp;#39;)
p &amp;lt;- p + labs(x=&amp;#39;value&amp;#39;, y=&amp;#39;density&amp;#39;)
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:fig54left&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-22-rstan-wonderful-r-multi-lm_files/figure-html/fig54left-1.png&#34; alt=&#34;每名學生的出勤率隨機誤差的分佈&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 5: 每名學生的出勤率隨機誤差的分佈
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;實際上我們只需要選取每名學生模型計算獲得的事後隨機誤差的代表值，比如可以是平均值，中央值，或者是MAP值（事後確率最大推定値，maximum a posteriori estimate），來觀察就可以了：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s_dens &amp;lt;- density(ms$s)
s_MAP &amp;lt;- s_dens$x[which.max(s_dens$y)]
bw &amp;lt;- 0.01
p &amp;lt;- ggplot(data=d_mode, aes(x=X))
p &amp;lt;- p + theme_bw(base_size=18)
p &amp;lt;- p + geom_histogram(binwidth=bw, color=&amp;#39;black&amp;#39;, fill=&amp;#39;white&amp;#39;)
p &amp;lt;- p + geom_density(eval(bquote(aes(y=..count..*.(bw)))), alpha=0.5, color=&amp;#39;black&amp;#39;, fill=&amp;#39;gray20&amp;#39;)
p &amp;lt;- p + stat_function(fun=function(x) nrow(d)*bw*dnorm(x, mean=0, sd=s_MAP), linetype=&amp;#39;dashed&amp;#39;)
p &amp;lt;- p + labs(x=&amp;#39;value&amp;#39;, y=&amp;#39;count&amp;#39;)
p &amp;lt;- p + xlim(range(density(d_mode$X)$x))
p&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 2 rows containing missing values (geom_bar).&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:fig54right&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-22-rstan-wonderful-r-multi-lm_files/figure-html/fig54right-1.png&#34; alt=&#34;每名學生事後出勤率隨機誤差的MAP值的柱狀圖，和相應的概率密度函數（灰色鐘罩），點狀虛線是均值爲0，標準差是模型計算的事後隨機誤差標準差的 MAP 值的正態分佈的形狀。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 6: 每名學生事後出勤率隨機誤差的MAP值的柱狀圖，和相應的概率密度函數（灰色鐘罩），點狀虛線是均值爲0，標準差是模型計算的事後隨機誤差標準差的 MAP 值的正態分佈的形狀。
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;step-4.-mcmc-樣本的散點圖矩陣&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 4. MCMC 樣本的散點圖矩陣&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(GGally)
library(hexbin)


d &amp;lt;- data.frame(b1=ms$b1, b2=ms$b2, b3=ms$b3, sigma=ms$sigma, mu1=ms$mu[,1], mu50=ms$mu[,50], lp__=ms$lp__)
N_col &amp;lt;- ncol(d)
ggp &amp;lt;- ggpairs(d, upper=&amp;#39;blank&amp;#39;, diag=&amp;#39;blank&amp;#39;, lower=&amp;#39;blank&amp;#39;)

label_list &amp;lt;- list(b1=&amp;#39;b1&amp;#39;, b2=&amp;#39;b2&amp;#39;, b3=&amp;#39;b3&amp;#39;, sigma=&amp;#39;sigma&amp;#39;, mu1=&amp;#39;mu[1]&amp;#39;, mu50=&amp;#39;mu[50]&amp;#39;, lp__=&amp;#39;lp__&amp;#39;)
for(i in 1:N_col) {
  x &amp;lt;- d[,i]
  bw &amp;lt;- (max(x)-min(x))/10
  p &amp;lt;- ggplot(data.frame(x), aes(x))
  p &amp;lt;- p + theme_bw(base_size=14)
  p &amp;lt;- p + theme(axis.text.x=element_text(angle=60, vjust=1, hjust=1))
  p &amp;lt;- p + geom_histogram(binwidth=bw, fill=&amp;#39;white&amp;#39;, color=&amp;#39;grey5&amp;#39;)
  p &amp;lt;- p + geom_line(eval(bquote(aes(y=..count..*.(bw)))), stat=&amp;#39;density&amp;#39;)
  p &amp;lt;- p + geom_label(data=data.frame(x=-Inf, y=Inf, label=label_list[[colnames(d)[i]]]), aes(x=x, y=y, label=label), hjust=0, vjust=1)
  ggp &amp;lt;- putPlot(ggp, p, i, i)
}

zcolat &amp;lt;- seq(-1, 1, length=81)
zcolre &amp;lt;- c(zcolat[1:40]+1, rev(zcolat[41:81]))

for(i in 1:(N_col-1)) {
  for(j in (i+1):N_col) {
    x &amp;lt;- as.numeric(d[,i])
    y &amp;lt;- as.numeric(d[,j])
    r &amp;lt;- cor(x, y, method=&amp;#39;spearman&amp;#39;, use=&amp;#39;pairwise.complete.obs&amp;#39;)
    zcol &amp;lt;- lattice::level.colors(r, at=zcolat, col.regions=grey(zcolre))
    textcol &amp;lt;- ifelse(abs(r) &amp;lt; 0.4, &amp;#39;grey20&amp;#39;, &amp;#39;white&amp;#39;)
    ell &amp;lt;- ellipse::ellipse(r, level=0.95, type=&amp;#39;l&amp;#39;, npoints=50, scale=c(.2, .2), centre=c(.5, .5))
    p &amp;lt;- ggplot(data.frame(ell), aes(x=x, y=y))
    p &amp;lt;- p + theme_bw() + theme(
      plot.background=element_blank(),
      panel.grid.major=element_blank(), panel.grid.minor=element_blank(),
      panel.border=element_blank(), axis.ticks=element_blank()
    )
    p &amp;lt;- p + geom_polygon(fill=zcol, color=zcol)
    p &amp;lt;- p + geom_text(data=NULL, x=.5, y=.5, label=100*round(r, 2), size=6, col=textcol)
    ggp &amp;lt;- putPlot(ggp, p, i, j)
  }
}

for(j in 1:(N_col-1)) {
  for(i in (j+1):N_col) {
    x &amp;lt;- d[,j]
    y &amp;lt;- d[,i]
    p &amp;lt;- ggplot(data.frame(x, y), aes(x=x, y=y))
    p &amp;lt;- p + theme_bw(base_size=14)
    p &amp;lt;- p + theme(axis.text.x=element_text(angle=60, vjust=1, hjust=1))
    p &amp;lt;- p + geom_hex()
    p &amp;lt;- p + scale_fill_gradientn(colours=gray.colors(7, start=0.1, end=0.9))
    ggp &amp;lt;- putPlot(ggp, p, i, j)
  }
}
ggp&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:fig55&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-22-rstan-wonderful-r-multi-lm_files/figure-html/fig55-1.png&#34; alt=&#34;MCMC樣本的事後矩陣。對角線上是各個參數事後樣本的柱狀圖和相應的概率密度曲線。右上角是各個參數事後樣本之間的 Spearman 秩相關係數。左下角是兩兩的散點圖。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 7: MCMC樣本的事後矩陣。對角線上是各個參數事後樣本的柱狀圖和相應的概率密度曲線。右上角是各個參數事後樣本之間的 Spearman 秩相關係數。左下角是兩兩的散點圖。
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Simple linear regression using Rstan--Rstan Wonderful R-(2)</title>
      <link>https://wangcc.me/post/simple-linear-regression-using-rstan/</link>
      <pubDate>Tue, 15 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/simple-linear-regression-using-rstan/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#step-1-確認數據分佈&#34;&gt;Step 1, 確認數據分佈&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-2-描述線性模型&#34;&gt;Step 2, 描述線性模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-3-寫下stan模型&#34;&gt;Step 3, 寫下Stan模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-4-診斷stan貝葉斯模型的收斂程度&#34;&gt;Step 4, 診斷Stan貝葉斯模型的收斂程度&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-5修改mcmc條件設定&#34;&gt;Step 5，修改MCMC條件設定&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-6-並行平行計算的設定&#34;&gt;Step 6, 並行（平行）計算的設定&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-7-計算貝葉斯可信區間和貝葉斯預測區間&#34;&gt;Step 7, 計算貝葉斯可信區間和貝葉斯預測區間&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#練習題&#34;&gt;練習題&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap04/input/data-salary.txt&#34;&gt;數據 data-salary.txt&lt;/a&gt;是架空的。&lt;/p&gt;
&lt;p&gt;某公司社員的年齡 &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;（歲），和年收入 &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;（萬日元）的數據如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;X,Y
24,472
24,403
26,454
32,575
33,546
35,781
38,750
40,601
40,814
43,792
43,745
44,837
48,868
52,988
56,1092
56,1007
57,1233
58,1202
59,1123
59,1314
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;年收入 &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; 被認爲是由基本年收 &lt;span class=&#34;math inline&#34;&gt;\(y_{base}\)&lt;/span&gt; 和其他影響因素 &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt; 構成。由於該公司是典型的年功序列式的日本傳統企業，所以基本年收本身和社員年齡成正比例。 &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt; 則被認爲是由該員工當年的業績等隨機誤差造成的，但是所有員工的 &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt; 的均值被認爲是零。&lt;/p&gt;
&lt;p&gt;g分析目的：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;借用這個數據來分析並回答如下的問題：在該公司如果採用了一名50歲的員工，他/她的年收入的預期值會是多少。&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;step-1-確認數據分佈&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 1, 確認數據分佈&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Salary &amp;lt;- read.table(&amp;quot;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap04/input/data-salary.txt&amp;quot;, 
                     sep = &amp;quot;,&amp;quot;, header = T)
library(ggplot2)

ggplot(Salary, aes(x = X, y = Y)) + 
  geom_point(shape = 1, size = 4)  + theme(plot.subtitle = element_text(vjust = 1), 
    plot.caption = element_text(vjust = 1), 
    axis.line = element_line(colour = &amp;quot;bisque4&amp;quot;, 
        size = 0.2, linetype = &amp;quot;solid&amp;quot;), 
    axis.ticks = element_line(size = 0.7), 
    axis.title = element_text(size = 16), 
    axis.text = element_text(size = 16, colour = &amp;quot;gray0&amp;quot;), 
    panel.background = element_rect(fill = &amp;quot;gray98&amp;quot;)) +
  scale_y_continuous(limits = c(200, 1400), breaks = c(200, 600, 1000, 1400))&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step1&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-15-simple-linear-regression-using-rstan_files/figure-html/step1-1.png&#34; alt=&#34;橫軸爲 $X$，縱軸爲 $Y$ 的散點圖&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: 橫軸爲 &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;，縱軸爲 &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; 的散點圖
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;從這個散點圖的特徵可以看出年收入確實似乎和年齡呈線性正相關。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-2-描述線性模型&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 2, 描述線性模型&lt;/h2&gt;
&lt;p&gt;這個簡單線性回歸模型的數學表達式可以描述如下：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{array}{l}
Y[n]        = y_{base}[n] + \varepsilon [n]&amp;amp;  n = 1,2,\dots,N \\
y_{base}[n] = a + bX[n]                    &amp;amp;  n = 1,2,\dots,N \\
\varepsilon[n] \sim \text{Normal}(0, \sigma) &amp;amp; n = 1,2,\dots,N \\ 
\end{array}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;同樣的模型你可以簡化描述成爲：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y[n] \sim \text{Normal}(a + bX[n], \sigma)\;\; n = 1,2,\dots,N
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;那麼如果一個統計師只有經過傳統概率論觀點的訓練，他/她會在R裏面這樣來分析這個數據：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res_lm &amp;lt;- lm(Y ~ X, data = Salary)
summary(res_lm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Y ~ X, data = Salary)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -155.471  -51.523   -6.663   52.822  141.349 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) -119.697     68.148  -1.756    0.096 .  
## X             21.904      1.518  14.428 2.47e-11 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 79.1 on 18 degrees of freedom
## Multiple R-squared:  0.9204, Adjusted R-squared:  0.916 
## F-statistic: 208.2 on 1 and 18 DF,  p-value: 2.466e-11&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 用這個線性回歸模型來對上面模型中的參數作出預測：

X_new &amp;lt;- data.frame(X=23:60)
conf_95 &amp;lt;- predict(res_lm, X_new, interval = &amp;quot;confidence&amp;quot;, level = 0.95)
pred_95 &amp;lt;- predict(res_lm, X_new, interval = &amp;quot;prediction&amp;quot;, level = 0.95)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;temp_var &amp;lt;- predict(res_lm, interval=&amp;quot;prediction&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in predict.lm(res_lm, interval = &amp;quot;prediction&amp;quot;): predictions on current data refer to _future_ responses&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_df &amp;lt;- cbind(Salary, temp_var)

ggplot(new_df, aes(x = X, y = Y)) + 
  geom_point(shape = 1, size = 4)  + theme(plot.subtitle = element_text(vjust = 1), 
    plot.caption = element_text(vjust = 1), 
    axis.line = element_line(colour = &amp;quot;bisque4&amp;quot;, 
        size = 0.2, linetype = &amp;quot;solid&amp;quot;), 
    axis.ticks = element_line(size = 0.7), 
    axis.title = element_text(size = 16), 
    axis.text = element_text(size = 16, colour = &amp;quot;gray0&amp;quot;), 
    panel.background = element_rect(fill = &amp;quot;gray98&amp;quot;)) + 
  geom_smooth(method = lm, se=TRUE, size = 0.3)+
  scale_y_continuous(limits = c(200, 1400), breaks = c(200, 600, 1000, 1400)) +
   geom_line(aes(y=lwr), color = &amp;quot;red&amp;quot;, linetype = &amp;quot;dashed&amp;quot;)+
    geom_line(aes(y=upr), color = &amp;quot;red&amp;quot;, linetype = &amp;quot;dashed&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step2&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-15-simple-linear-regression-using-rstan_files/figure-html/step2-1.png&#34; alt=&#34;用簡單線性回歸模型計算的基本年收的信賴區間(灰色陰影)和預測區間(紅色點線)。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: 用簡單線性回歸模型計算的基本年收的信賴區間(灰色陰影)和預測區間(紅色點線)。
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;step-3-寫下stan模型&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 3, 寫下Stan模型&lt;/h2&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;data {
    int N; 
    real X[N]; 
    real Y[N];
}

parameters {
    real a;
    real b;
    real&amp;lt;lower=0&amp;gt; sigma;
}

model {
    for(n in 1:N) {
        Y[n] ~ normal(a + b*X[n], sigma);
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;參數部分 &lt;code&gt;real&amp;lt;lower=0&amp;gt; sigma&lt;/code&gt; 的代碼表示標準差不可採集負數作爲樣本。&lt;/p&gt;
&lt;p&gt;實際運行上面的Stan代碼：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rstan)
data &amp;lt;- list(N=nrow(Salary), X=Salary$X, Y = Salary$Y)
fit &amp;lt;- sampling(model4_5, data, seed = 1234) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &amp;#39;5b73686886069c0bad70513d4ea4141a&amp;#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 7e-06 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.066103 seconds (Warm-up)
## Chain 1:                0.038316 seconds (Sampling)
## Chain 1:                0.104419 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &amp;#39;5b73686886069c0bad70513d4ea4141a&amp;#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 3e-06 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.060925 seconds (Warm-up)
## Chain 2:                0.040855 seconds (Sampling)
## Chain 2:                0.10178 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &amp;#39;5b73686886069c0bad70513d4ea4141a&amp;#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 3e-06 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.055785 seconds (Warm-up)
## Chain 3:                0.038069 seconds (Sampling)
## Chain 3:                0.093854 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &amp;#39;5b73686886069c0bad70513d4ea4141a&amp;#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 3e-06 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.057673 seconds (Warm-up)
## Chain 4:                0.04921 seconds (Sampling)
## Chain 4:                0.106883 seconds (Total)
## Chain 4:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Stan model: 5b73686886069c0bad70513d4ea4141a.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##          mean se_mean    sd    2.5%     25%     50%    75%  97.5% n_eff Rhat
## a     -121.53    2.05 75.97 -270.45 -167.02 -120.34 -73.00  26.46  1379    1
## b       21.96    0.05  1.69   18.71   20.84   21.93  23.00  25.30  1350    1
## sigma   85.09    0.37 15.38   61.62   73.63   83.07  94.33 121.28  1697    1
## lp__   -93.63    0.04  1.31  -96.87  -94.24  -93.29 -92.66 -92.13  1045    1
## 
## Samples were drawn using NUTS(diag_e) at Tue Nov 26 15:06:39 2019.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;輸出結果的前三行，是該次MCMC的設定條件，其中模型名稱是Rmarkdown文件中隨機產生的。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;第二行則說明的是該次MCMC進行了4條鏈的採樣，每條鏈2000次，其中前1000次被當作是 burn-in (或者叫 warmup)。可以看到一共獲得了4000個事後樣本。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;接下來的五行是參數的事後樣本的事後分析總結，一共有11列。
&lt;ul&gt;
&lt;li&gt;第1列是參數名稱，最後一個 &lt;code&gt;lp__&lt;/code&gt;是Stan特有的算法得到的產物，具體解釋爲對數事後概率 (log posterior)，當然它也需要得到收斂才行。&lt;/li&gt;
&lt;li&gt;第2列是獲得的4000個參數的事後樣本的事後平均值(posterior mean)。例如&lt;code&gt;b&lt;/code&gt;（回歸直線的斜率）的事後平均值是21.96，也就是說年齡每增加一歲，基本年收入平均增加21.96萬日元。你可以和之前的概率論算法相比較(&lt;code&gt;b = 21.904&lt;/code&gt;)。&lt;/li&gt;
&lt;li&gt;第3列&lt;code&gt;se_mean&lt;/code&gt;是事後平均值的標準誤(standard error of posterior mean)。說白了是MCMC事後樣本的方差除以第10列的有效樣本量&lt;code&gt;n_eff&lt;/code&gt;之後取根號獲得的值。&lt;/li&gt;
&lt;li&gt;第4列&lt;code&gt;sd&lt;/code&gt;是MCMC事後樣本的標準差(standard deviation of posterior MCMC sample)。&lt;/li&gt;
&lt;li&gt;第5-9列是MCMC事後樣本的四分位點。也就是貝葉斯統計算法獲得的事後可信區間。&lt;/li&gt;
&lt;li&gt;第10列&lt;code&gt;n_eff&lt;/code&gt;是Stan在基於事後樣本自相關程度來判斷的有效事後樣本量大小。爲了有效地計算和繪製事後分佈的統計量，這個有效樣本量需要至少有100個以上吧（作者觀點）。如果報告給出的事後有效樣本量過小的話也是模型收斂不佳的表現之一。&lt;/li&gt;
&lt;li&gt;第11列&lt;code&gt;Rhat&lt;/code&gt;&lt;span class=&#34;math inline&#34;&gt;\((\hat R)\)&lt;/span&gt;是主要用於判斷模型是否達到收斂的重要指標，每個參數都會被計算一個&lt;code&gt;Rhat&lt;/code&gt;值。當MCMC鏈條數在3以上，且同時所有的模型參數的 &lt;code&gt;Rhat &amp;lt; 1.1&lt;/code&gt;的話，可以認爲模型達到了良好的收斂。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;step-4-診斷stan貝葉斯模型的收斂程度&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 4, 診斷Stan貝葉斯模型的收斂程度&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggmcmc)

ggmcmc(ggs(fit, inc_warmup = TRUE, stan_include_auxiliar = TRUE), plot = &amp;quot;traceplot&amp;quot;, dev_type_html = &amp;quot;png&amp;quot;,        file = &amp;quot;trace.html&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;上面的代碼，會自動生成四個模型參數的軌跡MCMC鏈式圖報告。&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step4&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/img/traceplot-model4-5.png&#34; alt=&#34;用ggmcmc函數製作而成的MCMC鏈式圖報告。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: 用ggmcmc函數製作而成的MCMC鏈式圖報告。
&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(bayesplot)

color_scheme_set(&amp;quot;mix-brightblue-gray&amp;quot;)

posterior2 &amp;lt;- rstan::extract(fit, inc_warmup = TRUE, permuted = FALSE)

p &amp;lt;- mcmc_trace(posterior2, n_warmup = 0,
                facet_args = list(nrow = 2, labeller = label_parsed))
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step41&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-15-simple-linear-regression-using-rstan_files/figure-html/step41-1.png&#34; alt=&#34;用 bayesplot包數繪製的MCMC鏈式圖。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4: 用 bayesplot包數繪製的MCMC鏈式圖。
&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- mcmc_acf_bar(posterior2)
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step42&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-15-simple-linear-regression-using-rstan_files/figure-html/step42-1.png&#34; alt=&#34;用 bayesplot包數繪製的事後樣本自相關圖(autocorrelation)。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 5: 用 bayesplot包數繪製的事後樣本自相關圖(autocorrelation)。
&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- mcmc_dens_overlay(posterior2, color_chains = T)
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step43&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-15-simple-linear-regression-using-rstan_files/figure-html/step43-1.png&#34; alt=&#34;用 bayesplot包數繪製的事後樣本密度分佈圖。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 6: 用 bayesplot包數繪製的事後樣本密度分佈圖。
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;step-5修改mcmc條件設定&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 5，修改MCMC條件設定&lt;/h2&gt;
&lt;p&gt;進行貝葉斯模型擬合的過程中，常常需要不停地修改模型的條件，例如縮短warm-up等。下面的Rstan代碼可以實現簡便地頻繁修改MCMC條件設定：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# library(rstan) uncomment if run for the first time
data &amp;lt;- list(N=nrow(Salary), X=Salary$X, Y = Salary$Y)
fit2 &amp;lt;- sampling(
    model4_5, 
    data = data, 
    pars = c(&amp;quot;b&amp;quot;, &amp;quot;sigma&amp;quot;), 
    init = function(){
      list(a = runif(1, -10, 10), b = runif(1, 0, 10), sigma = 10)
    },
    seed = 123,
    chains = 3, iter = 1000, warmup = 200, thin = 2
) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &amp;#39;5b73686886069c0bad70513d4ea4141a&amp;#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 4e-06 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 1: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 1: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 1: Iteration: 201 / 1000 [ 20%]  (Sampling)
## Chain 1: Iteration: 300 / 1000 [ 30%]  (Sampling)
## Chain 1: Iteration: 400 / 1000 [ 40%]  (Sampling)
## Chain 1: Iteration: 500 / 1000 [ 50%]  (Sampling)
## Chain 1: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 1: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 1: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 1: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 1: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.031364 seconds (Warm-up)
## Chain 1:                0.020284 seconds (Sampling)
## Chain 1:                0.051648 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &amp;#39;5b73686886069c0bad70513d4ea4141a&amp;#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 3e-06 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 2: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 2: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 2: Iteration: 201 / 1000 [ 20%]  (Sampling)
## Chain 2: Iteration: 300 / 1000 [ 30%]  (Sampling)
## Chain 2: Iteration: 400 / 1000 [ 40%]  (Sampling)
## Chain 2: Iteration: 500 / 1000 [ 50%]  (Sampling)
## Chain 2: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 2: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 2: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 2: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 2: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.027437 seconds (Warm-up)
## Chain 2:                0.031304 seconds (Sampling)
## Chain 2:                0.058741 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &amp;#39;5b73686886069c0bad70513d4ea4141a&amp;#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 3e-06 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 3: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 3: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 3: Iteration: 201 / 1000 [ 20%]  (Sampling)
## Chain 3: Iteration: 300 / 1000 [ 30%]  (Sampling)
## Chain 3: Iteration: 400 / 1000 [ 40%]  (Sampling)
## Chain 3: Iteration: 500 / 1000 [ 50%]  (Sampling)
## Chain 3: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 3: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 3: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 3: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 3: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.037999 seconds (Warm-up)
## Chain 3:                0.032879 seconds (Sampling)
## Chain 3:                0.070878 seconds (Total)
## Chain 3:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Stan model: 5b73686886069c0bad70513d4ea4141a.
## 3 chains, each with iter=1000; warmup=200; thin=2; 
## post-warmup draws per chain=400, total post-warmup draws=1200.
## 
##         mean se_mean    sd   2.5%    25%    50%    75%  97.5% n_eff Rhat
## b      21.87    0.07  1.72  18.43  20.76  21.89  23.00  25.13   694    1
## sigma  85.13    0.53 15.70  60.14  74.45  82.93  93.48 119.94   887    1
## lp__  -93.68    0.05  1.34 -97.05 -94.29 -93.39 -92.70 -92.16   601    1
## 
## Samples were drawn using NUTS(diag_e) at Tue Nov 26 15:06:44 2019.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中&lt;code&gt;fit&lt;/code&gt;的最後一行是修改各種條件的示例：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;chains&lt;/code&gt;至少要三條；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;iter&lt;/code&gt;一開始可以設定在500~1000左右，確定模型可以收斂以後，再加大這個數值以獲得穩定的事後統計量，多多益善；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;warmup&lt;/code&gt;，也就MCMC採樣開始後多少樣本可以丟棄。這個數值需要參考trace plot；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;thin&lt;/code&gt;，通常只需要保持默認值 1。和WinBUGS, JAGS相比Stan算法採集的事後樣本自相關比較低。&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;step-6-並行平行計算的設定&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 6, 並行（平行）計算的設定&lt;/h2&gt;
&lt;p&gt;如果你寫出來的貝葉斯模型需要很長時間的計算和收斂，可以充分利用你的計算機的多核計算，把每條MCMC鏈單獨進行計算加速這個過程：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;parallel::detectCores() #我的桌上型電腦有8個核可以用於平行計算&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;但是平行計算時如果計算中出錯則由於每條鏈都是相互獨立地進行，報錯就減少了。所以如果要使用多核同時計算的話，建議先減少採樣數，確認不會報錯以後再用多核平行計算增加採樣量。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;step-7-計算貝葉斯可信區間和貝葉斯預測區間&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 7, 計算貝葉斯可信區間和貝葉斯預測區間&lt;/h2&gt;
&lt;p&gt;這一步就又回到一開始提出的研究問題上來，我們來計算基本年收的貝葉斯可信區間和貝葉斯預測區間。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ms &amp;lt;- rstan::extract(fit)

quantile(ms$b, probs = c(0.025, 0.975))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     2.5%    97.5% 
## 18.71095 25.29837&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d_mcmc &amp;lt;- data.frame(a = ms$a, b = ms$b, sigma = ms$sigma)

head(d_mcmc)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             a        b    sigma
## 1 -188.336995 23.54553 78.05735
## 2 -158.530408 22.41580 79.71338
## 3 -208.500067 24.04350 92.61271
## 4 -138.796486 22.10719 66.83120
## 5   -5.871562 19.36954 75.66852
## 6  -55.023869 20.74697 66.57393&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1 &amp;lt;- ggplot(d_mcmc, aes(x = a, y = b)) + 
 geom_point(shape = 1, size = 4)

ggExtra::ggMarginal(
  p = p1,
  type = &amp;#39;density&amp;#39;,
  margins = &amp;#39;both&amp;#39;,
  size = 4,
  colour = &amp;#39;black&amp;#39;,
  fill = &amp;#39;#2D077A&amp;#39;
)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step71&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-15-simple-linear-regression-using-rstan_files/figure-html/step71-1.png&#34; alt=&#34;MCMC樣本的兩個模型參數的事後散點圖，及它們之間的邊緣分佈密度圖。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 7: MCMC樣本的兩個模型參數的事後散點圖，及它們之間的邊緣分佈密度圖。
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;從圖&lt;a href=&#34;#fig:step71&#34;&gt;7&lt;/a&gt;中可觀察到該貝葉斯線性模型獲得的事後模型參數樣本中，截距&lt;code&gt;a&lt;/code&gt;，和斜率&lt;code&gt;b&lt;/code&gt;之間呈極強的負相關關係。也就是說，截距是工資的起點（年齡爲0歲時），這個起點的理論值越低，斜率越大（歲年齡增加工資上升的速度越大）。&lt;/p&gt;
&lt;p&gt;根據上面分析的結果，下面的R代碼可以計算一名50歲的人被這家公司採用的時候，她/他的預期基本年收入的分佈（中獲得的MCMC樣本），和她/他的預期總年收的預測分佈（中獲得的MCMC樣本）。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;N_mcmc &amp;lt;- length(ms$lp__)
y50_base &amp;lt;- ms$a + ms$b*50
y50 &amp;lt;- rnorm(n = N_mcmc, mean = y50_base, sd = ms$sigma)
d_mcmc &amp;lt;- data.frame(a = ms$a, b = ms$b, sigma = ms$sigma, y50_base, y50)
head(d_mcmc)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             a        b    sigma y50_base       y50
## 1 -188.336995 23.54553 78.05735 988.9397  911.2037
## 2 -158.530408 22.41580 79.71338 962.2597 1015.4449
## 3 -208.500067 24.04350 92.61271 993.6750 1043.2207
## 4 -138.796486 22.10719 66.83120 966.5631  935.2792
## 5   -5.871562 19.36954 75.66852 962.6053  890.3167
## 6  -55.023869 20.74697 66.57393 982.3246 1014.2237&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# the following codes are also available from the author&amp;#39;s page:
# https://github.com/MatsuuraKentaro/RStanBook/blob/master/chap04/fig4-8.R
# library(ggplot2)
source(&amp;#39;commonRstan.R&amp;#39;)

# load(&amp;#39;output/result-model4-5.RData&amp;#39;)
ms &amp;lt;- rstan::extract(fit)

X_new &amp;lt;- 23:60
N_X &amp;lt;- length(X_new)
N_mcmc &amp;lt;- length(ms$lp__)

set.seed(1234)
y_base_mcmc &amp;lt;- as.data.frame(matrix(nrow=N_mcmc, ncol=N_X))
y_mcmc &amp;lt;- as.data.frame(matrix(nrow=N_mcmc, ncol=N_X))
for (i in 1:N_X) {
  y_base_mcmc[,i] &amp;lt;- ms$a + ms$b * X_new[i]
  y_mcmc[,i] &amp;lt;- rnorm(n=N_mcmc, mean=y_base_mcmc[,i], sd=ms$sigma)
}

customize.ggplot.axis &amp;lt;- function(p) {
  p &amp;lt;- p + labs(x=&amp;#39;X&amp;#39;, y=&amp;#39;Y&amp;#39;)
  p &amp;lt;- p + scale_y_continuous(breaks=seq(from=200, to=1400, by=400))
  p &amp;lt;- p + coord_cartesian(xlim=c(22, 61), ylim=c(200, 1400))
  return(p)
}

d_est &amp;lt;- data.frame.quantile.mcmc(x=X_new, y_mcmc=y_base_mcmc)
p &amp;lt;- ggplot.5quantile(data=d_est)
p &amp;lt;- p + geom_point(data=Salary, aes(x=X, y=Y), shape=1, size=3)
p &amp;lt;- customize.ggplot.axis(p)
# ggsave(file=&amp;#39;output/fig4-8-left.png&amp;#39;, plot=p, dpi=300, w=4, h=3)
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step72&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-15-simple-linear-regression-using-rstan_files/figure-html/step72-1.png&#34; alt=&#34;MCMC樣本計算獲得的基本年收的貝葉斯可信區間。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 8: MCMC樣本計算獲得的基本年收的貝葉斯可信區間。
&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d_est &amp;lt;- data.frame.quantile.mcmc(x=X_new, y_mcmc=y_mcmc)
p &amp;lt;- ggplot.5quantile(data=d_est)
p &amp;lt;- p + geom_point(data=Salary, aes(x=X, y=Y), shape=1, size=3)
p &amp;lt;- customize.ggplot.axis(p)
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step73&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-15-simple-linear-regression-using-rstan_files/figure-html/step73-1.png&#34; alt=&#34;MCMC樣本計算獲得的預期總年收的貝葉斯預測區間。（顏色較深的是50%預測區間帶，黑線是事後樣本的中央值）&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 9: MCMC樣本計算獲得的預期總年收的貝葉斯預測區間。（顏色較深的是50%預測區間帶，黑線是事後樣本的中央值）
&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# ggsave(file=&amp;#39;output/fig4-8-right.png&amp;#39;, plot=p, dpi=300, w=4, h=3)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;練習題&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;練習題&lt;/h2&gt;
&lt;p&gt;用模擬數據來嘗試進行貝葉斯t檢驗&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)
N1 &amp;lt;- 30
N2 &amp;lt;- 20
Y1 &amp;lt;- rnorm(n=N1, mean=0, sd=5)
Y2 &amp;lt;- rnorm(n=N2, mean=1, sd=4)&lt;/code&gt;&lt;/pre&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;請繪製上面代碼生成的兩組數據的示意圖&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d1 &amp;lt;- data.frame(group=1, Y=Y1)
d2 &amp;lt;- data.frame(group=2, Y=Y2)
d &amp;lt;- rbind(d1, d2)
d$group &amp;lt;- as.factor(d$group)

p &amp;lt;- ggplot(data=d, aes(x=group, y=Y, group=group, col=group))
p &amp;lt;- p + geom_boxplot(outlier.size=0)
p &amp;lt;- p + geom_point(position=position_jitter(w=0.4, h=0), size=2)
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:exe11&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-15-simple-linear-regression-using-rstan_files/figure-html/exe11-1.png&#34; alt=&#34;隨機生成的兩組數據的散點圖和箱式圖。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 10: 隨機生成的兩組數據的散點圖和箱式圖。
&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#ggsave(file=&amp;#39;fig-ex1.png&amp;#39;, plot=p, dpi=300, w=4, h=3)&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;寫下相當於t檢驗的數學式，表示各組之間方差或者標準差如果相等時，均值比較的檢驗模型。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;hypotheses:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;observations in each group follow a normal distribution&lt;/li&gt;
&lt;li&gt;all observations are independent&lt;/li&gt;
&lt;li&gt;The two population variance/standard deviations are known (and can be considered equal)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{H}_0: \mu_2 - \mu_1 = 0 \\
\text{H}_1: \mu_2 - \mu_1 \neq 0 \\ 
\text{If H}_0 \text{ is true, then:} \\
Z=\frac{\bar{Y_2} - \bar{Y_1}}{\sqrt{(\sigma_2^2/n_2) + (\sigma_1^2/n_1)}} \\
\text{follows a standard normal distribution with zero mean} \\
\Rightarrow \text{ if two variances are considered the same}\\ 
Y_1[n] \sim N(\mu_1, \sigma) \;\; n = 1,2,\dots,N \\
Y_2[n] \sim N(\mu_2, \sigma) \;\; n = 1,2,\dots,N
\]&lt;/span&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;寫下上一步模型的Stan代碼，並嘗試在R裏運行&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Stan代碼如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data {
  int N1;
  int N2;
  real Y1[N1];
  real Y2[N2];
}

parameters {
  real mu1;
  real mu2;
  real&amp;lt;lower=0&amp;gt; sigma;
}

model {
  for (n in 1:N1)
    Y1[n] ~ normal(mu1, sigma);
  for (n in 1:N2)
    Y2[n] ~ normal(mu2, sigma);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;R代碼如下：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rstan)
data &amp;lt;- list(N1=N1, N2=N2, Y1=Y1, Y2=Y2)
exe13 &amp;lt;- stan_model(file = &amp;quot;stanfiles/ex3.stan&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## recompiling to avoid crashing R session&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit &amp;lt;- sampling(exe13, data=data, seed=1234)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &amp;#39;ex3&amp;#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 9e-06 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.021914 seconds (Warm-up)
## Chain 1:                0.017258 seconds (Sampling)
## Chain 1:                0.039172 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &amp;#39;ex3&amp;#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 4e-06 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.024198 seconds (Warm-up)
## Chain 2:                0.058766 seconds (Sampling)
## Chain 2:                0.082964 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &amp;#39;ex3&amp;#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 4e-06 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.021824 seconds (Warm-up)
## Chain 3:                0.020478 seconds (Sampling)
## Chain 3:                0.042302 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &amp;#39;ex3&amp;#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 5e-06 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.037682 seconds (Warm-up)
## Chain 4:                0.021954 seconds (Sampling)
## Chain 4:                0.059636 seconds (Total)
## Chain 4:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Stan model: ex3.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##         mean se_mean   sd    2.5%    25%    50%    75%  97.5% n_eff Rhat
## mu1    -0.24    0.01 0.83   -1.90  -0.79  -0.23   0.32   1.36  3550    1
## mu2     1.62    0.02 1.00   -0.29   0.93   1.62   2.28   3.62  3606    1
## sigma   4.49    0.01 0.46    3.69   4.17   4.44   4.77   5.52  3499    1
## lp__  -97.74    0.03 1.27 -100.95 -98.33 -97.40 -96.83 -96.33  1896    1
## 
## Samples were drawn using NUTS(diag_e) at Tue Nov 26 15:27:48 2019.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;從獲取到的事後參數的MCMC樣本計算 &lt;span class=&#34;math inline&#34;&gt;\(\text{Prob}[\mu_1 &amp;lt; \mu_2]\)&lt;/span&gt;：&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ms &amp;lt;- extract(fit)
prob &amp;lt;- mean(ms$mu1 &amp;lt; ms$mu2)  #=&amp;gt; 0.932
prob&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.932&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;所以可以認爲地一組均值，小於第二組均值的事後概率是93.2%&lt;/p&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;如果不能認爲兩組的方差相等的話，模型又該改成什麼樣子？&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y_1[n] \sim N(\mu_1, \sigma_1) \;\; n = 1,2,\dots,N \\
Y_2[n] \sim N(\mu_2, \sigma_2) \;\; n = 1,2,\dots,N
\]&lt;/span&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data {
  int N1;
  int N2;
  real Y1[N1];
  real Y2[N2];
}

parameters {
  real mu1;
  real mu2;
  real&amp;lt;lower=0&amp;gt; sigma1;
  real&amp;lt;lower=0&amp;gt; sigma2;
}

model {
  for (n in 1:N1)
    Y1[n] ~ normal(mu1, sigma1);
  for (n in 1:N2)
    Y2[n] ~ normal(mu2, sigma2);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;下面的代碼相當於實施Welch的t檢驗：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rstan)
data &amp;lt;- list(N1=N1, N2=N2, Y1=Y1, Y2=Y2)
exe15 &amp;lt;- stan_model(file = &amp;quot;stanfiles/ex5.stan&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## recompiling to avoid crashing R session&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit &amp;lt;- sampling(exe15, data=data, seed=1234)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &amp;#39;ex5&amp;#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 9e-06 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.023394 seconds (Warm-up)
## Chain 1:                0.028188 seconds (Sampling)
## Chain 1:                0.051582 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &amp;#39;ex5&amp;#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 6e-06 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.032678 seconds (Warm-up)
## Chain 2:                0.037585 seconds (Sampling)
## Chain 2:                0.070263 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &amp;#39;ex5&amp;#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 4e-06 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.027502 seconds (Warm-up)
## Chain 3:                0.019927 seconds (Sampling)
## Chain 3:                0.047429 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &amp;#39;ex5&amp;#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 4e-06 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.023821 seconds (Warm-up)
## Chain 4:                0.019475 seconds (Sampling)
## Chain 4:                0.043296 seconds (Total)
## Chain 4:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Stan model: ex5.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##          mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat
## mu1     -0.24    0.02 0.93  -2.07  -0.87  -0.23   0.39   1.62  3671    1
## mu2      1.64    0.01 0.85  -0.06   1.09   1.63   2.19   3.33  3657    1
## sigma1   5.12    0.01 0.69   3.98   4.63   5.04   5.53   6.68  3808    1
## sigma2   3.63    0.01 0.65   2.63   3.16   3.54   3.99   5.15  3226    1
## lp__   -95.37    0.03 1.44 -98.86 -96.13 -95.04 -94.28 -93.52  1732    1
## 
## Samples were drawn using NUTS(diag_e) at Tue Nov 26 15:28:31 2019.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ms &amp;lt;- rstan::extract(fit)
prob &amp;lt;- mean(ms$mu1 &amp;lt; ms$mu2)  #=&amp;gt; 0.93725
prob&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.93725&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Rstan Wonderful R-(1)</title>
      <link>https://wangcc.me/post/rstan-wonderful-r/</link>
      <pubDate>Thu, 22 Nov 2018 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/rstan-wonderful-r/</guid>
      <description>


&lt;p&gt;P16&lt;/p&gt;
&lt;p&gt;事後分布 &lt;span class=&#34;math inline&#34;&gt;\(p(\theta | Y)\)&lt;/span&gt;の値が最大になる点&lt;span class=&#34;math inline&#34;&gt;\(\theta^*\)&lt;/span&gt;を事後確率最大推定値 (maximum a posteriori estimate)と呼ぶ．略してMAP推定値 (MAP estimate)．&lt;/p&gt;
&lt;p&gt;我們把能夠將事後概率分布取極大值的參數點 &lt;span class=&#34;math inline&#34;&gt;\(\theta^*\)&lt;/span&gt; 稱爲事後概率的最大似然估計值 (maximum a posteriori estimate)，簡稱 MAP估計值 (MAP estimate)。&lt;/p&gt;
&lt;p&gt;P19&lt;/p&gt;
&lt;p&gt;統計建模的一般順序&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;確定分析目的&lt;/li&gt;
&lt;li&gt;確定數據分布&lt;/li&gt;
&lt;li&gt;想象數據產生本身的機制：思考數據與數據之間可能的關系&lt;/li&gt;
&lt;li&gt;寫下你所認爲的數據模型的數學表達式&lt;/li&gt;
&lt;li&gt;用 R 模擬(simulation)並確認前一步寫下的數學模型的性質，特點&lt;/li&gt;
&lt;li&gt;用 Stan 實際進行模型參數的推斷&lt;/li&gt;
&lt;li&gt;獲得推斷結果，解釋其事後概率分布的意義，繪制易於理解的模型示意圖&lt;/li&gt;
&lt;li&gt;繪制成功之後的模型示意圖和最先使用的模型之間進行比對，重新查缺補漏&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;P23&lt;/p&gt;
&lt;p&gt;ただいたずらにモデルを複雑化させるのは解釈のしにくさを招く．&lt;/p&gt;
&lt;p&gt;P30&lt;/p&gt;
&lt;p&gt;最初にmodel ブロックの尤度の部分（と事前分布の部分）を書く．その尤度の部分に登場した変数のうち，データの変数をdataブロックに，残りの変数をparametersブロックに書いていく．&lt;/p&gt;
&lt;p&gt;Stan的基本文法構成&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data {
 數據描述
}

parameters {
 想要進行MCMC事後樣本採集的參數描述
}

model {
 p(Y|theta) 似然的描述
 先驗概率分布 p(theta) 的描述
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;把下面的模型&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
Y[n] &amp;amp; \sim \text{Normal}(\mu, 1) \;\; n = 1, \dots, N \\
\mu  &amp;amp; \sim \text{Normal}(0, 100)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;翻譯成爲 Stan 模型語言是：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data {
  int N;
  real Y[N];
}

parameters {
  real mu;
}

model {
  for (n in 1:N) {
    Y[n] ~ normal(mu, 1);
  }
  mu ~ normal(0, 100);
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中我們按照實際模型書寫的順序 model -&amp;gt; data -&amp;gt; parameter 來逐個解釋：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;model&lt;/code&gt; 模塊中 &lt;code&gt;for (n in 1:N)&lt;/code&gt; 開始的循環部分（三行）對應數學模型的 $Y[n] (, 1) n = 1, , N $　部分。&lt;/li&gt;
&lt;li&gt;Stan 語言中，每一行描述的結尾需要用分號 &lt;code&gt;;&lt;/code&gt; 來結束。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mu ~ normal(0,100)&lt;/code&gt; 則對應數學模型中寫的先驗概率 &lt;span class=&#34;math inline&#34;&gt;\(\mu \sim \text{Normal}(0, 100)\)&lt;/span&gt; 部分。這裏給均值的先驗概率分佈是一個方差很大的無信息先驗概率分佈 (noninformative prior)。事實上在 Stan 軟件語言中，如果不特別指出先驗概率分佈，系統會默認給參數以無信息的先驗概率分佈，這樣即使沒有這一行，模型也是可以跑的。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;data&lt;/code&gt; 模塊中寫明的是 &lt;code&gt;model&lt;/code&gt; 模塊中描述的模型將要使用的數據。它包括宣示數據的個數（樣本量 &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;），以及數據本身。其中 &lt;code&gt;int N&lt;/code&gt; 意爲樣本量的數量是整數個 (integer)，&lt;code&gt;real Y[N]&lt;/code&gt; 則宣示實數有 N 個作爲數據。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;parameter&lt;/code&gt; 模塊是告訴軟件需要採樣且關注的未知參數 (parameter) 是 &lt;code&gt;mu&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;在 Stan 語言中，還可以和其他語言一樣爲模型加註解釋的文字，只需要在想要做註釋的文字最開始的部分增加 &lt;code&gt;//&lt;/code&gt;，如果註釋的文字超過一行，那麼在註釋的模塊前後加上 &lt;code&gt;/*&lt;/code&gt; 和 &lt;code&gt;*/&lt;/code&gt; 即可。&lt;/li&gt;
&lt;li&gt;另外，目前爲止主流的貝葉斯模型軟件中使用精確度 (precision) ，也就是方差的倒數來描述正態分佈 &lt;code&gt;normal(mean, 1/variance)&lt;/code&gt; ，但是在Stan的語法中使用的是 &lt;code&gt;normal(mean, sd)&lt;/code&gt;，也就是用標準差來描述正態分佈。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;寫Stan（或者說寫大多數的代碼）時，請遵守以下的原則：&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;適當縮進，以便於閱讀；&lt;/li&gt;
&lt;li&gt;表示數據的部分用大寫字母，表示參數的部分，用小寫字母；&lt;/li&gt;
&lt;li&gt;每個部分之間至少使用一個空行加以區分；&lt;/li&gt;
&lt;li&gt;請不要用&lt;code&gt;camelCase&lt;/code&gt;這樣的方式（單詞之間用大寫隔開），請在單詞之間用下劃線 &lt;code&gt;camel_case&lt;/code&gt; 的標記方法；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;~&lt;/code&gt;或者&lt;code&gt;=&lt;/code&gt;前後用一個字符大小的空格來隔開。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;最低限度的話，也請依照1,2兩個標準來書寫你的Stan代碼。不爲他人，也爲自己將來再讀代碼時能快速理解其涵義。往Stan的官方論壇投稿時，也必須遵守它們在手冊裏提供的 “Stan Program Style Guide” 代碼書寫規則，也是對其他寫，讀代碼的人的尊重。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Summer Project Schedule</title>
      <link>https://wangcc.me/post/summer-project-schedule/</link>
      <pubDate>Sun, 24 Jun 2018 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/summer-project-schedule/</guid>
      <description>&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Data analysis finish by 2018-07-&lt;/del&gt;24&lt;del&gt;31&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Paper structure confirm by 2018-08-01&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Paper draft complete by 2018-08-16&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;2018-06-24

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Read and try to repeat Rll&amp;rsquo;s method in R and familarize the dataset ASAP&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Two papers applying Repeated Measures LCA&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2018-06-25

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Meeting with supervisor and Susanna&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Confirm the cutoff of carborhydrate consumption&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Talk with Rll ask about the methodology and dataset&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2018-06-26

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Send the summarised memo of meeting to Supervisor and etc.&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Read the first part fundamentals of LCA.&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2018-06-27

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;a href=&#34;https://www.londonr.org/&#34; target=&#34;_blank&#34;&gt;London R in UCL&lt;/a&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; disabled class=&#34;task-list-item&#34;&gt; Germany lost their game against South Korea, UNBELIEVEABLE&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2018-06-28&lt;br /&gt;

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Read the book collins2010latent - Latent Class and Latent Transition Analysis: With Applications in the Social, Behavioral, and Health Sciences (Done until 4.2)&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Learn how to do LCA in R&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2018-06-29

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Read the book collins2010latent - Latent Class and Latent Transition Analysis: With Applications in the Social, Behavioral, and Health Sciences (Done until 4.3)&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Data management for NDNS 8 years data (70%)&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Learn how to do LCA in R&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Start to analysis the data according to the discussion on 25th(30%)&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Day1 data analysis results summary&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2018-07-01

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Relax and do nothing&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Buy some drink to enjoy the night with classmates(HB)&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2018-07-02/03

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Send some preliminary results to co-authors&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;&lt;a href=&#34;http://www.the-afc.com/competitions/fifa-world-cup/latest/news/japan-fa-president-proud-of-blue-samurai&#34; target=&#34;_blank&#34;&gt;Japan lost the game to Belgium, but they are the glory of Asia&amp;ndash;heartbreaking&lt;/a&gt;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2018-07-04&lt;br /&gt;

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;&amp;ldquo;consider separating weekdays from weekends if we are not averaging the four days?&amp;rdquo;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2018-07-05

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Test and confirm the availability of LCA in SAS&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Learn how to do LCA in SAS with NDNS data&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2018-07-06&lt;br /&gt;

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Learn how to do LCA with random effects in SAS&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Find whether there is any possibility of conducting the same method in R or STATA (no there is no way)&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2018-07-07~09

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;&amp;ldquo;Maybe we should try with the threshold at 25% only as per the existing guidelines (although those are per meal)?&amp;rdquo;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2018-07-10

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; ~~Meet with tutor;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; Start writing about the methodology;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; Try to start writing about the introduction;&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2018-07-11&lt;br /&gt;

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Try to summarise the meeting memo yesterday;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Re-analyse the data with new cut-off values (25, 50, 75);&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Re-analyse the data with new cut-off values (50);&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2018-07-12~22

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Use latent class growth analysis;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Use multilevel latent class analysis;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Think about the mathmatical theory behind the mixed LCA, write to PROC LCA group if necessary;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2018-07-23~25

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Learn about the survey package in R&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Finish writing about the methodology;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Write some introduction;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2018-07-26&lt;br /&gt;

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Let&amp;rsquo;s finish analysis of the classes and health outcomes.&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Read about the carbo-fibre ratio references.&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2018-08-15

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;PM review&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Finish most of the discussion outlines and 2 pages of them.&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2018-08-31

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Finish revising the report according to comments from LP and SAM;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Read RT&amp;rsquo;s report and send the comments;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Confirm the deadline for funding applications;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Prepare the abstract for conferences (UK and JP);&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Start preparing the paper for submit (MLCA part alone);&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Think about the schedules and plans after leaving London;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Finish the post of Scotland trip.&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>徒手打造一個假設檢驗</title>
      <link>https://wangcc.me/post/construction-of-a-hypothesis-test/</link>
      <pubDate>Wed, 08 Nov 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/construction-of-a-hypothesis-test/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#什麼是假設檢驗-hypothesis-testing&#34;&gt;什麼是假設檢驗 Hypothesis testing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#錯誤概率和效能方程&#34;&gt;錯誤概率和效能方程&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#如何選擇要檢驗的統計量&#34;&gt;如何選擇要檢驗的統計量&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#複合假設-composite-hypotheses&#34;&gt;複合假設 composite hypotheses&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#如何獲得反對零假設的證據-how-to-quantify-evidence-against-h_0&#34;&gt;如何獲得反對零假設的證據 how to quantify evidence against &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#雙側替代假設情況下雙側-p-值的定量方法&#34;&gt;雙側替代假設情況下，雙側 &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; 值的定量方法&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;什麼是假設檢驗-hypothesis-testing&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;什麼是假設檢驗 Hypothesis testing&lt;/h3&gt;
&lt;p&gt;一般來說，我們的&lt;strong&gt;假設&lt;/strong&gt;（或者叫&lt;strong&gt;假說&lt;/strong&gt;）是對與我們實驗觀察數據來自的總體（或人羣）的&lt;strong&gt;概率分佈&lt;/strong&gt;的描述。在參數檢驗的背景下，就是要檢驗描述這個總體（或人羣）的&lt;strong&gt;概率分佈&lt;/strong&gt;的參數 (parameters)。最典型的情況是，我們提出兩個互補的假設，一個叫作&lt;strong&gt;零假設&lt;/strong&gt;（或者叫&lt;strong&gt;原假設&lt;/strong&gt;），null hypothesis (&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;)；另一個是與之對應的（互補的）替代假設，althernative hypothesis (&lt;span class=&#34;math inline&#34;&gt;\(H_1/H_A\)&lt;/span&gt;)。&lt;/p&gt;
&lt;p&gt;例如，若 &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 是一個服從二項分佈的隨機離散變量 &lt;span class=&#34;math inline&#34;&gt;\(X\sim Bin(5, \theta)\)&lt;/span&gt;。可以考慮如下的零假設和替代假設：&lt;span class=&#34;math inline&#34;&gt;\(H_0: \theta=\frac{1}{2}; H_1: \theta=\frac{2}{3}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;當建立了零假設和替代假設以後，假設檢驗就是要建立如下的規則以確定：&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;從樣本中計算所得的參數估計值爲多少時，拒絕零假設。（接受替代假設爲“真”）&lt;/li&gt;
&lt;li&gt;從樣本中計算所得的參數估計值爲多少時，零假設不被拒絕。（接受零假設爲“真”）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;注意：（這一段很繞）&lt;/p&gt;
&lt;p&gt;上面的例子是零假設和替代假設均爲簡單假設的情況，實際操作中常常會設計更加複雜的（不對稱的）假設：即簡單的 &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;，複雜的 &lt;span class=&#34;math inline&#34;&gt;\(H_1\)&lt;/span&gt;。如此一來當零假設 &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; 不被拒絕時，我們並不一定就接受之。因爲無證據證明 &lt;span class=&#34;math inline&#34;&gt;\(H_1\)&lt;/span&gt; 不等於有證據證明 &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;。&lt;em&gt;&lt;strong&gt;(Absence of evidence is not evidence of absence).&lt;/strong&gt;&lt;/em&gt; 換句話說，無證據讓我們拒絕 &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; 本身並不成爲支持 &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; 爲“真”的證據。因爲在實際操作中，當我們設定的簡單的零假設沒有被拒絕，可能還存在其他符合樣本數據的零假設；相反地，當樣本數據的計算結果拒絕了零假設，我們只能接受替代假設。所以，反對零假設的證據，同時就是支持替代假設的證據。&lt;/p&gt;
&lt;p&gt;在樣本空間 sample space 中，決定了零假設 &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; 會被拒絕的子集 subset，被命名爲拒絕域 rejection region 或者 判別區域 critical region，用 &lt;span class=&#34;math inline&#34;&gt;\(\mathfrak{R}\)&lt;/span&gt; 來標記。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;錯誤概率和效能方程&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;錯誤概率和效能方程&lt;/h3&gt;
&lt;p&gt;這一部分可以參考之前&lt;a href=&#34;https://winterwang.github.io/post/sample-size-in-clinical-trial/&#34;&gt;臨牀試驗樣本量計算&lt;/a&gt;的部分。&lt;/p&gt;
&lt;table class=&#34;table table-striped table-bordered&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;caption&gt;
Table 1: Definition of Type I and Type II error
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;border-bottom:hidden&#34; colspan=&#34;2&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center; border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;&#34; colspan=&#34;2&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px;&#34;&gt;
SAMPLE
&lt;/div&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{x} \notin \mathfrak{R}\)&lt;/span&gt; Accept &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{x} \in \mathfrak{R}\)&lt;/span&gt; Reject &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;vertical-align: middle !important;&#34; rowspan=&#34;2&#34;&gt;
TRUTH
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; is true
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\checkmark\)&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; &lt;br&gt; Type I error
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(H_1\)&lt;/span&gt; is true
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; &lt;br&gt; Type II error
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\checkmark\)&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;假如一個假設檢驗是關於總體參數 &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; 的：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_0: \theta=\theta_0 \;vs.\; H_1: \theta=\theta_1 \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;這個檢驗的效能被定義爲當替代假設爲“真”時，拒絕零假設的概率（能夠檢驗出有真實差別的能力）：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Power&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(=Prob(\underline{x}\in\mathfrak{R}|H_1\; is\; true) = 1-Prob(Type \; II\; error)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;檢驗的顯著性水平用 &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; 來表示。&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; 的直觀意義就是，檢驗結果錯誤的拒絕了零假設 &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;，接受了替代假設 &lt;span class=&#34;math inline&#34;&gt;\(H_1\)&lt;/span&gt;，即假陽性的概率。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Prob(\underline{x}\in \mathfrak{R} |H_0 \;is\;true)=Prob(Type\;I\;error)\)&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;以二項分佈爲例&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;以二項分佈爲例&lt;/h4&gt;
&lt;p&gt;用本文開頭的例子： &lt;span class=&#34;math inline&#34;&gt;\(X\sim Bin(5,\theta)\)&lt;/span&gt;。和我們建立的零假設和替代假設：&lt;span class=&#34;math inline&#34;&gt;\(H_0: \theta=\frac{1}{2}; H_1: \theta=\frac{2}{3}\)&lt;/span&gt;：&lt;/p&gt;
&lt;p&gt;考慮兩種檢驗方法：&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;A 方法：當且僅當5次觀察都爲“成功”時才拒絕 &lt;span class=&#34;math inline&#34;&gt;\(H_0 (i.e.\; X=5)\)&lt;/span&gt;。所以此時判別區域 &lt;span class=&#34;math inline&#34;&gt;\(\mathfrak{R}\)&lt;/span&gt; 爲 &lt;span class=&#34;math inline&#34;&gt;\(5\)&lt;/span&gt;。檢驗效能爲：&lt;span class=&#34;math inline&#34;&gt;\(Prob(X=5|H_1 \;is\;true)=(\frac{2}{3})^5=0.1317\)&lt;/span&gt;。顯著性水平爲 &lt;span class=&#34;math inline&#34;&gt;\(Prob(X=5|H_0\;is\;true)=(\frac{1}{2})^5=0.03125\)&lt;/span&gt;。&lt;/li&gt;
&lt;li&gt;B 方法：當觀察到3,4,5次“成功”時，拒絕 &lt;span class=&#34;math inline&#34;&gt;\(H_0 (i.e.\; X=3,4,5)\)&lt;/span&gt;。此時判別區域 &lt;span class=&#34;math inline&#34;&gt;\(\mathfrak{R}\)&lt;/span&gt; 爲 &lt;span class=&#34;math inline&#34;&gt;\(3,4,5\)&lt;/span&gt;。檢驗效能爲：&lt;span class=&#34;math inline&#34;&gt;\(Prob(X=3,4,or\:5|H_1\;is\;ture)=\sum_{i=3}^5(\frac{2}{3})^i(\frac{1}{3})^{5-i}\approx0.7901\)&lt;/span&gt;；顯著性水平爲：&lt;span class=&#34;math inline&#34;&gt;\(Prob(X=3,4,5|H_0\;is\;true)=\sum_{i=3}^5(\frac{1}{2})^i(\frac{1}{2})^{5-i}=0.5\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# the power in test B
dbinom(3,5,2/3)+dbinom(4,5,2/3)+dbinom(5,5,2/3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7901235&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# the size in test B
dbinom(3,5,0.5)+dbinom(4,5,0.5)+dbinom(5,5,0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;比較上面兩種檢驗方法，可以看到，用B方法時，我們有更高的概率獲得假陽性結果（第一類錯誤，錯誤地拒絕 &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;，接受 &lt;span class=&#34;math inline&#34;&gt;\(H_1\)&lt;/span&gt;)，但是也有更高的檢驗效能（真陽性更高）。這個例子就說明了，試圖提高檢驗效能的同時，會提高犯第一類錯誤的概率。實際操作中我們常常將第一類錯誤的概率固定，例如 &lt;span class=&#34;math inline&#34;&gt;\(\alpha=0.05\)&lt;/span&gt;，然後儘可能選擇效能最高的檢驗方法。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;如何選擇要檢驗的統計量&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;如何選擇要檢驗的統計量&lt;/h3&gt;
&lt;p&gt;在上面的二項分佈的實驗中，“成功的次數” 是我們感興趣的要檢驗的統計量。但也可能是第一次出現 “成功” 之前的實驗次數，或者，任何與假設相關的統計量。相似的，如果觀察不是離散變量而是連續的，可以拿來檢驗的指標就有很多，如均值，中位數，衆數，幾何平均值等。&lt;/p&gt;
&lt;p&gt;幸運地是，當明確了零假設和替代假設後，我們可以利用 &lt;a href=&#34;https://en.wikipedia.org/wiki/Neyman%E2%80%93Pearson_lemma&#34;&gt;Neyman-Pearson lemma&lt;/a&gt; 似然比公式&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;來決定使用哪個統計量做檢驗&lt;strong&gt;最有效&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[=\frac{L_{H_0}}{L_{H_1}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;這公式很直觀，因爲當數據更加支持 &lt;span class=&#34;math inline&#34;&gt;\(H_1\)&lt;/span&gt; 時 (&lt;span class=&#34;math inline&#34;&gt;\(L_{H_1}\)&lt;/span&gt; 更大)，&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; 的可能性相對更小，就更應該被拒絕。而且，由於似然比越小，他的對數就越小，使用對數似然比常常更加直觀：&lt;span class=&#34;math inline&#34;&gt;\(\ell_{H_0}-\ell_{H_1}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;那到底要多小才算小？這個進入拒絕域的閾值由兩個指標來決定：&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;被檢驗統計量的樣本分佈&lt;/li&gt;
&lt;li&gt;第一類錯誤概率 &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;以已知方差的正態分佈爲例&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;以已知方差的正態分佈爲例&lt;/h4&gt;
&lt;p&gt;假如已知 &lt;span class=&#34;math inline&#34;&gt;\(X_1, \cdots, X_n \stackrel{i.i.d}{\sim} N(\mu, \sigma^2)\)&lt;/span&gt; 而且方差 &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; 也是已知的。如果令 &lt;span class=&#34;math inline&#34;&gt;\(H_0: \mu=5\; ;H_1: \mu=10\)&lt;/span&gt; 可以通過如下的方法找到我們需要的最佳檢驗統計量 &lt;u&gt;best statistic&lt;/u&gt; 根據之前的&lt;a href=&#34;https://winterwang.github.io/post/log-likelihood-ratio/&#34;&gt;推導&lt;/a&gt;可知正態分佈的似然方程如下：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\ell(\mu|\underline{x}) =-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;所以已知 &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; 時，我們的零假設和替代假設之間的對數似然比 &lt;span class=&#34;math inline&#34;&gt;\(\ell_{H_0}-\ell_{H_1}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\ell_{H_0}-\ell_{H_1}=-\frac{1}{2\sigma^2}(\sum_{i=1}^n(x_i-5)^2-\sum_{i=1}^n(x_i-10)^2)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;然俄，我們只需要考慮隨着數據變化的部分，所以忽略掉不變的部分&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\ell_{H_0}-\ell_{H_1} &amp;amp; = -(\sum_{i=1}^n(x_i-5)^2-\sum_{i=i}^n(x_i-10)^2)\\
                &amp;amp; = 75n - 2\times(10-5)\sum_{i=1}^nx_i \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;所以只要樣本和 &lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^nx_i\)&lt;/span&gt; &lt;u&gt;(最佳統計量 best statistic)&lt;/u&gt; 足夠大，零假設就會被拒絕。而且注意到最佳統計量可以乘以任何常數用作新的最佳統計量。所以爲了方便我們就用樣本均數 &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{n}\sum_{i=1}^nx_i\)&lt;/span&gt; 作此處的最佳統計量。所以此時，我們的最佳檢驗就是當樣本均值足夠大，超過某個閾值時，我們拒絕零假設。而且，樣本均值的樣本分佈是可以知道的，這樣就便於我們繼續計算下一步：拒絕域 （判別區域）。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;複合假設-composite-hypotheses&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;複合假設 composite hypotheses&lt;/h3&gt;
&lt;p&gt;目前爲止我們討論的假設檢驗限制太多，實際操作時，我們多考慮類似如下的假設：&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_0: \theta=\theta_0 \;v.s.\; H_1: \theta&amp;gt;\theta_0\)&lt;/span&gt; [&lt;strong&gt;單側&lt;/strong&gt;的替代假設]&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_0: \theta=\theta_0 \;v.s.\; H_1: \theta\neq\theta_0\)&lt;/span&gt; [&lt;strong&gt;雙側&lt;/strong&gt;的替代假設]&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;所以我們面臨的問題是簡單假設中用於判定的最佳統計量，是否還適用？我們一一來看：&lt;/p&gt;
&lt;div id=&#34;單側替代假設&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;單側替代假設&lt;/h4&gt;
&lt;p&gt;之前的推導中我們發現，樣本均值越大，零假設和替代假設的對數似然比 &lt;span class=&#34;math inline&#34;&gt;\(\ell_{H_0}-\ell_{H_1}\)&lt;/span&gt; 越小。所以我們在樣本均值較大時，拒絕零假設，那麼就可以把原來使用的簡單替代假設 &lt;span class=&#34;math inline&#34;&gt;\(H_1: \mu=10\)&lt;/span&gt; 擴展爲，任意大於 &lt;span class=&#34;math inline&#34;&gt;\(5\)&lt;/span&gt; 的 &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; ，即 &lt;span class=&#34;math inline&#34;&gt;\(\mu&amp;gt;5\)&lt;/span&gt; 。因爲大於 &lt;span class=&#34;math inline&#34;&gt;\(5\)&lt;/span&gt; 的任何均值，都提供了更小的對數似然比，都會讓我們拒絕零假設。所以在正態分佈時，單側替代假設的最佳檢驗統計量還是&lt;strong&gt;樣本均值&lt;/strong&gt;。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;雙側替代假設&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;雙側替代假設&lt;/h4&gt;
&lt;p&gt;雙側替代假設的情況下，我們無法繼續使用樣本均值作爲最佳統計量。因爲當我們想檢驗：&lt;span class=&#34;math inline&#34;&gt;\(H_0: \mu=5 \;v.s.\; H_1: \mu&amp;lt;5\)&lt;/span&gt; 時，必須獲得足夠小的樣本均值才能讓我們拒絕零假設。先按下不表。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;如何獲得反對零假設的證據-how-to-quantify-evidence-against-h_0&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;如何獲得反對零假設的證據 how to quantify evidence against &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;重新再考慮符合假設：&lt;span class=&#34;math inline&#34;&gt;\(H_0: \theta=\theta_0\;v.s.\;H_1: \theta&amp;gt;\theta_0\)&lt;/span&gt; 假如存在一個總是可用的最佳檢驗統計量，用 &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; 來標記 (或 &lt;span class=&#34;math inline&#34;&gt;\(T(x)\)&lt;/span&gt;)， 這個統計量足夠大時，我們拒絕 &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;。 別忘了我們還要定義判別區域：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Prob(\underline{x}\in\mathfrak{R}|H_0)=\alpha\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;如果我們知道 &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; 的樣本分佈，我們很容易就可以使用一個閾值 &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; 來定義這個判別區域：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Prob(T\geqslant c|H_0)=\alpha\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;更加正式的，我們定義判別區域 &lt;span class=&#34;math inline&#34;&gt;\(\mathfrak{R}\)&lt;/span&gt; 爲：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\{\underline{x}:Prob(T(x)\geqslant c|H_0)=\alpha\}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;換句話說，當統計量 &lt;span class=&#34;math inline&#34;&gt;\(T&amp;gt;c\)&lt;/span&gt; 時，我們拒絕 &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; 。如果先不考慮拒絕或不拒絕的二元判定，我們可以用一個連續型測量值來量化反對零假設 &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; 的證據。再考慮從觀察數據中獲得的 &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; ，即數據告訴我們的 &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; 。所以，當 &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; 值越大，說明觀察值相對零假設 &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; 越往極端的方向走。因此我們可以用 &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; 的樣本分佈來計算觀察值大大於等於這個閾值（極端值）時的概率：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p=Prob(T\geqslant t|H_0)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;這個概率公式被稱爲是單側 &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; 值 &lt;strong&gt;(one-side p-value)&lt;/strong&gt;。單側 &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; 值越小，統計量 &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; 的樣本空間就有越小比例（越強）的證據支持零假設 &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;我們把這以思想用到假設檢驗中時，就可以認爲：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p&amp;lt;\alpha \Leftrightarrow t&amp;gt;c\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;所以用我們一貫的設定 &lt;span class=&#34;math inline&#34;&gt;\(\alpha=0.05\)&lt;/span&gt;，所以如果計算獲得 &lt;span class=&#34;math inline&#34;&gt;\(p&amp;lt;0.05\)&lt;/span&gt; 我們就認爲獲得了足夠強的拒絕零假設 &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; 的證據。&lt;/p&gt;
&lt;div id=&#34;回到正態分佈的均值比較問題上來單側替代假設&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;回到正態分佈的均值比較問題上來（單側替代假設）&lt;/h4&gt;
&lt;p&gt;繼續考慮 &lt;span class=&#34;math inline&#34;&gt;\(X_1,\cdots,X_n\stackrel{i.i.d}{\sim} N(\mu, \sigma^2)\)&lt;/span&gt;，假設已知 &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2=10\)&lt;/span&gt;，我們要檢驗的是 &lt;span class=&#34;math inline&#34;&gt;\(H_0: \mu=5 \;v.s.\; H_1: \mu&amp;gt;5\)&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;確定最佳檢驗統計量：已經證明過，單側替代假設的最佳檢驗統計量是&lt;strong&gt;樣本均值&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;確定該統計量的樣本分佈：已知樣本均數的樣本分佈是 &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\sim N(\mu,\sigma^2/n)\)&lt;/span&gt; 。&lt;br&gt;&lt;span class=&#34;math inline&#34;&gt;\(\Rightarrow Z=\frac{\bar{X}-\mu}{\sigma/\sqrt{n}} \sim N(0,1)\)&lt;/span&gt;，所以在 &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; 條件下，&lt;span class=&#34;math inline&#34;&gt;\(\Rightarrow Z=\frac{\bar{X}-5}{\sqrt{10}/\sqrt{n}} \sim N(0,1)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;所以當一個檢驗的一類錯誤概率設定爲 &lt;span class=&#34;math inline&#34;&gt;\(\alpha=0.05\)&lt;/span&gt; 時，我們使用的判別區域使統計量據落在該判別區域內的概率爲 &lt;span class=&#34;math inline&#34;&gt;\(0.05\)&lt;/span&gt;：&lt;br&gt; &lt;span class=&#34;math inline&#34;&gt;\(Prob(\bar{X}\geqslant c|H_0) = 0.05\)&lt;/span&gt; &lt;br&gt; 已知在標準正態分佈時，&lt;span class=&#34;math inline&#34;&gt;\(Prob(Z\geqslant1.64)=0.05=Prob(\frac{\bar{X}-5}{\sqrt{10}/\sqrt{n}}\geqslant1.64)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;假設樣本量是 &lt;span class=&#34;math inline&#34;&gt;\(10\)&lt;/span&gt;，那麼數據的判別區域 &lt;span class=&#34;math inline&#34;&gt;\(\mathfrak{R}\)&lt;/span&gt; 就是 &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\geqslant6.64\)&lt;/span&gt;。&lt;/li&gt;
&lt;li&gt;假設觀察數據告訴我們，&lt;span class=&#34;math inline&#34;&gt;\(\bar{X}=7.76\)&lt;/span&gt; 。那麼這一組觀察數據計算得到的統計量落在了判別區域內，所以說是有足夠的證據拒絕接受 &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; 的。&lt;/li&gt;
&lt;li&gt;我們可以給這個觀察數據計算相應的單側 &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; 值：&lt;br&gt; &lt;span class=&#34;math inline&#34;&gt;\(p=Prob(\bar{X}\geqslant7.76|H_0)=Prob(Z+5\geqslant7.76)\\=Prob(Z\geqslant2.76)=0.003\)&lt;/span&gt; &lt;br&gt; 所以，數據告訴我們，在 &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; 的前提下，觀察值出現的概率是 &lt;span class=&#34;math inline&#34;&gt;\(0.3\%\)&lt;/span&gt; 。即，在無數次取樣實驗中，僅有 &lt;span class=&#34;math inline&#34;&gt;\(0.3\%\)&lt;/span&gt; 的結果可以給出支持 &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; 的證據。因此我們拒絕 &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; 接受 &lt;span class=&#34;math inline&#34;&gt;\(H_1\)&lt;/span&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;雙側替代假設情況下雙側-p-值的定量方法&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;雙側替代假設情況下，雙側 &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; 值的定量方法&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/post/2017-11-08-construction-of-a-hypothesis-test_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;p&gt;此處故意使用一個左右不對稱的概率密度分佈來解釋。&lt;/p&gt;
&lt;p&gt;現在的替代假設是雙側的：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_0: \theta=\theta_0 \;v.s.\; H_1:  \theta\neq\theta_0\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;正常來說，雙側的假設檢驗應該分成兩個單側檢驗。即：&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_1: \theta&amp;gt;\theta_0\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_1: \theta&amp;lt;\theta_0\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;每個單側檢驗都有自己的最佳檢驗統計量。令 &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; 是 1. 的最佳檢驗統計量，該統計量的樣本分佈如上圖所示（左右不對稱）。假如觀察數據給出的統計量爲 &lt;span class=&#34;math inline&#34;&gt;\(t2\)&lt;/span&gt;，那麼在概率上反對零假設的情況可以有兩種：&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(T\geqslant t2\)&lt;/span&gt; 其中， &lt;span class=&#34;math inline&#34;&gt;\(Prob(T\geqslant t2|H_0)=p1\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(T\leqslant t1\)&lt;/span&gt; 其中， &lt;span class=&#34;math inline&#34;&gt;\(Prob(T\leqslant t1|H_0) =p1\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;所以概率密度分佈兩側的距離可以不對稱，但是只要左右兩側概率密度分佈的面積(&lt;span class=&#34;math inline&#34;&gt;\(=p1\)&lt;/span&gt;)相同，那麼就可以直接認爲，雙側 &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; 值是兩側面積之和 (&lt;span class=&#34;math inline&#34;&gt;\(p=2\times p1\)&lt;/span&gt;)，且觀察數據提供的統計量落在這兩個面積內的話，都足以提供證據拒絕零假設 &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;回到上文中單側 &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; 值爲&lt;span class=&#34;math inline&#34;&gt;\(0.003\)&lt;/span&gt;，故雙側 &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; 值就是它的兩倍：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(p1=Prob(\bar{X}\geqslant7.76|H_0)=Prob(Z+5\geqslant7.76)\\=Prob(Z\geqslant2.76)=0.003\\ \Rightarrow p=2\times p1=0.006\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;區分與&lt;a href=&#34;https://winterwang.github.io/post/log-likelihood-ratio/&#34;&gt;之前討論的似然比&lt;/a&gt;，之前討論的似然比只是所有的似然和極大似然之間的比，此處的似然比只是純粹在探討兩個假設之間可能性之比。&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Rememer that &lt;span class=&#34;math inline&#34;&gt;\(\ell_{H_0}-\ell_{H_1}\)&lt;/span&gt; is a random variable: the data varies &lt;strong&gt;each time&lt;/strong&gt; we sample, with consequently varying relative support for the hypotheses, and so we are only interested in that part of &lt;span class=&#34;math inline&#34;&gt;\(\ell_{H_0}-\ell_{H_1}\)&lt;/span&gt; which depends on the results, the data, which vary with each sample (i.e. which contains the random part); the constant part provides no information on the relative support the data give to the hypotheses, so we ignore it.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>二次方程近似法求對數似然比 approximate log-likelihood ratios</title>
      <link>https://wangcc.me/post/approximate-log-likelihood-ratios/</link>
      <pubDate>Tue, 07 Nov 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/approximate-log-likelihood-ratios/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#正態近似法求對數似然-normal-approximation-to-the-log-likelihood&#34;&gt;正態近似法求對數似然 Normal approximation to the log-likelihood&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#參數轉化-parameter-transformations&#34;&gt;參數轉化 parameter transformations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#exercise&#34;&gt;Exercise&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;爲什麼要用二次方程近似對數似然比方程？&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;上節也看到，我們會碰上難以用代數學計算獲得對數似然比信賴區間的情況 (&lt;a href=&#34;https://winterwang.github.io/post/log-likelihood-ratio/&#34;&gt;binomial example&lt;/a&gt;)。&lt;/li&gt;
&lt;li&gt;我們同時知道，對數似然比方程會隨着樣本量增加而越來越漸進於二次方程，且左右對稱。&lt;/li&gt;
&lt;li&gt;所以，我們考慮當樣本量足夠大時，用二次方程來近似對數似然比方程從而獲得參數估計的信賴區間。&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;正態近似法求對數似然-normal-approximation-to-the-log-likelihood&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;正態近似法求對數似然 Normal approximation to the log-likelihood&lt;/h3&gt;
&lt;p&gt;根據&lt;a href=&#34;https://winterwang.github.io/post/log-likelihood-ratio/&#34;&gt;前一節&lt;/a&gt;，如果樣本均數的分佈符合正態分佈：&lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\sim N(\mu, \sigma^2/n)\)&lt;/span&gt;。那麼樣本均數的對數似然比爲：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[llr(\mu|\bar{X})=\ell(\mu|\bar{X})=-\frac{1}{2}(\frac{\bar{x}-\mu}{\sigma/\sqrt{n}})^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中， &lt;span class=&#34;math inline&#34;&gt;\(\bar{x}\)&lt;/span&gt; 是正態分佈總體均數 &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; 的極大似然估計 (maximum likelihood estimator, MLE)。如果已知總體的方差參數，那麼 &lt;span class=&#34;math inline&#34;&gt;\(\sigma/\sqrt{n}\)&lt;/span&gt; 是 &lt;span class=&#34;math inline&#34;&gt;\(\bar{x}\)&lt;/span&gt; 的標準誤 (standard error)。&lt;/p&gt;
&lt;p&gt;因此，假設 &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; 是我們想尋找的總體參數。有些人提議可以使用下面的關於 &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; 的二次方程來做近似：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[f(\theta|data)=-\frac{1}{2}(\frac{\theta-M}{S})^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;上述方程具有一個正態二次對數似然 (比) 的形式，而且該方程的極大似然估計(MLE)， &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; 的標準誤爲 &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;。如果我們正確地選用 &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; 和 &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;，那我們就可以用這樣的方程來近似求真實觀察數據的似然 &lt;span class=&#34;math inline&#34;&gt;\(\ell(\theta|data)\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;通過近似正態對數似然比，&lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; 應當選用使方程取最大值時，參數 &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; 的極大似然估計 &lt;span class=&#34;math inline&#34;&gt;\(M=\hat{\Theta}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;但是在選用標準誤 &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; 上必須滿足下列條件：&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; 是極大似然估計 &lt;span class=&#34;math inline&#34;&gt;\(\hat{\Theta}\)&lt;/span&gt; 的標準誤。&lt;/li&gt;
&lt;li&gt;被選擇的 &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; 必須儘可能的使該二次方程形成一個十分接近真實的對數似然比方程。特別是在最大值的部分必須與之無限接近或者一致。所以二者在 MLE 的位置應當有相同的曲率（二階導數）。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;由於，一個方程的曲率是該方程的二階導數（斜線斜率變化的速度）。所以對數似然比方程在 MLE 取最大值時的曲率（二階導數）爲：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\left.\frac{d^2}{d\theta^2}\ell(\theta)\right\vert_{\theta=\hat{\theta}}=\ell^{\prime\prime}(\hat{\theta})=-\frac{1}{S^2}\\
\Rightarrow S^2=\left.-\frac{1}{\ell^{\prime\prime}(\theta)}\right\vert_{\theta=\hat{\theta}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;在正態分佈的例子下，&lt;span class=&#34;math inline&#34;&gt;\(M=\bar{x}, S=\sigma/\sqrt{n}\)&lt;/span&gt;。對數似然比方程最大值時的曲率（二階導數）恰好就爲標準誤的平方的負倒數：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\ell^{\prime\prime}(\theta)=-\frac{1}{SE^2}\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\Rightarrow\)&lt;/span&gt; 被叫做 &lt;strong&gt;Fisher information&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;稍微總結一下：&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;任意的對數似然比方程 &lt;span class=&#34;math inline&#34;&gt;\(llr(\theta)\)&lt;/span&gt; 都可以考慮用一個二次方程來近似：
&lt;span class=&#34;math display&#34;&gt;\[f(\theta|data)=-\frac{1}{2}(\frac{\theta-M}{S})^2\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;其中&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\begin{aligned}  &amp;amp;M=\hat\theta\\  &amp;amp;S^2=\left.-\frac{1}{\ell^{\prime\prime}(\theta)}\right\vert_{\theta=\hat{\theta}}\\  &amp;amp;when \\  &amp;amp; n\rightarrow\infty \Rightarrow  \begin{cases}  S^2\rightarrow Var(\hat\theta) \\  S\rightarrow SE(\hat\theta)  \end{cases}  \end{aligned}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;近似法估算對數似然比的信賴區間&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;近似法估算對數似然比的信賴區間&lt;/h4&gt;
&lt;p&gt;一旦我們決定了使用正態近似法來模擬對數似然比方程，對數似然比的信賴區間算法就回到了前一節中我們算過的方法，也就是：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[-2f(\theta)&amp;lt;\mathcal{X}_{1,(1-\alpha)}^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;故信賴區間爲： &lt;span class=&#34;math inline&#34;&gt;\(m\pm\sqrt{\mathcal{X}_{1,(1-\alpha)}^2}S\)&lt;/span&gt;。求&lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; 水平的信賴區間時，&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{X}_{1,0.95}^2=3.84\)&lt;/span&gt;，所以就又看到了熟悉的 &lt;span class=&#34;math inline&#34;&gt;\(M\pm1.96S\)&lt;/span&gt;。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;以泊松分佈爲例&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;以泊松分佈爲例&lt;/h4&gt;
&lt;p&gt;一個被追蹤的樣本，經過了 &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; 人年的觀察，記錄到了 &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; 個我們要研究的事件：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[D\sim Poi(\mu), where \mu=\lambda p\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Step 1. 找極大似然估計 (MLE)，&lt;a href=&#34;https://winterwang.github.io/post/likelihood/&#34;&gt;之前介紹似然方程時推導過的泊松分佈的似然方程&lt;/a&gt;：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
P(D=d|\lambda) &amp;amp;= \frac{e^{-\mu}\cdot\mu^d}{d!} \\
 &amp;amp;=\frac{e^{-\lambda p}\cdot\lambda^d p^d}{d!} \\
omitting&amp;amp;\;terms\;not\;in\;\mu \\
&amp;amp;\Rightarrow \ell(\lambda) = dlog\lambda - \lambda p \\
&amp;amp;\Rightarrow \ell^\prime(\lambda) = \frac{d}{\lambda} -p \\
&amp;amp;\Rightarrow \hat\lambda=\frac{d}{p} = \textbf{M}
\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Step 2. 求似然方程的二階導數，確認 MLE 是使方程獲得最大值的點，然後確定 &lt;span class=&#34;math inline&#34;&gt;\(S^2\)&lt;/span&gt;：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
&amp;amp; \ell^\prime(\lambda) = \frac{d}{\lambda} -p \\
&amp;amp; \Rightarrow \ell^{\prime\prime}(\lambda) = -\frac{d}{\lambda^2}&amp;lt;0 \Rightarrow \textbf{MLE is maximum} \\
&amp;amp; S^2 = \left.-\frac{1}{\ell^{\prime\prime}(\lambda)}\right\vert_{\lambda=\hat{\lambda}=d/p} = -\frac{1}{-d/\hat\lambda^2} = -\frac{1}{-d/(d/p)^2} \\
&amp;amp;\Rightarrow S^2 = \frac{d}{p^2} \\
\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Step 3. 把前兩部求得的 &lt;span class=&#34;math inline&#34;&gt;\(MLE\)&lt;/span&gt; 和 &lt;span class=&#34;math inline&#34;&gt;\(S^2\)&lt;/span&gt; 代入近似的二次方程：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
&amp;amp; \hat\lambda=\frac{d}{p}=M,\; S^2 = \frac{d}{p^2}  \\
&amp;amp; using\;approximate\;quadratic\;llr \\
&amp;amp; q(\lambda) = -\frac{1}{2}(\frac{\lambda-M}{S})^2\\
&amp;amp;\Rightarrow q(\lambda) = -\frac{1}{2}(\frac{\lambda-\frac{d}{p}}{\frac{\sqrt{d}}{p}})^2\\
&amp;amp; let \; q(\lambda)=-1.92\\
&amp;amp;\Rightarrow -\frac{1}{2}(\frac{\lambda-\frac{d}{p}}{\frac{\sqrt{d}}{p}})^2=-1.92\\
&amp;amp;(\frac{\lambda-\frac{d}{p}}{\frac{\sqrt{d}}{p}})^2=3.84\\
&amp;amp;\frac{\lambda-\frac{d}{p}}{\frac{\sqrt{d}}{p}} = \pm1.96\\
&amp;amp;\Rightarrow 95\%CI \;for \;\lambda = \frac{d}{p}\pm1.96\frac{\sqrt{d}}{p}
\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;結論就是： 發病（死亡）率 &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; 的 &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; 信賴區間爲： &lt;span class=&#34;math inline&#34;&gt;\(M\pm1.96S\)&lt;/span&gt;。所以我們不需要每次都代入對數似然比方程，只要算出 &lt;span class=&#34;math inline&#34;&gt;\(MLE = M\)&lt;/span&gt; 和 &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; 之後代入這個公式就可以用二次方程近似法算出信賴區間。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;以二項分佈爲例&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;以二項分佈爲例&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[K\sim Bin(n,\pi)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Step 1. 找極大似然估計 (MLE)：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp; Prob(K=k) = \pi^k(1-\pi)\binom{n}{k}\\
&amp;amp;\Rightarrow L(\pi|k) = \pi^k(1-\pi)\binom{n}{k}\\
&amp;amp;omitting\;terms\;not\;in\;\pi \\
&amp;amp;\Rightarrow \ell(\pi) = k\:log\pi+(n-k)log(1-\pi) \\
&amp;amp;\ell^\prime(\pi) = \frac{k}{\pi}-\frac{n-k}{1-\pi} \\
&amp;amp; let\;\ell^\prime(\hat\pi) =0 \\
&amp;amp;\Rightarrow \frac{k}{\hat\pi}-\frac{n-k}{1-\hat\pi}=0\\
&amp;amp;\Rightarrow \frac{\hat\pi}{1-\hat\pi}=\frac{k}{n-k}\\
&amp;amp;\Rightarrow \frac{\hat\pi}{1-\hat\pi}=\frac{k/n}{1-k/n}\\
&amp;amp;\Rightarrow \hat\pi=\frac{k}{n} = p = \textbf{M}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Step 2. 將對數似然方程的二次微分 (二階導數)，確認在 MLE 爲極大值，並確認 &lt;span class=&#34;math inline&#34;&gt;\(S^2\)&lt;/span&gt;：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;\ell^\prime(\pi) = \frac{k}{\pi}-\frac{n-k}{1-\pi} \\
&amp;amp;\ell^{\prime\prime}(\pi)=\frac{-k}{\pi^2}-\frac{n-k}{(1-\pi)^2} &amp;lt;0 \\
&amp;amp;\therefore at\;\textbf{MLE}\;\ell(\pi)\;has\;maximum \\
S^2&amp;amp;=\left.-\frac{1}{\ell^{\prime\prime}(\pi)}\right\vert_{\pi=\hat\pi=k/n=p}\\
&amp;amp;=\frac{1}{\frac{k}{\hat\pi^2}+\frac{n-k}{(1-\hat\pi)^2}}\\
&amp;amp;=\frac{\hat\pi^2(1-\hat\pi)^2}{k(1-\hat\pi)^2+(n-k)\hat\pi^2}\\
&amp;amp;=\frac{P^2(1-P)^2}{np(1-p)^2+(n-np)p^2}\\
&amp;amp;=\frac{p(1-p)}{n(1-p)+np}\\
&amp;amp;=\frac{p(1-p)}{n}\\
&amp;amp;\Rightarrow S=\sqrt{\frac{p(1-p)}{n}}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Step 3. 將求得的 MLE 和 &lt;span class=&#34;math inline&#34;&gt;\(S^2\)&lt;/span&gt; 代入近似信賴區間：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
95\% CI \;for \; \pi:\\
M\pm1.96S=p\pm1.96\sqrt{\frac{p(1-p)}{n}}\\
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;參數轉化-parameter-transformations&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;參數轉化 parameter transformations&lt;/h3&gt;
&lt;p&gt;如果將參數 &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; 通過某種數學方程轉化成 &lt;span class=&#34;math inline&#34;&gt;\(g(\theta)\)&lt;/span&gt;，那麼我們可以認爲，轉化後的方程的 MLE 爲 &lt;span class=&#34;math inline&#34;&gt;\(g(\hat\theta)\)&lt;/span&gt;，其中 &lt;span class=&#34;math inline&#34;&gt;\(\hat\theta\)&lt;/span&gt; 是參數 &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; 的 MLE。&lt;/p&gt;
&lt;p&gt;類似地，如果 &lt;span class=&#34;math inline&#34;&gt;\(\theta_1 \sim \theta_2\)&lt;/span&gt; 是參數 &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; 的似然比信賴區間，那麼 &lt;span class=&#34;math inline&#34;&gt;\(g(\theta_1)\sim g(\theta_2)\)&lt;/span&gt; 就是 &lt;span class=&#34;math inline&#34;&gt;\(g(\theta)\)&lt;/span&gt; 的似然比信賴區間。&lt;/p&gt;
&lt;p&gt;以下爲轉換參數以後獲取信賴區間的步驟：&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;將參數通過某些數學方程（通常是取對數）轉化，使新的對數似然比方程更加接近二次方程的對稱圖形。&lt;br&gt; Transform parameter so that &lt;span class=&#34;math inline&#34;&gt;\(llr\)&lt;/span&gt; is closer to a quadratic shape.&lt;/li&gt;
&lt;li&gt;用本節學到的二次方程近似法，求得轉化後的參數的似然比信賴區間。 &lt;br&gt; Use our quadratic approximation on the transformed parameter to calculate our likelihood ratio confidence intervals.&lt;/li&gt;
&lt;li&gt;將第2步計算獲得的似然比信賴區間再通過轉化參數時的逆函數轉換回去，以獲得原參數的似然比信賴區間。&lt;br&gt; Transform the confidence intervals back, or to any scale we wish – they remain valid.&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;以泊松分佈爲例-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;以泊松分佈爲例&lt;/h4&gt;
&lt;p&gt;當我們用泊松分佈模擬事件在某段時間內發生率 &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; 時，注意到這個事件發生率必須滿足 &lt;span class=&#34;math inline&#34;&gt;\(\lambda&amp;gt;0\)&lt;/span&gt;。當事件發生次數較低時，會讓似然方程的圖形被擠壓在低值附近。如果嘗試用對數轉換 &lt;span class=&#34;math inline&#34;&gt;\(\lambda \rightarrow log(\lambda)\)&lt;/span&gt; 此時 &lt;span class=&#34;math inline&#34;&gt;\(log(\lambda)\)&lt;/span&gt; 就不再被限制與 &lt;span class=&#34;math inline&#34;&gt;\(&amp;gt;0\)&lt;/span&gt;。下面我們嘗試尋找對數轉換過後的 &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; 和 &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;令 &lt;span class=&#34;math inline&#34;&gt;\(\beta=log(\lambda), \Rightarrow e^\beta=\lambda\)&lt;/span&gt; 從本文上半部分中我們已知 &lt;span class=&#34;math inline&#34;&gt;\(\hat\lambda=\frac{d}{p}\)&lt;/span&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;對數轉換以後的 &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; 是什麼? &lt;br&gt;根據定義，&lt;span class=&#34;math inline&#34;&gt;\(MLE(\beta)=MLE[log(\lambda)]=log(\hat\lambda)\)&lt;/span&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\Rightarrow M=\hat\beta=log(\frac{d}{p})\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;對數轉換以後的 &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; 是什麼? &lt;br&gt; 泊松分佈的對數似然方程是：&lt;span class=&#34;math inline&#34;&gt;\(\ell(\lambda|d)=d log(\lambda) - \lambda p\)&lt;/span&gt; 用 &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; 替換掉 &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;&lt;/p&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\begin{aligned}  &amp;amp; \ell(\beta|d)=d \beta - pe^\beta\\  &amp;amp; \Rightarrow \ell^\prime(\beta)=d-pe^\beta \Rightarrow \ell^{\prime\prime}(\beta)=-pe^\beta \\  &amp;amp; S^2 = \left.-\frac{1}{\ell^{\prime\prime}(\beta)}\right\vert_{\beta=\hat{\beta}} = \left.\frac{1}{pe^\beta}\right\vert_{\beta=\hat{\beta}} = \frac{1}{pe^{log(d/p)}}\\  &amp;amp;\Rightarrow S^2=\frac{1}{d} \therefore S=\frac{1}{\sqrt{d}} \end{aligned}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;轉換後的近似二次方程：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\begin{aligned}  &amp;amp; q(\beta) = -\frac{1}{2}(\frac{\beta-M}{S})^2 = -\frac{1}{2}(\frac{\beta-log(\frac{d}{p})}{\frac{1}{\sqrt{d}}})^2  \end{aligned}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; 的 &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; 信賴區間 &lt;span class=&#34;math inline&#34;&gt;\(=log(\frac{d}{p})\pm1.96\frac{1}{\sqrt{d}}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; 的 &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; 信賴區間 &lt;span class=&#34;math inline&#34;&gt;\(=exp(log(\frac{d}{p})\pm1.96\frac{1}{\sqrt{d}})\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;以二項分佈爲例-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;以二項分佈爲例&lt;/h4&gt;
&lt;p&gt;在研究對象 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 人中觀察到 &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; 個人患有某種疾病。&lt;/p&gt;
&lt;p&gt;令 &lt;span class=&#34;math inline&#34;&gt;\(\beta=log(\pi) \Rightarrow \pi=e^\beta\)&lt;/span&gt; 從上文的推倒也已知 &lt;span class=&#34;math inline&#34;&gt;\(\hat\pi=\frac{k}{n}=p\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\begin{aligned} &amp;amp;\Rightarrow \ell(\beta)=klog\pi+(n-k)log(1-\pi)=k\beta+(n-k)log(1-e^\beta) \\ &amp;amp;\Rightarrow \ell^{\prime}(\beta)=k-\frac{(n-k)(e^\beta)}{1-e^\beta} \\ &amp;amp;\Rightarrow \ell^{\prime\prime}(\beta)=-(n-k)\frac{e^\beta(1-e^\beta)+e^{2\beta}}{(1-e^\beta)^2} \\ &amp;amp; \ell^{\prime\prime}(\beta)= -(n-k)\frac{e^\beta}{(1-e^\beta)^2}\\ &amp;amp;\Rightarrow S^2 = \left.-\frac{1}{\ell^{\prime\prime}(\beta)}\right\vert_{\beta=\hat{\beta}} = \frac{(1-e^{\hat\beta})^2}{(n-k)e^{\hat\beta}} \\ &amp;amp;\because \hat\beta=log(\hat\pi) \\ &amp;amp;\therefore e^{\hat\beta} = \frac{k}{n}\\ &amp;amp;\Rightarrow S^2=\frac{(1-\frac{k}{n})^2}{(n-k)\frac{k}{n}}=\frac{n-k}{nk}=\frac{1}{k}-\frac{1}{n}\\ &amp;amp; \Rightarrow S=\sqrt{\frac{1}{k}-\frac{1}{n}}\\ \end{aligned}\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Exercise&lt;/h3&gt;
&lt;div id=&#34;q1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Q1&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;在&lt;span class=&#34;math inline&#34;&gt;\(n=100\)&lt;/span&gt;人中觀察到有&lt;span class=&#34;math inline&#34;&gt;\(k=40\)&lt;/span&gt;人患病，假設每個人只有患病，不患病兩個狀態，用二項分佈來模擬這個數據，&lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; 爲患病的概率。下面是 &lt;span class=&#34;math inline&#34;&gt;\(\pi \in [0.2,0.6]\)&lt;/span&gt; 區間的對數似然比方程曲線。&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pi &amp;lt;- seq(0.2, 0.6, by=0.01)
L &amp;lt;- (pi^40)*((1-pi)^60)
Lmax &amp;lt;- rep(max(L), 41)
LR &amp;lt;- L/Lmax
logLR &amp;lt;- log(LR)

plot(pi, logLR, type = &amp;quot;l&amp;quot;, ylim = c(-11, 0),yaxt=&amp;quot;n&amp;quot;,
     frame.plot = FALSE, ylab = &amp;quot;logLR(\U03C0)&amp;quot;, xlab = &amp;quot;\U03C0&amp;quot;)
grid(NA, 5, lwd = 2) # add some horizontal grid on the background
axis(2, at=seq(-12,0,2), las=2)
title(main = &amp;quot;Figure 1. Binomial log-likelihood ratio&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/post/2017-11-07-approximate-log-likelihood-ratios_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;用一個二次方程來模擬上面的對數似然比曲線：&lt;span class=&#34;math inline&#34;&gt;\(f(\pi)=-\frac{(\pi-M)^2}{2S^2}\)&lt;/span&gt;，其中 &lt;span class=&#34;math inline&#34;&gt;\(M=\hat\pi=\frac{k}{n}=0.4\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(S^2=\frac{p(1-p)}{n}=0.0024\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mai = c(1.2, 0.5, 1, 0.7))
quad &amp;lt;- -(pi-0.4)^2/(2*0.0024)
plot(pi, quad, type = &amp;quot;l&amp;quot;, ylim = c(-4, 0),yaxt=&amp;quot;n&amp;quot;, col=&amp;quot;red&amp;quot;,
     frame.plot = FALSE, ylab = &amp;quot;&amp;quot;, xlab = &amp;quot;\U03C0&amp;quot;)
lines(pi, logLR, col=&amp;quot;black&amp;quot;)
grid(NA, 4, lwd = 1) # add some horizontal grid on the background
axis(2, at=seq(-4,0,1), las=2)
title(main = &amp;quot;Figure 2. Quadratic approximation\n of binomial log-likelihood ratio \n 40 out of 100 subjects&amp;quot;)
abline(h=-1.92, lty=1, col=&amp;quot;red&amp;quot;)
axis(4, at=-1.92, las=2)

legend(x=0.27, y= -5.5 ,xpd = TRUE,  legend=c(&amp;quot;logLR&amp;quot;,&amp;quot;Quadratic&amp;quot;), bty = &amp;quot;n&amp;quot;,
       col=c(&amp;quot;black&amp;quot;,&amp;quot;red&amp;quot;), lty=c(1,1), horiz = TRUE) #the legend is below the graph&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/post/2017-11-07-approximate-log-likelihood-ratios_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;q2&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Q2&lt;/h4&gt;
&lt;p&gt;依舊使用二項分佈數據來模擬，觀察不同的事件數量和樣本量對近似計算的影響。&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;類比上面的問題，用同樣的 &lt;span class=&#34;math inline&#34;&gt;\(\hat\pi=0.4\)&lt;/span&gt;，但是 &lt;span class=&#34;math inline&#34;&gt;\(n=10, k=4\)&lt;/span&gt; 時的圖形：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/post/2017-11-07-approximate-log-likelihood-ratios_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat\pi=0.4, n=1000, k=400\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/post/2017-11-07-approximate-log-likelihood-ratios_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat\pi=0.01, n=100, k=1\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;注意此圖中紅線提示的近似二次曲線，信賴區間的下限已經低於0，是無法接受的近似。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/post/2017-11-07-approximate-log-likelihood-ratios_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat\pi=0.01, n=1000, k=10\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/post/2017-11-07-approximate-log-likelihood-ratios_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat\pi=0.01, n=10000, k=100\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/post/2017-11-07-approximate-log-likelihood-ratios_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat\pi=0.99, n=100, k=99\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;注意此圖中紅線提示的近似二次曲線，信賴區間的上限已經大於1，和上面的 Figure 5. 一樣也是無法接受的近似。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/post/2017-11-07-approximate-log-likelihood-ratios_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;總結： 二次方程近似時，在二項分佈的情況下，隨着 &lt;span class=&#34;math inline&#34;&gt;\(n, k\)&lt;/span&gt; 增加，近似越理想。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>對數似然比 Log-likelihood ratio</title>
      <link>https://wangcc.me/post/log-likelihood-ratio/</link>
      <pubDate>Sun, 05 Nov 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/log-likelihood-ratio/</guid>
      <description>
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;對數似然比-log-likelihood-ratio&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;對數似然比 Log-likelihood ratio&lt;/h3&gt;
&lt;p&gt;對數似然比的想法來自於將對數似然方程圖形的 &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; 軸重新調節 (rescale) 使之最大值爲零。這可以通過計算該分佈方程的&lt;strong&gt;對數似然比 (log-likelihood ratio)&lt;/strong&gt; 來獲得：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[llr(\theta)=\ell(\theta|data)-\ell(\hat{\theta}|data)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;由於 &lt;span class=&#34;math inline&#34;&gt;\(\ell(\theta)\)&lt;/span&gt; 的最大值在 &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt; 時， 所以，&lt;span class=&#34;math inline&#34;&gt;\(llr(\theta)\)&lt;/span&gt; 就是個當 &lt;span class=&#34;math inline&#34;&gt;\(\theta=\hat{\theta}\)&lt;/span&gt; 時取最大值，且最大值爲零的方程。很容易理解我們叫這個方程爲對數似然比，因爲這個方程就是將似然比 &lt;span class=&#34;math inline&#34;&gt;\(LR(\theta)=\frac{L(\theta)}{L(\hat{\theta})}\)&lt;/span&gt; 取對數而已。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://winterwang.github.io/post/likelihood/&#34;&gt;之前&lt;/a&gt;我們也確證了，不包含我們感興趣的參數的方程部分可以忽略掉。還是用上一節 10人中4人患病的例子：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[L(\pi|X=4)=\binom{10}{4}\pi^4(1-\pi)^{10-4}\\
\Rightarrow \ell(\pi)=log[\pi^4(1-\pi)^{10-4}]\\
\Rightarrow llr(\pi)=\ell(\pi)-\ell(\hat{\pi})=log\frac{\pi^4(1-\pi)^{10-4}}{0.4^4(1-0.4)^{10-4}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其實由上也可以看出 &lt;span class=&#34;math inline&#34;&gt;\(llr(\theta)\)&lt;/span&gt; 只是將對應的似然方程的 &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; 軸重新調節了一下而已。形狀是沒有改變的：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow=c(1,2))
x &amp;lt;- seq(0,1,by=0.001)
y &amp;lt;- (x^4)*((1-x)^6)/(0.4^4*0.6^6)
z &amp;lt;- log((x^4)*((1-x)^6))-log(0.4^4*0.6^6)
plot(x, y, type = &amp;quot;l&amp;quot;, ylim = c(0,1.1),yaxt=&amp;quot;n&amp;quot;,
     frame.plot = FALSE, ylab = &amp;quot;LR(\U03C0)&amp;quot;, xlab = &amp;quot;\U03C0&amp;quot;)
axis(2, at=seq(0,1, 0.2), las=2)
title(main = &amp;quot;Binomial likelihood ratio&amp;quot;)
abline(h=1.0, lty=2)
segments(x0=0.4, y0=0, x1=0.4, y1=1, lty = 2)
plot(x, z, type = &amp;quot;l&amp;quot;, ylim = c(-10, 1), yaxt=&amp;quot;n&amp;quot;, frame.plot = FALSE,
     ylab = &amp;quot;llr(\U03C0)&amp;quot;, xlab = &amp;quot;\U03C0&amp;quot; )
axis(2, at=seq(-10, 0, 2), las=2)
title(main = &amp;quot;Binomial log-likelihood ratio&amp;quot;)
abline(h=0, lty=2)
segments(x0=0.4, y0=-10, x1=0.4, y1=0, lty = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/post/2017-11-05-log-likelihood-ratio_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;正態分佈數據的最大似然和對數似然比&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;正態分佈數據的最大似然和對數似然比&lt;/h4&gt;
&lt;p&gt;假設單個樣本 &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; 是來自一組服從正態分佈數據的觀察值：&lt;span class=&#34;math inline&#34;&gt;\(Y\sim N(\mu, \tau^2)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;那麼有：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(y|\mu) &amp;amp;= \frac{1}{\sqrt{2\pi\tau^2}}e^{(-\frac{1}{2}(\frac{y-\mu}{\tau})^2)} \\
\Rightarrow L(\mu|y) &amp;amp;=\frac{1}{\sqrt{2\pi\tau^2}}e^{(-\frac{1}{2}(\frac{y-\mu}{\tau})^2)} \\
\Rightarrow \ell(\mu)&amp;amp;=log(\frac{1}{\sqrt{2\pi\tau^2}})-\frac{1}{2}(\frac{y-\mu}{\tau})^2\\
omitting&amp;amp;\;terms\;not\;in\;\mu \\
&amp;amp;= -\frac{1}{2}(\frac{y-\mu}{\tau})^2 \\
\Rightarrow \ell^\prime(\mu) &amp;amp;= 2\cdot[-\frac{1}{2}(\frac{y-\mu}{\tau})\cdot\frac{-1}{\tau}] \\
&amp;amp;=\frac{y-\mu}{\tau^2} \\
let \; \ell^\prime(\mu) &amp;amp;= 0 \\
\Rightarrow \frac{y-\mu}{\tau^2} &amp;amp;= 0 \Rightarrow \hat{\mu} = y\\
\because \ell^{\prime\prime}(\mu) &amp;amp;=  \frac{-1}{\tau^2} &amp;lt; 0 \\
\therefore \hat{\mu} &amp;amp;= y \Rightarrow \ell(\hat{\mu}=y)_{max}=0 \\
llr(\mu)&amp;amp;=\ell(\mu)-\ell(\hat{\mu})=\ell(\mu)\\
&amp;amp;=-\frac{1}{2}(\frac{y-\mu}{\tau})^2
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;n-個獨立正態分佈樣本的對數似然比&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 個獨立正態分佈樣本的對數似然比&lt;/h3&gt;
&lt;p&gt;假設一組觀察值來自正態分佈 &lt;span class=&#34;math inline&#34;&gt;\(X_1,\cdots,X_n\stackrel{i.i.d}{\sim}N(\mu,\sigma^2)\)&lt;/span&gt;，先假設 &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; 已知。將觀察數據 &lt;span class=&#34;math inline&#34;&gt;\(x_1,\cdots, x_n\)&lt;/span&gt; 標記爲 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}\)&lt;/span&gt;。 那麼：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
L(\mu|\underline{x}) &amp;amp;=\prod_{i=1}^nf(x_i|\mu)\\
\Rightarrow \ell(\mu|\underline{x}) &amp;amp;=\sum_{i=1}^nlogf(x_i|\mu)\\
&amp;amp;=\sum_{i=1}^n[-\frac{1}{2}(\frac{x_i-\mu}{\sigma})^2]\\
&amp;amp;=-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2\\
&amp;amp;=-\frac{1}{2\sigma^2}[\sum_{i=1}^n(x_i-\bar{x})^2+\sum_{i=1}^n(\bar{x}-\mu)^2]\\
omitting&amp;amp;\;terms\;not\;in\;\mu \\
&amp;amp;=-\frac{1}{2\sigma^2}\sum_{i=1}^n(\bar{x}-\mu)^2\\
&amp;amp;=-\frac{n}{2\sigma^2}(\bar{x}-\mu)^2 \\
&amp;amp;=-\frac{1}{2}(\frac{\bar{x}-\mu}{\sigma/\sqrt{n}})^2\\
\because \ell(\hat{\mu}) &amp;amp;= 0 \\
\therefore llr(\mu) &amp;amp;= \ell(\mu)-\ell(\hat{\mu}) = \ell(\mu)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;n-個獨立正態分佈樣本的對數似然比的分佈&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 個獨立正態分佈樣本的對數似然比的分佈&lt;/h3&gt;
&lt;p&gt;假設我們用 &lt;span class=&#34;math inline&#34;&gt;\(\mu_0\)&lt;/span&gt; 表示總體均數這一參數的值。要注意的是，每當樣本被重新取樣，似然，對數似然方程，對數似然比都隨着觀察值而變 (即有自己的分佈)。&lt;/p&gt;
&lt;p&gt;考慮一個服從正態分佈的單樣本 &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;: &lt;span class=&#34;math inline&#34;&gt;\(Y\sim N(\mu_0,\tau^2)\)&lt;/span&gt;。那麼它的對數似然比：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[llr(\mu_0|Y)=\ell(\mu_0)-\ell(\hat{\mu})=-\frac{1}{2}(\frac{Y-\mu_0}{\tau})^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;根據&lt;a href=&#34;https://winterwang.github.io/post/chi-square-distribution/&#34;&gt;卡方分佈&lt;/a&gt;的定義：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\because \frac{Y-\mu_0}{\tau}\sim N(0,1)\\
\Rightarrow (\frac{Y-\mu_0}{\tau})^2 \sim \mathcal{X}_1^2\\
\therefore -2llr(\mu_0|Y) \sim \mathcal{X}_1^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;所以，如果有一組服從正態分佈的觀察值：&lt;span class=&#34;math inline&#34;&gt;\(X_1,\cdots,X_n\stackrel{i.i.d}{\sim}N(\mu_0,\sigma^2)\)&lt;/span&gt;，且 &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; 已知的話：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[-2llr(\mu_0|\bar{X})\sim \mathcal{X}_1^2\]&lt;/span&gt;&lt;/p&gt;
根據&lt;a href=&#34;https://winterwang.github.io/post/central-limit-theory/&#34;&gt;中心極限定理&lt;/a&gt;，可以將上面的結論一般化：

&lt;div class=&#34;theorem&#34;&gt;
&lt;span id=&#34;thm:unnamed-chunk-2&#34; class=&#34;theorem&#34;&gt;&lt;strong&gt;Theorem 1  &lt;/strong&gt;&lt;/span&gt;如果 &lt;span class=&#34;math inline&#34;&gt;\(X_1,\cdots,X_n\stackrel{i.i.d}{\sim}f(x|\theta)\)&lt;/span&gt;。 那麼當重複多次從參數爲 &lt;span class=&#34;math inline&#34;&gt;\(\theta_0\)&lt;/span&gt; 的總體中取樣時，那麼統計量 &lt;span class=&#34;math inline&#34;&gt;\(-2llr(\theta_0)\)&lt;/span&gt; 會漸進於自由度爲 &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; 的卡方分佈： &lt;span class=&#34;math display&#34;&gt;\[-2llr(\theta_0)=-2\{\ell(\theta_0)-\ell(\hat{\theta})\}\xrightarrow[n\rightarrow\infty]{}\;\sim \mathcal{X}_1^2\]&lt;/span&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div id=&#34;似然比信賴區間&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;似然比信賴區間&lt;/h3&gt;
&lt;p&gt;如果樣本量 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 足夠大 (通常應該大於 &lt;span class=&#34;math inline&#34;&gt;\(30\)&lt;/span&gt;)，根據上面的定理：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[-2llr(\theta_0)=-2\{\ell(\theta_0)-\ell(\hat{\theta})\}\sim \mathcal{X}_1^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;所以：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Prob(-2llr(\theta_0)\leqslant \mathcal{X}_{1,0.95}^2=3.84) = 0.95\\
\Rightarrow Prob(llr(\theta_0)\geqslant-3.84/2=-1.92) = 0.95\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;故似然比的 &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; 信賴區間就是能夠滿足 &lt;span class=&#34;math inline&#34;&gt;\(llr(\theta)=-1.92\)&lt;/span&gt; 的兩個 &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; 值。&lt;/p&gt;
&lt;div id=&#34;以二項分佈數據爲例&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;以二項分佈數據爲例&lt;/h4&gt;
&lt;p&gt;繼續用本文開頭的例子：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[llr(\pi)=\ell(\pi)-\ell(\hat{\pi})=log\frac{\pi^4(1-\pi)^{10-4}}{0.4^4(1-0.4)^{10-4}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;如果令 &lt;span class=&#34;math inline&#34;&gt;\(llr(\pi)=-1.92\)&lt;/span&gt; 在代數上可能較難獲得答案。然而從圖形上，如果我們在 &lt;span class=&#34;math inline&#34;&gt;\(y=-1.92\)&lt;/span&gt; 畫一條橫線，和該似然比方程曲線相交的兩個點就是我們想要求的信賴區間的上限和下限：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- seq(0,1,by=0.001)
z &amp;lt;- log((x^4)*((1-x)^6))-log(0.4^4*0.6^6)
plot(x, z, type = &amp;quot;l&amp;quot;, ylim = c(-10, 1), yaxt=&amp;quot;n&amp;quot;, frame.plot = FALSE,
     ylab = &amp;quot;llr(\U03C0)&amp;quot;, xlab = &amp;quot;\U03C0&amp;quot; )
axis(2, at=seq(-10, 0, 2), las=2)
abline(h=0, lty=2)
abline(h=-1.92, lty=2)
segments(x0=0.15, y0=-12, x1=0.15, y1=-1.92, lty = 2)
segments(x0=0.7, y0=-12, x1=0.7, y1=-1.92, lty = 2)
axis(1, at=c(0.15,0.7))
text(0.9, -1, &amp;quot;-1.92&amp;quot;)
arrows(0.8, -1.92, 0.8, 0, lty = 1, length = 0.08)
arrows( 0.8, 0, 0.8, -1.92, lty = 1, length = 0.08)
title(main = &amp;quot;Log-likelihood ratio for binomial example, \n with 95% likelihood confidence interval shown&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/post/2017-11-05-log-likelihood-ratio_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;從上圖中可以讀出，&lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; 對數似然比信賴區間就是 &lt;span class=&#34;math inline&#34;&gt;\((0.15, 0.7)\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;以正態分佈數據爲例&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;以正態分佈數據爲例&lt;/h4&gt;
&lt;p&gt;本文前半部分證明過，
&lt;span class=&#34;math inline&#34;&gt;\(X_1,\cdots,X_n\stackrel{i.i.d}{\sim}N(\mu,\sigma^2)\)&lt;/span&gt;，先假設 &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; 已知。將觀察數據 &lt;span class=&#34;math inline&#34;&gt;\(x_1,\cdots, x_n\)&lt;/span&gt; 標記爲 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}\)&lt;/span&gt;。 那麼：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[llr(\mu|\underline{x}) = \ell(\mu|\underline{x})-\ell(\hat{\mu}) = \ell(\mu|\underline{x}) \\
=-\frac{1}{2}(\frac{\bar{x}-\mu}{\sigma/\sqrt{n}})^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;很顯然，這是一個關於 &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; 的二次方程，且最大值在 MLE &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mu}=\bar{x}\)&lt;/span&gt; 時取值 &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;。所以可以通過對數似然比法求出均值的 &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; 信賴區間公式：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[-2\times[-\frac{1}{2}(\frac{\bar{x}-\mu}{\sigma/\sqrt{n}})^2]=3.84\\
\Rightarrow L=\bar{x}-\sqrt{3.84}\frac{\sigma}{\sqrt{n}} \\
U=\bar{x}+\sqrt{3.84}\frac{\sigma}{\sqrt{n}} \\
note: \;\sqrt{3.84}=1.96\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;注意到這和我們&lt;a href=&#34;https://winterwang.github.io/post/frequentist-statistical-inference02/&#34;&gt;之前&lt;/a&gt;求的正態分佈均值的信賴區間公式完全一致。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Exercise&lt;/h3&gt;
&lt;div id=&#34;q1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Q1&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;假設十個對象中有三人死亡，用二項分佈模型來模擬這個例子，求這個例子中參數 &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; 的似然方程和圖形 (likelihood) ?&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;解&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;解&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\begin{aligned}  L(\pi|3) &amp;amp;= \binom{10}{3}\pi^3(1-\pi)^{10-3} \\  omitting\;&amp;amp;terms\;not\;in\;\mu \\  \Rightarrow \ell(\pi|3) &amp;amp;= log[\pi^3(1-\pi)^7] \\  &amp;amp;= 3log\pi+7log(1-\pi)\\  \Rightarrow \ell^\prime(\pi|3)&amp;amp;= \frac{3}{\pi}-\frac{7}{1-\pi} \\  let \; \ell^\prime&amp;amp; =0\\  &amp;amp;\frac{3}{\pi}-\frac{7}{1-\pi} = 0 \\  &amp;amp;\frac{3-10\pi}{\pi(1-\pi)} = 0 \\  \Rightarrow MLE &amp;amp;= \hat\pi = 0.3 \end{aligned}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/post/2017-11-05-log-likelihood-ratio_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;計算似然比，並作圖，注意方程圖形未變，&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; 軸的變化；取對數似然比，並作圖&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;LR &amp;lt;- L/max(L) ; head(LR)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0000000000 0.0004191759 0.0031233631 0.0098110584 0.0216286076
## [6] 0.0392577320&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(pi, LR, type = &amp;quot;l&amp;quot;, ylim = c(0, 1),yaxt=&amp;quot;n&amp;quot;, col=&amp;quot;darkblue&amp;quot;,
     frame.plot = FALSE, ylab = &amp;quot;&amp;quot;, xlab = &amp;quot;\U03C0&amp;quot;)
grid(NA, 5, lwd = 1)
axis(2, at=seq(0,1,0.2), las=2)
title(main = &amp;quot;Binomial likelihood ratio function\n 3 out of 10 subjects&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/post/2017-11-05-log-likelihood-ratio_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logLR &amp;lt;- log(L/max(L))
plot(pi, logLR, type = &amp;quot;l&amp;quot;, ylim = c(-4, 0),yaxt=&amp;quot;n&amp;quot;, col=&amp;quot;darkblue&amp;quot;,
     frame.plot = FALSE, ylab = &amp;quot;&amp;quot;, xlab = &amp;quot;\U03C0&amp;quot;)
grid(NA, 5, lwd = 1)
axis(2, at=seq(-4,0,1), las=2)
title(main = &amp;quot;Binomial log-likelihood ratio function\n 3 out of 10 subjects&amp;quot;)
abline(h=-1.92, lty=1, col=&amp;quot;red&amp;quot;)
axis(4, at=-1.92, las=0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/post/2017-11-05-log-likelihood-ratio_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;q2&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Q2&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;與上面用同樣的模型，但是觀察人數變爲 &lt;span class=&#34;math inline&#34;&gt;\(100\)&lt;/span&gt; 人 患病人數爲 &lt;span class=&#34;math inline&#34;&gt;\(30\)&lt;/span&gt; 人，試作對數似然比方程之圖形，與上圖對比：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/post/2017-11-05-log-likelihood-ratio_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;p&gt;可以看出，兩組數據的 MLE 都是一致的， &lt;span class=&#34;math inline&#34;&gt;\(\hat\pi=0.3\)&lt;/span&gt;，但是對數似然比方程圖形在 樣本量爲 &lt;span class=&#34;math inline&#34;&gt;\(n=100\)&lt;/span&gt; 時比 &lt;span class=&#34;math inline&#34;&gt;\(n=10\)&lt;/span&gt; 時窄很多，由此產生的似然比信賴區間也就窄很多（精確很多）。所以對數似然比方程的曲率（二階導數），反映了觀察獲得數據提供的對總體參數 &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; 推斷過程中的信息量。而且當樣本量較大時，對數似然比方程也更加接近左右對稱的二次方程曲線。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;q3&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Q3&lt;/h4&gt;
&lt;p&gt;在一個實施了160人年的追蹤調查中，觀察到8個死亡案例。使用泊松分佈模型，繪製對數似然比方程圖形，從圖形上目視推測極大似然比的 &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; 信賴區間。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;解-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;解&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\begin{aligned}  d = 8, \;p &amp;amp;= 160\; person\cdot year \\  \Rightarrow D\sim Poi(\mu &amp;amp;=\lambda p) \\  L(\lambda|data) &amp;amp;= Prob(D=d=8) \\  &amp;amp;= e^{-\mu}\frac{\mu^d}{d!} \\  &amp;amp;= e^{-\lambda p}\frac{\lambda^d p^d}{d!} \\  omitting&amp;amp;\;terms\;not\;in\;\lambda \\  &amp;amp;= e^{-\lambda p}\lambda^d \\ \Rightarrow \ell(\lambda|data)&amp;amp;= log(e^{-\lambda p}\lambda^d) \\  &amp;amp;= d\cdot log(\lambda)-\lambda p \\  &amp;amp; = 8\times log(\lambda) - 160\times\lambda \end{aligned}\)&lt;/span&gt;&lt;/p&gt;
&lt;img src=&#34;https://wangcc.me/post/2017-11-05-log-likelihood-ratio_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;480&#34; /&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
lambda
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
LogLR
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.010
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-6.4755033
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.011
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-5.8730219
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.012
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-5.3369308
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.013
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-4.8565892
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.014
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-4.4237254
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.015
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-4.0317824
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.016
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-3.6754743
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.017
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-3.3504773
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.018
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-3.0532100
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.019
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.7806722
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.020
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.5303259
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.021
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.3000045
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;&#34;&gt;
0.022
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;&#34;&gt;
-2.0878444
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;&#34;&gt;
0.023
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;&#34;&gt;
-1.8922303
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.024
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.7117534
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.025
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.5451774
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.026
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.3914117
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.027
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.2494891
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.028
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.1185480
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.029
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.9978174
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.030
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.8866050
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.031
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.7842864
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.032
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.6902968
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.033
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.6041236
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.034
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.5252998
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.035
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.4533996
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.036
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.3880325
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.037
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.3288407
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.038
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.2754948
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.039
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.2276909
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.040
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.1851484
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.041
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.1476075
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.042
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.1148271
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.043
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0865831
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.044
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0626670
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.045
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0428841
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.046
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0270529
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.047
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0150032
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.048
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0065760
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.049
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0016217
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.050
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000000
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.051
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0015790
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.052
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0062343
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.053
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0138487
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.054
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0243117
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.055
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0375186
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.056
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0533705
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.057
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0717739
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.058
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0926400
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.059
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.1158845
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.060
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.1414275
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.061
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.1691931
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.062
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.1991090
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.063
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.2311062
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.064
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.2651194
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.065
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.3010859
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.066
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.3389461
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.067
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.3786431
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.068
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.4201224
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.069
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.4633320
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.070
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.5082221
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.071
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.5547450
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.072
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.6028551
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.073
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.6525085
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.074
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.7036633
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.075
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.7562791
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.076
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.8103173
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.077
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.8657407
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.078
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.9225134
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.079
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.9806012
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.080
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.0399710
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.081
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.1005908
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.082
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.1624301
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.083
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.2254592
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.084
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.2896497
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.085
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.3549740
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.086
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.4214057
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.087
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.4889191
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.088
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.5574895
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.089
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.6270931
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.090
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.6977067
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.091
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.7693080
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.092
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.8418754
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;&#34;&gt;
0.093
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;&#34;&gt;
-1.9153881
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;&#34;&gt;
0.094
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;&#34;&gt;
-1.9898258
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.095
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.0651689
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.096
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.1413985
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.097
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.2184962
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.098
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.2964442
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.099
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.3752252
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.100
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.4548226
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;所以從列表數據結合圖形， 可以找到信賴區間的下限在 0.022~0.023 之間， 上限在 0.093～0.094 之間。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>似然非然 Likelihood</title>
      <link>https://wangcc.me/post/likelihood/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/likelihood/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#概率-vs.推斷probability-vs.inference&#34;&gt;概率 vs. 推斷/Probability vs. Inference&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#似然和極大似然估計&#34;&gt;似然和極大似然估計&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#似然方程的一般化定義&#34;&gt;似然方程的一般化定義&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#對數似然方程-log-likelihood&#34;&gt;對數似然方程 log-likelihood&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#極大似然估計-maximum-likelihood-estimator-mle-的性質&#34;&gt;極大似然估計 (maximum likelihood estimator, MLE) 的性質：&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#率的似然估計-likelihood-for-a-rate&#34;&gt;率的似然估計 Likelihood for a rate&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#有-n-個獨立觀察時的似然方程和對數似然方程&#34;&gt;有 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 個獨立觀察時的似然方程和對數似然方程&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;概率-vs.推斷probability-vs.inference&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;概率 vs. 推斷/Probability vs. Inference&lt;/h3&gt;
&lt;p&gt;在概率論的環境下，我們常常被告知的前提是：某某事件發生的概率是多少。例如： 一枚硬幣正面朝上的概率是 &lt;span class=&#34;math inline&#34;&gt;\(0.5\; Prob(coin\;landing\;heads)=0.5\)&lt;/span&gt;。然後在這個前提下，我們又繼續去計算複雜的事件發生的概率（例如，10次投擲硬幣以後4次正面朝上的概率是多少？）。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\binom{10}{4}\times(0.5^4)\times(0.5^{10-4}) = 0.205
\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dbinom(4, 10, 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2050781&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# or you can calculate by hand:
factorial(10)*(0.5^10)/(factorial(4)*(factorial(6)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2050781&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在統計推斷的理論中，我們考慮實際的情況，這樣的實際情況就是，我們通過觀察獲得數據，然而我們並不知道某事件發生的概率到底是多少（神如果存在話，只有神知道）。故這個 &lt;span class=&#34;math inline&#34;&gt;\(Prob(coin\;landing\;heads)\)&lt;/span&gt; 的概率大小對於“人類”來說是未知的。我們可能觀察到投擲了10次硬幣，其中有4次是正面朝上的。那麼我們從這一次觀察實驗中，需要計算的是能夠符合觀察結果的“最佳”概率估計 (best estimate)。在這種情況下，&lt;strong&gt;似然法 (likelihood)&lt;/strong&gt; 就是我們進行參數估計的最佳手段。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;似然和極大似然估計&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;似然和極大似然估計&lt;/h3&gt;
&lt;p&gt;此處用二項分佈的例子來理解似然法的概念：假設我們觀察到10個對象中有4個患病，我們假定這個患病的概率爲 &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt;。於是我們就有了下面的模型：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;模型：&lt;/strong&gt; 我們假定患病與否是一個服從&lt;strong&gt;二項分佈的隨機變量&lt;/strong&gt;，&lt;span class=&#34;math inline&#34;&gt;\(X\sim Bin(10,\pi)\)&lt;/span&gt;。同時也默認每個人之間是否患病是相互獨立的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;數據：&lt;/strong&gt; 觀察到的數據是，10人中有4人患病。於是 &lt;span class=&#34;math inline&#34;&gt;\(x=4\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;現在按照觀察到的數據，參數 &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; 變成了未知數：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Prob(X=4|\pi)=\binom{10}{4}\pi^4(1-\pi)^{10-4}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;此時我們會很自然的考慮，當 &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; 是未知數的時候，&lt;strong&gt;它取值爲多大的時候才能讓這個事件（即：10人中4人患病）發生的概率最大？&lt;/strong&gt; 所以我們可以將不同的數值代入 &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; 來計算該事件在不同概率的情況下發生的可能性到底是多少：&lt;/p&gt;
&lt;table class=&#34;table table-striped table-bordered&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;caption&gt;
Table 1: The probability of observing &lt;span class=&#34;math inline&#34;&gt;\(X=4\)&lt;/span&gt;
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
事件 &lt;span class=&#34;math inline&#34;&gt;\(X=4\)&lt;/span&gt; 發生的概率
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.000
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.088
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;0.4&lt;/strong&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;0.251&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.205
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.6
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.111
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.8
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.006
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.000
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;很顯然，如果 &lt;span class=&#34;math inline&#34;&gt;\(\pi=0.4\)&lt;/span&gt; 時，我們觀察到的事件發生的概率要比 &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; 取其它值時更大。於是小總結一下目前爲止的步驟如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;觀察到實驗數據（10人中4個患病）；&lt;/li&gt;
&lt;li&gt;假定這數據服從二項分佈的概率模型，計算不同（&lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; 的取值不同的）情況下，該事件按照假定模型發生的概率；&lt;/li&gt;
&lt;li&gt;通過比較，我們選擇了能夠讓觀察事件發生概率最高的參數取值 (&lt;span class=&#34;math inline&#34;&gt;\(\pi=0.4\)&lt;/span&gt;)。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;至此，我們可以知道，似然方程，是一個關於未知參數 &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; 的函數，我們目前位置做的就是找到這個函數的最大值 (maximised)，和使之成爲最大值時的 &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; ：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[L(\pi|X=4)=\binom{10}{4}\pi^4(1-\pi)^{10-4}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;我們可以畫出這個似然方程的形狀， &lt;span class=&#34;math inline&#34;&gt;\(\pi\in[0,1]\)&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- seq(0,1,by=0.001)
y &amp;lt;- (factorial(10)/(factorial(4)*(factorial(6))))*(x^4)*((1-x)^6)
plot(x, y, type = &amp;quot;l&amp;quot;, ylim = c(0,0.3), ylab = &amp;quot;L(\U03C0)&amp;quot;, xlab = &amp;quot;\U03C0&amp;quot;)
title(&amp;quot;Figure 1. Binomial Likelihood&amp;quot;)
abline(h=0.251, lty=2)
abline(v=0.4, lty=2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/post/2017-11-02-likelihood_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;p&gt;從圖形上我們也能確認，&lt;span class=&#34;math inline&#34;&gt;\(\pi=0.4\)&lt;/span&gt; 時能夠讓這個似然方程取得最大值。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;似然方程的一般化定義&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;似然方程的一般化定義&lt;/h3&gt;
&lt;p&gt;對於一個概率模型，如果其參數爲 &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;，那麼在給定觀察數據 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}\)&lt;/span&gt; 時，該參數的似然方程被定義爲：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(L(\theta|\underline{x})=P(\underline{x}|\theta)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;注意：&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(\underline{x}|\theta)\)&lt;/span&gt; 可以是概率（離散分佈）方程，也可以是概率密度（連續型變量）方程。對於此方程，&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; 是給定的，然後再計算某些事件發生的概率。&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(L(\theta|\underline{x})\)&lt;/span&gt; 是一個關於參數 &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; 的方程，此時，&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}\)&lt;/span&gt; 是固定不變的（觀察值）。我們希望通過這個方程求出能夠使觀察到的事件發生概率最大的參數值。&lt;/li&gt;
&lt;li&gt;似然方程&lt;strong&gt;不是&lt;/strong&gt;一個概率密度方程。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;另一個例子：&lt;/p&gt;
&lt;p&gt;有一組觀察數據是離散型隨機變量 &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;，它符合概率方程 &lt;span class=&#34;math inline&#34;&gt;\(f(x|\theta)\)&lt;/span&gt;。下表羅列了當 &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; 分別取值 &lt;span class=&#34;math inline&#34;&gt;\(1,2,3\)&lt;/span&gt; 時的概率方程的值，試求每個觀察值 &lt;span class=&#34;math inline&#34;&gt;\(X = 0,1,2,3,4\)&lt;/span&gt; 的最大似然參數估計：&lt;/p&gt;
&lt;table class=&#34;table table-striped table-bordered&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;caption&gt;
Exercise 1
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(f(x|1)\)&lt;/span&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(f(x|2)\)&lt;/span&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(f(x|3)\)&lt;/span&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1/3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1/4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1/3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1/4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1/4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1/6
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1/6
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1/4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1/2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1/6
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1/3
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class=&#34;table table-striped table-bordered&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;caption&gt;
Exercise 1 answer
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(f(x|1)\)&lt;/span&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(f(x|2)\)&lt;/span&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(f(x|3)\)&lt;/span&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1/3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1/4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;1&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1/3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1/4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;1&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1/4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1/6
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;2&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1/6
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1/4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1/2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;3&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1/6
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1/3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;3&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;對數似然方程-log-likelihood&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;對數似然方程 log-likelihood&lt;/h3&gt;
&lt;p&gt;似然方程的最大值，可通過求 &lt;span class=&#34;math inline&#34;&gt;\(L(\theta|data)\)&lt;/span&gt; 的最大值獲得，也可以通過求該方程的對數方程 &lt;span class=&#34;math inline&#34;&gt;\(\ell(\theta|data)\)&lt;/span&gt; 的最大值獲得。傳統上，我們估計最大方程的最大值的時候，會給參數戴一頂“帽子”（因爲這是觀察獲得的數據告訴我們的參數）： &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt;。並且我們發現對數似然方程比一般的似然方程更加容易微分，因此求似然方程的最大值就變成了求對數似然方程的最大值：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{d\ell}{d\theta}=\ell^\prime(\theta)=0\\
AND\\
\frac{d^2\ell}{d\theta^2}&amp;lt;0\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;要注意的是，微分不一定總是能幫助我們求得似然方程的最大值。如果說參數本身的定義域是有界限的話，微分就行不通了：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- seq(0,3,by=0.001)
y &amp;lt;- (x-1)^2-5
plot(x, y, type = &amp;quot;l&amp;quot;, ylim = c(-5,0-1), ylab = &amp;quot;L(\U03B8)&amp;quot;, xlab = &amp;quot;\U03B8&amp;quot;)
title(&amp;quot;Figure 2. Likelihood function with \n a limited domain&amp;quot;)
abline(v=3, lty=2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/post/2017-11-02-likelihood_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;證明當-lthetadata-取最大值時該方程的對數方程-ellthetadata-也是最大值&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;證明：當 &lt;span class=&#34;math inline&#34;&gt;\(L(\theta|data)\)&lt;/span&gt; 取最大值時，該方程的對數方程 &lt;span class=&#34;math inline&#34;&gt;\(\ell(\theta|data)\)&lt;/span&gt; 也是最大值：&lt;/h4&gt;
&lt;p&gt;如果似然方程是連續可導，只有一個最大值，且可以二次求導，假設 &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt; 使該方程取最大值，那麼：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{dL}{d\theta}=0, \frac{d^2L}{d\theta^2}&amp;lt;0 \Rightarrow \theta=\hat{\theta}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;令 &lt;span class=&#34;math inline&#34;&gt;\(\ell=logL\)&lt;/span&gt; 那麼 &lt;span class=&#34;math inline&#34;&gt;\(\frac{d\ell}{dL}=\ell^\prime=\frac{1}{L}\)&lt;/span&gt;：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{d\ell}{d\theta}=\frac{d\ell}{dL}\cdot\frac{dL}{d\theta}=\frac{1}{L}\cdot\frac{dL}{d\theta}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;當 &lt;span class=&#34;math inline&#34;&gt;\(\ell(\theta|data)\)&lt;/span&gt; 取最大值時：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{d\ell}{d\theta}=0\Leftrightarrow\frac{1}{L}\cdot\frac{dL}{d\theta}=0\\
\because \frac{1}{L}\neq0 \\
\therefore \frac{dL}{d\theta}=0\\
\Leftrightarrow \theta=\hat{\theta}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\frac{d^2\ell}{d\theta^2} &amp;amp;= \frac{d}{d\theta}(\frac{d\ell}{dL}\cdot\frac{dL}{d\theta})\\
 &amp;amp;= \frac{d\ell}{dL}\cdot\frac{d^2L}{d\theta^2} + \frac{dL}{d\theta}\cdot\frac{d}{d\theta}(\frac{d\ell}{dL})
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;當 &lt;span class=&#34;math inline&#34;&gt;\(\theta=\hat{\theta}\)&lt;/span&gt; 時，&lt;span class=&#34;math inline&#34;&gt;\(\frac{dL}{d\theta}=0\)&lt;/span&gt; 且 &lt;span class=&#34;math inline&#34;&gt;\(\frac{d^2L}{d\theta^2}&amp;lt;0 \Rightarrow \frac{d^2\ell}{d\theta^2}&amp;lt;0\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;所以，求獲得 &lt;span class=&#34;math inline&#34;&gt;\(\ell(\theta|data)\)&lt;/span&gt; 最大值的 &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; 即可令 &lt;span class=&#34;math inline&#34;&gt;\(L(\theta|data)\)&lt;/span&gt; 獲得最大值。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;極大似然估計-maximum-likelihood-estimator-mle-的性質&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;極大似然估計 (maximum likelihood estimator, MLE) 的性質：&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;漸進無偏 Asymptotically unbiased: &lt;br&gt; &lt;span class=&#34;math inline&#34;&gt;\(n\rightarrow \infty \Rightarrow E(\hat{\Theta}) \rightarrow \theta\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;漸進最高效能 Asymptotically efficient: &lt;br&gt; &lt;span class=&#34;math inline&#34;&gt;\(n\rightarrow \infty \Rightarrow Var(\hat{\Theta})\)&lt;/span&gt; 是所有參數中方差最小的估計&lt;/li&gt;
&lt;li&gt;漸進正態分佈 Asymptotically normal: &lt;br&gt; &lt;span class=&#34;math inline&#34;&gt;\(n\rightarrow \infty \Rightarrow \hat{\Theta} \sim N(\theta, Var(\hat{\Theta}))\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;變形後依然保持不變 Transformation invariant: &lt;br&gt; &lt;span class=&#34;math inline&#34;&gt;\(\hat{\Theta}\)&lt;/span&gt; 是 &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; 的MLE時 &lt;span class=&#34;math inline&#34;&gt;\(\Rightarrow g(\hat{\Theta})\)&lt;/span&gt; 是 &lt;span class=&#34;math inline&#34;&gt;\(g(\theta)\)&lt;/span&gt; 的 MLE&lt;/li&gt;
&lt;li&gt;信息足夠充分 Sufficient：&lt;br&gt; &lt;span class=&#34;math inline&#34;&gt;\(\hat{\Theta}\)&lt;/span&gt; 包含了觀察數據中所有的能夠用於估計參數的信息&lt;/li&gt;
&lt;li&gt;始終不變 consistent: &lt;br&gt; &lt;span class=&#34;math inline&#34;&gt;\(n\rightarrow\infty\Rightarrow\hat{\Theta}\rightarrow\theta\)&lt;/span&gt; 或者可以寫成：&lt;span class=&#34;math inline&#34;&gt;\(\varepsilon&amp;gt;0, lim_{n\rightarrow\infty}P(|\hat{\Theta}-\theta|&amp;gt;\varepsilon)=0\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;率的似然估計-likelihood-for-a-rate&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;率的似然估計 Likelihood for a rate&lt;/h3&gt;
&lt;p&gt;如果在一項研究中，參與者有各自不同的追蹤隨訪時間（長度），那麼我們應該把事件（疾病）的發病率用率的形式（多少事件每單位人年, e.g. per person year of observation）。如果這個發病率的參數用 &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; 來表示，所有參與對象的隨訪時間之和爲 &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; 人年。那麼這段時間內的期望事件（疾病發病）次數爲：&lt;span class=&#34;math inline&#34;&gt;\(\mu=\lambda p\)&lt;/span&gt;。假設事件（疾病發病）發生是相互獨立的，可以使用泊松分佈來模擬期望事件（疾病發病）次數 &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt;：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[D\sim Poi(\mu)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;假設我們觀察到了 &lt;span class=&#34;math inline&#34;&gt;\(D=d\)&lt;/span&gt; 個事件，我們獲得這個觀察值的概率應該用以下的模型：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Prob(D=d)=e^{-\mu}\frac{\mu^d}{d!}=e^{-\lambda p}\frac{\lambda^dp^d}{d!}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;因此，&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; 的似然方程是：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[L(\lambda|observed \;data)=e^{-\lambda p}\frac{\lambda^dp^d}{d!}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;所以，&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; 的對數似然方程是：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\ell(\lambda|observed\;data) &amp;amp;= log(e^{-\lambda p}\frac{\lambda^dp^d}{d!}) \\
  &amp;amp;= -\lambda p+d\:log(\lambda)+d\:log(p)-log(d!) \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;解 &lt;span class=&#34;math inline&#34;&gt;\(\ell^\prime(\lambda|data)=0\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\ell^\prime(\lambda|data) &amp;amp;= -p+\frac{d}{\lambda}=0\\
\Rightarrow \hat{\lambda} &amp;amp;= \frac{d}{p} \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;注意：&lt;/strong&gt;
在對數似然方程中，不包含參數的部分，對與似然方程的形狀不產生任何影響，我們在微分對數似然方程的時候，這部分也都自動消失。所以不包含參數的部分，與我們如何獲得極大似然估計是無關的。因此，我們常常在寫對數似然方程的時候就把其中沒有參數的部分直接忽略了。例如上面泊松分佈的似然方程中，&lt;span class=&#34;math inline&#34;&gt;\(d\:log(p)-log(d!)\)&lt;/span&gt; 不包含參數 &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; 可以直接不寫出來。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;有-n-個獨立觀察時的似然方程和對數似然方程&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;有 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 個獨立觀察時的似然方程和對數似然方程&lt;/h3&gt;
&lt;p&gt;當有多個獨立觀察時，總體的似然方程等於各個觀察值的似然方程之&lt;strong&gt;乘積&lt;/strong&gt;。如果 &lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\stackrel{i.i.d}{\sim}f(\cdot|\theta)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[L(\theta|x_1,\cdots,x_n)=f(x_1,\cdots,x_n|\theta)=\prod_{i=1}^nf(x_i|\theta)\\
\Rightarrow \ell(\theta|x_1,\cdots,x_n)=\sum_{i=1}^nlog(f(x_i|\theta))\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>臨牀實驗的樣本量計算問題 Sample Size in Clinical Trial</title>
      <link>https://wangcc.me/post/sample-size-in-clinical-trial/</link>
      <pubDate>Sun, 29 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/sample-size-in-clinical-trial/</guid>
      <description>
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;背景&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;背景&lt;/h3&gt;
&lt;p&gt;計劃臨牀實驗的時候，爲了避免偏倚和帶有偏見的結論，應當將注意力放在&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;如何將實驗對象隨機分配 (randomisation)&lt;/li&gt;
&lt;li&gt;設計對照組 (control group)&lt;/li&gt;
&lt;li&gt;合適（且必須）的貫徹盲法 (blinding)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;另外一個同樣重要的問題是–&lt;strong&gt;“我到底需要多少樣本?”&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;一項臨牀實驗，應該提供足夠的證據來證明新藥物（新治療方法）是否有效，是否安全。影響一個實驗設計的樣本量的因素可能有如下幾種：&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;統計學方案。&lt;/strong&gt;
&lt;br&gt; 從統計學上可以推算出，需要多少樣本來獲得一個堅實可信的證據來證明藥物的實際有效性。&lt;/li&gt;
&lt;li&gt;經濟上的因素。
&lt;br&gt; 然而實際上可能還有經濟上，時間上，人力物力資源上的現實因素，會制約到底一個實驗能夠收集到多少樣本量。&lt;/li&gt;
&lt;li&gt;倫理道德上的因素。
&lt;br&gt; 許多臨牀實驗還必須受制於醫學倫理因素。在倫理上一個實驗到底可以維持多久。或者說，要考慮當實驗中一些受試者的結果不理想，或者是有副作用的時候，我們何時該及時停止該實驗？&lt;/li&gt;
&lt;li&gt;實驗本身的可信度。
&lt;br&gt; 如果一個臨牀實驗的規模在設計上就很小，可能它本身的可信度就很低。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;這裏我們只考慮沒有其他任何因素的影響下，&lt;strong&gt;1. 統計學方案&lt;/strong&gt;上該如何計算準確的所需樣本量的大小。&lt;/p&gt;
比較下列兩個同樣比較了溶栓酶和安慰劑在預防心肌梗塞患者死亡的臨牀實驗：
&lt;table class=&#34;table table-striped table-bordered&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-1&#34;&gt;Table 1: &lt;/span&gt;Results from the 1st Australian and ISIS-2 trials for reducing mortality from post-MI
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
治療組
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
溶栓酶
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
安慰劑
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
p.value
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1st Australian
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
n=264
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
n=253
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
死亡人數
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
26 (9.8%)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
32 (12.6%)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
p = 0.32
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
評價
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Risk ratio
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0.78 (95% CI: 0.48 to 1.27)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
ISIS-2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
n=8592
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
n=8595
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
死亡人數
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
791 (9.2%)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1029 (12.0%)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
p &amp;lt; 0.001
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
評價
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Risk ratio
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0.77 (95% CI: 0.70 to 0.84)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;這兩個臨牀實驗獲得的治療效果 (treatment effect)，在數字的百分比上幾乎十分接近。然而由於樣本兩巨大的差距，可以看到第一個實驗的信賴區間十分的大，使得實驗結果是無意義的。而第二個大樣本的實驗結果就告訴我們，溶栓酶的治療效果是有效降低了心肌梗死患者死亡概率（降低了23%）。第一個實驗收集了近500個病例，卻仍然不能提供確實有效的證據證明溶栓酶的治療效果（提供了強的關聯結果，卻是極弱的證據。strong correlation, but weak evidence) 。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;決定所需樣本量大小的統計學因素&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;決定所需樣本量大小的統計學因素&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;實驗主要結果的測量/比較方法是什麼？ What is the principal outcome measure of the trial?
&lt;br&gt; 一項臨牀實驗的主要結果，應該是切合該實驗的主要目的的。並且應當能夠客觀評價。(如死亡率的改善，治癒率的提高等等)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;實驗數據準備分析的方案是什麼？ How will the data be analysed to detect a treatment difference?
&lt;br&gt; 實驗結果獲得的數據是連續型的 (血壓，血糖值，BMI)？還是分類的離散變量 (死亡的發生與否，疾病的治癒與否)？統計學上認爲的，治療結果提示有意義的差別時的概率。通常定爲 5%。(p &amp;lt; 0.05)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;對照組的試驗期望結果是怎樣的？ What results are expected in the control group?
&lt;br&gt; 當然我們不可能事先預知實驗對照組可能出現的結果。此處只討論我們的預期結果。大多數情況下，我們可以從已經進行過的類似臨牀試驗報告中獲得，或者是從非臨牀干預型研究（觀察型研究）報告中獲得對照組的期望結果。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;如果實驗藥物在治療上確實有差異，當這個差異最小爲多少時希望能從設計的實驗中被檢測到？ How small a treatment difference, if it exists, is important to detect?
&lt;br&gt; 這一條恐怕是每個臨牀實驗在設計階段最重要，最敏感也是最難做出決定的。如果我們已知這個藥物療效和對照相比差別很大，那麼樣本量不用很大，就足以提供值得信賴的證據。不過臨牀上常常會認爲療效差距不必&lt;strong&gt;非常的&lt;/strong&gt;顯著，但是在臨牀意義上也是十分重要的。
&lt;br&gt; 常常在這個問題上會引起衆多討論，因爲醫生和患者可能認爲任何一點差異都是有臨牀意義的。但是如果我們想檢測出較小的差距，會需要非常巨大的樣本量，這將會是十分不切合實際的。&lt;strong&gt;What needs to be decided upon is the smallest clinically relevant difference that would be important to detect if it were true.&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;在上面第 4 條被決定了以後，還要確定的是我們需要多大的把握來相信這個被檢測出來的療效差別？ With what degree of certainty is needed to be able to detect the treatment difference in 4?
&lt;br&gt; 在實際臨牀實驗中，結論是從觀察數據中得來的，而不是從我們預想的那個“未知的實驗效果”。觀察獲得的療效差別，可能比預想的大（有效），也很可能比預想的小（無效）。設計較好的臨牀實驗應該有足夠機率觀察到有意義的療效差別，即使觀察得到的結果不如預期的大。當然要增加我們觀察到有意義的療效差別，最簡單的辦法是增加樣本量。這個條件的含義是，當療效真差別真實存在，我們要有足夠大的把握把它通過實驗觀察到。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;第一類和第二類錯誤-type-i-and-type-ii-errors&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;第一類和第二類錯誤 Type I and type II errors&lt;/h3&gt;
&lt;p&gt;下面羅列一下我們在進行實驗設計時要用到的概念和相應的標記，注意雖然我們無法知道真正的人羣裏真實參數 (parameter) 的大小，但是我們需要用一些估計 (estimator) 來代替：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(p_1=\)&lt;/span&gt; the &lt;strong&gt;observed percentage&lt;/strong&gt; in those on standard treatment &lt;br&gt; 意爲施行標準治療法時觀察到的（治癒/有效）百分比&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(p_2=\)&lt;/span&gt; the &lt;strong&gt;observed percentage&lt;/strong&gt; in those on “new” treatment &lt;br&gt; 意爲施行“新療法”時觀察到的（治癒/有效）的百分比&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\Rightarrow p_1-p_2=\)&lt;/span&gt; &lt;strong&gt;observed treatment effect&lt;/strong&gt; &lt;br&gt; 意爲可以觀察到的治療效果。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\pi_1=\)&lt;/span&gt; the &lt;strong&gt;anticipated percentage&lt;/strong&gt; in those on standard treatment &lt;br&gt; 意爲施行標準治療法時，我們預期的（治癒/有效）百分比&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\pi_2=\)&lt;/span&gt; the &lt;strong&gt;anticipated percentage&lt;/strong&gt; in those on “new” treatment &lt;br&gt; 意爲施行“新療法”時，我們預期的（治療/有效）百分比&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\Rightarrow \pi_1-\pi_2=\)&lt;/span&gt; is the true difference which has been decided it is important to detect &lt;br&gt; 意爲上面第 4 條中我們設定好的希望通過實驗證實的真實的療效差別。&lt;/p&gt;
&lt;p&gt;其餘的數學標記包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\alpha=\)&lt;/span&gt; 有意義的療效差異，在統計學上的水平 (概率水平，通常設定爲 0.05 or 5%)&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(1-\beta=\)&lt;/span&gt; Degree of certainty that a true difference of &lt;span class=&#34;math inline&#34;&gt;\(\pi_1 - \pi_2\)&lt;/span&gt; would be detected. &lt;br&gt; 效能, power。意爲有多大的把握能通過實驗檢測出療效差別。（通常將目標值設定爲 &lt;span class=&#34;math inline&#34;&gt;\(1-\beta=90\%\)&lt;/span&gt;）&lt;/li&gt;
&lt;/ul&gt;
&lt;table class=&#34;table table-striped table-bordered&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;caption&gt;
Table 2: Observed trial results compared to the &lt;code&gt;truth&lt;/code&gt; of 1) no difference; 2) a true &lt;span class=&#34;math inline&#34;&gt;\(\pi_1-\pi_2\)&lt;/span&gt; diffrence
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;border-bottom:hidden&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center; border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;&#34; colspan=&#34;2&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px;&#34;&gt;
真實情況 &lt;br&gt; Truth
&lt;/div&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
無差別
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
真實差別存在 &lt;span class=&#34;math inline&#34;&gt;\(\pi_1-\pi_2\)&lt;/span&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
觀察到不存在有意義差別
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(1−\alpha\)&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; &lt;br&gt; Type II error
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
觀察到存在有意義差別
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; &lt;br&gt; Type I error
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(1-\beta\)&lt;/span&gt; &lt;br&gt; Power
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;考慮上面這個表格，可以很容易想到，一個理想的實驗設計，我們希望這個臨牀實驗獲得的結果儘可能地落在上表中的&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;左上角：即如果真實情況是無差別的，實驗結果也應該觀察到不存在有意義的差別。&lt;/li&gt;
&lt;li&gt;右下角：即如果真實情況是是存在真實差別 &lt;span class=&#34;math inline&#34;&gt;\(\pi_1-\pi_2\)&lt;/span&gt; 的，試驗結果也應該觀察到有意義的差別。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;然而，我們在獲得臨牀實驗結果之後常常犯的兩類錯誤，同樣在上面的表格中顯示：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Type I error:&lt;/strong&gt; A type I error is when a treatment difference is claimed based on a statistically significant observed result when in truth no such difference exists, i.e. a false positive result. &lt;br&gt; 左下角爲&lt;strong&gt;一類錯誤&lt;/strong&gt;，即實驗結果觀察到有顯著的療效差異，然而，真實情況是並沒有差異的話，被認爲是假陽性判斷。&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; 表示一類錯誤發生的概率。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Type II error:&lt;/strong&gt; A type II error is when in truth there exists a difference of &lt;span class=&#34;math inline&#34;&gt;\(\pi_1-\pi_2\)&lt;/span&gt; but the observed results fail to reach statistical significance, i.e. a false negative result. &lt;br&gt; 右上角爲&lt;strong&gt;二類錯誤&lt;/strong&gt;，即實驗結果觀察到沒有顯著的療效差異，然而，真實情況是有差異的話，被認爲是假陰性判斷。&lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; 表示二類錯誤發生的概率。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Alternative ways of describing &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; is the risk of a Type I error; &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; 也被叫做檢驗的顯著水平, significant level。&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; is the risk of a Type II error. &lt;span class=&#34;math inline&#34;&gt;\(1-\beta\)&lt;/span&gt; is termed statistical power. 其中 &lt;span class=&#34;math inline&#34;&gt;\(1-\beta\)&lt;/span&gt; 被叫做檢驗效能。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\alpha, 1-\beta\)&lt;/span&gt; 的水平需要事先被確定，否則無法進行進一步的樣本量的計算。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;比較兩組之間的百分比-percentages-or-proportions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;比較兩組之間的百分比 (percentages or proportions)&lt;/h3&gt;
&lt;div id=&#34;樣本量計算公式-使用顯著水平-5-和檢驗效能-90&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;樣本量計算公式 (使用顯著水平 5%, 和檢驗效能 90%)&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[n=10.5\times\frac{[\pi_1\times(100-\pi_1)+\pi_2\times(100-\pi_2)]}{(\pi_1-\pi_2)^2}\times2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;注意：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;上面的公式後面有 &lt;span class=&#34;math inline&#34;&gt;\(\times2\)&lt;/span&gt; 是因爲前一半公式計算的只是一組（治療或對照組）所需的樣本量。&lt;/li&gt;
&lt;li&gt;這裏使用的是百分比。所以當使用比例的時候，要把 &lt;span class=&#34;math inline&#34;&gt;\(100\)&lt;/span&gt; 改成 &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;。&lt;/li&gt;
&lt;li&gt;使用公式計算的所需樣本量，並不是說我們需要的病例數就是計算出來的結果。上面的公式獲得的結果只是對所需樣本量的估算。&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;樣本量計算公式的一般化-不同的顯著水平和檢驗效能條件下&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;樣本量計算公式的一般化 (不同的顯著水平和檢驗效能條件下)&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[n=f(\alpha, \beta)\times\frac{[\pi_1\times(100-\pi_1)+\pi_2\times(100-\pi_2)]}{(\pi_1-\pi_2)^2}\times2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中， &lt;span class=&#34;math inline&#34;&gt;\(f(\alpha, \beta)\)&lt;/span&gt; 指的是關於檢驗顯著水平 &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; 和檢驗效能 &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; 的函數。 可以參考下面的表格：&lt;/p&gt;
&lt;table class=&#34;table table-striped table-bordered&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;caption&gt;
Table 3: Values of &lt;span class=&#34;math inline&#34;&gt;\(f(\alpha, \beta)\)&lt;/span&gt; for different levels of &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center; border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;&#34; colspan=&#34;1&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;
&lt;/div&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center; border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;&#34; colspan=&#34;4&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;
&lt;/div&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
0.05
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
0.1
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
0.2
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
0.5
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
(&lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; power)
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
(&lt;span class=&#34;math inline&#34;&gt;\(90\%\)&lt;/span&gt; power)
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
(&lt;span class=&#34;math inline&#34;&gt;\(80\%\)&lt;/span&gt; power)
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
(&lt;span class=&#34;math inline&#34;&gt;\(50\%\)&lt;/span&gt; power)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.05
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
13.0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
10.5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
7.85
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.84
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.01
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
17.8
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
14.9
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
11.7
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
6.63
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;要注意的是，除了上面表格中提供的 &lt;span class=&#34;math inline&#34;&gt;\(f(\alpha, \beta)\)&lt;/span&gt; 數值，可以通過以下公式計算得出：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[f(\alpha, \beta)=(Z_{1-\frac{\alpha}{2}}+Z_{1-\beta})^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;例如：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\alpha=0.05, \beta=0.1\)&lt;/span&gt; 時：&lt;span class=&#34;math inline&#34;&gt;\(f(\alpha, \beta)=(1.96+1.282)^2=10.5\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\alpha=0.05, \beta=0.2\)&lt;/span&gt; 時：&lt;span class=&#34;math inline&#34;&gt;\(f(\alpha, \beta)=(1.96+0.84)^2=7.85\)&lt;/span&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;比較兩組之間的均值&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;比較兩組之間的均值&lt;/h3&gt;
&lt;p&gt;許多臨牀實驗不光關心患者是否被治癒或者死亡，另外還有許多實驗的主要結果是連續變量：例如，腎功能（腎小球濾過率），或收縮期血壓。然而背後的原理其實還是一樣的。&lt;/p&gt;
&lt;div id=&#34;樣本量計算公式&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;樣本量計算公式&lt;/h4&gt;
&lt;p&gt;然而，另外一個必須考慮的因素：治療組對照組測量結果的標準差 (standard deviation, &lt;span class=&#34;math inline&#34;&gt;\(sd, \sigma\)&lt;/span&gt;)。這裏先考慮兩者標準差相同的情況。標準差的數據通常來自與先行研究的科學文獻，有些（土豪）實驗會先進行預實驗獲得想要的實驗數據–標準差。通常，建議像比較百分比那樣，調整改變一下不同的檢驗顯著水品和檢驗效能，計算多個所需樣本量來互相比較參考。&lt;/p&gt;
&lt;p&gt;比較兩組均值時需要用到的數學標記：&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mu_1=\)&lt;/span&gt; 標準治療法（對照組）的期待平均值；&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mu_2=\)&lt;/span&gt; 新治療法（治療組）的期待平均值；&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sigma=\)&lt;/span&gt; 兩組的標準差（假設兩組標準差相同）；&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\alpha=\)&lt;/span&gt; 一類錯誤發生的概率，檢驗顯著水平；&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta=\)&lt;/span&gt; 二類錯誤發生的概率，&lt;span class=&#34;math inline&#34;&gt;\(1-\beta\)&lt;/span&gt; 是檢驗效能。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;用上面標記表示的公式如下：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[n=f(\alpha, \beta)\times\frac{2\sigma^2}{(\mu_1-\mu_2)^2}\times2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;可以認爲，上面的公式中 &lt;span class=&#34;math inline&#34;&gt;\(\mu_1-\mu_2\)&lt;/span&gt; ，各組的平均值本身並不重要，兩組之間均值的差是我們關心的。如果用 &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt; 表示兩組之間均值差的期待值，那麼公式可以改寫爲：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[n=f(\alpha, \beta)\times\frac{2\sigma^2}{\delta^2}\times2\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;樣本量計算的調整&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;樣本量計算的調整&lt;/h3&gt;
&lt;p&gt;如果我們無法成功隨訪部分患者，那麼這部分人的數據就無法獲得，實驗數據的說服力就會下降。如果我們預估計有 &lt;span class=&#34;math inline&#34;&gt;\(Q\%\)&lt;/span&gt; 的人會失去隨訪，那麼我們可以將之前步驟中計算獲得的數字乘以 &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{1-Q\%}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;如果實驗設計是我們會在某個時間點允許治療組或對照組中的部分人變更自己的實驗方案（即治療組的參與者改進入對照組，反之亦然）。那麼所需樣本量的計算調整的方法爲：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;令 &lt;span class=&#34;math inline&#34;&gt;\(Q_1=\)&lt;/span&gt; 第一組中改成第二組治療方案的人數比例；&lt;/li&gt;
&lt;li&gt;令 &lt;span class=&#34;math inline&#34;&gt;\(Q_2=\)&lt;/span&gt; 第二組中改成第一組治療方案的人數比例；&lt;/li&gt;
&lt;li&gt;將之前步驟中計算獲得的樣本量數字乘以 &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{(1-Q_1-Q_2)^2}\)&lt;/span&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果預期參與實驗治療組（而不是對照組）的人中有部分人（比例爲 &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt;）會中斷實驗進程，那麼調整公式爲：&lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{(1-Q)^2}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;還有的實驗會使用大於 &lt;span class=&#34;math inline&#34;&gt;\(1:1\)&lt;/span&gt; 的比例設計對照組和實驗組的人數。假設這一比例爲 &lt;span class=&#34;math inline&#34;&gt;\(r:1\)&lt;/span&gt; 那麼調整的樣本量數字還要乘以：&lt;span class=&#34;math inline&#34;&gt;\(\frac{(r+1)^2}{4r}\)&lt;/span&gt;。&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>卡方分佈 chi square distribution</title>
      <link>https://wangcc.me/post/chi-square-distribution/</link>
      <pubDate>Thu, 26 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/chi-square-distribution/</guid>
      <description>


&lt;div id=&#34;卡方分佈的期望和方差的證明&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;卡方分佈的期望和方差的證明：&lt;/h3&gt;
&lt;p&gt;當 &lt;span class=&#34;math inline&#34;&gt;\(X\sim N(0,1)\)&lt;/span&gt; 時， &lt;span class=&#34;math inline&#34;&gt;\(X^2\sim \mathcal{X}_1^2\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;如果 &lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots, X_n\stackrel{i.i.d}{\sim} N(0,1)\)&lt;/span&gt;，
那麼 &lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^nX_i^2\sim\mathcal{X}_n^2\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中： &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{X}_n^2\)&lt;/span&gt; 表示自由度爲 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 的卡方分佈。&lt;/p&gt;
&lt;p&gt;且 &lt;span class=&#34;math inline&#34;&gt;\(X_m^2+X_n^2=\mathcal{X}_{m+n}^2\)&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;卡方分佈的期望&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;卡方分佈的期望：&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[E(X_1^2)=Var(X)+[E(X)]^2=1+0=1\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\Rightarrow E(X_n^2)=n\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;卡方分佈的方差&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;卡方分佈的方差：&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
Var(X_1^2) &amp;amp;= E(X_1^{2^2}) - E(X_1^2)^2 \\
           &amp;amp;= E(X_1^4)-1
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;下面來求-ex_14&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;下面來求 &lt;span class=&#34;math inline&#34;&gt;\(E(X_1^4)\)&lt;/span&gt;&lt;/h5&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\because E(X_1) &amp;amp;= \int_{-\infty}^{+\infty} xf(x)dx \\
\therefore E(X_1^4) &amp;amp;= \int_{-\infty}^{+\infty} x^4f(x)dx
\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;已知： &lt;span class=&#34;math inline&#34;&gt;\(f(x)=\frac{1}{\sqrt{2\pi}}e^{(-\frac{x^2}{2})}\)&lt;/span&gt; 代入上式：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
E(X_1^4) &amp;amp;= \int_{-\infty}^{+\infty} x^4f(x)dx \\
         &amp;amp;= \int_{-\infty}^{+\infty} x^4\frac{1}{\sqrt{2\pi}}e^{(-\frac{x^2}{2})}dx\\
         &amp;amp;=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}x^4e^{(-\frac{x^2}{2})}dx\\
         &amp;amp;=\frac{-1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}x^3(-x)e^{(-\frac{x^2}{2})}dx
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;令 &lt;span class=&#34;math inline&#34;&gt;\(u=x^3, v=e^{(-\frac{x^2}{2})},t=-\frac{x^2}{2}\)&lt;/span&gt;
可以推導：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\frac{dv}{dx} &amp;amp;= \frac{dv}{dt}\frac{dt}{dx} \\
              &amp;amp;= e^t(-\frac{1}{2}\times2x) \\
              &amp;amp;= (-x)e^{(-\frac{x^2}{2})} \\
\Rightarrow dv &amp;amp;= (-x)e^{(-\frac{x^2}{2})}dx
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;再代入上面的式子：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
E(X_1^4) &amp;amp;= \frac{-1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}u\:dv \\
integrate\; &amp;amp;by\; parts:\\
E(X_1^4) &amp;amp;= \frac{-1}{\sqrt{2\pi}}\{[u\:v] \rvert_{-\infty}^{+\infty}-\int_{-\infty}^{+\infty}v\:du\} \\
&amp;amp;= \frac{-1}{\sqrt{2\pi}}\{[x^3e^{(-\frac{x^2}{2})}]\rvert_{-\infty}^{+\infty} -\int_{-\infty}^{+\infty}v\:du\} \\
&amp;amp;=\frac{-1}{\sqrt{2\pi}}\{0-0-\int_{-\infty}^{+\infty}e^{(-\frac{x^2}{2})}dx^3\} \\
&amp;amp;=\frac{-1}{\sqrt{2\pi}}[-3\int_{-\infty}^{+\infty}x^2e^{(-\frac{x^2}{2})}dx] \\
&amp;amp;=\frac{-3}{\sqrt{2\pi}}[\int_{-\infty}^{+\infty}x(-x)e^{(-\frac{x^2}{2})}dx] \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;再來一次分部積分：&lt;/p&gt;
&lt;p&gt;令 &lt;span class=&#34;math inline&#34;&gt;\(a=x,b=e^{(-\frac{x^2}{2})},d\:b = (-x)e^{(-\frac{x^2}{2})}dx\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
E(X_1^4) &amp;amp;= \frac{-3}{\sqrt{2\pi}}\{[a\:b] \rvert_{-\infty}^{+\infty} - \int_{-\infty}^{+\infty}b\:da\} \\
&amp;amp;=\frac{-3}{\sqrt{2\pi}}\{[xe^{(-\frac{x^2}{2})}]\rvert_{-\infty}^{+\infty} -\int_{-\infty}^{+\infty}b\:da\} \\
&amp;amp;=\frac{-3}{\sqrt{2\pi}}\{0-0-\int_{-\infty}^{+\infty}e^{(-\frac{x^2}{2})}dx\} \\
&amp;amp;=\frac{-3}{\sqrt{2\pi}}[-\int_{-\infty}^{+\infty}e^{(-\frac{x^2}{2})}dx] \\
&amp;amp;=\frac{3}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}e^{(-\frac{x^2}{2})}dx
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;下面令 &lt;span class=&#34;math inline&#34;&gt;\(I=\int_{-\infty}^{+\infty}e^{(-\frac{x^2}{2})}dx\\ \Rightarrow I^2=\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}e^{(-\frac{x^2+y^2}{2})}dxdy\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;接下來需要用到 &lt;a href=&#34;https://www.youtube.com/watch?v=r0fv9V9GHdo&#34;&gt;座標轉換&lt;/a&gt;的知識，將 &lt;span class=&#34;math inline&#34;&gt;\(x,y\)&lt;/span&gt; 表示的笛卡爾座標，轉換爲用角度 &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; 和半徑 &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; 表示的形式。之後的證明可以在&lt;a href=&#34;https://www.youtube.com/watch?v=fWOGfzC3IeY&#34;&gt;油管&lt;/a&gt;上看到，但是我還是繼續證明下去。&lt;/p&gt;
&lt;p&gt;直角座標系 (cartesian coordinators) 和
極座標系 (polar coordinators) 之間轉換的關係如下：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
x&amp;amp;=r\:cos\theta\\
y&amp;amp;=r\:sin\theta\\
r^2&amp;amp;=x^2+y^2\\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;座標轉換以後可以繼續求 &lt;span class=&#34;math inline&#34;&gt;\(E(X_1^4)\)&lt;/span&gt;。 在那之前我們先求 &lt;span class=&#34;math inline&#34;&gt;\(I^2\)&lt;/span&gt;。
注意轉換座標系統以後，&lt;span class=&#34;math inline&#34;&gt;\(\theta\in[0,2\pi], r\in[0,+\infty]\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I^2 &amp;amp;= \int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}e^{(-\frac{x^2+y^2}{2})}dxdy \\
&amp;amp;= \int_{0}^{+\infty}\int_{0}^{2\pi}e^{(-\frac{r^2}{2})}rd\theta dr \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;由於先從中間的 &lt;span class=&#34;math inline&#34;&gt;\(\int_{0}^{2\pi}e^{(-\frac{r^2}{2})}rd\theta\)&lt;/span&gt; 開始積分，&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; 以外都可以視爲常數，那麼這個 &lt;span class=&#34;math inline&#34;&gt;\([0,2\pi]\)&lt;/span&gt; 上的積分就的等於 &lt;span class=&#34;math inline&#34;&gt;\(2\pi e^{(-\frac{r^2}{2})}r\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;因此上面的式子又變爲：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
I^2 &amp;amp;=  2\pi\int_{0}^{+\infty}e^{(-\frac{r^2}{2})}r\:dr \\
\because \frac{d(e^{\frac{-r^2}{2}})}{dr} &amp;amp;= -e^{(-\frac{r^2}{2})}r \\
\therefore I^2 &amp;amp;= 2\pi(-e^{\frac{-r^2}{2}})\rvert_0^{+\infty} \\
               &amp;amp;= 0-(2\pi\times(-1)) \\
               &amp;amp;= 2\pi\\
\Rightarrow I  &amp;amp;= \sqrt{2\pi}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;所以，&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
E(X_1^4) &amp;amp;= \frac{3}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}e^{(-\frac{x^2}{2})}dx \\
&amp;amp;= \frac{3}{\sqrt{2\pi}}\times I \\
&amp;amp;= 3 \\
\Rightarrow Var(X_1^2) &amp;amp;= E(X_1^4) - 1 \\
                       &amp;amp;= 3-1 =2 \\
\Rightarrow Var(X_n^2) &amp;amp;= 2n
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;結論：&lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots, X_n\stackrel{i.i.d}{\sim} N(0,1)\)&lt;/span&gt; 時，&lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^nX_i^2\sim\mathcal{X}_n^2\)&lt;/span&gt; 服從卡方分佈，其期望 &lt;span class=&#34;math inline&#34;&gt;\(E(X_n^2)=n\)&lt;/span&gt;，方差 &lt;span class=&#34;math inline&#34;&gt;\(Var(X_n^2)=2n\)&lt;/span&gt;。
根據&lt;a href=&#34;https://winterwang.github.io/post/central-limit-theory/&#34;&gt;中心極限定理&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[n\rightarrow \infty, X_n^2\sim N(n, 2n)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>估計和精確度的概念</title>
      <link>https://wangcc.me/post/frequentist-statistical-inference02/</link>
      <pubDate>Tue, 24 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/frequentist-statistical-inference02/</guid>
      <description>


&lt;div id=&#34;估計量和他們的樣本分佈&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;估計量和他們的樣本分佈&lt;/h3&gt;
&lt;p&gt;例子： 最大呼氣量 (Forced Expoiratory Volume in one second, FEV1) 用於測量一個人的肺功能，它的測量值是連續的。我們從前來門診的人中隨機抽取 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 人作爲樣本，用這個樣本的 FEV1 平均值來估計這個診所的患者的平均肺功能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;模型假設：&lt;/strong&gt; 在這個例子中，我們的假設有如下：每個隨機抽取的 FEV1 測量值都是從同一個總體（人羣）中抽取，每一個觀察值 &lt;span class=&#34;math inline&#34;&gt;\(Y_i\)&lt;/span&gt; 都互相獨立互不影響。我們用縮寫 iid 表示這些隨機抽取的樣本是服從獨立同分佈 (independent and identically distributed)。另外，總體的分佈也假定爲正態分佈，且總體均值爲 &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;，總體方差爲 &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;。那麼這個模型可以簡單的被寫成：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y_i \stackrel{i.i.d}{\sim} N(\mu, \sigma^2), i=1,2,\dots,n\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;總體均值 &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; 的估計量：&lt;/strong&gt; 顯然算術平均值: &lt;span class=&#34;math inline&#34;&gt;\(\bar{Y}=\frac{1}{n}\sum_{i=1}^ny_i\)&lt;/span&gt; 是我們用於估計總體均值的估計量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;估計量的樣本分佈：&lt;/strong&gt;
&lt;span class=&#34;math display&#34;&gt;\[\bar{Y}\stackrel{i.i.d}{\sim}N(\mu, \frac{\sigma^2}{n})\]&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;證明&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;證明&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
E(\bar{Y}) &amp;amp;= E(\frac{1}{n}\sum Y_i) \\
           &amp;amp;= \frac{1}{n}E(\sum Y_i) \\
           &amp;amp;= \frac{1}{n}\sum E(Y_i) \\
           &amp;amp;= \frac{1}{n}n\mu = \mu \\
Var(\bar{Y}) &amp;amp;= Var(\frac{1}{n}\sum Y_i) \\
\because Y_i \;are &amp;amp;\; independent   \\
            &amp;amp;= \frac{1}{n^2}\sum Var(Y_i) \\
            &amp;amp;= \frac{1}{n^2} n Var(Y_i) \\
            &amp;amp;= \frac{\sigma^2}{n}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;證明當-zfracbary-musqrtvarbary-時-zsim-n01&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;證明當 &lt;span class=&#34;math inline&#34;&gt;\(Z=\frac{\bar{Y}-\mu}{\sqrt{Var(\bar{Y})}}\)&lt;/span&gt; 時， &lt;span class=&#34;math inline&#34;&gt;\(Z\sim N(0,1)\)&lt;/span&gt;:&lt;/h4&gt;
&lt;p&gt;由式子可知， &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; 只是由一組服從正態分佈的數據 &lt;span class=&#34;math inline&#34;&gt;\(\bar{Y}\)&lt;/span&gt; 線性轉換 (linear transformation) 而來，所以 &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; 本身也服從正態分佈
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
E(Z) &amp;amp;= \frac{1}{\sqrt{Var(\bar{Y})}}E[\bar{Y}-\mu] \\
     &amp;amp;= \frac{1}{\sqrt{Var(\bar{Y})}}[\mu-\mu] = 0 \\
Var(Z) &amp;amp;= \frac{1}{Var(\bar{Y})}Var[\bar{Y}-\mu] \\
       &amp;amp;= \frac{1}{Var(\bar{Y})}Var(\bar{Y}) =1 \\
\therefore Z \;&amp;amp;\sim N(0,1)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;均值 &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; 的信賴區間：&lt;/strong&gt; 上節說道，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;信賴區間通常是成對成對的出現的，即有上限和下限。這樣的一對從樣本數據中計算得來的統計量，同樣也是有樣本分佈的。&lt;strong&gt;每次我們重新從總體或人羣中抽樣，計算獲得的信賴區間都不同，這些信賴區間就組成了信賴區間的樣本分佈。總體和人羣的參數落在這些信賴區間範圍內的概率，就是我們常說的信賴區間的水平（&lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt;）。&lt;/strong&gt; 常用的這個概率值就是 &lt;span class=&#34;math inline&#34;&gt;\(95\%, 90\%, 99\%\)&lt;/span&gt;。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;假定我們用 &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; 作爲信賴區間的水平。那麼下面我們嘗試推導一下信賴區間的計算公式。從長遠來說（也就是假設我們從總體中抽樣無數次，每次都進行信賴區間的計算，也獲得無數個信賴區間），這些信賴區間中有 &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; 是包含了總體的真實均值（但是卻是未知）的，而且這些信賴區間由於是從一個服從正態分佈的數據而來，它們也服從正態分佈（對真實均值左右對稱）。所以我們有理由相信，可以找到一個數值 &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Prob(\bar{Y} &amp;gt; \mu+c) = 0.025 \\
  Prob(\bar{Y} &amp;lt; \mu-c) = 0.025\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;因此，我們可以定義 &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; 信賴區間的上限和下限分別是：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[L=\bar{Y}-c \Rightarrow Prob(L&amp;gt;\mu)=0.025 \\
  U=\bar{Y}+c \Rightarrow Prob(U&amp;lt;\mu)=0.025\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/img/Selection_082.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;接下來就是推倒（故意的）&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; 的過程啦：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
Prob(\bar{Y}&amp;gt;\mu+c)=Prob(\bar{Y}-\mu&amp;gt;c) \;&amp;amp;= 0.025 \\
\Rightarrow Prob(\frac{\bar{Y}-\mu}{\sqrt{Var(\bar{Y})}} &amp;gt; \frac{c}{\sqrt{Var(\bar{Y})}}) \;&amp;amp;= 0.025 \\
\Rightarrow Prob(Z&amp;gt;\frac{c}{\sqrt{Var(\bar{Y})}}) \;&amp;amp;= 0.025 \\
上面已經證明了 Z\sim N(0,1) \\
而且我們也已知 Prob(Z&amp;gt;1.96) \;&amp;amp;= 0.025 \\
所以只要令 \frac{c}{\sqrt{Var(\bar{Y})}} =1.96 \\
\Rightarrow c=1.96\sqrt{Var(\bar{Y})} \\
所以總體均值\mu 的 95\% 信賴區間就是: \\
\mu = \bar{Y}\pm1.96\sqrt{Var(\bar{Y})}=\bar{Y}\pm &amp;amp; 1.96\frac{\sigma}{\sqrt{n}}\\
其中，\sqrt{Var(\bar{Y})} 就是我們熟知的估計量 \bar{Y} &amp;amp;的標準誤。
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;估計量的特質&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;估計量的特質&lt;/h3&gt;
&lt;p&gt;考慮以下的問題：&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;什麼因素決定了一個估計量 (estimator) 的好壞，是否實用？&lt;/li&gt;
&lt;li&gt;如果有其他的可選擇估計量，該如何取捨呢？&lt;/li&gt;
&lt;li&gt;當情況複雜的時候，我們該如何尋找合適的估計量？&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;偏倚&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;偏倚&lt;/h4&gt;
&lt;p&gt;假設 &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; 是我們估計總體參數 &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; 的一個估計量。一般來說我們希望估計量的樣本分佈可以在 &lt;code&gt;“正確的位置”&lt;/code&gt; 左右均勻分佈。換句話說我們希望：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[E(T)=\theta\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;如果實現了這個條件，我們說這樣的估計量是無偏的 (&lt;code&gt;unbiased&lt;/code&gt;)。然而，天下哪有這等好事，我們叫真實值和估計量之間的差距爲偏倚：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[bias(T) = E(T)-\theta\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其實偏倚完全等於零並不是最重要，許多常見的估計量都是有偏倚的。重要的是，這個偏倚會隨着樣本量的增加而逐漸趨近於零。所以我們就可以認爲這樣的估計量是漸進無偏的 (asymptotically unbiased)：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[T\;is\;an\;\textbf{unbiased}\;estimator\;for\;\theta\;if\;\\E(T)=\theta\\
T\;is\;an\;\textbf{asymptotically unbiased}\;estimator\;for\;\theta\;if\;\\lim_{n\rightarrow\infty}E(T)=\theta\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;估計量的效能-efficiency&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;估計量的效能 Efficiency&lt;/h4&gt;
&lt;p&gt;通常，我們希望一個估計量 (estimator) 的偏倚要小，同時，它的樣本分佈也希望能儘可能的不要波動太大。換句話說，我們還希望估計量的方差越小越好。&lt;/p&gt;
&lt;p&gt;如果說，兩個估計量有相同的偏倚，均可以選擇來推斷總體，我們說，其中樣本分佈的方差小的那個（波動幅度小）的那個估計量是相對更好的。因爲樣本分佈方差越小，說明可以&lt;strong&gt;更加精確的&lt;/strong&gt;估計總體參數。這兩個估計量的方差之比：&lt;span class=&#34;math inline&#34;&gt;\(Var(S)/Var(T)\)&lt;/span&gt; 被叫做這兩個估計量的&lt;strong&gt;相對效能 (relative efficiency)&lt;/strong&gt;。所以我們用估計量去推斷總體時，需要選用效能最高，精確度最好的估計量 &lt;strong&gt;(the minimum variance unbiased estimator/an efficient estimator)&lt;/strong&gt;。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;均值和中位數的相對效能&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;均值和中位數的相對效能&lt;/h4&gt;
&lt;p&gt;在一個服從 &lt;span class=&#34;math inline&#34;&gt;\(N(\mu,\sigma^2)\)&lt;/span&gt; 正態分佈的數據中，中位數和均值是一樣的，也都同時等於總體均值參數 &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;。而且，樣本均數 &lt;span class=&#34;math inline&#34;&gt;\(\bar{Y}\)&lt;/span&gt; 和樣本中位數 &lt;span class=&#34;math inline&#34;&gt;\(\dot{Y}\)&lt;/span&gt; 都是對總體均值的無偏估計量。那麼應該選用中位數還是平均值呢？&lt;/p&gt;
&lt;p&gt;之前證明過當 &lt;span class=&#34;math inline&#34;&gt;\(Y_i \sim N(\mu,\sigma^2)\)&lt;/span&gt; 時， &lt;span class=&#34;math inline&#34;&gt;\(Var(\bar{Y}=\sigma^2/n)\)&lt;/span&gt;。然而，當 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 較大的時候，可以證明的是：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Var(\dot{Y})=\frac{\pi}{2}\frac{\sigma^2}{n}\approx1.571\frac{\sigma^2}{n}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;因此，這兩個估計量的相對效能就是：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{Var(\dot{Y})}{Var(\bar{Y})}\approx1.571\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;所以總體是正態分佈時，平均值就是較中位數更適合用來估計總體的估計量。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;均方差-mean-square-error-mse&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;均方差 mean square error (MSE)&lt;/h3&gt;
&lt;p&gt;兩個估計量的偏倚不同時，可以比較他們和總體參數之間的差距，這被叫做均方差, Mean Square Error (MSE)。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[MSE(T)=E[(T-\theta)^2]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;這裏用一個數學技巧，將式子中的估計量和總體參數之間的差，分成兩個部分：一是估計量本身的方差 (&lt;span class=&#34;math inline&#34;&gt;\(T-E(T)\)&lt;/span&gt;)，一是估計量的偏倚 (&lt;span class=&#34;math inline&#34;&gt;\(E(T)-\theta\)&lt;/span&gt;)。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
MSE(T) &amp;amp;= E[(T-\theta)^2] \\
       &amp;amp;= E\{[T-E(T)+E(T)-\theta]^2\} \\
       &amp;amp;= E\{[T-E(T)]^2+[E(T)-\theta]^2 \\
       &amp;amp; \;\;\;\;\; \;\;+2[T-E(T)][E(T)-\theta]\} \\
       &amp;amp;= E\{[T-E(T)]^2\}+E\{[E(T)-\theta]^2\} + 0\\
       &amp;amp;= Var(T) + [bias(T)^2]
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;總體方差的估計自由度&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;總體方差的估計，自由度&lt;/h3&gt;
&lt;p&gt;如果 &lt;span class=&#34;math inline&#34;&gt;\(Y_i \sim (\mu, \sigma^2)\)&lt;/span&gt;，並不需要默認或者假定它服從正態分佈或者任何分佈。那麼它的方差我們會用：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[V_{\mu}=\frac{1}{n}\sum_{i=1}^n(Y_i-\mu)\]&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;證明-v_mu-是-sigma2-的無偏估計&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;證明 &lt;span class=&#34;math inline&#34;&gt;\(V_{\mu}\)&lt;/span&gt; 是 &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; 的無偏估計：&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
V_{\mu} &amp;amp;= \frac{1}{n}\sum_{i=1}^n(Y_i-\mu) \\
需要證明 &amp;amp;E(V_{\mu}) = \sigma^2 \\
\Rightarrow E(V_{\mu}) &amp;amp;= \frac{1}{n}\sum_{i=1}^nE(Y_i-\mu)^2 \\
        &amp;amp;= \frac{1}{n}\sum_{i=1}^nVar(Y_i) \\
        &amp;amp;= \frac{1}{n}\sum_{i=1}^n\sigma^2 \\
        &amp;amp;= \sigma^2
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;然而通常情況下，我們並不知道總體的均值 &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;。因此，只好用樣本的均值 &lt;span class=&#34;math inline&#34;&gt;\(\bar{Y}\)&lt;/span&gt; 來估計 &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;。所以上面的方程就變成了：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[V_{\mu}=\frac{1}{n}\sum_{i=1}^n(Y_i-\bar{Y})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;你如果仔細觀察認真思考，就會發現，上面這個式子是&lt;code&gt;有問題的&lt;/code&gt;。這個大問題就在於，&lt;span class=&#34;math inline&#34;&gt;\(Y_i-\bar{Y}\)&lt;/span&gt; 中我們忽略掉了樣本均值 &lt;span class=&#34;math inline&#34;&gt;\(\bar{Y}\)&lt;/span&gt; 和總體均值 &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; 之間的差 (&lt;span class=&#34;math inline&#34;&gt;\(\bar{Y}-\mu\)&lt;/span&gt;)。因此上面的計算式來估計總體方差時，很顯然是會低估平均平方差，從而低估了總體方差。&lt;/p&gt;
&lt;p&gt;這裏需要引入&lt;strong&gt;自由度 (degree of freedom)&lt;/strong&gt; 在參數估計中的概念。&lt;/p&gt;
&lt;p&gt;字面上可以理解爲：自由度是估計過程中使用了多少互相獨立的信息。所以在上面第一個公式中：&lt;span class=&#34;math inline&#34;&gt;\(V_{\mu}=\frac{1}{n}\sum_{i=1}^n(Y_i-\mu)\)&lt;/span&gt;。所有的 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 個觀察值互相獨立，不僅如此，他們還對總體均值獨立。然而在第二個我們用 &lt;span class=&#34;math inline&#34;&gt;\(\bar{Y}\)&lt;/span&gt; 取代了 &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; 的公式中，樣本均數則與觀察值不互相獨立。因爲&lt;strong&gt;樣本均數必然總是落在觀察值的中間&lt;/strong&gt;。然而總體均數並不一定就會落在觀察值中間。總體均數，和觀察值之間是自由，獨立的。因此，當我們觀察到 &lt;span class=&#34;math inline&#34;&gt;\(n-1\)&lt;/span&gt; 個觀察值時，剩下的最後一個觀察值，決定了樣本均值的大小。所以說，樣本均值的自由度，是 &lt;span class=&#34;math inline&#34;&gt;\(n-1\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;所以，加入了自由度的討論，我們可以相信，用樣本估計總體的方差時，使用下面的公式將會是總體方差的無偏估計：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[V_{n-1}=\frac{1}{n-1}\sum_{i=1}^n(Y_i-\bar{Y})=\frac{n}{n-1}V_n\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;證明-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;證明&lt;/h4&gt;
&lt;p&gt;利用上面也用到過的證明方法 – 把樣本和總體均值之間的差分成兩部分：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
V_{\mu} &amp;amp;= \frac{1}{n}\sum_{i=1}^n(Y_i-\mu)^2 \\
        &amp;amp;= \frac{1}{n}\sum_{i=1}^n[(Y_i-\bar{Y})+(\bar{Y}-\mu)]^2 \\
        &amp;amp;= \frac{1}{n}\sum_{i=1}^n[(Y_i-\bar{Y})^2+(\bar{Y}-\mu)^2\\
        &amp;amp;\;\;\;\;\;\;\;\;\;\;\;\;+2(Y_i-\bar{Y})(\bar{Y}-\mu)]\\
        &amp;amp;=\frac{1}{n}\sum_{i=1}^n(Y_i-\bar{Y})^2+\frac{1}{n}\sum_{i=1}^n(\bar{Y}-\mu)^2\\
        &amp;amp;\;\;\;\;\;\;\;\;\;\;\;\;+\frac{2}{n}(\bar{Y}-\mu)\sum_{i=1}^n(Y_i-\bar{Y}) \\
        &amp;amp;= V_n+(\bar{Y}-\mu)^2 \\ &amp;amp;\;\;\;\;\;\;\;\;\;\;\;\;(note\;that\;\sum_{i=1}^n(Y_i-\bar{Y})=0) \\
\Rightarrow  V_n &amp;amp;= V_{\mu}-(\bar{Y}-\mu)^2  \\
\therefore E(V_n)&amp;amp;= E(V_{\mu}) - E[(\bar{Y}-\mu)^2] \\
                 &amp;amp;= Var(Y)-Var(\bar{Y}) \\
                 &amp;amp;= \sigma^2-\frac{\sigma^2}{n} \\
                 &amp;amp;= \sigma^2(\frac{n-1}{n})
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;因此，我們看見 &lt;span class=&#34;math inline&#34;&gt;\(V_n\)&lt;/span&gt; 正如上面討論的那樣，是低估了總體方差的。雖然當 &lt;span class=&#34;math inline&#34;&gt;\(n\rightarrow\infty\)&lt;/span&gt; 時無限接近 &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; 但是依然是低估了的。所以，我們可以對之進行修正：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
E[\frac{n}{n-1}V_n]     &amp;amp;= \frac{n}{n-1}E[V_n] =\sigma^2 \\
\Rightarrow E[V_{n-1}]  &amp;amp;= \sigma^2
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;樣本方差的樣本分佈&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;樣本方差的樣本分佈&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(S^2\)&lt;/span&gt; 常用來標記樣本方差，取代上面我們用到的 &lt;span class=&#34;math inline&#34;&gt;\(V_{n-1}\)&lt;/span&gt;：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[S^2=\frac{1}{n-1}\sum_{i=1}^n(Y_i-\bar{Y})^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;而且上面也證明了，&lt;span class=&#34;math inline&#34;&gt;\(E(S^2)=\sigma^2\)&lt;/span&gt; 是總體方差的無偏估計。然而，要注意的是，樣本標準差 &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{S^2}\)&lt;/span&gt; 卻不是總體標準差 &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; 的無偏估計（因爲並不是線性變換，而是開了根號）。&lt;/p&gt;
&lt;div id=&#34;證明樣本標準差-s-不是總體標準差-sigma-的無偏估計&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;證明樣本標準差 &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; 不是總體標準差 &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; 的無偏估計&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
Var(S)               &amp;amp;=E(S^2)-[E(S)]^2 \\
\Rightarrow [E(S)]^2 &amp;amp;=E(S^2)-Var(S) \\
\because E(S^2)      &amp;amp;=\sigma^2 \\
\therefore   [E(S)]^2 &amp;amp;=\sigma^2-Var(S) \\
             E(S)     &amp;amp;=\sqrt{\sigma^2-Var(S)} \\
\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;可見樣本標準差是低估了總體標準差的。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;另外可以被證明的是：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{n-1}{\sigma^2}S^2\sim \mathcal{X}_{n-1}^2\\
Var(S^2)=\frac{2\sigma^4}{n-1}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{X}^2_m\)&lt;/span&gt;： 自由度爲 &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; 的&lt;a href=&#34;https://winterwang.github.io/post/chi-square-distribution/&#34;&gt;卡方分佈&lt;/a&gt;。是在圖形上向右歪曲的分佈。當自由度增加時，會越來越接近正態分佈。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>概率論者統計推斷入門之-被門夾住</title>
      <link>https://wangcc.me/post/frequentist-statistical-inference01/</link>
      <pubDate>Tue, 24 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/frequentist-statistical-inference01/</guid>
      <description>


&lt;div id=&#34;人羣與樣本-population-and-sample&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;人羣與樣本 (population and sample)&lt;/h3&gt;
&lt;p&gt;討論樣本時，需考慮下面幾個問題：&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;樣本是否具有代表性？&lt;/li&gt;
&lt;li&gt;人羣被準確定義了嗎？&lt;/li&gt;
&lt;li&gt;我們感興趣的“人羣”是否可以是無限大（多）的？&lt;/li&gt;
&lt;li&gt;我們研究的樣本，是僅僅用來觀察，亦或是計劃對之進行某種干預呢？&lt;/li&gt;
&lt;li&gt;我們從所有可能的人羣中抽樣了嗎？&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;樣本和統計量-sample-and-statistic&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;樣本和統計量 (sample and statistic)&lt;/h3&gt;
&lt;p&gt;通常我們在進行實驗或觀察時只是獲得了樣本的數據。而希望從樣本數據去推斷 (inference) 總體（或人羣）的一些特徵。我們也許只是想用樣本的平均值來估計整體人羣的某個特徵的平均值。不管是何種估計和推斷，都是基於對樣本數據的計算，從樣本中獲得想要推斷總體的&lt;strong&gt;統計量 (statistics)&lt;/strong&gt;。我們用已知樣本去推斷未知總體的過程就叫做&lt;strong&gt;估計 (estimate)&lt;/strong&gt;。這個想要被推斷的總體或人羣的值，被叫做&lt;strong&gt;參數 (parameter)&lt;/strong&gt;，常常使用希臘字母來標記。用來估計總體或人羣的，從樣本數據計算得來的統計量，叫做&lt;strong&gt;估計量 (estimator)&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;所有的統計量，都有&lt;strong&gt;樣本分佈 (sampling distributions，意爲重複無限次取樣後獲得的無限次統計量的分佈)&lt;/strong&gt;。推斷的過程歸納如下：&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;從總體或人羣中抽樣 (樣本量 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;)&lt;/li&gt;
&lt;li&gt;計算這個樣本的合適統計量，從而用於估計它在整體或人羣中的值。&lt;/li&gt;
&lt;li&gt;我們還需要決定計算獲得的統計量的樣本分佈（假定會抽樣無數次）。&lt;/li&gt;
&lt;li&gt;一旦可以精確地確認樣本分佈，我們就可以定量地計算出使用步驟2中獲得的統計量估計總體或人羣的參數時的準確度。&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;估計-estimation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;估計 Estimation&lt;/h3&gt;
&lt;p&gt;從樣本的均值，推斷總體或人羣的均值是一種估計。我們的目的是，從已知樣本中計算一個儘可能接近那個未知的總體或人羣參數的值。一個估計量有兩個與生俱來的性質 (properties)：1) 偏倚 (bias); 2) 精確度 (precision)。這兩個性質都可以從樣本分佈和估計量獲得。&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;偏倚： 偏倚簡單說就是樣本分佈的均值，也就是我們從樣本中計算獲得的估計量，和我們想要拿它來估計的總體或人羣的參數之間的差距。(The bias is the difference between the mean of the sampling distribution – the expected or average value of the estimator – and the population parameter being estimated.) 一個小的偏倚，確保了我們從樣本中計算獲得的估計值（假設我們抽樣無數次，計算無數個樣本估計值）&lt;strong&gt;均勻地&lt;/strong&gt;分佈在總體或人羣參數的左右兩邊。偏倚本身並不是太大的問題，但是假如樣本量增加，偏倚依然存在（估計量不一致, inconsistent），那常常意味着是抽樣過程出現了問題。例如：&lt;br&gt;用簡單隨機抽樣法獲得的樣本均值，就是總體或人羣均值的無偏估計 (unbiased estimator)。如果抽樣時由於某些主觀客觀的原因導致較小的樣本很少被抽樣（抽樣過程出了問題，脫離了簡單隨機抽樣原則），那麼此時得到的樣本均值就會是一個過高的估計值 (upward biased estimator)。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;精確度：估計值的精確度可以通過樣本分佈的方差或標準差來評價（簡單說是樣本分佈的方差越低，波動越小，精確度越高）。樣本分佈的標準差被定義爲估計值的標準誤。假如估計量是樣本均值，那麼樣本分佈的標準差（估計量的標準誤）和樣本數據之間有如下的關係：
&lt;span class=&#34;math display&#34;&gt;\[均值的標準誤 = \frac{樣本數據的標準差}{\sqrt{樣本量大小}}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在一些簡單的情況下，通常估計值的選用不言自明（例如均值，或者百分比）。但是在複雜的情況下，我們可能可以有多個不同類型的估計量可以選擇，他們也常常各有利弊，需要我們做出取捨。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;信賴區間-confidence-intervals&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;信賴區間 confidence intervals&lt;/h3&gt;
&lt;p&gt;從樣本中計算估計量獲得的一個估計值，只是一個&lt;strong&gt;點估計 (point estimate)&lt;/strong&gt;。對比之下，信賴區間就是一個對這個點估計的精確度的體現。信賴區間越窄，說明我們對於總體或人羣的參數的可能取值的範圍估計越精確。&lt;/p&gt;
&lt;p&gt;信賴區間通常是成對成對的出現的，即有上限和下限。這樣的一對從樣本數據中計算得來的統計量，同樣也是有樣本分佈的。&lt;strong&gt;每次我們重新從總體或人羣中抽樣，計算獲得的信賴區間都不同，這些信賴區間就組成了信賴區間的樣本分佈。總體和人羣的參數落在這些信賴區間範圍內的概率，就是我們常說的信賴區間的水平（&lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt;）。&lt;/strong&gt; 常用的這個概率值就是 &lt;span class=&#34;math inline&#34;&gt;\(95\%, 90\%, 99\%\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;當從樣本數據計算獲得的估計量的信賴區間很寬，說明了這個收集來的數據提供了很少的參數信息，導致估計變得很不精確。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;看到這裏的都是好漢一條啊！ 我不知道你暈了麼有，反正我是已經暈了。。。。&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>中心極限定理的應用</title>
      <link>https://wangcc.me/post/central-limit-theorem-application/</link>
      <pubDate>Sat, 21 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/central-limit-theorem-application/</guid>
      <description>


&lt;div id=&#34;二項分佈的正態分佈近似&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;二項分佈的正態分佈近似&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;假設我們有大量(&lt;span class=&#34;math inline&#34;&gt;\(n\rightarrow\infty\)&lt;/span&gt;)的二項分佈實驗 &lt;span class=&#34;math inline&#34;&gt;\(X\sim Bin(n, \pi)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;根據&lt;a href=&#34;https://winterwang.github.io/post/probability3/&#34;&gt;二項分佈的概率公式&lt;/a&gt;，計算將會變得很繁瑣複雜。&lt;/li&gt;
&lt;li&gt;解決辦法：應用中心極限定理。&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://winterwang.github.io/post/probability3/&#34;&gt;中心極限定理&lt;/a&gt;告訴我們，當樣本量足夠大時:
&lt;span class=&#34;math display&#34;&gt;\[X\sim N（n\pi, n\pi(1-\pi))\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;問題在於，多大的 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 才能算大樣本呢？
&lt;ul&gt;
&lt;li&gt;當且僅當 (only and if only) &lt;span class=&#34;math inline&#34;&gt;\(n&amp;gt;20\)&lt;/span&gt; AND &lt;span class=&#34;math inline&#34;&gt;\(n\pi&amp;gt;5\)&lt;/span&gt; AND &lt;span class=&#34;math inline&#34;&gt;\(n(1-\pi)&amp;gt;5\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;泊松分佈的正態分佈近似&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;泊松分佈的正態分佈近似&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;假設時間 &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; 內某事件的發生次數服從泊松分佈 &lt;span class=&#34;math inline&#34;&gt;\(X\sim Po(\mu)\)&lt;/span&gt;。&lt;/li&gt;
&lt;li&gt;考慮將這段時間 &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; 等分成 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 個時間段。那麼第 &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; 時間段內事件發生次數依舊服從泊松分佈 &lt;span class=&#34;math inline&#34;&gt;\(X_i\sim Po(\frac{\mu}{n})\)&lt;/span&gt;。且 &lt;span class=&#34;math inline&#34;&gt;\(E(X_i)=\mu/n, Var(X_i)=\mu/n\)&lt;/span&gt;。&lt;/li&gt;
&lt;li&gt;&lt;p&gt;那麼原先的 &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 可以被視爲是將這無數的小時間段的 &lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt; 相加。應用中心極限定理：
&lt;span class=&#34;math display&#34;&gt;\[X=\sum_{i=1}^nX_i\sim N(\frac{n\mu}{n}, \frac{n\mu}{n})\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;需要注意的是，這段時間 (&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;) 內發生的事件次數 (&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;) : &lt;span class=&#34;math inline&#34;&gt;\(\lambda t =\mu&amp;gt;10\)&lt;/span&gt; ，這樣的正態分佈模擬才能成立。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;正態分佈模擬的校正continuity-corrections&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;正態分佈模擬的校正：continuity corrections&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;如果我們使用正態分佈來模擬離散變量的分佈，常常需要用到正態分佈模擬的矯正。&lt;/li&gt;
&lt;li&gt;例如：我們如果用正態分佈模擬來計算 &lt;span class=&#34;math inline&#34;&gt;\(P(X=15)\)&lt;/span&gt;，那麼實際上我們應該計算的是 &lt;span class=&#34;math inline&#34;&gt;\(P(14.5&amp;lt;X&amp;lt;15.5)\)&lt;/span&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;例題&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;例題&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;已知 &lt;span class=&#34;math inline&#34;&gt;\(X\sim Bin(100,0.5)\)&lt;/span&gt;，求 &lt;span class=&#34;math inline&#34;&gt;\(P(X&amp;gt;60)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;解&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;解&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
\because X&amp;amp;\sim Bin(100, 0.5) \\ \therefore E(X) &amp;amp;=n\pi=50 \\
Var(X) &amp;amp;= n\pi(1-\pi) =25=5^2\\
P(X&amp;gt;60)  &amp;amp;= 1-P(X\leqslant60) \\
         &amp;amp;= 1-P(Z\leqslant\frac{60.5-50}{\sqrt{25}}) \\
         &amp;amp;= 1-P(Z\leqslant2.1) \\
         &amp;amp;= 1-\Phi(2.1) \\
         &amp;amp;= 1-0.982 = 0.018
\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 快來看實際用傻瓜算法計算獲得的概率：
1-pbinom(60, size=100, prob=0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0176001&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 快來看用中心極限定理模擬正態分佈獲得的概率：
1-pnorm((60.5-50)/sqrt(25))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.01786442&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/post/2017-10-21-central-limit-theorem-application_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;已知 &lt;span class=&#34;math inline&#34;&gt;\(X\sim Bin(48, 0.75)\)&lt;/span&gt;, 求 &lt;span class=&#34;math inline&#34;&gt;\(P(30&amp;lt;X&amp;lt;39)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;解-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;解&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\because B &amp;amp;\; \sim Bin(48, 0.75) \\
\therefore E(X) &amp;amp;\; =n\pi=36 \\
           Var(X) &amp;amp;\; =n\pi(1-\pi)=9=3^2 \\
P(30&amp;lt;X&amp;lt;39) &amp;amp;\; = P(31\leqslant X\leqslant 38)\\
     &amp;amp;\; = P(30.5\leqslant Y \leqslant 38.5) \\
     Y\;is\;the&amp;amp;\;normal\;approximation \\
     &amp;amp;\;= P(Y&amp;lt;38.5) - P(Y&amp;lt;30.5) \\
     &amp;amp;\;= P(Z\leqslant\frac{38.5-36}{3})-
          P(Z\leqslant\frac{30.5-36}{3}) \\
     &amp;amp;\;= P(Z\leqslant0.833) - P(Z\leqslant-1.833) \\
     &amp;amp;\;= \Phi(0.833)-\Phi(-1.833) \\
     &amp;amp;\;= 0.798-0.033 = 0.764
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 快來看實際用傻瓜算法計算獲得的概率：
pbinom(38, size=48, prob=0.75)-pbinom(30, size=48, prob=0.75)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7578159&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 快來看用中心極限定理模擬正態分佈獲得的概率：
pnorm((38.5-36)/sqrt(9)) - pnorm((30.5-36)/sqrt(9))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7642951&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/post/2017-10-21-central-limit-theorem-application_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;從上面兩個例題也能看出，&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 越小，正態分佈模擬的誤差就越大。&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;已知 &lt;span class=&#34;math inline&#34;&gt;\(X \sim Poisson(30)\)&lt;/span&gt; 求 &lt;span class=&#34;math inline&#34;&gt;\(P(X\leqslant20)\)&lt;/span&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;解-2&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;解&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\because E(X)=\mu=30, \;Var(X)=\mu=30=(\sqrt{30})^2 \\
 \begin{aligned}
 Pr(X\leqslant20) &amp;amp;= P(Z\leqslant\frac{20.5-30}{\sqrt{30}}) \\
                  &amp;amp;= P(Z\leqslant-1.734) \\
                  &amp;amp;= \Phi(-1.734) \\
                  &amp;amp;= 0.0414
 \end{aligned}
 \]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 快來看實際用傻瓜算法計算獲得的概率：
ppois(20, lambda=30)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.03528462&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 快來看用中心極限定理模擬正態分佈獲得的概率：
pnorm((20.5-30)/sqrt(30))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.04141871&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;這兩個其實有些小差距。不過看下圖，其模擬還是很到位的。只是正態分佈的面積明顯確實比泊松分佈的小柱子面積要大一些。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/post/2017-10-21-central-limit-theorem-application_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;已知 &lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2 \stackrel{i.i.d}{\sim} Poi(30)\)&lt;/span&gt; 求 &lt;span class=&#34;math inline&#34;&gt;\(P(X_1+X_2\leqslant40)\)&lt;/span&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;解-3&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;解&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
E(X_1+X_2) &amp;amp;\;= E(X_1)+E(X_2) = 30+30 = 60\\
Var(X_1+X_2) &amp;amp;\;= Var(X_1)+Var(X_2) = 30+30 \\
             &amp;amp;\;= (\sqrt{60})^2 \\
P(X_1+X_2\leqslant 40) &amp;amp;\;= P(Z \leqslant \frac{40.5-60}{\sqrt{60}}) \\
           &amp;amp;\;= P(Z\leqslant-2.517) \\
           &amp;amp;\;= \Phi(-2.517) \\
           &amp;amp;\;= 0.006
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 快來看實際用傻瓜算法計算獲得的概率：
ppois(40, lambda=60)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.00398281&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 快來看用中心極限定理模擬正態分佈獲得的概率：
pnorm((40.5-60)/sqrt(60))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.005910569&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/post/2017-10-21-central-limit-theorem-application_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;
又一次，正態分佈的面積比泊松分佈的小柱子面積要大一些。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;兩個連續隨機變量&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;兩個連續隨機變量&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;假定 &lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2\)&lt;/span&gt; 是兩個連續隨機變量：
&lt;span class=&#34;math display&#34;&gt;\[E(X_1)=\mu_1, Var(X_1)=\sigma_1^2 \\
E(X_2)=\mu_2, Var(X_2)=\sigma_2^2 \\
Corr(X_1, X_2)=\rho \Rightarrow Cov(X_1, X_2)=\rho\sigma_1\sigma_2=\sigma_{12}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;利用矩陣的標記法，可以將 &lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2\)&lt;/span&gt; 標記爲 &lt;span class=&#34;math inline&#34;&gt;\(\textbf{X}=(X_1, X_2)^T\)&lt;/span&gt;, 即：
&lt;span class=&#34;math display&#34;&gt;\[\textbf{X}=\left(
\begin{array}{c}
X_1\\
X_2\\
\end{array}
\right)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;上面的所有內容都可以標記爲：
&lt;span class=&#34;math display&#34;&gt;\[E(\textbf{X})=\mathbf{\mu}=\left(
\begin{array}{c}
\mu_1\\
\mu_2\\
\end{array}
\right)\\
Covariance \;matrix: \\
Var(\textbf{X})=\mathbf{\Sigma}=\left(
\begin{array}{c}
\sigma_1^2 &amp;amp; \sigma_{12}\\
\sigma_{12} &amp;amp; \sigma_1^2\\
\end{array}
\right)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;兩個連續隨機變量-例子&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;兩個連續隨機變量 例子：&lt;/h2&gt;
&lt;p&gt;假如要看收縮期血壓 (&lt;span class=&#34;math inline&#34;&gt;\(SBP\)&lt;/span&gt;) 和舒張期血壓 (&lt;span class=&#34;math inline&#34;&gt;\(DBP\)&lt;/span&gt;) 之間的關係：&lt;/p&gt;
&lt;p&gt;下列爲已知條件：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(SBP\)&lt;/span&gt; 的均值爲 &lt;span class=&#34;math inline&#34;&gt;\(130\)&lt;/span&gt;， 標準差爲 &lt;span class=&#34;math inline&#34;&gt;\(15\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(DBP\)&lt;/span&gt; 的均值爲 &lt;span class=&#34;math inline&#34;&gt;\(90\)&lt;/span&gt;, 標準差爲 &lt;span class=&#34;math inline&#34;&gt;\(10\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(SBP\)&lt;/span&gt; 和 &lt;span class=&#34;math inline&#34;&gt;\(DBP\)&lt;/span&gt; 之間的相關係數爲 &lt;span class=&#34;math inline&#34;&gt;\(0.75\)&lt;/span&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;那麼， 我們可以把這些信息用下面的方法來標記：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[E(\textbf{X})=\mathbf{\mu}=\left(
\begin{array}{c}
130\\
90\\
\end{array}
\right)\\
Var(\textbf{X})=\mathbf{\Sigma}=\left(
\begin{array}{c}
225 &amp;amp; 112.5\\
112.5 &amp;amp; 225\\
\end{array}
\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;條件分佈和邊緣分佈的概念&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;條件分佈和邊緣分佈的概念&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;如果 &lt;span class=&#34;math inline&#34;&gt;\(\textbf{X}=(X_1, X_2)^T\)&lt;/span&gt; 的兩個變量都服從正態分佈；&lt;/li&gt;
&lt;li&gt;&lt;p&gt;那麼這兩個變量的邊緣分佈 (marginal distribution) 也服從正態分佈:
&lt;span class=&#34;math display&#34;&gt;\[X_1\sim N(\mu_1,\sigma_1^2), X_2\sim N(\mu, \sigma_2^2)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;同樣的，&lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt; 的給出 &lt;span class=&#34;math inline&#34;&gt;\(X_2\)&lt;/span&gt; 的條件分佈 (condition distribution) 也服從正態分佈：
&lt;span class=&#34;math display&#34;&gt;\[E(X_1|X_2)=\mu_1+\frac{\rho\sigma_1}{\sigma_2}(X_2-\mu_2) \\
 Var(X_1|X_2)=\sigma_1^2(1-\rho^2)\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;反之亦然。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;條件分佈和邊緣分佈的例子&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;條件分佈和邊緣分佈的例子&lt;/h2&gt;
&lt;p&gt;上面的概念過於抽象，用血壓的例子：&lt;/p&gt;
&lt;p&gt;收縮期血壓和舒張期血壓各自服從正態分佈。那麼可以用上面的概念來寫出已知舒張期血壓時，收縮期血壓的分佈。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;條件期望:
&lt;span class=&#34;math display&#34;&gt;\[E(SBP|DBP)=130+\frac{0.75\times15}{10}(DBP-90)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;實際如果來了一個病人，他說他只記得自己測的舒張期血壓是95：&lt;br&gt;
他的收縮期血壓的期望值就可以用上面的式子計算：
&lt;span class=&#34;math display&#34;&gt;\[E(SBP|DBP=95)=136\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;條件方差爲：
&lt;span class=&#34;math display&#34;&gt;\[Var(SBP|DBP)=15^2(1-0.75^2)=98.4\approx9.92^2&amp;lt;15^2\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;所以當我們知道了這個人的一部分信息以後，推測他的另一個相關連的變量變得更加準確(&lt;strong&gt;方差變小&lt;/strong&gt;)了。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;例題-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;例題&lt;/h3&gt;
&lt;p&gt;有 (閒) 人記錄了 &lt;span class=&#34;math inline&#34;&gt;\(1494\)&lt;/span&gt; 名兒童在 &lt;span class=&#34;math inline&#34;&gt;\(2, 4, 6\)&lt;/span&gt; 歲時的腿長度。已知在記錄的這三個年齡時的平均腿長度分別爲 &lt;span class=&#34;math inline&#34;&gt;\(85 cm, 103cm, 114cm\)&lt;/span&gt;。協方差矩陣如下:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\left(
\begin{array}{c}
22.2 &amp;amp; 11.8 &amp;amp; 13.7\\
11.8 &amp;amp; 26.3 &amp;amp; 21.5\\
13.7 &amp;amp; 21.5 &amp;amp; 29.0
\end{array}
\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;假定，這三個年齡記錄的這些兒童的腿長度數據（聯合分佈, joint distribution）服從三個變量正態分佈。&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;求 &lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt; 歲時這些兒童的腿長度的邊緣分佈 (marginal distribution)&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;解-4&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;解&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X_{age=2} \sim N(85, \sigma_{age=2}^2=22.2)\]&lt;/span&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;求他們 &lt;span class=&#34;math inline&#34;&gt;\(6\)&lt;/span&gt; 歲時腿長度的 &lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt; 歲時的條件分佈。(Find the distribution of leg length age 6 conditional on leg length at age 2.)&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;解-5&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;解&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(6\)&lt;/span&gt; 歲時和 &lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt; 歲時腿長的相關係數 (correlation, &lt;span class=&#34;math inline&#34;&gt;\(\rho_{6,2}\)&lt;/span&gt;) 爲：
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\rho_{6,2} &amp;amp;= \frac{Cov_{6,2}}{\sqrt{Var(length_6)}\sqrt{Var(length_2)}}\\
&amp;amp;= \frac{13.7}{\sqrt{22.2}\sqrt{29}}=0.54
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;條件分佈套用上面提到的公式：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
E(length_6 | length_2) &amp;amp;= \mu_6+\frac{\rho_{6,2}\sigma_6}{\sigma_2}(length_2-\mu_2) \\
&amp;amp;= 114+\frac{0.54\times\sqrt{29.0}}{\sqrt{22.2}}(length_2-85)\\
Var(length_6 | length_2) &amp;amp;= \sigma_6^2(1-\rho_{6,2}^2) \\
                         &amp;amp;= 29.0\times(1-0.54^2) =20.5
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>偉大的中心極限定理</title>
      <link>https://wangcc.me/post/central-limit-theory/</link>
      <pubDate>Thu, 19 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/central-limit-theory/</guid>
      <description>


&lt;p&gt;最近明顯可以感覺到課程的步驟開始加速。看我的課表：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/img/IMG_0522.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;手機畫面太小了。早上都是9點半開始，下午基本都是到5點。週一更慘，到7點。週二-週五中午都被統計中心的講座佔據。簡直是非人的生活。&lt;/p&gt;
&lt;p&gt;這周概率論基礎結束。中心極限定理講完以後我們正式進入了 Inference 統計推斷的課程。我們花了一天時間講什麼是樣本估計 (Estimation)，什麼是參數精確度 (Precision)，什麼是自由度 (degree of freedom)，怎樣進行不偏的估計 (unbiased inference)。然後還有似然方程 (likelihood function)。&lt;/p&gt;
&lt;p&gt;今天的更新還是簡單的把概率論掃尾一下。感受一下中心極限定理的偉大。&lt;/p&gt;
&lt;div id=&#34;協方差-covariance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;協方差 Covariance&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://winterwang.github.io/post/probability2-4/&#34;&gt;之前我們定義過&lt;/a&gt;，兩個獨立連續隨機變量 &lt;span class=&#34;math inline&#34;&gt;\(X,Y\)&lt;/span&gt; 之和的方差 Variance ：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Var(X+Y)=Var(X)+Var(Y)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;然而如果他們並不相互獨立的話：&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
Var(X+Y) &amp;amp;= E[((X+Y)-E(X+Y))^2] \\
         &amp;amp;= E[(X+Y)-(E(X)+E(Y))^2] \\
         &amp;amp;= E[(X-E(X)) - (Y-E(Y))^2] \\
         &amp;amp;= E[(X-E(X))^2+(Y-E(Y))^2 \\
         &amp;amp; \;\;\; +2(X-E(X))(Y-E(Y))] \\
         &amp;amp;= Var(X)+Var(Y)+2E[(X-E(X))(Y-E(Y))]
\end{aligned}\]&lt;/span&gt;
&lt;p&gt;可以發現在兩者和的方差公式展開之後多了一部分 &lt;span class=&#34;math inline&#34;&gt;\(E[(X-E(X))(Y-E(Y))]\)&lt;/span&gt;。 這個多出來的一部分就說明了二者 &lt;span class=&#34;math inline&#34;&gt;\((X, Y)\)&lt;/span&gt; 之間的關係。它被定義爲協方差 (Covariance):
&lt;span class=&#34;math display&#34;&gt;\[Cov(X,Y) = E[(X-E(X))(Y-E(Y))]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;所以：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Var(X+Y)=Var(X)+Var(Y)+2Cov(X,Y)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    要記住，協方差只能用於評價&lt;span class=&#34;math inline&#34;&gt;(X,Y)&lt;/span&gt;之間的線性關係 (Linear Association)。
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;以下是協方差 (Covariance) 的一些特殊性質：&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Cov(X,X)=Var(X)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Cov(X,Y)=Cov(Y,X)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Cov(aX,bY)=ab\:Cov(X,Y)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Cov(aR+bS,cX+dY)=ac\:Cov(R,X)+ad\:Cov(R,Y)\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+bc\:Cov(S,X)+bd\:Cov(S,Y)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Cov(aX+bY,cX+dY)=ac\:Var(X)+ad\:Var(Y)\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+(ad+bc)Cov(X,Y)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Cov(X+Y,X-Y)=Var(X)-Var(Y)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;If &lt;span class=&#34;math inline&#34;&gt;\(X, Y\)&lt;/span&gt; are independent. &lt;span class=&#34;math inline&#34;&gt;\(Cov(X,Y)=0\)&lt;/span&gt; &lt;span class=&#34;diff_alert&#34;&gt;But not vise-versa !&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;相關-correlation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;相關 Correlation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;協方差雖然&lt;span class=&#34;math inline&#34;&gt;\(Cov(X,Y)\)&lt;/span&gt; 的大小很大程度上會被他們各自的單位和波動大小左右。&lt;/li&gt;
&lt;li&gt;我們將協方差標準化(除以各自的標準差 s.d.) (standardization) 之後，就可以得到相關係數 Corr (&lt;span class=&#34;math inline&#34;&gt;\(-1\sim1\)&lt;/span&gt;):
&lt;span class=&#34;math display&#34;&gt;\[Corr(X,Y)=\frac{Cov(X,Y)}{SD(X)SD(Y)}=\frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;中心極限定理-the-central-limit-theory&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;中心極限定理 the Central Limit Theory&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;diff_add&#34;&gt;&lt;strong&gt;如果從人羣中多次選出樣本量爲 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 的樣本，並計算樣本均值, &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}_n\)&lt;/span&gt;。那麼這個樣本均值 &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}_n\)&lt;/span&gt; 的分佈，會隨着樣本量增加 &lt;span class=&#34;math inline&#34;&gt;\(n\rightarrow\infty\)&lt;/span&gt;，而接近正態分佈。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;偉大的中心極限定理告訴我們：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;diff_alert&#34;&gt;&lt;strong&gt;當樣本量足夠大時，樣本均值 &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}_n\)&lt;/span&gt; 的分佈爲正態分佈，這個特性與樣本來自的人羣的分佈 &lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt; 無關。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;再說一遍：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;如果對象是獨立同分佈 i.i.d (identically and independently distributed)。那麼它的總體期望和方差分別是: &lt;span class=&#34;math inline&#34;&gt;\(E(X)=\mu;\;Var(X)=\sigma^2\)&lt;/span&gt;。
根據中心極限定理，可以得到：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;當樣本量增加，樣本均值的分佈服從正態分佈：
&lt;span class=&#34;math display&#34;&gt;\[\bar{X}_n\sim N(\mu, \frac{\sigma^2}{n})\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;也可以寫作，當樣本量增加：
&lt;span class=&#34;math display&#34;&gt;\[\sum_{i=1}^nX_i \sim N(n\mu,n\sigma^2)\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;有了這個定理，我們可以拋開樣本空間(&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;)的分佈，也不用假定它服從正態分佈。&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;diff_alert&#34;&gt;但是樣本的均值，卻總是服從正態分佈的。&lt;/span&gt;簡直是太完美了！！！！！！&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>你買的彩票中獎概率到底有多少？</title>
      <link>https://wangcc.me/post/probability3/</link>
      <pubDate>Wed, 11 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/probability3/</guid>
      <description>


&lt;div id=&#34;二項分佈的概念-binomial-distribution&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;二項分佈的概念 Binomial distribution&lt;/h3&gt;
&lt;p&gt;二項分佈在醫學研究中至關重要，一組二項分佈的數據，指的通常是 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 次相互獨立的&lt;a href=&#34;https://winterwang.github.io/post/probability2-4/&#34;&gt;成功率爲 &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; 的伯努利實驗&lt;/a&gt; (&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; independent Bernoulli trials) 中成功的次數。&lt;/p&gt;
&lt;p&gt;當 &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 服從二項分佈，記爲 &lt;span class=&#34;math inline&#34;&gt;\(X \sim binomial(n, \pi)\)&lt;/span&gt; 或&lt;span class=&#34;math inline&#34;&gt;\(X \sim bin(n, \pi)\)&lt;/span&gt;。它的(第 &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; 次實驗的)概率被定義爲：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
P(X=x) &amp;amp;= ^nC_x\pi^x(1-\pi)^{n-x} \\
       &amp;amp;= \binom{n}{x}\pi^x(1-\pi)^{n-x} \\
       &amp;amp; for\;\; x = 0,1,2,\dots,n
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;二項分佈的期望和方差&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;二項分佈的期望和方差&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;期望 &lt;span class=&#34;math inline&#34;&gt;\(E(X)\)&lt;/span&gt;
&lt;ul&gt;
&lt;li&gt;若 &lt;span class=&#34;math inline&#34;&gt;\(X \sim bin(n,\pi)\)&lt;/span&gt;，那麼 &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 就是這一系列獨立伯努利實驗中成功的次數。&lt;/li&gt;
&lt;li&gt;用 &lt;span class=&#34;math inline&#34;&gt;\(X_i, i =1,\dots, n\)&lt;/span&gt; 標記每個相互獨立的伯努利實驗。&lt;/li&gt;
&lt;li&gt;那麼我們可以知道 &lt;span class=&#34;math inline&#34;&gt;\(X=\sum_{i=1}^nX_i\)&lt;/span&gt;。
&lt;span class=&#34;math display&#34;&gt;\[\begin{align} E(X) &amp;amp;= E(\sum_{i=1}^nX_i)\\
                   &amp;amp;= E(X_1+X_2+\cdots+X_n) \\
                   &amp;amp;= E(X_1)+E(X_2)+\cdots+E(X_n)\\
                   &amp;amp;= \sum_{i=1}^nE(X_i)\\
                   &amp;amp;= \sum_{i=1}^n\pi \\
                   &amp;amp;= n\pi
\end{align}\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;方差 &lt;span class=&#34;math inline&#34;&gt;\(Var(X)\)&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
Var(X) &amp;amp;= Var(\sum_{i=1}^nX_i) \\
      &amp;amp;= Var(X_i+X_2+\cdots+X_n) \\
      &amp;amp;= Var(X_i)+Var(X_2)+\cdots+Var(X_n) \\
      &amp;amp;= \sum_{i=1}^nVar(X_i) \\
      &amp;amp;= n\pi(1-\pi) \\
\end{align}\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;超幾何分佈-hypergeometric-distribution&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;超幾何分佈 hypergeometric distribution&lt;/h3&gt;
&lt;p&gt;假設我們從總人數爲 &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; 的人羣中，採集一個樣本 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;。假如已知在總體人羣中(&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;)有 &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; 人患有某種疾病。請問採集的樣本 &lt;span class=&#34;math inline&#34;&gt;\(X=n\)&lt;/span&gt; 中患有這種疾病的人，服從怎樣的分佈？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;從人羣(&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;)中取出樣本(&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;)，有 &lt;span class=&#34;math inline&#34;&gt;\(^NC_n\)&lt;/span&gt; 種方法。&lt;/li&gt;
&lt;li&gt;從患病人羣(&lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt;)中取出患有該病的人(&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;)有 &lt;span class=&#34;math inline&#34;&gt;\(^MC_x\)&lt;/span&gt; 種方法。&lt;/li&gt;
&lt;li&gt;樣本中不患病的人(&lt;span class=&#34;math inline&#34;&gt;\(n-x\)&lt;/span&gt;)被採樣的方法有 &lt;span class=&#34;math inline&#34;&gt;\(^{N-M}C_{n-x}\)&lt;/span&gt; 種。&lt;/li&gt;
&lt;li&gt;採集一次 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 人作爲樣本的概率都一樣。因此：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(X=x)=\frac{\binom{M}{x}\binom{N-M}{n-x}}{\binom{N}{n}}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;樂透中獎概率問題&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;樂透中獎概率問題：&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;從數字 &lt;span class=&#34;math inline&#34;&gt;\(1\sim59\)&lt;/span&gt; 中選取 &lt;span class=&#34;math inline&#34;&gt;\(6\)&lt;/span&gt; 個任意號碼&lt;/li&gt;
&lt;li&gt;開獎時從 &lt;span class=&#34;math inline&#34;&gt;\(59\)&lt;/span&gt; 個號碼球中隨機抽取 &lt;span class=&#34;math inline&#34;&gt;\(6\)&lt;/span&gt; 個&lt;/li&gt;
&lt;li&gt;如果六個號碼全部猜中(不分順序)，你可以成爲百萬富翁。請問一次猜中全部 &lt;span class=&#34;math inline&#34;&gt;\(6\)&lt;/span&gt; 個號碼的概率是多少？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;從 &lt;span class=&#34;math inline&#34;&gt;\(59\)&lt;/span&gt; 個號碼中隨機取出任意 &lt;span class=&#34;math inline&#34;&gt;\(6\)&lt;/span&gt; 個號碼的方法有 &lt;span class=&#34;math inline&#34;&gt;\(^{59}C_6\)&lt;/span&gt; 種。
&lt;span class=&#34;math display&#34;&gt;\[^{59}C_6=\frac{59!}{6!(59-6)!}=45,057,474\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;每次選取六個號碼做爲一組的可能性相同，所以，你買了一組樂透號碼，能中獎的概率就是 &lt;span class=&#34;math inline&#34;&gt;\(1/45,057,474 = 0.00000002219\)&lt;/span&gt;。你還會再去買彩票麼？&lt;/p&gt;
&lt;div id=&#34;如果我只想中其中的-3-個號碼概率有多大&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;如果我只想中其中的 &lt;span class=&#34;math inline&#34;&gt;\(3\)&lt;/span&gt; 個號碼，概率有多大？&lt;/h4&gt;
&lt;p&gt;用超幾何分佈的概率公式：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
P(X=3) &amp;amp;= \frac{^6C_3\times ^{53}C_3}{^{59}C_6} \\
       &amp;amp;= 0.010
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;你有 &lt;span class=&#34;math inline&#34;&gt;\(1\%\)&lt;/span&gt; 的可能中獎。換句話說，如果中三個以上的數字算中獎的話，你買的彩票中獎的概率低於 &lt;span class=&#34;math inline&#34;&gt;\(1\%\)&lt;/span&gt;。是不是覺得下次送錢給博彩公司的時候還不如跟我一起喝一杯咖啡划算？&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;泊松分佈-poisson-distribution&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;泊松分佈 Poisson Distribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;當一個事件，在一段時間 (&lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;) 中可能發生的次數是 &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; 。那麼我們可以認爲，經過時間 &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;，該時間發生的期望次數是 &lt;span class=&#34;math inline&#34;&gt;\(E(X)=\lambda T\)&lt;/span&gt;。&lt;/li&gt;
&lt;li&gt;利用微分思想，將這段時間 &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; 等分成 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 個時間段，當 &lt;span class=&#34;math inline&#34;&gt;\(n\rightarrow\infty\)&lt;/span&gt; 直到每個微小的時間段內最多發生一次該事件。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;那麼&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;每個微小的時間段，可以視爲是一個伯努利實驗（有事件發生或者沒有）&lt;/li&gt;
&lt;li&gt;那麼這整段時間 &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; 內發生的事件可以視爲是一個二項分佈實驗。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;令 &lt;span class=&#34;math inline&#34;&gt;\(X=\)&lt;/span&gt; 一次事件發生時所經過的所有時間段。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(X \sim Bin(n, \pi)\)&lt;/span&gt;，其中 &lt;span class=&#34;math inline&#34;&gt;\(n\rightarrow\infty\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 爲時間段。&lt;/li&gt;
&lt;li&gt;在每個分割好的時間段內，事件發生的概率都是：&lt;span class=&#34;math inline&#34;&gt;\(\pi=\frac{\lambda T}{n}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;期望 &lt;span class=&#34;math inline&#34;&gt;\(\mu=\lambda T \Rightarrow \pi=\mu/n\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;所以 &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 的概率方程就是：
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
P(X=x) &amp;amp;= \binom{n}{x}\pi^x(1-\pi)^{n-x} \\
     &amp;amp;= \binom{n}{x}(\frac{\mu}{n})^x(1-\frac{\mu}{n})^{n-x} \\
     &amp;amp;= \frac{n!}{x!(n-x)!}(\frac{\mu}{n})^x(1-\frac{\mu}{n})^{n-x} \\
     &amp;amp;=\frac{n!}{n^x(n-x)!}\frac{\mu^x}{x!}(1-\frac{\mu}{n})^{n-x}\\
當 n\rightarrow\infty   &amp;amp;\; x \ll n (x遠小於n) 時\\
\frac{n!}{n^x(n-x)!} &amp;amp;=\frac{n(n-1)\dots(n-x+1)}{n^x} \rightarrow 1\\
(1-\frac{\mu}{n})^{n-x} &amp;amp;\approx  (1-\frac{\mu}{n})^n \rightarrow e^{-\mu}\\
所以 我們可&amp;amp;以得到泊松分佈的概率公式：   \\
P(X=x) &amp;amp;\rightarrow \frac{\mu^x}{x!}e^{-\mu}
\end{align}\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;當數據服從泊松分佈時，記爲 &lt;span class=&#34;math inline&#34;&gt;\(X\sim Poisson(\mu=\lambda T)\;\; or\;\; X\sim Poi(\mu)\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;證明泊松分佈的參數特徵&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;證明泊松分佈的參數特徵：&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(E(X)=\mu\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
E(X)  &amp;amp;=  \sum_{x=0}^\infty xP(X=x) \\
      &amp;amp;=  \sum_{x=0}^\infty x\frac{\mu^x}{x!}e^{-\mu} \\
      &amp;amp;= 0+ \sum_{x=1}^\infty x\frac{\mu^x}{x!}e^{-\mu} \\
      &amp;amp;=  \sum_{x=1}^\infty \frac{\mu^x}{(x-1)!}e^{-\mu} \\
      &amp;amp;=  \mu\sum_{x=1}^\infty \frac{\mu^{x-1}}{(x-1)!}e^{-\mu} \\
這個時候我們用i&amp;amp;=x-1 替換掉所有的 x \\
      &amp;amp;=  \mu\sum_{i=0}^\infty \frac{\mu^{i}}{i!}e^{-\mu} \\
注意到右半部分 &amp;amp;\sum_{i=0}^\infty \frac{\mu^{i}}{i!}e^{-\mu}=1 是一個\\泊松分佈的所有&amp;amp;概率和 \\
      &amp;amp;= \mu
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Var(x)=\mu\)&lt;/span&gt;
爲了找到 &lt;span class=&#34;math inline&#34;&gt;\(Var(X)\)&lt;/span&gt;，我們用公式 &lt;span class=&#34;math inline&#34;&gt;\(Var(X)=E(X^2)-E(X)^2\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;我們需要找到 &lt;span class=&#34;math inline&#34;&gt;\(E(X^2)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
E(X^2) &amp;amp;= \sum_{x=0}^\infty x^2\frac{\mu^x}{x!}e^{-\mu} \\
       &amp;amp;= \mu \sum_{x=1}^\infty x\frac{\mu^{x-1}}{(x-1)!}e^{-\mu} \\
這個時候我們用i&amp;amp;=x-1 替換掉所有的 x \\
       &amp;amp;= \mu \sum_{i=0}^\infty (i+1)\frac{\mu^{i}}{i!}e^{-\mu} \\
       &amp;amp;= \mu(\sum_{i=0}^\infty i\frac{\mu^i}{i!}e^{-\mu} + \sum_{i=0}^\infty \frac{\mu^i}{i!}e^{-\mu}) \\
       &amp;amp;= \mu(E(X)+1) \\
       &amp;amp;= \mu^2+\mu \\
因此，代入上面&amp;amp;提到的方差公式： \\
Var(X) &amp;amp;= E(X^2) - E(X)^2 \\
       &amp;amp;= \mu^2 + \mu -\mu^2 \\
       &amp;amp;= \mu
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>正態分佈</title>
      <link>https://wangcc.me/post/normal-distribution/</link>
      <pubDate>Wed, 11 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/normal-distribution/</guid>
      <description>


&lt;div id=&#34;概率密度曲線-probability-density-function-pdf&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;概率密度曲線 probability density function， PDF&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;一個隨機連續型變量 &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 它的性質由一個對應的&lt;strong&gt;概率密度方程 (probability density function, PDF)&lt;/strong&gt; 決定。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;在給定的範圍區間內，如 &lt;span class=&#34;math inline&#34;&gt;\(a\sim b, (a &amp;lt; b)\)&lt;/span&gt;，它的概率滿足:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(a\leqslant X \leqslant b) = \int_a^bf(x)dx\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;這個相關的方程，在 &lt;span class=&#34;math inline&#34;&gt;\(a\sim b\)&lt;/span&gt; 區間內的積分，就是這個連續變量在這個區間內取值的概率。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R codes for drawing a standard normal distribution by using ggplot2
library(ggplot2)
p &amp;lt;- ggplot(data.frame(x=c(-3,3)), aes(x=x)) +
  stat_function(fun = dnorm)
p + annotate(&amp;quot;text&amp;quot;, x=2, y=0.3, parse=TRUE, label=&amp;quot;frac(1, sqrt(2*pi)) * e ^(-z^2/2)&amp;quot;) +
  theme(plot.subtitle = element_text(vjust = 1),
        plot.caption = element_text(vjust = 1),
        axis.text.x = element_text(size = 12),
        axis.text.y = element_text(size = 12),
        plot.title = element_text(size = 10, face = &amp;quot;bold&amp;quot;, hjust = 0.5),
        panel.background = element_rect(fill = &amp;quot;ivory&amp;quot;)) +
  labs(title = &amp;quot;Probability density functions \n for standard normal distribution&amp;quot;,
       x = NULL, y = NULL) +
  stat_function(fun = dnorm,
                xlim = c(-1.3,0.4),
                geom = &amp;quot;area&amp;quot;,fill=&amp;quot;#00688B&amp;quot;, alpha= 0.2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/post/2017-10-11-normal-distribution_files/figure-html/normal%20distribution%20graph-1.png&#34; width=&#34;528&#34; /&gt;&lt;/p&gt;
&lt;p&gt;注意：整個方程的曲線下面積等於 &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;：
&lt;span class=&#34;math display&#34;&gt;\[\int_{-\infty}^\infty f(x)dx=1\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;期望 &lt;span class=&#34;math inline&#34;&gt;\(E(X)=\int_{-\infty}^\infty xf(x)dx\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;方差 &lt;span class=&#34;math inline&#34;&gt;\(Var(X)=\int_{-\infty}^\infty (x-\mu)^2f(x)dx\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;正態分佈&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;正態分佈&lt;/h3&gt;
&lt;p&gt;如果一組數據服從正態分佈，我們通常用它的期望（或者叫平均值）&lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;，和它的方差 &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;，來描述這組數據。記爲：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X \sim N(\mu, \sigma^2)\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;它的概率密度方程可以表述爲：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[f(x)=\frac{1}{\sqrt{2\pi\sigma^2}}exp(-\frac{(x-\mu)^2}{2\sigma^2})\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(E(x) =\mu\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Var(x)=\sigma^2\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;標準正態分佈&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;標準正態分佈&lt;/h3&gt;
&lt;p&gt;標準正態分佈的期望（或者均值）爲 &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;，方差爲 &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;記爲：&lt;span class=&#34;math inline&#34;&gt;\(Z \sim N(0,1)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;它的概率密度方程表述爲：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{1}{\sqrt{2\pi}}exp(-\frac{z^2}{2})\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;它的累積分佈方程 (cumulative distribution function， CDF)，是將概率密度方程 (PDF) 積分以後獲得的方程。通常我們記爲 &lt;span class=&#34;math inline&#34;&gt;\(\Phi(z)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;再看一下標準正態分佈的概率密度方程曲線：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/post/2017-10-11-normal-distribution_files/figure-html/normal%20distribution%20graph2-1.png&#34; width=&#34;528&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;95% 的曲線下面積在標準差 standard deviation &lt;span class=&#34;math inline&#34;&gt;\(-1.96\sim1.96\)&lt;/span&gt; 之間的區域。&lt;/li&gt;
&lt;li&gt;而且，&lt;span class=&#34;math inline&#34;&gt;\(\phi(-x)=1-\phi(x)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;任何一個正態分佈都可以通過下面的公式，標準化成爲標準正態分佈：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Z=\frac{X-\mu}{\sigma}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>概率論2</title>
      <link>https://wangcc.me/post/probability2-4/</link>
      <pubDate>Tue, 10 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/probability2-4/</guid>
      <description>


&lt;div id=&#34;bayes-理論的概念&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bayes 理論的概念&lt;/h3&gt;
&lt;p&gt;許多時候，我們需要將概率中的條件相互對調。
例如：
在已知該人羣中有20%的人有吸菸習慣(&lt;span class=&#34;math inline&#34;&gt;\(P(S)\)&lt;/span&gt;)，吸菸的人有9%的概率有哮喘(&lt;span class=&#34;math inline&#34;&gt;\(P(A|S)\)&lt;/span&gt;)，不吸菸的人有7%的概率有哮喘(&lt;span class=&#34;math inline&#34;&gt;\(P(A|\bar{S})\)&lt;/span&gt;)的前提下，有個人前來門診，發現是哮喘患者，那麼這個人有多大的概率是一個菸民？也就是要求 &lt;span class=&#34;math inline&#34;&gt;\(P(S|A)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;這裏先引入貝葉斯的概念：&lt;/p&gt;
&lt;p&gt;我們可以將 &lt;span class=&#34;math inline&#34;&gt;\(P(A\cap S)\)&lt;/span&gt; 寫成：
&lt;span class=&#34;math display&#34;&gt;\[P(A\cap S)=P(A|S)P(S)\\or\\
P(A\cap S)=P(S|A)P(A)\]&lt;/span&gt;
這兩個等式是完全等價的。我們將他們連起來：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(S|A)P(A)=P(A|S)P(S)\\
\Rightarrow P(S|A)=\frac{P(A|S)P(S)}{P(A)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;是不是看起來又像是寫了一堆&lt;strong&gt;廢話&lt;/strong&gt;？
沒錯，你看出來是一堆廢話的時候，證明你也同意這背後的簡單邏輯。&lt;/p&gt;
&lt;p&gt;再繼續，我們可以利用另外一個&lt;strong&gt;廢話&lt;/strong&gt;：&lt;span class=&#34;math inline&#34;&gt;\(\because S+\bar{S}=1\\ \therefore P(A)=P(A\cap S)+P(A\cap\bar{S})\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;用上面的公式替換掉 &lt;span class=&#34;math inline&#34;&gt;\(P(A\cap S)+P(A\cap\bar{S}） \\ \therefore P(A)=P(A|S)P(S)+P(A|\bar{S})P(\bar{S})\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;可以得到&lt;strong&gt;貝葉斯理論公式&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(S|A)=\frac{P(A|S)P(S)}{P(A|S)P(S)+P(A|\bar{S})P(\bar{S})}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;回到上面說到的哮喘人中有多少比例吸菸的問題。可以繼續使用概率樹來方便的計算：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/img/Selection_073.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
P(S|A) &amp;amp;= \frac{P(A|S)P(S)}{P(A|S)P(S)+P(A|\bar{S})P(\bar{S})} \\
        &amp;amp;= \frac{0.09\times0.2}{0.09\times0.2+0.07\times0.8} \\
        &amp;amp;= 0.24
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;所以我們的結論就是，在已知該人羣中有20%的人有吸菸習慣(&lt;span class=&#34;math inline&#34;&gt;\(P(S)\)&lt;/span&gt;)，吸菸的人有9%的概率有哮喘(&lt;span class=&#34;math inline&#34;&gt;\(P(A|S)\)&lt;/span&gt;)，不吸菸的人有7%的概率有哮喘(&lt;span class=&#34;math inline&#34;&gt;\(P(A|\bar{S})\)&lt;/span&gt;)的前提下，有個人前來門診，發現是哮喘患者，那麼這個人有24% 的概率是一個菸民(&lt;span class=&#34;math inline&#34;&gt;\(P(S|A)\)&lt;/span&gt;)。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;期望-expectation-或均值-or-mean-和-方差-variance&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;期望 Expectation (或均值 or mean) 和 方差 Variance&lt;/h3&gt;
&lt;p&gt;期望（或均值）是用來描述一組數據中心位置的指標（另一個是中位數 Median）。
對於離散型隨機變量 &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; (discrete random variables)，它的期望被定義爲：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[E(X)=\sum_x xP(X=x)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;所以就是將所有 &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 可能取到的值乘以相應的概率後求和。這個期望（或均值）常常用希臘字母 &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; 來標記。&lt;/p&gt;
&lt;p&gt;方差 Variance 是衡量一組數據變化幅度(dispersion/variability)的指標之一。 方差的定義是：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Var(X)=E((X-\mu)^2)\\其中，\mu=E(x)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;實際上我們更加常用的是它的另外一個公式：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Var(X)=E(X^2)-E(X)^2\]&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;證明-上面兩個方差公式相等&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;證明 上面兩個方差公式相等&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
Var(x)  &amp;amp;= E((X-\mu)^2) \\
        &amp;amp;= E(X^2-2X\mu+\mu^2)\\
        &amp;amp;= E(X^2) - 2\mu E(X) + \mu^2\\
        &amp;amp;= E(X^2) - 2\mu^2 + \mu^2 \\
        &amp;amp;= E(X^2) - \mu^2 \\
        &amp;amp;= E(X^2) - E(X)^2
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;方差的性質&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;方差的性質：&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Var(X+b)=Var(X)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Var(aX)=a^2Var(X)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Var(aX+b)=a^2Var(X)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;伯努利分佈-bernoulli-distribution&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;伯努利分佈 Bernoulli distribution&lt;/h3&gt;
&lt;p&gt;伯努利分佈，說的就是一個簡單的二分變量 (1, 0)，它取1時的概率如果是 &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt;。那麼我們可以計算這個分佈的期望值:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
E(X) &amp;amp;=\sum_x xP(X=x) \\
     &amp;amp;=1\times\pi + 0\times(1-\pi)\\
     &amp;amp;=\pi
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;由於 &lt;span class=&#34;math inline&#34;&gt;\(x=x^2\)&lt;/span&gt;，因爲 &lt;span class=&#34;math inline&#34;&gt;\(x=0,1\)&lt;/span&gt;, 所以 &lt;span class=&#34;math inline&#34;&gt;\(E[X^2]=E[X]\)&lt;/span&gt;，那麼方差爲：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
Var(X) &amp;amp;=E[X^2]-E[X]^2 \\
       &amp;amp;=E[X]-E[X]^2 \\
       &amp;amp;=\pi - \pi^2 \\
       &amp;amp;=\pi(1-\pi)
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;證明xy-爲互爲獨立的隨機離散變量時-a-exyexey-b-varxyvarxvary&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;證明，&lt;span class=&#34;math inline&#34;&gt;\(X,Y\)&lt;/span&gt; 爲互爲獨立的隨機離散變量時，&lt;br&gt;a) &lt;span class=&#34;math inline&#34;&gt;\(E(XY)=E(X)E(Y)\)&lt;/span&gt; ; &lt;br&gt;b) &lt;span class=&#34;math inline&#34;&gt;\(Var(X+Y)=Var(X)+Var(Y)\)&lt;/span&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;strong&gt;證明&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
E(XY) &amp;amp;= \sum_x\sum_y xyP(X=x, Y=y) \\
\because &amp;amp;\; X,Y are\;independent\;to\;each\;other \\
\therefore &amp;amp;= \sum_x\sum_y xyP(X=x)P(Y=y)\\
      &amp;amp;=\sum_x xP(X=x)\sum_y yP(Y=y)\\
      &amp;amp;=E(X)E(Y)
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;ol start=&#34;2&#34; style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;strong&gt;證明&lt;/strong&gt;
根據方差的定義：
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
Var(X+Y) &amp;amp;= E((X+Y)^2)-E(X+Y)^2 \\
    &amp;amp; \; Expand \\
    &amp;amp;=E(X^2+2XY+Y^2)-(E(X)+E(Y))^2\\
    &amp;amp;=E(X^2)+E(Y^2)+2E(XY)\\
    &amp;amp;\;\;\; - E(X)^2-E(Y)^2-2E(X)E(Y)\\
    &amp;amp;\; We\;just\;showed\; E(XY)=E(X)E(Y)\\
    &amp;amp;=E(X^2)-E(X)^2+E(Y^2)-E(Y)^2 \\
    &amp;amp;=Var(X)+Var(Y)
\end{align}\]&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>“你會用概率論來賭博嗎？”之解答</title>
      <link>https://wangcc.me/post/probability-gambling-answers/</link>
      <pubDate>Mon, 09 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/probability-gambling-answers/</guid>
      <description>


&lt;p&gt;前情提要：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/img/Selection_071.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;假設你在一個電視遊戲節目。有上圖一樣的三扇門。其中一扇門後面有一輛保時捷，另兩扇門後面則是&lt;a href=&#34;https://winterwang.github.io/post/black-meal/&#34;&gt;(味道奇特的)山羊&lt;/a&gt;。遊戲規則是主持人會讓你先選擇其中一扇門（先不打開你選的這扇門）。主持人隨後打開另外兩扇門中的一扇沒有保時捷的門。主持人問你，你要堅持選擇之前選中的那扇門，還是要改變主意換一扇門去猜是否可以猜中保時捷。
請問，堅持選擇之前選中的門猜中保時捷的概率高，還是主持人打開一扇門以後改變主意猜中保時捷的概率更高呢？&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;答案是：必須改變主意才能提高中獎概率。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;上述情況下，最簡單的是用概率樹 (probability tree) 來做決定：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/img/Selection_072.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;解說一下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;假定保時捷在1號門後，你第一次選擇了1號門，那麼此時主持人可以任意打開2號或者三號門（因爲他們後面都沒有保時捷）。&lt;/li&gt;
&lt;li&gt;假定保時捷在1號門後，你第一次選了2號門，那麼此時主持人只能打開3號門（因爲一號門後是保時捷，按照遊戲規則主持人不能打開）。&lt;/li&gt;
&lt;li&gt;假定保時捷在1號門後，你第一次選了3號門，那麼此時主持人只能打開2號門（因爲一號門後是保時捷，按照遊戲規則主持人不能打開）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;所以按照圖中給出的計算概率樹的過程可以得到:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P[改變主意以後贏得保時捷的概率]\\=\frac{1}{3}+\frac{1}{3}=\frac{2}{3}\\
P[不改主意，贏得保時捷的概率]\\=\frac{1}{6}+\frac{1}{6}=\frac{1}{3}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;你是否選擇了改變主意了呢？&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>你會用概率論來賭博嗎？</title>
      <link>https://wangcc.me/post/probability-gambling/</link>
      <pubDate>Sun, 08 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/probability-gambling/</guid>
      <description>


&lt;p&gt;轉眼我已經進入課程的第二週了，總體來說，我們一半的時間都在電腦房練習 Stata 的數據清理和簡單的描述統計 (descriptive statistics)。從我個人的經驗來說，數據分析的過程，其實一大半的時間是消耗在 data cleaning 上的，即使手頭拿到了所謂的乾淨的數據，到真正要分析的時候就會發現一大堆的問題在裏面，需要重新整理，重新添加標記以使之變得更加讓人類可以讀懂。電腦是機器，他是不管你的數據是否乾淨的。只要你放了數據進去，邏輯還可以，沒有編程上的語法錯誤，它總歸會出來一些報告和結果的。如果就這麼直接用的話，大部分的人就會掉進陷阱。畢竟數據不光會說出事實真相，&lt;strong&gt;更多的情況下還會把真相給掩蓋住了。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;我的其餘大部分時間都用在了複習高等數學的微積分上了。感覺好似回到了高中時代。其實大學的時候線性代數得分還是接近滿分的。後來多年不用，生疏了。剛打開複習的書的時候，許多微分積分的規則都已經忘記。通過這一週的辛苦練習，終於是找回了一點狀態。如果你也想有空的時候複習以下高中數學知識，這本書可以推薦給你：&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.amazon.co.uk/gp/product/0471827223/ref=oh_aui_detailpage_o04_s00?ie=UTF8&amp;amp;psc=1&#34;&gt;Quick Calculus: Short Manual of Self-instruction&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/img/Selection_070.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;上面這本書的內容可以一邊閱讀，一邊練習。實在是複習的一本好書。我花了一週的課餘時間，從頭到尾把裏面的習題和解答全部完成。收穫很大。感覺年輕時的數學思維又開始在大腦裏復甦了。一身輕鬆。&lt;/p&gt;
&lt;p&gt;下面想介紹一下上週學習的概率的基礎問題。&lt;/p&gt;
&lt;div id=&#34;首先是最基礎的三個概率的公理&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;首先是最基礎的&lt;strong&gt;三個概率的公理&lt;/strong&gt;：&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;對於任意事件 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;，它發生的概率 &lt;span class=&#34;math inline&#34;&gt;\(P(A)\)&lt;/span&gt; 滿足這樣的不等式： &lt;span class=&#34;math inline&#34;&gt;\(0 \leqslant P(A) \leqslant 1\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(\Omega)=1\)&lt;/span&gt; , &lt;span class=&#34;math inline&#34;&gt;\(\Omega\)&lt;/span&gt; 是全樣本空間 (total sample space)&lt;/li&gt;
&lt;li&gt;對於互斥（相互獨立）的事件 &lt;span class=&#34;math inline&#34;&gt;\(A_1, A_2, \dots, A_n\)&lt;/span&gt; 有如下的等式關係： &lt;span class=&#34;math inline&#34;&gt;\(P(A_1\cup A_2 \cup \cdots \cup A_n)=P(A_1)+P(A_2)+\cdots+P(A_n)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;你是不是覺得上面三條公理都是&lt;strong&gt;廢話&lt;/strong&gt;。
不用擔心，我也是這麼覺得的。因爲所有人都認同的道理，才能成爲公理 (axiom)，因爲它們是不需要證明的自然而然形成的人人都接受的觀念。&lt;code&gt;(axiom: a saying that is widely accepted on its own merits; its truth is assumed to be self-evident)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;然而，正是這樣顯而易見的道理，確是拿來建築理論的基石，千萬不能小看了他們。例如，我們看下面這個看似也應該成爲公理的公式，你能證明嗎：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(A_1\cup A_2) = P(A_1) + P(A_2) - P(A_1 \cap A_2)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/img/venngram.png&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;證明&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;證明：&lt;/h4&gt;
&lt;p&gt;先考慮 &lt;span class=&#34;math inline&#34;&gt;\(A_1 \cup A_2\)&lt;/span&gt; 是什麼（拆分成三個互斥事件）&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(A_1 \cup A_2 = (A_1\cap \bar{A_2})\cup(\bar{A_1}\cap A_2)\cup(A_1\cap A_2)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;運用上面的公理&lt;del&gt;2&lt;/del&gt; 3&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\therefore P(A_1 \cup A_2) = P(A_1\cap \bar{A_2}) + P(\bar{A_1}\cap A_2) + P(A_1\cap A_2) \;\;\;\;\;\;(1)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;再考慮 &lt;span class=&#34;math inline&#34;&gt;\(A_1=(A_1\cap A_2)\cup(A_1\cap\bar{A_2})\)&lt;/span&gt; 繼續拆分成兩個互斥事件&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\therefore P(A_1)=P(A_1\cap A_2)+P(A_1\cap\bar{A_2})\)&lt;/span&gt; 整理一下：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(A_1\cap\bar{A_2})=P(A_1)-P(A_1\cap A_2)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;同理可得: &lt;span class=&#34;math inline&#34;&gt;\(P(\bar{A_1}\cap A_2)=P(A_2)-P(A_1\cap A_2)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;代入上面第(1)式可得：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(A_1 \cup A_2) =P(A_1)-P(A_1\cap A_2)\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+P(A_2)-P(A_1\cap A_2)\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+P(A_1\cap A_2)\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;=P(A_1) + P(A_2) - P(A_1 \cap A_2)\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;條件概率-conditional-probability&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;條件概率 Conditional probability&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(A|S)=\frac{P(A\cap S)}{P(S)}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(A\cap S) = P(A|S)P(S)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;獨立-independence-的定義&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;獨立 (independence) 的定義&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;兩個事件定義爲互爲獨立時 (&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; are said to be independent &lt;strong&gt;if and only if&lt;/strong&gt;)
&lt;span class=&#34;math display&#34;&gt;\[P(A\cap B)=P(A)P(B)\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;因爲從條件概率的概念我們已知&lt;br&gt; &lt;span class=&#34;math inline&#34;&gt;\(P(A\cap B) = P(A|B)P(B)\)&lt;/span&gt; &lt;br&gt;所以&lt;span class=&#34;math inline&#34;&gt;\(P(A|B)=P(A)\)&lt;/span&gt; 即：事件 &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; 無法提供事件 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 的任何有效訊息 (&lt;strong&gt;&lt;span class=&#34;math inline&#34;&gt;\(A, B\)&lt;/span&gt; 互相獨立&lt;/strong&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;賭博問題&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;賭博問題&lt;/h2&gt;
&lt;p&gt;終於來到本次話題的重點了。我要扣題了哦。語文老師快在此加分。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/img/Selection_071.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;假設你在一個電視遊戲節目。有上圖一樣的三扇門。其中一扇門後面有一輛保時捷，另兩扇門後面則是&lt;a href=&#34;https://winterwang.github.io/post/black-meal/&#34;&gt;(味道奇特的)山羊&lt;/a&gt;。遊戲規則是主持人會讓你先選擇其中一扇門（先不打開你選的這扇門）。主持人隨後打開另外兩扇門中的一扇沒有保時捷的門。主持人問你，你要堅持選擇之前選中的那扇門，還是要改變主意換一扇門去猜是否可以猜中保時捷。
請問，堅持選擇之前選中的門猜中保時捷的概率高，還是主持人打開一扇門以後改變主意猜中保時捷的概率更高呢？&lt;/p&gt;
&lt;p&gt;答案明天揭曉。&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Matrix Revisions</title>
      <link>https://wangcc.me/post/matrix-revision/</link>
      <pubDate>Sat, 30 Sep 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/matrix-revision/</guid>
      <description>


&lt;div id=&#34;basic-definition-and-notations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Basic Definition and notations:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;An &lt;span class=&#34;math inline&#34;&gt;\(m\times n\)&lt;/span&gt; &lt;strong&gt;matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;&lt;/strong&gt; is a rectangular array of numbers with &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; rows and &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; columns.&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;elements&lt;/strong&gt; of a matrix &lt;span class=&#34;math inline&#34;&gt;\(A_{m\times n}\)&lt;/span&gt; are &lt;span class=&#34;math inline&#34;&gt;\(a_{ij}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;order&lt;/strong&gt; of a matrix is the number of rows by the number of columns, i.e. &lt;span class=&#34;math inline&#34;&gt;\(m\times n\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;A &lt;strong&gt;column vector&lt;/strong&gt; with &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; elements, &lt;span class=&#34;math inline&#34;&gt;\(y = \left( \begin{array}{c} y_1\\ y_2\\ \vdots\\ y_n \end{array} \right)\)&lt;/span&gt;, is a matrix with only one column i.e. an &lt;span class=&#34;math inline&#34;&gt;\(m\times 1\)&lt;/span&gt; matrix.&lt;/li&gt;
&lt;li&gt;A &lt;strong&gt;row vector&lt;/strong&gt; with &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; elements, &lt;span class=&#34;math inline&#34;&gt;\(x=(x_1,x_2,x_3, \cdots, x_n)\)&lt;/span&gt;, is a matrix with only one row, i.e. an &lt;span class=&#34;math inline&#34;&gt;\(1\times n\)&lt;/span&gt; matrix.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Transposed matrix&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(A^T\)&lt;/span&gt; (or &lt;span class=&#34;math inline&#34;&gt;\(A&amp;#39;\)&lt;/span&gt;) arises from the matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; by interchanging the column vectors and the row vectors i.e. &lt;span class=&#34;math inline&#34;&gt;\(a_{ij}^T = a_{ji}\)&lt;/span&gt; (so a column vector is converted into a row vector and vise versa)&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;A partitioned matrix&lt;/strong&gt; is a matrix written in terms of sub-matrices. &lt;span class=&#34;math inline&#34;&gt;\(A=\left( \begin{array}{c} A_{11} &amp;amp; A_{12}\\ A_{21} &amp;amp; A_{22}\\ \end{array} \right)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(A_{11},A_{12},A_{21},A_{22}\)&lt;/span&gt; are sub-matrices&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(A_{11}, A_{21}\)&lt;/span&gt; have the same number of columns, so do &lt;span class=&#34;math inline&#34;&gt;\(A_{12}, A_{22}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(A_{11}, A_{12}\)&lt;/span&gt; have the same number of rows, so do &lt;span class=&#34;math inline&#34;&gt;\(A_{21}, A_{22}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;partitioning is not restricted to dividing a matrix into just four sub-matrices&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;A square matrix&lt;/strong&gt; has exactly as many rows as it has columns i.e. the order of the matrix is &lt;span class=&#34;math inline&#34;&gt;\(n\times n\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The main diagonal&lt;/strong&gt; (or leading diagnonal) of a square matrix &lt;span class=&#34;math inline&#34;&gt;\(A (n\times n)\)&lt;/span&gt; are the elements lying on the diagnoal &lt;strong&gt;from top left to bottom right.&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(a_{11},a_{22},a_{33},\cdots,a_{nn}\)&lt;/span&gt; i.e. all &lt;span class=&#34;math inline&#34;&gt;\(a_{ii}, i= 1,\cdots, n\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;The trace &lt;/strong&gt; of a square matrix is the sum of the diagonal elements &lt;span class=&#34;math inline&#34;&gt;\(tr(A)=a_{11}+a_{22}+\cdots+a_{nn}=\sum_{i=1}^na_{ii}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;special-matrices&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Special matrices&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;A symmetric matrix&lt;/strong&gt; is a square matrix for which the following is true for all the off diagonal elements. &lt;span class=&#34;math inline&#34;&gt;\(a_{ij}=a_{ji}\)&lt;/span&gt; i.e. &lt;span class=&#34;math inline&#34;&gt;\(A^T=A\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Diagonal matrix&lt;/strong&gt; is a square matrix having zero for all the non-diagonal elements i.e. &lt;span class=&#34;math inline&#34;&gt;\(A=\left( \begin{array}{c} a_{11} &amp;amp; \cdots &amp;amp; 0\\ \vdots &amp;amp; \ddots &amp;amp; \vdots\\ 0 &amp;amp; \cdots &amp;amp; a_{nn} \end{array} \right)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Zero matrix&lt;/strong&gt; (null matrix) is a matrix whose all elements are zero&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Identity matrix&lt;/strong&gt; (or unit matrix) is a diagonal matrix having all diagonal elements equal to 1 and off diagonal elements equal to zero. i.e. &lt;span class=&#34;math inline&#34;&gt;\(I=\left( \begin{array}{c} 1 &amp;amp; \cdots &amp;amp; 0\\ \vdots &amp;amp; \ddots &amp;amp; \vdots\\ 0 &amp;amp; \cdots &amp;amp; 1 \end{array} \right)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;“Summing vector”&lt;/strong&gt; is a vector whose every element is 1 i.e. &lt;span class=&#34;math inline&#34;&gt;\(1_{n}=(1\cdots1)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;“J matrix”&lt;/strong&gt; is a matrix (not necessarily square) whose every element is 1 i.e. &lt;span class=&#34;math inline&#34;&gt;\(J_{m\times n}=\left( \begin{array}{c} 1 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 1\\ 1 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 1 \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ 1 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 1 \end{array} \right)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;basic-operations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Basic Operations&lt;/h2&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;Addition (Substraction)&lt;/strong&gt; can take place only when the matrices involved are of the same order.
i.e. Two matrices can be added (subtracted) only if they have the same numbers of rows and the same numbers of columns.
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(A+B=B+A\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((A+B)+C=A+(B+C)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(A+0=0+A=A\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(A+(-A)=0\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((A+B)^T=A^T+B^T\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Multiplication by scalar:&lt;/strong&gt;
- &lt;span class=&#34;math inline&#34;&gt;\(cA=Ac\)&lt;/span&gt;
- &lt;span class=&#34;math inline&#34;&gt;\(c(dA)=(cd)A\)&lt;/span&gt;
- &lt;span class=&#34;math inline&#34;&gt;\((c\pm d)A=cA\pm dA\)&lt;/span&gt;
- &lt;span class=&#34;math inline&#34;&gt;\(c(A\pm B)=cA \pm cB\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Multiplication of an &lt;span class=&#34;math inline&#34;&gt;\(2\times2\)&lt;/span&gt; matrix by a column vector which has 2 rows yields a column vector with &lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt; rows.&lt;/strong&gt;
&lt;span class=&#34;math display&#34;&gt;\[Ax=\left(
\begin{array}{c}
a_{11} &amp;amp; a_{12}\\
a_{21} &amp;amp; a_{22}\\
\end{array}
\right)\left(
\begin{array}{c}
x_{1}\\
x_{2}\\
\end{array}
\right)=\left(
\begin{array}{c}
a_{11}x_1+a_{12}x_2\\
a_{21}x_1+a_{22}x_2\\
\end{array}
\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;generally&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Generally:&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Multiplication of an &lt;span class=&#34;math inline&#34;&gt;\(m\times n\)&lt;/span&gt; matrix&lt;/strong&gt; by a &lt;strong&gt;column vector&lt;/strong&gt; which has &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; rows &lt;strong&gt;yields a column vector&lt;/strong&gt; with &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; rows.
&lt;span class=&#34;math display&#34;&gt;\[Ax=\left(
\begin{array}{c}
a_{11} &amp;amp; \cdots &amp;amp; a_{1n} \\
\vdots &amp;amp; \ddots &amp;amp; \vdots \\
a_{m1} &amp;amp; \cdots &amp;amp; a_{mn}
\end{array}
\right)\left(
\begin{array}{c}
x_{1}\\
x_{2}\\
\vdots \\
x_{n}
\end{array}
\right)=\left(
\begin{array}{c}
a_{11}x_{1}+a_{12}x_2+\cdots+a_{1n}x_n\\
\vdots \\
a_{m1}x_{1}+a_{m2}x_2+\cdots+a_{mn}x_n
\end{array}
\right)=y \\
i.e. y_i=\sum_{j=1}^na_{ij}x_j, \; i=1,\cdots, m\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://winterwang.github.io/post/2017-02-22/&#34;&gt;Multiplication of matrices&lt;/a&gt;:&lt;/strong&gt; The product &lt;span class=&#34;math inline&#34;&gt;\(AB=C\)&lt;/span&gt; is &lt;strong&gt;defined only when &lt;span class=&#34;math inline&#34;&gt;\(A_{m\times r}\)&lt;/span&gt; has exactly as many columns as &lt;span class=&#34;math inline&#34;&gt;\(B_{r\times n}\)&lt;/span&gt; has rows&lt;/strong&gt;. And the elements of &lt;span class=&#34;math inline&#34;&gt;\(C_{m\times n}\)&lt;/span&gt; are given as
&lt;span class=&#34;math display&#34;&gt;\[c_{ij}=\sum_{l=1}^na_{il}b_{lj}, \;\; i=1,\cdots,m \; and \; j=1,\cdots, n\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(AB \neq BA\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((AB)C=A(BC)=ABC\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(A(B+C)=AB+AC\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((B+C)A=BA+CA\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(IA=AI=A\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;further-definitions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Further definitions&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://winterwang.github.io/post/2017-03-15/&#34;&gt;&lt;strong&gt;The determinant&lt;/strong&gt;&lt;/a&gt; of a second order square matrix is &lt;span class=&#34;math inline&#34;&gt;\(det(A)=|A|=\begin{vmatrix} a_{11} &amp;amp; a_{12} \\ a_{21} &amp;amp; a_{22} \end{vmatrix}=a_{11}a_{22}-a_{12}a_{21}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://winterwang.github.io/post/2017-07-06/&#34;&gt;The inverse of a matrix&lt;/a&gt;&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(A^{-1}\)&lt;/span&gt; if it exists, is a matrix whose product with &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is the identity matrix i.e. &lt;span class=&#34;math inline&#34;&gt;\(AA^{-1}=A^{-1}A=I\)&lt;/span&gt;. (&lt;strong&gt;Note: both &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(A^{-1}\)&lt;/span&gt; have to be square&lt;/strong&gt;) For second order matrices:&lt;span class=&#34;math inline&#34;&gt;\(A^{-1}=\frac{1}{det(A)}\left( \begin{array}{c} a_{22} &amp;amp; -a_{12}\\ -a_{21} &amp;amp; a_{11}\\ \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://winterwang.github.io/post/2017-07-06/&#34;&gt;Singular or non-invertible matrix&lt;/a&gt;&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(det(A)=0\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Idempotent matrices(冪等矩陣)&lt;/strong&gt; are square and the following is true: &lt;span class=&#34;math inline&#34;&gt;\(AA=A^2=A\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://winterwang.github.io/post/2017-03-08/&#34;&gt;Orthogonal matrices&lt;/a&gt;&lt;/strong&gt; have the following property: &lt;span class=&#34;math inline&#34;&gt;\(AA^T=A^TA=I\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記 28</title>
      <link>https://wangcc.me/post/plus-equations-and-matrix-multiplication/</link>
      <pubDate>Wed, 16 Aug 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/plus-equations-and-matrix-multiplication/</guid>
      <description>


&lt;div id=&#34;行向量乘以矩陣乘以列向量可得標量&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;行向量乘以矩陣，乘以列向量可得標量&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underset{1\times m}{\underline{x}^t}\underset{m\times m}{M}\underset{m\times1}{\underline{x}}\)&lt;/span&gt; 或者 &lt;span class=&#34;math inline&#34;&gt;\(\underset{1\times m}{\underline{x}^t}\underset{m\times n}{N}\underset{n\times1}{\underline{y}}\)&lt;/span&gt; 的形式其實質上均爲 &lt;span class=&#34;math inline&#34;&gt;\(1\times1\)&lt;/span&gt;的標量，即最早我們接觸到的&lt;a href=&#34;https://winterwang.github.io/post/2017-02-06/&#34;&gt;加法算式&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;這樣的乘法計算通過&lt;a href=&#34;https://winterwang.github.io/post/2017-02-22/&#34;&gt;矩陣&lt;/a&gt;（包括&lt;a href=&#34;https://winterwang.github.io/post/2017-02-19/&#34;&gt;向量&lt;/a&gt;）的積的定義很容易進行。然而，反過來的話，（即從乘法算式反寫變形成爲行向量，矩陣，列向量相乘的形式），如果沒有練習的話，常常讓人覺得很困難。&lt;/p&gt;
&lt;p&gt;在這裏，我們將多元變量分析中常常遭遇的加法算式拿出來舉例，練習變形成爲矩陣的積的形式。當然，爲了簡便起見，我們用三個元素的向量來練習：
&lt;span class=&#34;math display&#34;&gt;\[\underline{x}=\left(
\begin{array}{c}
x_1 \\
x_2 \\
x_3
\end{array}\right), \underline{a}=\left(
\begin{array}{c}
a_1 \\
a_2 \\
a_3
\end{array}\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(a_1x_1^2+a_2x_2^2+a_3x_3^3\\ =x_1\times a_1x_1+x_2\times a_2x_2+x_3\times a_3x_3\\ =(x_1,x_2,x_3)\left( \begin{array}{c} a_1x_1 \\ a_2x_2 \\ a_3x_3 \end{array}\right)\\ =(x_1,x_2,x_3)\left( \begin{array}{c} a_1 &amp;amp; 0 &amp;amp; 0 \\ 0 &amp;amp; a_2 &amp;amp; 0 \\ 0 &amp;amp; 0 &amp;amp; a_3 \end{array}\right)\left( \begin{array}{c} x_1 \\ x_2 \\ x_3 \end{array}\right)\\ =\underline{x}^tD\underline{x}\)&lt;/span&gt; &lt;br&gt;其中
&lt;span class=&#34;math display&#34;&gt;\[D=\left(
\begin{array}{c}
a_1 &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; a_2 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; a_3
\end{array}\right)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\((a_1x_1+a_2x_2+a_3x_3)^2\\ =(a_1x_1+a_2x_2+a_3x_3)^t(a_1x_1+a_2x_2+a_3x_3)\\=\left\{(a_1, a_2, a_3)\left( \begin{array}{c} x_1 \\ x_2 \\ x_3 \end{array}\right)\right\}^t(a_1, a_2, a_3)\left( \begin{array}{c} x_1 \\ x_2 \\ x_3 \end{array}\right)\\ =(\underline{a}^t\underline{x})^t\underline{a}^t\underline{x}\\ =\underline{x}^t\underline{a}\underline{a}^t\underline{x}\\ \Rightarrow\\ =\underline{x}^t\left( \begin{array}{c} a_1 &amp;amp; 0 &amp;amp; 0 \\ a_2 &amp;amp; 0 &amp;amp; 0 \\ a_3 &amp;amp; 0 &amp;amp; 0 \end{array}\right)\left( \begin{array}{c} a_1 &amp;amp; a_2 &amp;amp; a_3 \\ 0 &amp;amp; 0 &amp;amp; 0 \\ 0 &amp;amp; 0 &amp;amp; 0 \end{array}\right)\underline{x}\\ =\underline{x}^tAA^t\underline{x}\\ =\underline{x}A_0\underline{x}\)&lt;/span&gt; &lt;br&gt;
其中
&lt;span class=&#34;math display&#34;&gt;\[A=\left(
\begin{array}{c}
a_1 &amp;amp; 0 &amp;amp; 0 \\
a_2 &amp;amp; 0 &amp;amp; 0 \\
a_3 &amp;amp; 0 &amp;amp; 0
\end{array}\right), A_0=\left(
\begin{array}{c}
a_1^2 &amp;amp; a_1a_2 &amp;amp; a_1a_3 \\
a_2a_1 &amp;amp; a_2^2 &amp;amp; a_2a_3 \\
a_3a_1 &amp;amp; a_3a_2 &amp;amp; a_3^2
\end{array}\right)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\((a_1x_1+a_2x_2+a_3x_3)^2+(b_1x_1+b_2x_2+b_3x_3)^2\\+(c_1x_1+c_2x_2+c_3x_3)^2\\ =\underline{x}^t\underline{a}\underline{a}^t\underline{x}+\underline{x}^t\underline{b}\underline{b}^t\underline{x}+\underline{x}^t\underline{c}\underline{c}^t\underline{x}\\ =\underline{x}^t(\underline{a}\underline{a}^t+\underline{b}\underline{b}^t+\underline{c}\underline{c}^t)\underline{x}\\ =\underline{x}^t(\underline{a} \underline{b} \underline{c})\left( \begin{array}{c} \underline{a}^t \\ \underline{b}^t \\ \underline{c}^t \end{array}\right)\underline{x}\\ =\underline{x}^t\left( \begin{array}{c} a_1 &amp;amp; b_1 &amp;amp; c_1 \\ a_2 &amp;amp; b_2 &amp;amp; c_2 \\ a_3 &amp;amp; b_3 &amp;amp; c_3 \end{array}\right)\left( \begin{array}{c} a_1 &amp;amp; a_1 &amp;amp; a_1 \\ b_2 &amp;amp; b_2 &amp;amp; b_2 \\ c_3 &amp;amp; c_3 &amp;amp; c_3 \end{array}\right)\underline{x}\\ =\underline{x}^tBB^t\underline{x}\)&lt;/span&gt; &lt;br&gt;
其中
&lt;span class=&#34;math display&#34;&gt;\[B=\left(
\begin{array}{c}
a_1 &amp;amp; b_1 &amp;amp; c_1 \\
a_2 &amp;amp; b_2 &amp;amp; c_2 \\
a_3 &amp;amp; b_3 &amp;amp; c_3
\end{array}\right)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{(a_1x_1+a_2x_2+a_3x_3)^2}{d_1}+ \frac{(b_1x_1+b_2x_2+b_3x_3)^2}{d_2}\\ +\frac{(c_1x_1+c_2x_2+c_3x_3)^2}{d_3}\)&lt;/span&gt;&lt;br&gt;
根據上面第1個練習
&lt;span class=&#34;math inline&#34;&gt;\(=(a_1x_1+a_2x_2+a_3x_3, b_1x_1+b_2x_2+b_3x_3, c_1x_1+c_2x_2+c_3x_3)\\ \left( \begin{array}{c} 1/d_1 &amp;amp; 0 &amp;amp; 0 \\ 0 &amp;amp; 1/d_2 &amp;amp; 0 \\ 0 &amp;amp; 0 &amp;amp; 1/d_3 \end{array}\right)\left( \begin{array}{c} a_1x_1+a_2x_2+a_3x_3 \\ b_1x_1+b_2x_2+b_3x_3 \\ c_1x_1+c_2x_2+c_3x_3 \end{array}\right)\\ =(x_1,x_2,x_3)\left( \begin{array}{c} a_1 &amp;amp; b_1 &amp;amp; c_1 \\ a_2 &amp;amp; b_2 &amp;amp; c_2 \\ a_3 &amp;amp; b_3 &amp;amp; c_3 \end{array}\right)\left( \begin{array}{c} 1/d_1 &amp;amp; 0 &amp;amp; 0 \\ 0 &amp;amp; 1/d_2 &amp;amp; 0 \\ 0 &amp;amp; 0 &amp;amp; 1/d_3 \end{array}\right)\\ \left( \begin{array}{c} a_1 &amp;amp; a_1 &amp;amp; a_1 \\ b_2 &amp;amp; b_2 &amp;amp; b_2 \\ c_3 &amp;amp; c_3 &amp;amp; c_3 \end{array}\right)\left( \begin{array}{c} x_1 \\ x_2 \\ x_3 \end{array}\right)\\ =\underline{x}^tBD^{-1}B^t\underline{x}\)&lt;/span&gt; &lt;br&gt;
其中
&lt;span class=&#34;math display&#34;&gt;\[D^{-1}=\left(
\begin{array}{c}
1/d_1 &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 1/d_2 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 1/d_3
\end{array}\right) \;(d_1d_2d_3\neq0), B 與上面3. 相同。\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記 27</title>
      <link>https://wangcc.me/post/homogeneouse-linear-equations/</link>
      <pubDate>Fri, 11 Aug 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/homogeneouse-linear-equations/</guid>
      <description>


&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X=\left(
\begin{array}{c}
x_{1} &amp;amp; x_{12} &amp;amp; \cdots &amp;amp; x_{1n}\\
x_{21} &amp;amp; x_{22} &amp;amp; \cdots &amp;amp; x_{2n}\\
\vdots &amp;amp; \vdots &amp;amp; \cdots &amp;amp; \vdots \\
x_{n1}  &amp;amp; x_{n2} &amp;amp; \cdots &amp;amp; x_{nn}
\end{array}
\right), \underline{a}=\left(
\begin{array}{c}
a_1 \\
a_2 \\
\vdots \\
a_n
\end{array}
\right), \underline{0}=\left(
\begin{array}{c}
\underline{0}\\
\underline{0}\\
\vdots\\
\underline{0}\\
\end{array}
\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;用上述來表達的同次連立一次方程式 (system of homogeneouse linear equations)：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X\underline{a}=\underline{0}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;即：
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
\left\{
\begin{array}{ll}
x_{11}a_1+x_{12}a_2+\cdots+x_{1n}a_n  = 0\\
x_{21}a_1+x_{22}a_2+\cdots+x_{2n}a_n  = 0\\
\cdots\\
x_{n1}a_1+x_{n2}a_2+\cdots+x_{nn}a_n  = 0\\
\end{array}
\right.
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;這樣的方程式與之前的不同，等號右邊全部都是 &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;.
如果係數矩陣的行列式 &lt;span class=&#34;math inline&#34;&gt;\(|X|\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[|X|=\begin{vmatrix}
x_{1} &amp;amp; x_{12} &amp;amp; \cdots &amp;amp; x_{1n}\\
x_{21} &amp;amp; x_{22} &amp;amp; \cdots &amp;amp; x_{2n}\\
\vdots &amp;amp; \vdots &amp;amp; \cdots &amp;amp; \vdots \\
x_{n1}  &amp;amp; x_{n2} &amp;amp; \cdots &amp;amp; x_{nn}
\end{vmatrix}\neq0\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;那麽這個連立方程式的解，我們通過&lt;a href=&#34;https://winterwang.github.io/post/cramers-formula/&#34;&gt;克萊姆法則&lt;/a&gt;公式知道有且僅有 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=\underline{0}\)&lt;/span&gt;， 即 &lt;span class=&#34;math inline&#34;&gt;\(a_1=a_2=\cdots=a_n=0\)&lt;/span&gt; (自明解, trivial solution)&lt;/p&gt;
&lt;p&gt;此外，如果此連立方程組有非自明解 (&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}\neq0\)&lt;/span&gt;) 那麽相應的， &lt;span class=&#34;math inline&#34;&gt;\(|X|=0\)&lt;/span&gt;。多元變量分析時我們多關心的是非自明解。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;diff_alert&#34;&gt;重點記住：&lt;br&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}\neq0\)&lt;/span&gt; 時， &lt;span class=&#34;math inline&#34;&gt;\(|X|=0\)&lt;/span&gt;； &lt;br&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=0\)&lt;/span&gt; 時, &lt;span class=&#34;math inline&#34;&gt;\(|X|\neq0\)&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;方程組 &lt;span class=&#34;math inline&#34;&gt;\(\begin{align} \left\{ \begin{array}{ll} a_1+2a_2 = 0\\ 2a_1+a_2 = 0\\ \end{array} \right. \end{align}\)&lt;/span&gt; 的係數矩陣 &lt;span class=&#34;math inline&#34;&gt;\(\left( \begin{array}{c} 1 &amp;amp; 2 \\ 2 &amp;amp; 1 \\ \end{array} \right)\)&lt;/span&gt; 的行列式 &lt;span class=&#34;math inline&#34;&gt;\(\begin{vmatrix} 1 &amp;amp; 2\\ 2 &amp;amp; 1\\ \end{vmatrix}=-3\neq0\)&lt;/span&gt; 所以此同次連立一次方程組的解有且僅有 &lt;span class=&#34;math inline&#34;&gt;\(a_1=a_2=0\)&lt;/span&gt; (自明解)。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;方程組 &lt;span class=&#34;math inline&#34;&gt;\(\begin{align} \left\{ \begin{array}{ll} -a_1+2a_2 = 0 \;\;\;\;\;\;\;(1)\\ 2a_1-4a_2 = 0 \;\;\;\;\;\;\;\;(2)\\ \end{array} \right. \end{align}\)&lt;/span&gt; &lt;br&gt;的係數行列式 &lt;span class=&#34;math inline&#34;&gt;\(\begin{vmatrix} -1 &amp;amp; 2\\ 2 &amp;amp; -4\\ \end{vmatrix}=0\)&lt;/span&gt;，因此除了自明解 &lt;span class=&#34;math inline&#34;&gt;\(a_1=a_2=0\)&lt;/span&gt; 之外，還有非自明解。由於 &lt;span class=&#34;math inline&#34;&gt;\((2)=-2\times(1)\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\((1),(2)\)&lt;/span&gt; 實質上是一個相同的方程式。 按照&lt;a href=&#34;https://winterwang.github.io/post/2017-02-12-t/&#34;&gt;前述方法&lt;/a&gt;，我們設定 &lt;span class=&#34;math inline&#34;&gt;\(a_2=s\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; 為非零任意實數)，則 &lt;span class=&#34;math inline&#34;&gt;\(a_1=2s\)&lt;/span&gt;。所以可以將這個方程組的非自明解寫作：
&lt;span class=&#34;math display&#34;&gt;\[\underline{a}=s\left(
\begin{array}{c}
2\\
1
\end{array}
\right), s 為非零任意實數。\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\star 自明解和非自明解同時表述時可以寫作: \underline{a}=s\left( \begin{array}{c} 2\\ 1 \end{array} \right),\\ s 為任意實數。\)&lt;/span&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;方程組 &lt;span class=&#34;math inline&#34;&gt;\(\begin{align} \left\{ \begin{array}{ll} (1-\lambda)a_1+2a_2 = 0 \;\;\;\;\;\;\;(1)\\ 4a_1+(3-\lambda)a_2 = 0 \;\;\;\;\;\;\;(2)\\ \end{array} \right. \end{align}\)&lt;/span&gt; 有非自明解的條件為：
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
\begin{vmatrix}
1-\lambda &amp;amp; 2\\
4 &amp;amp; 3-\lambda
\end{vmatrix} &amp;amp; = (1-\lambda)(3-\lambda)-4\times2 \\
&amp;amp; = 3-4\lambda+\lambda^2-8 \\
&amp;amp; = \lambda^2-4\lambda-5 \\
&amp;amp; = (\lambda+1)(\lambda-5) \\
&amp;amp; = 0
\end{align}\\
\therefore \lambda=-1, 5\]&lt;/span&gt;
&lt;br&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\lambda=-1\)&lt;/span&gt; 時，代入 &lt;span class=&#34;math inline&#34;&gt;\((1),(2)\)&lt;/span&gt; 得&lt;br&gt; &lt;span class=&#34;math inline&#34;&gt;\(\begin{align}  \left\{  \begin{array}{ll}  2a_1+2a_2 = 0 \;\;\;\;\;\;\;(1)\\  4a_1+4a_2 = 0 \;\;\;\;\;\;\;(2)\\  \end{array}  \right.  \end{align}\)&lt;/span&gt; 為實質上只有一個方程 &lt;span class=&#34;math inline&#34;&gt;\(a_1+a_2=0\)&lt;/span&gt; 的方程組。令 &lt;span class=&#34;math inline&#34;&gt;\(a_2=s\)&lt;/span&gt;， 則 &lt;span class=&#34;math inline&#34;&gt;\(a_1=-s\)&lt;/span&gt;。因此此時方程組的非自明解為 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=s\left(  \begin{array}{c}  1\\  2  \end{array}  \right)\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; 為 &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; 意外的任意實數)。&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\lambda=5\)&lt;/span&gt; 時，同理可得， &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=t\left(  \begin{array}{c}  1\\  2  \end{array}  \right)\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; 為 &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; 意外的任意實數)。&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;方程組 &lt;span class=&#34;math inline&#34;&gt;\((\zeta)\begin{align} \left\{ \begin{array}{r} (5-\lambda)a_1+2a_2-4a_3 = 0\\ -3a_1-\lambda a_2+4a_3 = 0\\ 6a_1+6a_2-(1+\lambda)a_3 = 0\\ \end{array} \right. \end{align}\)&lt;/span&gt; 擁有非自明解的條件為：
&lt;span class=&#34;math display&#34;&gt;\[\begin{vmatrix}
5-\lambda &amp;amp; 2        &amp;amp; -4 \\
-3        &amp;amp; -\lambda &amp;amp; 4  \\
6         &amp;amp; 6        &amp;amp; -(1+\lambda)
\end{vmatrix}=0\]&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;按照以前我們做過的&lt;a href=&#34;https://winterwang.github.io/post/2017-03-15/&#34;&gt;練習&lt;/a&gt;將行列式按照 &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; 展開：
&lt;span class=&#34;math display&#34;&gt;\[-\lambda^3+4\lambda^2-\lambda-6=\\
-(\lambda+1)(\lambda-2)(\lambda-3)=0\\
\therefore \lambda=-1,2,3\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\lambda = -1\)&lt;/span&gt; 時， &lt;span class=&#34;math inline&#34;&gt;\((\zeta)\)&lt;/span&gt; 變為：&lt;span class=&#34;math inline&#34;&gt;\(\begin{align}  \left\{  \begin{array}{r}  6a_1+2a_2-4a_3 = 0\\  -3a_1+ a_2+4a_3 = 0\\  6a_1+6a_2 = 0\\  \end{array}  \right.  \end{align}\)&lt;/span&gt;，此時的非自明解為 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=s\left(  \begin{array}{c}  1\\  -1\\  1  \end{array}  \right)\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; 為 &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; 以外的任意實數)&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\lambda = 2\)&lt;/span&gt; 時， &lt;span class=&#34;math inline&#34;&gt;\((\zeta)\)&lt;/span&gt; 變為：&lt;span class=&#34;math inline&#34;&gt;\(\begin{align}  \left\{  \begin{array}{r}  3a_1+2a_2-4a_3 = 0\\  -3a_1-2a_2+4a_3 = 0\\  6a_1+6a_2-3a_3 = 0\\  \end{array}  \right.  \end{align}\)&lt;/span&gt;，此時的非自明解為 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=t\left(  \begin{array}{c}  6\\  -5\\  2  \end{array}  \right)\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; 為 &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; 以外的任意實數)&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\lambda = 3\)&lt;/span&gt; 時， &lt;span class=&#34;math inline&#34;&gt;\((\zeta)\)&lt;/span&gt; 變為：&lt;span class=&#34;math inline&#34;&gt;\(\begin{align}  \left\{  \begin{array}{r}  2a_1+2a_2-4a_3 = 0\\  -3a_1-3a_2+4a_3 = 0\\  6a_1+6a_2-4a_3 = 0\\  \end{array}  \right.  \end{align}\)&lt;/span&gt;，此時的非自明解為 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=u\left(  \begin{array}{c}  1\\  -1\\  0  \end{array}  \right)\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; 為 &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; 以外的任意實數)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(3, 4\)&lt;/span&gt; 與後面的固有值問題關係深刻。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記 26</title>
      <link>https://wangcc.me/post/elementary-row-operations/</link>
      <pubDate>Thu, 10 Aug 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/elementary-row-operations/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;https://winterwang.github.io/post/linear-simultaneous-equation/&#34;&gt;擴大係數矩陣&lt;/a&gt; &lt;span class=&#34;math inline&#34;&gt;\((X \underline{y})\)&lt;/span&gt; 通過行的基本變形，轉化成爲 &lt;span class=&#34;math inline&#34;&gt;\((E \underline{y}^*)\)&lt;/span&gt; 的時候，寫在右側的 &lt;span class=&#34;math inline&#34;&gt;\(\underline{y}^*\)&lt;/span&gt; 就是所求的 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}\)&lt;/span&gt;。&lt;/p&gt;
&lt;hr /&gt;
&lt;div id=&#34;練習-解下列連立一次方程式&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;練習 解下列連立一次方程式&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
\left\{
\begin{array}{ll}
a_1+2a_2+a_3  = 2\\
2a_1+a_2+a_3  = 3\\
a_1+a_2+2a_3  = 3
\end{array}
\right.
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;解&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;解&lt;/h4&gt;
&lt;p&gt;此連立方程組的擴大係數矩陣爲：
&lt;span class=&#34;math display&#34;&gt;\[(X \underline{y})=\left(
\begin{array}{c}
1 &amp;amp; 2 &amp;amp; 1 &amp;amp; 2\\
2 &amp;amp; 1 &amp;amp; 1 &amp;amp; 3\\
1 &amp;amp; 1 &amp;amp; 2 &amp;amp; 3
\end{array}
\right)\]&lt;/span&gt;
下面開始&lt;a href=&#34;https://winterwang.github.io/post/2017-07-07/&#34;&gt;行變形&lt;/a&gt;：
&lt;span class=&#34;math display&#34;&gt;\[\left(\begin{array}{c}
1&amp;amp; 2&amp;amp; 1 &amp;amp; \vdots &amp;amp; 2\\
2&amp;amp; 1&amp;amp; 1 &amp;amp; \vdots &amp;amp; 3\\
1&amp;amp; 1&amp;amp; 2 &amp;amp; \vdots &amp;amp; 3\\
\end{array}\right) \begin{align}
\left\{
\begin{array}{rr}
(1)\\
(2)\\
(3)
\end{array}
\right.
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\left(\begin{array}{r}
1&amp;amp; 2&amp;amp; 1 &amp;amp; \vdots &amp;amp; 2\\
0&amp;amp; -3&amp;amp; -1 &amp;amp; \vdots &amp;amp; -1\\
0&amp;amp; -1&amp;amp; 1 &amp;amp; \vdots &amp;amp; -1\\
\end{array}\right) \begin{align}
\left\{
\begin{array}{l}
(1)\\
(2)=(2)-2\times(1)\\
(3)=(3)-(1)
\end{array}
\right.
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\left(\begin{array}{r}
1&amp;amp; 0&amp;amp; 3 &amp;amp; \vdots &amp;amp; 4\\
0&amp;amp; -4&amp;amp; 0 &amp;amp; \vdots &amp;amp; 0\\
0&amp;amp; 1&amp;amp; -1 &amp;amp; \vdots &amp;amp; -1\\
\end{array}\right) \begin{align}
\left\{
\begin{array}{l}
(1)=(1)+2\times(3)\\
(2)=(2)+(3)\\
(3)=-1\times(3)
\end{array}
\right.
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\left(\begin{array}{r}
1&amp;amp; 0&amp;amp; 3 &amp;amp; \vdots &amp;amp; 4\\
0&amp;amp; 1&amp;amp; 0 &amp;amp; \vdots &amp;amp; 0\\
0&amp;amp; 1&amp;amp; -1 &amp;amp; \vdots &amp;amp; -1\\
\end{array}\right) \begin{align}
\left\{
\begin{array}{l}
(1)\\
(2)=(2)\div(-4)\\
(3)
\end{array}
\right.
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\left(\begin{array}{r}
1&amp;amp; 0&amp;amp; 3 &amp;amp; \vdots &amp;amp; 4\\
0&amp;amp; 1&amp;amp; 0 &amp;amp; \vdots &amp;amp; 0\\
0&amp;amp; 0&amp;amp; -1 &amp;amp; \vdots &amp;amp; -1
\end{array}\right) \begin{align}
\left\{
\begin{array}{l}
(1)\\
(2)\\
(3)=(3)-(2)
\end{array}
\right.
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\left(\begin{array}{r}
1&amp;amp; 0&amp;amp; 0 &amp;amp; \vdots &amp;amp; 1\\
0&amp;amp; 1&amp;amp; 0 &amp;amp; \vdots &amp;amp; 0\\
0&amp;amp; 0&amp;amp; 1 &amp;amp; \vdots &amp;amp; 1
\end{array}\right) \begin{align}
\left\{
\begin{array}{l}
(1)=(1)+3\times(3)\\
(2)\\
(3)
\end{array}
\right.
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;至此，點線的左側矩陣變成了單位矩陣以後轉換結束。所求的 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}\)&lt;/span&gt; 便是點線右側的向量。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\therefore \underline{a}=\left(
\begin{array}{c}
a_1 \\
a_2 \\
a_3 \\
\end{array}
\right)=\left(
\begin{array}{c}
1 \\
0 \\
1 \\
\end{array}
\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記 25</title>
      <link>https://wangcc.me/post/cramers-formula/</link>
      <pubDate>Wed, 09 Aug 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/cramers-formula/</guid>
      <description>


&lt;div id=&#34;克萊姆法則-cramers-formula&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;克萊姆法則 Cramer’s Formula&lt;/h3&gt;
&lt;p&gt;當 &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 爲&lt;a href=&#34;https://winterwang.github.io/post/2017-07-06/&#34;&gt;正則矩陣&lt;/a&gt;（&lt;span class=&#34;math inline&#34;&gt;\(|X|\neq0\)&lt;/span&gt;）時 連立一次方程式：&lt;span class=&#34;math inline&#34;&gt;\(X\underline{a}=\underline{y}\)&lt;/span&gt; 的解可以寫作：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[a_j=\frac{|X_j|}{|X|} (j=1,2,\cdots, n)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中： &lt;span class=&#34;math inline&#34;&gt;\(|X_j|\)&lt;/span&gt; 爲矩陣 &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 的第 &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; 列替換爲 &lt;span class=&#34;math inline&#34;&gt;\(\underline{y}\)&lt;/span&gt; 以後的矩陣的行列式。&lt;/p&gt;
&lt;hr /&gt;
&lt;div id=&#34;練習-解下列連立一次方程式&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;練習 解下列連立一次方程式&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
\left\{
\begin{array}{ll}
a_1+2a_2+a_3  = 2\\
2a_1+a_2+a_3  = 3\\
a_1+a_2+2a_3  = 3
\end{array}
\right.
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;解&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;解&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X=\left(
\begin{array}{c}
1 &amp;amp; 2 &amp;amp; 1 \\
2 &amp;amp; 1 &amp;amp; 1 \\
1 &amp;amp; 1 &amp;amp; 2
\end{array}
\right), \underline{a}=\left(
\begin{array}{c}
a_1 \\
a_2 \\
a_3 \\
\end{array}
\right), \underline{y}=\left(
\begin{array}{c}
2 \\
3 \\
3 \\
\end{array}
\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中 &lt;span class=&#34;math inline&#34;&gt;\(|X|=-4\)&lt;/span&gt; &lt;a href=&#34;https://winterwang.github.io/post/2017-03-15/&#34;&gt;(三次行列式的計算)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 的第一列置換成 &lt;span class=&#34;math inline&#34;&gt;\(\underline{y}\)&lt;/span&gt; 則:
&lt;span class=&#34;math display&#34;&gt;\[|X_1|=\begin{vmatrix}
2 &amp;amp; 2 &amp;amp;  1\\
3 &amp;amp; 1 &amp;amp;  1\\
3 &amp;amp; 1 &amp;amp;  2\\
\end{vmatrix}=-4\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 的第二列置換成 &lt;span class=&#34;math inline&#34;&gt;\(\underline{y}\)&lt;/span&gt; 則:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[|X_2|=\begin{vmatrix}
1 &amp;amp; 2 &amp;amp;  1\\
2 &amp;amp; 3 &amp;amp;  1\\
1 &amp;amp; 3 &amp;amp;  2\\
\end{vmatrix}=0\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 的第三列置換成 &lt;span class=&#34;math inline&#34;&gt;\(\underline{y}\)&lt;/span&gt; 則:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[|X_3|=\begin{vmatrix}
1 &amp;amp; 2 &amp;amp;  2\\
2 &amp;amp; 1 &amp;amp;  3\\
1 &amp;amp; 1 &amp;amp;  3\\
\end{vmatrix}=-4\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\therefore a_1=\frac{|X_1|}{|X|}=1, \\
a_2=\frac{|X_2|}{|X|}=0, \\
a_3=\frac{|X_3|}{|X|}=1\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記 24</title>
      <link>https://wangcc.me/post/inverse-matrix-method/</link>
      <pubDate>Sun, 06 Aug 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/inverse-matrix-method/</guid>
      <description>


&lt;div id=&#34;逆矩陣法解連立一次方程式&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;逆矩陣法解連立一次方程式&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 為&lt;a href=&#34;https://winterwang.github.io/post/2017-07-06/&#34;&gt;正則矩陣&lt;/a&gt;時(&lt;span class=&#34;math inline&#34;&gt;\(|X|\neq0\)&lt;/span&gt;)，給 &lt;span class=&#34;math inline&#34;&gt;\(X\underline{a}=\underline{y}\)&lt;/span&gt; 等式兩邊同時乘以 &lt;span class=&#34;math inline&#34;&gt;\(X^{-1}\)&lt;/span&gt;，可以得到 &lt;span class=&#34;math inline&#34;&gt;\(X^{-1}X\underline{a}=X^{-1}\underline{y}\rightarrow E\underline{a}=X^{-1}\underline{y}\)&lt;/span&gt;。由此方法可以得到 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=X^{-1}\underline{y}\)&lt;/span&gt;。&lt;/p&gt;
&lt;hr /&gt;
&lt;div id=&#34;練習-解下列連立一次方程式&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;練習 解下列連立一次方程式&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
\left\{
\begin{array}{ll}
a_1+2a_2+a_3  = 2\\
2a_1+a_2+a_3  = 3\\
a_1+a_2+2a_3  = 3
\end{array}
\right.
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;解&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;解&lt;/h4&gt;
&lt;p&gt;元連立方程式可以寫作&lt;span class=&#34;math inline&#34;&gt;\(X\underline{a}=\underline{y}\)&lt;/span&gt;，其中
&lt;span class=&#34;math display&#34;&gt;\[X=\left(
\begin{array}{c}
1 &amp;amp; 2 &amp;amp; 1 \\
2 &amp;amp; 1 &amp;amp; 1 \\
1 &amp;amp; 1 &amp;amp; 2
\end{array}
\right), \underline{a}=\left(
\begin{array}{c}
a_1 \\
a_2 \\
a_3 \\
\end{array}
\right), \underline{y}=\left(
\begin{array}{c}
2 \\
3 \\
3 \\
\end{array}
\right)\]&lt;/span&gt;
之前我們已經用&lt;a href=&#34;https://winterwang.github.io/post/2017-07-07/&#34;&gt;行的基本變形法&lt;/a&gt;和&lt;a href=&#34;https://winterwang.github.io/post/inverse-matrix/&#34;&gt;逆矩陣法&lt;/a&gt;分別計算過了 &lt;span class=&#34;math inline&#34;&gt;\(X^{-1}\)&lt;/span&gt; ：
&lt;span class=&#34;math display&#34;&gt;\[X^{-1}=\left(\begin{array}{c}
-1/4 &amp;amp; 3/4 &amp;amp; -1/4\\
3/4 &amp;amp; -1/4 &amp;amp; -1/4\\
-1/4 &amp;amp; -1/4 &amp;amp; -3/4\\
\end{array}\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\therefore\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
\underline{a} &amp;amp; =X^{-1}\underline{y} \\
&amp;amp; =\left(\begin{array}{c}
-1/4 &amp;amp; 3/4 &amp;amp; -1/4\\
3/4 &amp;amp; -1/4 &amp;amp; -1/4\\
-1/4 &amp;amp; -1/4 &amp;amp; 3/4\\
\end{array}\right)\left(
\begin{array}{c}
2 \\
3 \\
3 \\
\end{array}
\right)\\
&amp;amp;=\left(
\begin{array}{c}
-1/4\times2+3/4\times3-1/4\times3 \\
3/4\times1+(-1/4)\times3-1/4\times3 \\
-1/4\times2-1/4\times3+3/4\times3 \\
\end{array}
\right) \\
&amp;amp; = \left(
\begin{array}{c}
1 \\
0 \\
1 \\
\end{array}
\right)
\end{align} \]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記 23</title>
      <link>https://wangcc.me/post/linear-simultaneous-equation/</link>
      <pubDate>Sat, 05 Aug 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/linear-simultaneous-equation/</guid>
      <description>


&lt;p&gt;連立一次方程式：
&lt;span class=&#34;math inline&#34;&gt;\(\begin{align} \left\{ \begin{array}{ll} x_{11}a_1+x_{12}a_2+\cdots+x_{1n}a_n = y_1\\ x_{21}a_1+x_{22}a_2+\cdots+x_{2n}a_n = y_2\\ \cdots \\ x_{n1}a_1+x_{n2}a_2+\cdots+x_{nn}a_n = y_n \end{array} \right. \end{align}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;可以看成是利用：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(X=\left( \begin{array}{c} x_{11} &amp;amp; x_{12} &amp;amp; \cdots &amp;amp; x_{1n} \\ x_{21} &amp;amp; x_{22} &amp;amp; \cdots &amp;amp; x_{2n} \\ \vdots &amp;amp; \vdots &amp;amp; \cdots &amp;amp; \vdots \\ x_{n1} &amp;amp; x_{n2} &amp;amp; \cdots &amp;amp; x_{nn} \end{array} \right), \underline{a}=\left( \begin{array}{c} a_1 \\ a_2 \\ \vdots \\ a_n \\ \end{array} \right), \underline{y}=\left( \begin{array}{c} y_1 \\ y_2 \\ \vdots \\ y_n \\ \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;表達為： &lt;span class=&#34;math inline&#34;&gt;\(X\underline{a}=\underline{y}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中 &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 為&lt;a href=&#34;https://winterwang.github.io/post/2017-02-28/&#34;&gt;係數矩陣 (coefficient matrix)&lt;/a&gt;。如果將 &lt;span class=&#34;math inline&#34;&gt;\(\underline{y}\)&lt;/span&gt; 加入 &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 的最後一列，寫成下面的形式:
&lt;span class=&#34;math display&#34;&gt;\[(X \underline{y})=\left(
\begin{array}{c}
x_{11} &amp;amp; x_{12} &amp;amp; \cdots &amp;amp; x_{1n}  &amp;amp; y_1\\
x_{21} &amp;amp; x_{22} &amp;amp; \cdots &amp;amp; x_{2n}  &amp;amp; y_2\\
\vdots &amp;amp; \vdots &amp;amp; \cdots &amp;amp; \vdots  &amp;amp; \vdots\\
x_{n1} &amp;amp; x_{n2} &amp;amp; \cdots &amp;amp; x_{nn} &amp;amp; y_n
\end{array}
\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;那麽這樣的矩陣 &lt;span class=&#34;math inline&#34;&gt;\((X \underline{y})\)&lt;/span&gt; 被稱作是擴大&lt;a href=&#34;https://winterwang.github.io/post/2017-02-28/&#34;&gt;係數矩陣 (augmented coefficient matrix)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;在多元變量分析時，通常出現的情況是， &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 和 &lt;span class=&#34;math inline&#34;&gt;\(\underline{y}\)&lt;/span&gt; 為已知 (觀測收集得來的數據)，&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}\)&lt;/span&gt; 為未知。&lt;/p&gt;
&lt;p&gt;一般情況下， &lt;span class=&#34;math inline&#34;&gt;\(X\underline{a}=\underline{y}\)&lt;/span&gt; 的解答方法就是(1)&lt;a href=&#34;https://winterwang.github.io/post/inverse-matrix/&#34;&gt;使用逆矩陣&lt;/a&gt;，(2)克萊姆法則法，(3)&lt;a href=&#34;https://winterwang.github.io/post/2017-07-07/&#34;&gt;行的基本變形法&lt;/a&gt; 三種手法。但是，前兩種手法只適用於 &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 是正方形矩陣且同時是&lt;a href=&#34;https://winterwang.github.io/post/2017-07-06/&#34;&gt;正則矩陣&lt;/a&gt;。另外， &lt;span class=&#34;math inline&#34;&gt;\((X \underline{y})\)&lt;/span&gt; 中間加上逗號的寫法 &lt;span class=&#34;math inline&#34;&gt;\((X,\underline{y})\)&lt;/span&gt; 也表達相同含義。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記22</title>
      <link>https://wangcc.me/post/inverse-matrix/</link>
      <pubDate>Thu, 03 Aug 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/inverse-matrix/</guid>
      <description>&lt;p&gt;正方形矩陣 $A$ 的行列式滿足 $|A| \neq 0$ 時，逆矩陣可以表達爲(當 $|A|=0$ 時，正方形矩陣 $A$ 沒有逆矩陣)：
$$A^{-1}=\frac{1}{|A|}adj(A)=\frac{1}{|A|}(A_{ij})^t$$&lt;/p&gt;

&lt;p&gt;$$=\frac{1}{|A|}\lbrace(-1)^{i+j}D_{ij}\rbrace^t$$&lt;/p&gt;

&lt;p&gt;其中:&lt;br&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$adj(A)$ 爲&lt;a href=&#34;https://winterwang.github.io/post/2017-03-15/&#34; target=&#34;_blank&#34;&gt;餘因子矩陣&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;$A_{ij}$ 爲&lt;a href=&#34;https://winterwang.github.io/post/2017-03-15/&#34; target=&#34;_blank&#34;&gt;餘因子&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;$D_{ij}$ &lt;a href=&#34;https://winterwang.github.io/post/2017-03-15/&#34; target=&#34;_blank&#34;&gt;爲小行列式&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;(1) 之前舉過的例子再拿來試試看：&lt;/p&gt;

&lt;p&gt;$$X=\left(
\begin{array}{c}
1 &amp;amp; 2 &amp;amp; 1 \newline
2 &amp;amp; 1 &amp;amp; 1 \newline
1 &amp;amp; 1 &amp;amp; 2
\end{array}
\right)=\left(\begin{array}{c}
x_{11} &amp;amp; x_{12} &amp;amp; x_{13} \newline
x_{21} &amp;amp; x_{22} &amp;amp; x_{23} \newline
x_{31} &amp;amp; x_{32} &amp;amp; x_{33}
\end{array}\right)$$
&lt;br&gt;&lt;/p&gt;

&lt;p&gt;元素 $x_{ij}$ 的餘因子 $X_{ij}(i,j=1,2,3)$ 爲：&lt;/p&gt;

&lt;p&gt;$$X_{11}=(-1)^{1+1}\left|
\begin{array}{c}
1 &amp;amp; 1 \newline
1 &amp;amp; 2
\end{array}\right|=1$$&lt;/p&gt;

&lt;p&gt;$$X_{12}=(-1)^{1+2}\left|
\begin{array}{c}
2 &amp;amp; 1 \newline
1 &amp;amp; 2
\end{array}\right|=-3$$&lt;/p&gt;

&lt;p&gt;$$X_{13}=(-1)^{1+3}\left|
\begin{array}{c}
2 &amp;amp; 1 \newline
1 &amp;amp; 1
\end{array}\right|=1$$&lt;/p&gt;

&lt;p&gt;$$X_{21}=(-1)^{2+1}\left|
\begin{array}{c}
2 &amp;amp; 1 \newline
1 &amp;amp; 2
\end{array}\right|=-3$$&lt;/p&gt;

&lt;p&gt;$$X_{22}=(-1)^{2+2}\left|
\begin{array}{c}
1 &amp;amp; 1 \newline
1 &amp;amp; 2
\end{array}\right|=1$$&lt;/p&gt;

&lt;p&gt;$$X_{23}=(-1)^{2+3}\left|
\begin{array}{c}
1 &amp;amp; 2 \newline
1 &amp;amp; 1
\end{array}\right|=1$$&lt;/p&gt;

&lt;p&gt;$$X_{31}=(-1)^{3+1}\left|
\begin{array}{c}
2 &amp;amp; 1 \newline
1 &amp;amp; 1
\end{array}\right|=1$$&lt;/p&gt;

&lt;p&gt;$$X_{32}=(-1)^{3+2}\left|
\begin{array}{c}
1 &amp;amp; 1 \newline
2 &amp;amp; 1
\end{array}\right|=1$$&lt;/p&gt;

&lt;p&gt;$$X_{33}=(-1)^{3+3}\left|
\begin{array}{c}
1 &amp;amp; 2 \newline
2 &amp;amp; 1
\end{array}\right|=-3$$&lt;/p&gt;

&lt;p&gt;因此餘因子矩陣爲：$adj(X)=\left(
\begin{array}{c}
1 &amp;amp; -3 &amp;amp; 1 \newline
-3 &amp;amp; 1 &amp;amp; 1 \newline
1 &amp;amp; 1 &amp;amp; -3
\end{array}
\right)^t=\left(
\begin{array}{c}
1 &amp;amp; -3 &amp;amp; 1 \newline
-3 &amp;amp; 1 &amp;amp; 1 \newline
1 &amp;amp; 1 &amp;amp; -3
\end{array}
\right)$&lt;/p&gt;

&lt;p&gt;我們看見這個餘因子矩陣是一個對稱矩陣，這是由於原矩陣 $X$ 本身就是一個對稱矩陣。另外，行列式爲：&lt;/p&gt;

&lt;p&gt;$$\begin{align}|X|&amp;amp;=1\times X_{11}+2\times X_{12}+1\times X_{13}\newline&amp;amp;=1\times1+2\times(-3)+1\times1\newline&amp;amp;=-4\end{align}$$&lt;/p&gt;

&lt;p&gt;因此所求的逆矩陣爲：&lt;/p&gt;

&lt;p&gt;$$\begin{align}X^{-1}&amp;amp;=\frac{1}{|X|}adj(X)\newline
&amp;amp;=\frac{1}{-4}\left(
\begin{array}{c}
1 &amp;amp; -3 &amp;amp; 1 \newline
-3 &amp;amp; 1 &amp;amp; 1 \newline
1 &amp;amp; 1 &amp;amp; -3
\end{array}
\right)\newline
&amp;amp;=\left(
\begin{array}{c}
-\frac{1}{4} &amp;amp; \frac{3}{4} &amp;amp; -\frac{1}{4} \newline
\frac{3}{4} &amp;amp; -\frac{1}{4} &amp;amp; -\frac{1}{4} \newline
-\frac{1}{4} &amp;amp; -\frac{1}{4} &amp;amp; \frac{3}{4}
\end{array}
\right)\end{align}$$&lt;/p&gt;

&lt;p&gt;(2) 試求矩陣 $A=\left(
\begin{array}{c}
1 &amp;amp; 2 &amp;amp; 1 \newline
2 &amp;amp; 3 &amp;amp; 1 \newline
1 &amp;amp; 2 &amp;amp; 2
\end{array}
\right)=\left(
\begin{array}{c}
a_{11} &amp;amp; a_{12}  &amp;amp; a_{13} \newline
a_{21} &amp;amp; a_{22}  &amp;amp; a_{23} \newline
a_{31} &amp;amp; a_{32}  &amp;amp; a_{33}
\end{array}
\right)$ 的逆矩陣 $A^{-1}$:&lt;/p&gt;

&lt;p&gt;$$\begin{array}
=A_{11}=6-2=4, &amp;amp; A_{12}=-(4-1)=-3, &amp;amp; A_{13}=4-3=1 \newline
A_{21}=-(4-2)=-2, &amp;amp; A_{22}=2-1=1, &amp;amp; A_{23}=-(2-2)=0 \newline
A_{31}=2-3=-1, &amp;amp; A_{32}=-(1-2)=1, &amp;amp; A_{33}=3-4=-1
\end{array}$$&lt;/p&gt;

&lt;p&gt;$$adj(A)=\left(
\begin{array}{c}
4 &amp;amp; -3 &amp;amp; 1 \newline
-2 &amp;amp; 1 &amp;amp; 0 \newline
-1 &amp;amp; 1 &amp;amp; -1
\end{array}
\right)^t=\left(
\begin{array}{c}
4 &amp;amp; -2 &amp;amp; -1 \newline
-3 &amp;amp; 1 &amp;amp; 1 \newline
1 &amp;amp; 0 &amp;amp; -1
\end{array}
\right)$$&lt;/p&gt;

&lt;p&gt;$$\begin{align}
|A| &amp;amp;=1\times A_{11}+2\times A_{12}+1\times A_{13} \newline
    &amp;amp;=1\times4+2\times(-3)+1\times1 \newline
    &amp;amp;=4-6+1 \newline
    &amp;amp;=-1
\end{align}$$&lt;/p&gt;

&lt;p&gt;$$
\therefore
\begin{align}
A^{-1} &amp;amp;= \frac{1}{(-1)}\left(
\begin{array}{c}
4 &amp;amp; -2 &amp;amp; -1 \newline
-3 &amp;amp; 1 &amp;amp; 1 \newline
1 &amp;amp; 0 &amp;amp; -1
\end{array}
\right) \newline
&amp;amp;=\left(
\begin{array}{c}
-4 &amp;amp; 2 &amp;amp; 1 \newline
3 &amp;amp; -1 &amp;amp; -1 \newline
-1 &amp;amp; 0 &amp;amp; 1
\end{array}
\right)
\end{align}$$&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記21</title>
      <link>https://wangcc.me/post/2017-07-07/</link>
      <pubDate>Fri, 07 Jul 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/2017-07-07/</guid>
      <description>


&lt;div id=&#34;行的基本變形&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;行的基本變形&lt;/h2&gt;

&lt;div class=&#34;theorem&#34;&gt;
&lt;p&gt;&lt;span id=&#34;thm:line&#34; class=&#34;theorem&#34;&gt;&lt;strong&gt;Theorem 1  (行的基本變形)  &lt;/strong&gt;&lt;/span&gt;對矩陣進行下列操作的過程，被稱爲是行的基本變形（行的基本操作, elementary row operations）。&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;給任意一行乘以/除以一個非零的數。&lt;/li&gt;
&lt;li&gt;給任意一行加上/減去另外任意行的倍數。&lt;/li&gt;
&lt;li&gt;將任意兩行的對應元素互換。
&lt;/div&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;練習基本變形&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;練習基本變形：&lt;/h3&gt;
&lt;hr /&gt;
&lt;p&gt;用行的基本變形求矩陣 &lt;span class=&#34;math inline&#34;&gt;\(X=\left(\begin{array}{c} 1&amp;amp; 2&amp;amp; 1\\ 2&amp;amp; 1&amp;amp; 1\\ 1&amp;amp; 1&amp;amp; 2\\ \end{array}\right)\)&lt;/span&gt; 的逆矩陣 &lt;span class=&#34;math inline&#34;&gt;\(X^{-1}\)&lt;/span&gt; &lt;br&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;首先，將矩陣 &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 和同次單位矩陣 &lt;span class=&#34;math inline&#34;&gt;\(E_3\)&lt;/span&gt; 的元素寫成如下的左右並列的形式（用點隔開）&lt;span class=&#34;math inline&#34;&gt;\((X, E)\)&lt;/span&gt;。數字 (1) (2) (3) 表示行數：&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\left(\begin{array}{c}
1&amp;amp; 2&amp;amp; 1 &amp;amp; \vdots &amp;amp; 1 &amp;amp; 0 &amp;amp; 0\\
2&amp;amp; 1&amp;amp; 1 &amp;amp; \vdots &amp;amp; 0 &amp;amp; 1 &amp;amp; 0\\
1&amp;amp; 1&amp;amp; 2 &amp;amp; \vdots &amp;amp; 0 &amp;amp; 0 &amp;amp; 1\\
\end{array}\right) \begin{align}
\left\{
\begin{array}{rr}
(1)\\
(2)\\
(3)
\end{array}
\right.
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;可以變形成爲下面的形式：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\left(\begin{array}{c}
1&amp;amp; 2&amp;amp; 1 &amp;amp; \vdots &amp;amp; 1 &amp;amp; 0 &amp;amp; 0\\
0&amp;amp; -3&amp;amp; -1 &amp;amp; \vdots &amp;amp; -2 &amp;amp; 1 &amp;amp; 0\\
0&amp;amp; -1&amp;amp; 1 &amp;amp; \vdots &amp;amp; -1 &amp;amp; 0 &amp;amp; 1\\
\end{array}\right) \begin{align}
\left\{
\begin{array}{l}
(1)\\
(2)=(2)-2\times(1)\\
(3)=(3)-(1)
\end{array}
\right.
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;繼續變形成如下的形式：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\left(\begin{array}{c}
1&amp;amp; 0&amp;amp; 3 &amp;amp; \vdots &amp;amp; -1 &amp;amp; 0 &amp;amp; 2\\
0&amp;amp; -4&amp;amp; 0 &amp;amp; \vdots &amp;amp; -3 &amp;amp; 1 &amp;amp; 1\\
0&amp;amp; 1&amp;amp; -1 &amp;amp; \vdots &amp;amp; 1 &amp;amp; 0 &amp;amp; -1\\
\end{array}\right) \begin{align}
\left\{
\begin{array}{l}
(1)=(1)+2\times(3)\\
(2)=(2)+(3)\\
(3)=-1\times(3)
\end{array}
\right.
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Next:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\left(\begin{array}{c}
1&amp;amp; 0&amp;amp; 3 &amp;amp; \vdots &amp;amp; -1 &amp;amp; 0 &amp;amp; 2\\
0&amp;amp; 1&amp;amp; 0 &amp;amp; \vdots &amp;amp; 3/4 &amp;amp; -1/4 &amp;amp; -1/4\\
0&amp;amp; 1&amp;amp; -1 &amp;amp; \vdots &amp;amp; 1 &amp;amp; 0 &amp;amp; -1\\
\end{array}\right) \begin{align}
\left\{
\begin{array}{l}
(1)=(1)\\
(2)=(2)\div(-4)\\
(3)=(3)
\end{array}
\right.
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Next:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\left(\begin{array}{c}
1&amp;amp; 0&amp;amp; 3 &amp;amp; \vdots &amp;amp; -1 &amp;amp; 0 &amp;amp; 2\\
0&amp;amp; 1&amp;amp; 0 &amp;amp; \vdots &amp;amp; 3/4 &amp;amp; -1/4 &amp;amp; -1/4\\
0&amp;amp; 0&amp;amp; -1 &amp;amp; \vdots &amp;amp; 1/4 &amp;amp; 1/4 &amp;amp; -3/4\\
\end{array}\right) \begin{align}
\left\{
\begin{array}{l}
(1)=(1)\\
(2)=(2)\\
(3)=(3)-(2)
\end{array}
\right.
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Next:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\left(\begin{array}{c}
1&amp;amp; 0&amp;amp; 0 &amp;amp; \vdots &amp;amp; -1/4 &amp;amp; 3/4 &amp;amp; -1/4\\
0&amp;amp; 1&amp;amp; 0 &amp;amp; \vdots &amp;amp; 3/4 &amp;amp; -1/4 &amp;amp; -1/4\\
0&amp;amp; 0&amp;amp; 1 &amp;amp; \vdots &amp;amp; -1/4 &amp;amp; -1/4 &amp;amp; -3/4\\
\end{array}\right) \begin{align}
\left\{
\begin{array}{l}
(1)=(1)+3\times(3)\\
(2)=(2)\\
(3)=-1\times(3)
\end{array}
\right.
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;點 “&lt;span class=&#34;math inline&#34;&gt;\(\vdots\)&lt;/span&gt;” 的左側變形成爲單位矩陣時，行變形結束。右側便是所求的逆矩陣 &lt;span class=&#34;math inline&#34;&gt;\(X^{-1}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X^{-1}=\left(\begin{array}{c}
-1/4 &amp;amp; 3/4 &amp;amp; -1/4\\
3/4 &amp;amp; -1/4 &amp;amp; -1/4\\
-1/4 &amp;amp; -1/4 &amp;amp; 3/4\\
\end{array}\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;q-如果有行的基本變形請問有沒有列的基本變形-elementary-column-operations&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Q: 如果有行的基本變形，請問有沒有列的基本變形 (elementary column operations)？&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;a-有把行的基本變形中的定義-refthmline-的行改成列既是列的基本變形的定義&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A: 有。把行的基本變形中的定義 (&lt;a href=&#34;#thm:line&#34;&gt;1&lt;/a&gt;) 的行改成列，既是列的基本變形的定義。&lt;/h3&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記20</title>
      <link>https://wangcc.me/post/2017-07-06/</link>
      <pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/2017-07-06/</guid>
      <description>


&lt;div id=&#34;逆矩陣&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;逆矩陣&lt;/h2&gt;
&lt;div id=&#34;逆矩陣定義&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;逆矩陣定義&lt;/h3&gt;

&lt;div class=&#34;theorem&#34;&gt;
&lt;span id=&#34;thm:unnamed-chunk-1&#34; class=&#34;theorem&#34;&gt;&lt;strong&gt;Theorem 1  &lt;/strong&gt;&lt;/span&gt;如果對於正方形矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;，存在一個&lt;strong&gt;正方形矩陣&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 滿足 &lt;span class=&#34;math inline&#34;&gt;\(AX=XA=E\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(E\)&lt;/span&gt; 爲單位矩陣) 時，這個正方形矩陣 &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 被叫做 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 的&lt;strong&gt;逆矩陣&lt;/strong&gt;，寫作 &lt;span class=&#34;math inline&#34;&gt;\(A^{-1}\)&lt;/span&gt;。&lt;br&gt;
存在逆矩陣 &lt;span class=&#34;math inline&#34;&gt;\((A^{-1})\)&lt;/span&gt; 的 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; ，被叫做&lt;strong&gt;正則矩陣&lt;/strong&gt; (regular matrix, nonsingular matrix)。&lt;br&gt;
不存在逆矩陣的 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;，被叫做&lt;strong&gt;奇異矩陣&lt;/strong&gt; (singular matrix)。&lt;br&gt;
滿足 &lt;span class=&#34;math inline&#34;&gt;\(|A|\neq 0\)&lt;/span&gt; 的矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 被叫做正則矩陣。滿足 &lt;span class=&#34;math inline&#34;&gt;\(|A|=0\)&lt;/span&gt; 的矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 被叫做奇異矩陣。&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 爲正則矩陣時，滿足：&lt;span class=&#34;math inline&#34;&gt;\(A^{-1}A=AA^{-1}=E\)&lt;/span&gt; 。&lt;br&gt;
顯然，單位矩陣的逆矩陣也是一個單位矩陣: &lt;br&gt;
&lt;span class=&#34;math display&#34;&gt;\[E^{-1}E=EE^{-1}=E, E^{-1}=E\]&lt;/span&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div id=&#34;逆矩陣的性質&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;逆矩陣的性質&lt;/h3&gt;
&lt;p&gt;對於正則矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A, B\)&lt;/span&gt; 有以下性質：&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((AB)^{-1}=B^{-1}A^{-1}\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;diff_alert&#34;&gt;注意此處矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A，B\)&lt;/span&gt; 的順序對調了。&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((A^{-1})^{-1}=A\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((A^{t})^{-1}=(A^{-1})^t\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((\lambda A)^{-1}=\frac{1}{\lambda}A^{-1} (\lambda \ne 0)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;對角矩陣 &lt;span class=&#34;math inline&#34;&gt;\(D_n=diag(a_{11},a_{22},\dotsm,a_{nn})\)&lt;/span&gt; 的逆矩陣寫作： &lt;span class=&#34;math inline&#34;&gt;\(D_n^{-1}=diag(1/a_{11}, 1/a_{22},\dotsm,1/a_{nn})\)&lt;/span&gt;；&lt;br&gt;
&lt;span class=&#34;diff_alert&#34;&gt;注意此處的條件爲所有對角成分均非零: &lt;span class=&#34;math inline&#34;&gt;\(a_{11}a_{22}\dotsm a_{nn}\neq 0\)&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;證明&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;證明&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\((AB)(AB)^{-1}=E\)&lt;/span&gt; &lt;br&gt;
&lt;span class=&#34;diff_alert&#34;&gt;等式兩邊從左往右乘以 &lt;span class=&#34;math inline&#34;&gt;\(A^{-1}\)&lt;/span&gt;&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\((A^{-1}A)B(AB)^{-1}=A^{-1}E\\ B(AB)^{-1}=A^{-1}\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;diff_alert&#34;&gt;等式兩邊從左往右乘以 &lt;span class=&#34;math inline&#34;&gt;\(B^{-1}\)&lt;/span&gt;&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\((B^{-1}B)(AB)^{-1}=B^{-1}A^{-1}\\ E(AB)^{-1}=B^{-1}A^{-1}\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;diff_alert&#34;&gt;&lt;a href=&#34;https://winterwang.github.io/post/2017-03-08/&#34;&gt;根據單位矩陣的性質：&lt;/a&gt;&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\therefore (AB)^{-1}=B^{-1}A^{-1}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(E=E^{-1}=(A^{-1}A)^{-1}=A^{-1}(A^{-1})^{-1}\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;diff_alert&#34;&gt;等式兩邊從左往右乘以 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(AE=AA^{-1}(A^{-1})^{-1}\\ \therefore A=(A^{-1})^{-1}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(E=E^t=(A^{-1}A)^t=A^t(A^{-1})^t\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;diff_alert&#34;&gt;等式兩邊從左往右乘以 &lt;span class=&#34;math inline&#34;&gt;\((A^t)^{-1}\)&lt;/span&gt;&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\((A^t)^{-1}E=(A^t)^{-1}A^t(A^{-1})^t\\ \therefore (A^t)^{-1}=(A^{-1})^t\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記19</title>
      <link>https://wangcc.me/post/2017-04-02/</link>
      <pubDate>Sun, 02 Apr 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/2017-04-02/</guid>
      <description>


&lt;div id=&#34;行列式的性質&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;行列式的性質&lt;/h2&gt;
&lt;p&gt;具體的行列式的值，可以通過以下介紹的行列式性質，儘量簡潔地求解。本節也是爲了簡易示範，僅僅使用3次行列式作例子。4次以上的行列式性質依然相同，依此類推即可。&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;轉置矩陣的行列式，與轉置前的行列式一致。即：&lt;span class=&#34;math inline&#34;&gt;\(|A^t|=|A|\)&lt;/span&gt;。 &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(|A|=\begin{vmatrix}  1 &amp;amp; 2 &amp;amp; 3 \\  4 &amp;amp; 5 &amp;amp; 6 \\  7 &amp;amp; 8 &amp;amp; 9 \\ \end{vmatrix}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;任意一列（或者任意一行）若乘以 &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; 倍，那麼這個矩陣的行列式結果也將是乘以 &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; 倍。&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(|A|=\begin{vmatrix} a_{11} &amp;amp; a_{12} &amp;amp; a_{13}\\ \lambda a_{21} &amp;amp;\lambda a_{22} &amp;amp; \lambda a_{23}\\ a_{31} &amp;amp; a_{32} &amp;amp; a_{33}\\ \end{vmatrix}\\ \;\;\;\;=|A|=\lambda \begin{vmatrix} a_{11} &amp;amp; a_{12} &amp;amp; a_{13}\\ a_{21} &amp;amp; a_{22} &amp;amp; a_{23}\\ a_{31} &amp;amp; a_{32} &amp;amp; a_{33}\\ \end{vmatrix}\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(|A|=\begin{vmatrix} a_{11} &amp;amp; a_{12} &amp;amp; \lambda a_{13}\\ a_{21} &amp;amp; a_{22} &amp;amp; \lambda a_{23}\\ a_{31} &amp;amp; a_{32} &amp;amp; \lambda a_{33}\\ \end{vmatrix}\\ \;\;\;\;=|A|=\lambda \begin{vmatrix} a_{11} &amp;amp; a_{12} &amp;amp; a_{13}\\ a_{21} &amp;amp; a_{22} &amp;amp; a_{23}\\ a_{31} &amp;amp; a_{32} &amp;amp; a_{33}\\ \end{vmatrix}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;任意一列（或者任意一行）的各成分乘以 &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; 倍，與其他任意一列（或者任意一行）的各成分進行加運算（或者減運算）獲得的矩陣的行列式與原矩陣的行列式相同。&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\begin{vmatrix} a_{11} &amp;amp; a_{12} &amp;amp; a_{13}\\ a_{21}\pm \lambda a_{11} &amp;amp; a_{22}\pm \lambda a_{12} &amp;amp; a_{23}\pm \lambda a_{13}\\ a_{31} &amp;amp; a_{32} &amp;amp; a_{33}\\ \end{vmatrix}，\\ \begin{vmatrix} a_{11} &amp;amp; a_{12}\pm \lambda a_{11} &amp;amp; a_{13}\\ a_{21} &amp;amp; a_{22}\pm \lambda a_{21} &amp;amp; a_{23}\\ a_{31} &amp;amp; a_{32}\pm \lambda a_{31} &amp;amp; a_{33}\\ \end{vmatrix},\)&lt;/span&gt; &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\begin{vmatrix} a_{11} &amp;amp; a_{12} &amp;amp; a_{13}\\ a_{21} \pm \frac{a_{11}}{\lambda} &amp;amp; a_{22}\pm \frac{a_{12}}{\lambda} &amp;amp; a_{23}\pm \frac{a_{13}}{\lambda}\\ a_{31} &amp;amp; a_{32} &amp;amp; a_{33}\\ \end{vmatrix}, \\ \begin{vmatrix} a_{11} &amp;amp; a_{12}\pm \frac{a_{11}}{\lambda} &amp;amp; a_{13}\\ a_{21} &amp;amp; a_{22}\pm \frac{a_{21}}{\lambda} &amp;amp; a_{23}\\ a_{31} &amp;amp; a_{32}\pm \frac{a_{31}}{\lambda} &amp;amp; a_{33}\\ \end{vmatrix}\)&lt;/span&gt;&lt;br&gt;
上述行列式與行列式 &lt;span class=&#34;math inline&#34;&gt;\(\begin{vmatrix} a_{11} &amp;amp; a_{12} &amp;amp; a_{13}\\ a_{21} &amp;amp; a_{22} &amp;amp; a_{23}\\ a_{31} &amp;amp; a_{32} &amp;amp; a_{33}\\ \end{vmatrix}\)&lt;/span&gt; 結果相同。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;符合下列條件時，行列式的值爲 &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;
&lt;ul&gt;
&lt;li&gt;任意一行（或者列）的全部成分均爲 &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; 時。&lt;/li&gt;
&lt;li&gt;矩陣中若有兩行（或者兩列）的對應成分全部相同時。&lt;/li&gt;
&lt;li&gt;矩陣中若有兩行（或者兩列）的對應成分均成一定比例時。&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\begin{vmatrix} a_{11} &amp;amp; a_{12} &amp;amp; a_{13}\\ a_{21} &amp;amp; a_{22} &amp;amp; a_{23}\\ 0 &amp;amp; 0 &amp;amp; 0\\ \end{vmatrix}=0\\ \begin{vmatrix} a &amp;amp; b &amp;amp; c\\ a &amp;amp; b &amp;amp; c\\ d &amp;amp; e &amp;amp; f\\ \end{vmatrix}=0\\ \begin{vmatrix} a &amp;amp; b &amp;amp; c\\ ka &amp;amp; kb &amp;amp; kc\\ d &amp;amp; e &amp;amp; f\\ \end{vmatrix}=0\)&lt;/span&gt;&lt;br&gt;
由於上面的後兩條成立，所以當矩陣中任意兩列（或者兩行）的對應成分幾乎相等，或者比值無限接近時，行列式的值也可以說就接近爲 &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;。此性質與多重線性迴歸的多重共線性有直接關係。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;一個矩陣中其中兩列（或者兩行）的成分交換以後獲得的矩陣，其行列式值爲原矩陣的行列式的值的相反數。（即符號相反）&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\begin{vmatrix} a_{11} &amp;amp; a_{12} &amp;amp; a_{13}\\ a_{21} &amp;amp; a_{22} &amp;amp; a_{23}\\ a_{31} &amp;amp; a_{32} &amp;amp; a_{33}\\ \end{vmatrix}=-\begin{vmatrix} a_{21} &amp;amp; a_{22} &amp;amp; a_{23}\\ a_{11} &amp;amp; a_{12} &amp;amp; a_{13}\\ a_{31} &amp;amp; a_{32} &amp;amp; a_{33}\\ \end{vmatrix}\)&lt;/span&gt; （第一行和第二行對調成分）&lt;/li&gt;
&lt;li&gt;對角矩陣，上三角矩陣，下三角矩陣的行列式的值，等於對角成分的積&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\begin{vmatrix} a_{11} &amp;amp; 0 &amp;amp; 0\\ 0 &amp;amp; a_{22} &amp;amp; 0\\ 0 &amp;amp; 0 &amp;amp; a_{33}\\ \end{vmatrix}=a_{11}a_{22}a_{33},\\ \begin{vmatrix} a_{11} &amp;amp; a_{12} &amp;amp; a_{13}\\ 0 &amp;amp; a_{22} &amp;amp; a_{23}\\ 0 &amp;amp; 0 &amp;amp; a_{33}\\ \end{vmatrix}=a_{11}a_{22}a_{33},\\ \begin{vmatrix} a_{11} &amp;amp; 0 &amp;amp; 0\\ a_{21} &amp;amp; a_{22} &amp;amp; 0\\ a_{31} &amp;amp; a_{32} &amp;amp; a_{33}\\ \end{vmatrix}=a_{11}a_{22}a_{33}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;矩陣中如果有任意一行（或列），衹有一個成分為非零成分，可以將該矩陣的行列式降次：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\begin{vmatrix} a_{11} &amp;amp; 0 &amp;amp; 0\\ a_{21} &amp;amp; a_{22} &amp;amp; a_{23}\\ a_{31} &amp;amp; a_{32} &amp;amp; a_{33}\\ \end{vmatrix}=a_{11}(-1)^{1+1}\begin{vmatrix} a_{22} &amp;amp; a_{23} \\ a_{32} &amp;amp; a_{33} \end{vmatrix}\)&lt;/span&gt; &lt;br&gt;
&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\begin{vmatrix} a_{11} &amp;amp; a_{12} &amp;amp; 0\\ a_{21} &amp;amp; a_{22} &amp;amp; a_{23}\\ a_{31} &amp;amp; a_{32} &amp;amp; 0\\ \end{vmatrix}=a_{23}(-1)^{1+1}\begin{vmatrix} a_{11} &amp;amp; a_{12} \\ a_{31} &amp;amp; a_{32} \end{vmatrix}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(A, B\)&lt;/span&gt; 同時都是正方形矩陣時，&lt;span class=&#34;math inline&#34;&gt;\(|AB|=|A|·|B|\)&lt;/span&gt;。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;
&lt;div id=&#34;證明&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;strong&gt;證明&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(A=\left(\begin{array}{c} 0 &amp;amp; 4 &amp;amp; 2\\ -1 &amp;amp; 3 &amp;amp; 7\\ 6 &amp;amp; 5 &amp;amp; 9\\  \end{array}\right)\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(B=\left(\begin{array}{c} 2 &amp;amp; 3 &amp;amp; 4\\ -2 &amp;amp; 7 &amp;amp; 1\\ 4 &amp;amp; 6 &amp;amp; 0\\  \end{array}\right)\)&lt;/span&gt; 時， &lt;span class=&#34;math inline&#34;&gt;\(|AB|=|A|·|B|\)&lt;/span&gt; 成立&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;解&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;解&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\because AB=\left(\begin{array}{c}  0 &amp;amp; 40 &amp;amp; 4\\  20 &amp;amp; 60 &amp;amp; -1\\  38 &amp;amp; 107 &amp;amp; 29\\  \end{array}\right)\)&lt;/span&gt; &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\therefore |AB|=\begin{vmatrix}  0 &amp;amp; 40 &amp;amp; 4\\  20 &amp;amp; 60 &amp;amp; -1\\  38 &amp;amp; 107 &amp;amp; 29\\  \end{vmatrix}\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;diff_alert&#34;&gt;利用性質3： 第2列 - 第3列 &lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt; 10 作新的第2列&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(=\begin{vmatrix} 0 &amp;amp; 0 &amp;amp; 4\\ 20 &amp;amp; 70 &amp;amp; -1\\ 38 &amp;amp; -183 &amp;amp; 29\\ \end{vmatrix}\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;diff_alert&#34;&gt;利用性質7： 第一行衹有第三個元素非零，可以降次。&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(=4(-1)^{1+3}\begin{vmatrix}  20 &amp;amp; 70 \\  38 &amp;amp; -183\end{vmatrix}\)&lt;/span&gt; &lt;br&gt;
&lt;span class=&#34;diff_alert&#34;&gt;利用性質2: 第一行所有元素除以10, 將 10 提前。&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(=4\times10\begin{vmatrix}  2 &amp;amp; 7 \\  38 &amp;amp; -183\end{vmatrix}\\  =40(-366-266)\\=-25280\)&lt;/span&gt;&lt;br&gt;
&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(|A|=\begin{vmatrix}  0 &amp;amp; 4 &amp;amp; 2\\  -1 &amp;amp; 3 &amp;amp; 7\\  6 &amp;amp; 5 &amp;amp; 9\\  \end{vmatrix}\)&lt;/span&gt; &lt;br&gt;
&lt;span class=&#34;diff_alert&#34;&gt;利用性質3: 第2列 - 第3列 &lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt; 2 作爲新的第二列元素&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(=\begin{vmatrix} 0 &amp;amp; 0 &amp;amp; 2\\ -1 &amp;amp; -11 &amp;amp; 7\\ 6 &amp;amp; -13 &amp;amp; 9\\ \end{vmatrix}\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;diff_alert&#34;&gt;利用性質7: 第一行衹有第三個元素非零，降次。&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(=2(-1)^{1+3}\begin{vmatrix} -1 &amp;amp; -11 \\ 6 &amp;amp; -13\end{vmatrix}\\=2(13+66)=158\)&lt;/span&gt; &lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(|B|=\begin{vmatrix} 2 &amp;amp; 3 &amp;amp; 4\\ -2 &amp;amp; 7 &amp;amp; 1\\ 4 &amp;amp; 6 &amp;amp; 0\\ \end{vmatrix}\)&lt;/span&gt; &lt;br&gt;
&lt;span class=&#34;diff_alert&#34;&gt;利用性質3: 第1行 &lt;span class=&#34;math inline&#34;&gt;\(+\)&lt;/span&gt; 第2行作新的第1行； 第3行 - 第1行 &lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt; 2 作新的第三行&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(=\begin{vmatrix} 0 &amp;amp; 10 &amp;amp; 5\\ -2 &amp;amp; 7 &amp;amp; 1\\ 0 &amp;amp; 0 &amp;amp; -8\\ \end{vmatrix}\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;diff_alert&#34;&gt;利用性質7: 第三行衹有第三個元素非零，降次。&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(=-8(-1)^{3+3}\begin{vmatrix} 0 &amp;amp; 10 \\ -2 &amp;amp; 7\end{vmatrix}\\=-8\times20=-160\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;綜上可得&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;綜上可得&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(|A|·|B|=158\times(-160)=-25280=|AB|\)&lt;/span&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;試用這一節介紹的行列式性質求解前一節例3的行列式值&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;試用這一節介紹的行列式性質，求解前一節&lt;a href=&#34;https://winterwang.github.io/post/2017-03-15/&#34;&gt;例(3)&lt;/a&gt;的行列式值。&lt;/h3&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;解-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;解&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\begin{vmatrix}  -2 &amp;amp; 3 &amp;amp;4 &amp;amp; 1\\  4 &amp;amp; 2&amp;amp; 0&amp;amp; 5\\  2 &amp;amp;-3&amp;amp; -4&amp;amp; 2\\  2 &amp;amp; 1&amp;amp; 2&amp;amp; -3 \end{vmatrix}\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;diff_alert&#34;&gt;利用性質3&lt;br&gt;
1. 第1行 &lt;span class=&#34;math inline&#34;&gt;\(+\)&lt;/span&gt; 第3行，作新的第一行；
2. 第2行 &lt;span class=&#34;math inline&#34;&gt;\(-\)&lt;/span&gt; 第3行 &lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt; 2，作新的第2行；
3. 第4行 &lt;span class=&#34;math inline&#34;&gt;\(-\)&lt;/span&gt; 第3行 作新的第4行&lt;/span&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(=\begin{vmatrix}  0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 3\\  0 &amp;amp; 8 &amp;amp; 8 &amp;amp; 1\\  2 &amp;amp; -3 &amp;amp; -4 &amp;amp; 2\\  0 &amp;amp; 4 &amp;amp; 6 &amp;amp; -5 \end{vmatrix}\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;diff_alert&#34;&gt;利用性質7: 第1行衹有第4個元素非零，降次。&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(=3(-1)^{1+4}\begin{vmatrix} 0 &amp;amp; 8 &amp;amp; 8 \\ 2 &amp;amp; -3 &amp;amp; -4 \\ 0 &amp;amp; 4 &amp;amp; 6 \end{vmatrix}\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;diff_alert&#34;&gt;利用性質7: 第1列衹有第2個元素非零，降次。&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(=-3 \times 2(-1)^{1+2}\begin{vmatrix} 8 &amp;amp; 8 \\ 4 &amp;amp; 6 \end{vmatrix}\\ =6 \times (8 \times 6 - 4\times 8)\\ =96\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記18</title>
      <link>https://wangcc.me/post/2017-03-15/</link>
      <pubDate>Wed, 15 Mar 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/2017-03-15/</guid>
      <description>


&lt;div id=&#34;行列式的定義與計算&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;行列式的定義與計算&lt;/h2&gt;

&lt;div class=&#34;theorem&#34;&gt;
&lt;span id=&#34;thm:unnamed-chunk-1&#34; class=&#34;theorem&#34;&gt;&lt;strong&gt;Theorem 1  (determinant)  &lt;/strong&gt;&lt;/span&gt;&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 次正方形矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A= (a_{ij})=\left( \begin{array}{c} a_{11}&amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n}\\ a_{21}&amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n}\\ \vdots&amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ a_{n1}&amp;amp; a_{n2} &amp;amp; \cdots &amp;amp; a_{nn} \end{array} \right)\)&lt;/span&gt; 的&lt;strong&gt;行列式(determinant)&lt;/strong&gt;被定義爲是，&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 的全部成分 &lt;span class=&#34;math inline&#34;&gt;\(a_{11},a_{12},\cdots,a_{nn}\)&lt;/span&gt; 的函數，這個函數是一個&lt;strong&gt;標量(scalar)&lt;/strong&gt;。
&lt;/div&gt;

&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;次正方形矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 的行列式(&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;次行列式)，被記作：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(|A|, |a_{ij}|, \det(A), \det(a_{ij})， \begin{vmatrix} a_{11}&amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n}\\ a_{21}&amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n}\\ \vdots&amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ a_{n1}&amp;amp; a_{n2} &amp;amp; \cdots &amp;amp; a_{nn} \notag \end{vmatrix}\)&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;1次行列式：
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
A=(a_{11}), |A|=a_{11}
(\#eq:determinant1)
\end{equation}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;2次行列式：
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
A=\left(
\begin{array}{}
a_{11} &amp;amp; a_{12}\\
a_{21} &amp;amp; a_{22}\\
\end{array}
\right), |A|=a_{11}a_{12}-a_{12}a_{21}
(\#eq:determinant2)
\end{equation}\]&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(n-1\)&lt;/span&gt; 次行列式
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
A_{(n-1)\times(n-1)}, 假設行列式 |A| 有被定義
(\#eq:determinant3)
\end{equation}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 次行列式&lt;br&gt;
假如&lt;a href=&#34;#eq:determinant3&#34;&gt;(&lt;strong&gt;??&lt;/strong&gt;)&lt;/a&gt;成立：&lt;br&gt;
對於：&lt;span class=&#34;math inline&#34;&gt;\(A_{n\times n}=\left( \begin{array}{c} a_{11}&amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n}\\ a_{21}&amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n}\\ \vdots&amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ a_{n1}&amp;amp; a_{n2} &amp;amp; \cdots &amp;amp; a_{nn} \end{array} \right)\\ |A|=\begin{vmatrix} a_{11}&amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n}\\ a_{21}&amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n}\\ \vdots&amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ a_{n1}&amp;amp; a_{n2} &amp;amp; \cdots &amp;amp; a_{nn} \notag \end{vmatrix}\\ \left\{ \begin{array}{} (4)\;=a_{i1}A_{i1}+a_{i2}A_{i2}+\cdots+a_{ij}A_{ij}+\cdots+a_{in}A_{in}\\ (5)\;=a_{1j}A_{i1}+a_{2j}A_{2j}+\cdots+a_{ij}A_{ij}+\cdots+a_{nj}A_{nj}\\  \end{array} \right.\)&lt;/span&gt;&lt;br&gt;
式子 &lt;span class=&#34;math inline&#34;&gt;\((4)\)&lt;/span&gt; 被稱爲行列式 &lt;span class=&#34;math inline&#34;&gt;\(|A|\)&lt;/span&gt; 的第 &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; 行&lt;strong&gt;展開式(expansion of &lt;span class=&#34;math inline&#34;&gt;\(|A|\)&lt;/span&gt; according to elements of row &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;)&lt;/strong&gt;。同樣的，式子 &lt;span class=&#34;math inline&#34;&gt;\((5)\)&lt;/span&gt; 被稱爲行列式 &lt;span class=&#34;math inline&#34;&gt;\(|A|\)&lt;/span&gt; 的第 &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; 列展開式。&lt;span class=&#34;math inline&#34;&gt;\(|A_{ij}|(i=1,2,\cdots,n;j=1,2,\cdots,n)\)&lt;/span&gt; 被稱爲 成分 &lt;span class=&#34;math inline&#34;&gt;\(a_{ij}\)&lt;/span&gt; 的&lt;strong&gt;餘因子(cofactor)&lt;/strong&gt;，定義如下：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(A_{ij}=(-1)^{i+j}D_{ij}\\ \;\;\;\;=(-1)^{i+j}\begin{vmatrix} a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1,j-1} &amp;amp; a_{1,j+1} &amp;amp; \cdots &amp;amp; a_{1n}\\ a_{21} &amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2,j-1} &amp;amp; a_{2,j+1} &amp;amp; \cdots &amp;amp; a_{2n}\\ \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots\\ a_{i-1,1} &amp;amp; a_{i-1,2} &amp;amp; \cdots &amp;amp; a_{i-1,j-1} &amp;amp; a_{i-1,j+1} &amp;amp; \cdots &amp;amp; a_{i-1,n}\\ a_{i+1,1} &amp;amp; a_{i+1,2} &amp;amp; \cdots &amp;amp; a_{i+1,j-1} &amp;amp; a_{i+1,,j+1} &amp;amp; \cdots &amp;amp; a_{i+1,n}\\ \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots\\ a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1,j-1} &amp;amp; a_{1,j+1} &amp;amp; \cdots &amp;amp; a_{1n}\\ \end{vmatrix}\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(D_{ij}\)&lt;/span&gt; 正如上面等式最右端所寫，其實是行列式 &lt;span class=&#34;math inline&#34;&gt;\(A_{n\times n}\)&lt;/span&gt; 剔除了第 &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; 行和第 &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; 列的 &lt;span class=&#34;math inline&#34;&gt;\((n-1)\)&lt;/span&gt; 次行列式，又被叫做行列式 &lt;span class=&#34;math inline&#34;&gt;\(A_{n\times n}\)&lt;/span&gt; 的&lt;strong&gt;小行列式(minor)&lt;/strong&gt;。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;餘因子矩陣&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;餘因子矩陣&lt;/h2&gt;
&lt;p&gt;以 &lt;span class=&#34;math inline&#34;&gt;\(A_{n\times n}\)&lt;/span&gt; 的成分 &lt;span class=&#34;math inline&#34;&gt;\(a_{ij}\)&lt;/span&gt; 的餘因子 &lt;span class=&#34;math inline&#34;&gt;\(A_{ij}\)&lt;/span&gt; 作成分&lt;strong&gt;的矩陣&lt;/strong&gt;&lt;span class=&#34;diff_alert&#34;&gt;的轉置矩陣&lt;/span&gt;作被稱爲 &lt;span class=&#34;math inline&#34;&gt;\(A_{n\times n}\)&lt;/span&gt; 的&lt;strong&gt;餘因子矩陣(adjoint matrix, adjugate matrix)&lt;/strong&gt;。標記爲 &lt;span class=&#34;math inline&#34;&gt;\(adj(A)\)&lt;/span&gt;。也就是說：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(adj(A)=\left( \begin{array}{c} A_{11}&amp;amp; A_{12} &amp;amp; \cdots &amp;amp; A_{1n}\\ A_{21}&amp;amp; A_{22} &amp;amp; \cdots &amp;amp; A_{2n}\\ \vdots&amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ A_{n1}&amp;amp; A_{n2} &amp;amp; \cdots &amp;amp; A_{nn} \end{array} \right)^t=\left( \begin{array}{c} A_{11}&amp;amp; A_{21} &amp;amp; \cdots &amp;amp; A_{n1}\\ A_{12}&amp;amp; A_{22} &amp;amp; \cdots &amp;amp; A_{n2}\\ \vdots&amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ A_{1n}&amp;amp; A_{2n} &amp;amp; \cdots &amp;amp; A_{nn} \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;我們來試着計算行列式：&lt;br&gt;
1. &lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt; 次行列式&lt;br&gt;
以方程&lt;a href=&#34;#eq:determinant2&#34;&gt;(&lt;strong&gt;??&lt;/strong&gt;)&lt;/a&gt;的定義計算：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\begin{vmatrix}  a_{11} &amp;amp; a_{12}\\  a_{21} &amp;amp; a_{22}\\  \end{vmatrix}=a_{11}a_{22}-a_{12}a_{21}\)&lt;/span&gt;&lt;br&gt;
此公式可以用下列 &lt;strong&gt;示意圖(薩呂法則, Sarrus’ rule)&lt;/strong&gt; 來記憶:
　&lt;img src=&#34;https://wangcc.me/img/sarrus.png&#34; /&gt;&lt;br&gt;
也就是，沿着右下方向將所有成分相乘以後用加號 &lt;span class=&#34;math inline&#34;&gt;\(+\)&lt;/span&gt; 號連接起來，沿着左下的方向的所有成分則相乘以後用減號 &lt;span class=&#34;math inline&#34;&gt;\(-\)&lt;/span&gt; 號連接起來。最後將這兩者相加獲得行列式的值。&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;練習： 求 &lt;span class=&#34;math inline&#34;&gt;\(A=\left( \begin{array}{} 4 &amp;amp; 2\\ 1 &amp;amp; 3\\ \end{array} \right)\)&lt;/span&gt; 的行列式和餘因子矩陣。&lt;/p&gt;
&lt;p&gt;解： &lt;span class=&#34;math inline&#34;&gt;\(|A|=\begin{vmatrix} 4 &amp;amp; 2\\ 1 &amp;amp; 3\\ \end{vmatrix}=4\times3-2\times1=10\)&lt;/span&gt; &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(adj(A)=\left( \begin{array}{} A_{11} &amp;amp; A_{21}\\ A_{12} &amp;amp; A_{22}\\ \end{array} \right)\)&lt;/span&gt; &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\because A_{11}=(-1)^{(1+1)}\times3\\ A_{21}=(-1)^{(2+1)}\times2\\ A_{12}=(-1)^{(1+2)}\times1\\ A_{22}=(-1)^{2+2}\times4\\ \therefore adj(A)=\left( \begin{array}{r} 3 &amp;amp; -2\\ -1 &amp;amp; 4\\ \end{array} \right)\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;diff_alert&#34;&gt; 注意：餘因子&lt;strong&gt;矩陣&lt;/strong&gt;，終究是一個矩陣而非行列式。&lt;/span&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;三次矩陣&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(A=\left( \begin{array}{c}  a_{11} &amp;amp; a_{12} &amp;amp; a_{13}\\  a_{21} &amp;amp; a_{22} &amp;amp; a_{23}\\  a_{31} &amp;amp; a_{32} &amp;amp; a_{33}\\ \end{array} \right)\)&lt;/span&gt; 的行列式 &lt;span class=&#34;math inline&#34;&gt;\(|A|\)&lt;/span&gt; 要如何用 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 的成分來表示呢？ &lt;br&gt;
我們發現，代入上面第 &lt;span class=&#34;math inline&#34;&gt;\((4)\)&lt;/span&gt; 個式子 &lt;span class=&#34;math inline&#34;&gt;\(n=3\)&lt;/span&gt; 的情況來計算。&lt;br&gt;
在這裏，我們就按照 &lt;span class=&#34;math inline&#34;&gt;\(i=1\)&lt;/span&gt; 的情況來展開。&lt;span class=&#34;diff_alert&#34;&gt; (注意：&lt;span class=&#34;math inline&#34;&gt;\(i=2, i=3\)&lt;/span&gt; 的情況展開，結果也是一樣的。)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
 |A| &amp;amp;= a_{11}A_{11}+a_{12}A_{12}+a_{13}A_{13}\\
     &amp;amp;= a_{11}(-1)^{1+1}D_{11}+a_{12}(-1)^{1+2}D_{12}\\
     &amp;amp;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+a_{13}(-1)^{1+3}D_{13}\\
     &amp;amp;= a_{11}\begin{vmatrix}a_{22} &amp;amp; a_{23}\\ a_{32} &amp;amp; a_{33}\\\end{vmatrix}-a_{12}\begin{vmatrix}a_{21} &amp;amp; a_{23}\\ a_{31} &amp;amp; a_{33}\\\end{vmatrix}\\
     &amp;amp;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+a_{13}\begin{vmatrix}a_{21} &amp;amp; a_{22}\\ a_{31} &amp;amp; a_{32}\\\end{vmatrix}\\
     &amp;amp;= a_{11}(a_{22}a_{23}-a_{23}a_{32})-a_{12}(a_{21}a_{33}-a_{31}a_{23})\\
     &amp;amp;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+a_{13}(a_{21}a_{32}-a_{22}a_{31})\\
     &amp;amp;=a_{11}a_{22}a_{33}+a_{12}a_{23}a_{31}+a_{13}a_{21}a_{32}-a_{11}a_{23}a_{32}\\
     &amp;amp;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;-a_{12}a_{21}a_{33}-a_{13}a_{22}a_{31}
\end{align}\]&lt;/span&gt;&lt;br&gt;
我們也可以利用薩呂法則（下圖）來記住計算過程：&lt;br&gt;
&lt;img src=&#34;https://wangcc.me/img/sarrus33.png&#34; /&gt;&lt;br&gt;
另外，可以得到如下的餘因子：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(A_{11}=\begin{vmatrix}a_{22} &amp;amp; a_{23}\\ a_{32} &amp;amp; a_{33}\\\end{vmatrix}, A_{12}=-\begin{vmatrix}a_{21} &amp;amp; a_{23}\\ a_{31} &amp;amp; a_{33}\\\end{vmatrix}, A_{13}=\begin{vmatrix}a_{21} &amp;amp; a_{22}\\ a_{31} &amp;amp; a_{32}\\\end{vmatrix}\\ A_{21}=-\begin{vmatrix}a_{12} &amp;amp; a_{13}\\ a_{32} &amp;amp; a_{33}\\\end{vmatrix}, A_{22}=\begin{vmatrix}a_{11} &amp;amp; a_{13}\\ a_{31} &amp;amp; a_{33}\\\end{vmatrix}, A_{23}=-\begin{vmatrix}a_{11} &amp;amp; a_{12}\\ a_{31} &amp;amp; a_{32}\\\end{vmatrix}\\ A_{31}=\begin{vmatrix}a_{12} &amp;amp; a_{13}\\ a_{22} &amp;amp; a_{23}\\\end{vmatrix}, A_{32}=-\begin{vmatrix}a_{11} &amp;amp; a_{13}\\ a_{21} &amp;amp; a_{23}\\\end{vmatrix}, A_{33}=\begin{vmatrix}a_{11} &amp;amp; a_{12}\\ a_{21} &amp;amp; a_{22}\\\end{vmatrix}\)&lt;/span&gt;&lt;br&gt;
因此餘因子矩陣爲：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(adj(A)=\left( \begin{array}{c}  A_{11} &amp;amp; A_{12} &amp;amp; A_{13}\\  A_{21} &amp;amp; A_{22} &amp;amp; A_{23}\\  A_{31} &amp;amp; A_{32} &amp;amp; A_{33}\\ \end{array} \right)^t=\left( \begin{array}{c}  A_{11} &amp;amp; A_{21} &amp;amp; A_{23}\\  A_{12} &amp;amp; A_{22} &amp;amp; A_{32}\\  A_{13} &amp;amp; A_{23} &amp;amp; A_{33}\\ \end{array} \right)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;table style=&#34;width:7%;&#34;&gt;
&lt;colgroup&gt;
&lt;col width=&#34;6%&#34; /&gt;
&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;練習：試求矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A=\left( \begin{array}{r} 6 &amp;amp; 1 &amp;amp; -3\\ 3 &amp;amp; 5 &amp;amp; 7\\ 2 &amp;amp; -1 &amp;amp; 3\\ \end{array} \right)\)&lt;/span&gt; 的行列式和餘因子矩陣&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;解： &lt;span class=&#34;math inline&#34;&gt;\(|A|= \begin{vmatrix} 6 &amp;amp; 1 &amp;amp; -3\\ 3 &amp;amp; 5 &amp;amp; 7\\ 2 &amp;amp; -1 &amp;amp; 3\\ \end{vmatrix}\\ \;\;\;\;\:=6\times5\times3+1\times7\times2+(-3)\times3\times(-1)\\ \;\;\;\;\;\:\;\;\;\;\:-\{6\times7\times(-1)+1\times3\times3+(-3)\times5\times2\}\\ \;\;\;\;\:=113-(-63)=176\\ \\ A_{11}=\begin{vmatrix}5 &amp;amp; 7\\ -1 &amp;amp; 3\\\end{vmatrix}=15-(-7)=22\\ A_{12}=\begin{vmatrix}3 &amp;amp; 7\\ 2&amp;amp; 3\\\end{vmatrix}=9-14=-5\\ A_{13}=\begin{vmatrix}3 &amp;amp; 5\\ 2 &amp;amp; -1\\\end{vmatrix}=-3-10=-13\\ A_{21}=\begin{vmatrix}1 &amp;amp; -3\\ -1 &amp;amp; 3\\\end{vmatrix}=3-3=0\\ A_{22}=\begin{vmatrix}6 &amp;amp; -3\\ 2 &amp;amp; 3\\\end{vmatrix}=18-(-6)=24\\ A_{23}=\begin{vmatrix}6 &amp;amp; 1\\ 2 &amp;amp; -1\\\end{vmatrix}=-6-2=-8\\ A_{31}=\begin{vmatrix}1 &amp;amp; -3\\5 &amp;amp; 7\\\end{vmatrix}=7-(-15)=22\\ A_{32}=\begin{vmatrix}6 &amp;amp; -3\\3 &amp;amp; 7\\\end{vmatrix}=42-(-9)=51\\ A_{33}=\begin{vmatrix}6 &amp;amp; 1\\ 3 &amp;amp; 5\\\end{vmatrix}=30-3=27\\ \Longrightarrow adj(A)=\left( \begin{array}{r} 22 &amp;amp; 5 &amp;amp; -13 \\ 0 &amp;amp; 24&amp;amp; -8 \\ 22 &amp;amp; 51&amp;amp; 27 \\ \end{array} \right)^t=\left( \begin{array}{r} 22 &amp;amp; 0 &amp;amp; 22\\ 5 &amp;amp; 24 &amp;amp; 51\\ -13 &amp;amp; -8 &amp;amp; 27\\ \end{array} \right)\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table style=&#34;width:7%;&#34;&gt;
&lt;colgroup&gt;
&lt;col width=&#34;6%&#34; /&gt;
&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;練習： 求3次矩陣的固有值時(將來敘述)需要的行列式&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\begin{vmatrix} a-\lambda &amp;amp; b &amp;amp; c\\ d &amp;amp; e-\lambda &amp;amp; f\\ g &amp;amp; h &amp;amp; i-\lambda \end{vmatrix}\)&lt;/span&gt;&lt;br&gt;
展開以後，整理爲關於 &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; 的式子：&lt;br&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;解： &lt;span class=&#34;math inline&#34;&gt;\(\begin{vmatrix} a-\lambda &amp;amp; b &amp;amp; c\\ d &amp;amp; e-\lambda &amp;amp; f\\ g &amp;amp; h &amp;amp; i-\lambda \end{vmatrix}\\ =(a-\lambda)(e-\lambda)(i-\lambda)+bfg+dhc\\ \;\;\;\;\;-\{g(e-\lambda)c+bd(i-\lambda)+(a-\lambda)fh\}\\ =-\lambda^3+(a+e+i)\lambda^2+(bd+cg+fh-ae-ei-ai)\lambda\\ \;\;\;\;\;+(aei+bfg+cdh-afh-bdi-ecg)\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;4次行列式：&lt;br&gt;
試求&lt;span class=&#34;math inline&#34;&gt;\(A=\left( \begin{array}{}  a_{11}&amp;amp; a_{12}&amp;amp; a_{13}&amp;amp; a_{14}\\  a_{21}&amp;amp; a_{22}&amp;amp; a_{23}&amp;amp; a_{24}\\  a_{31}&amp;amp; a_{32}&amp;amp; a_{33}&amp;amp; a_{34}\\  a_{41}&amp;amp; a_{42}&amp;amp; a_{43}&amp;amp; a_{44} \end{array} \right)\\ \;\;=\left( \begin{array}{r}  -2 &amp;amp; 3 &amp;amp;4 &amp;amp; 1\\  4 &amp;amp; 2&amp;amp; 0&amp;amp; 5\\  2 &amp;amp;-3&amp;amp; -4&amp;amp; 2\\  2 &amp;amp; 1&amp;amp; 2&amp;amp; -3 \end{array} \right)\)&lt;/span&gt; &lt;br&gt;的行列式 &lt;span class=&#34;math inline&#34;&gt;\(|A|\)&lt;/span&gt;：&lt;br&gt;
由於第2行有成分 &lt;span class=&#34;math inline&#34;&gt;\(a_{23}=0\)&lt;/span&gt; 我們以第二行展開行列式，因爲 &lt;span class=&#34;math inline&#34;&gt;\(a_{23}=0\)&lt;/span&gt;，所以 &lt;span class=&#34;math inline&#34;&gt;\(a_{23}A_{23}=0\)&lt;/span&gt; 可以省略：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(|A|=a_{21}A_{21}+a_{22}A_{22}+a_{24}A_{24}\\ \;\;\;\;\:=a_{21}(-1)^{2+1}D_{21}+a_{22}(-1)^{2+2}D_{22}+a_{24}(-1)^{2+4}D_{24}\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\because A_{21}=(-1)^{2+1}\begin{vmatrix}  a_{12} &amp;amp; a_{13} &amp;amp; a_{14}\\  a_{32} &amp;amp; a_{33} &amp;amp; a_{34}\\  a_{42} &amp;amp; a_{43} &amp;amp; a_{44}\\ \end{vmatrix}\\ \;\;\;\;\;\;\;\;\;=-\begin{vmatrix}  3 &amp;amp; 4 &amp;amp; 1\\  -3 &amp;amp; -4 &amp;amp; 2\\  1 &amp;amp; 2&amp;amp; -3\\ \end{vmatrix}=6\\ A_{22}=(-1)^{2+2}\begin{vmatrix}  a_{11} &amp;amp; a_{13} &amp;amp; a_{14}\\  a_{31} &amp;amp; a_{33} &amp;amp; a_{34}\\  a_{41} &amp;amp; a_{43} &amp;amp; a_{44}\\ \end{vmatrix}\\ \;\;\;\;\;\;\;\;\;=\begin{vmatrix}  -2 &amp;amp; 4 &amp;amp;1\\  2 &amp;amp; -4 &amp;amp;2\\  2 &amp;amp; 2 &amp;amp;-3\\ \end{vmatrix}=36\\ A_{24}=(-1)^{2+4}\begin{vmatrix}  a_{11} &amp;amp; a_{12} &amp;amp; a_{13}\\  a_{31} &amp;amp; a_{32} &amp;amp; a_{33}\\  a_{41} &amp;amp; a_{42} &amp;amp; a_{43}\\ \end{vmatrix}\\ \;\;\;\;\;\;\;\;\;=-\begin{vmatrix}  -2 &amp;amp; 3 &amp;amp;4\\  2 &amp;amp; -3 &amp;amp;-4\\  2 &amp;amp; 1 &amp;amp;2\\ \end{vmatrix}=0\\ \therefore |A|=4\times6+2\times36+5\times0=96\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;diff_alert&#34;&gt;然而，&lt;strong&gt;4次以上的矩陣的行列式計算，沒有類似薩呂法則的計算方法。&lt;/strong&gt;&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記17</title>
      <link>https://wangcc.me/post/2017-03-13/</link>
      <pubDate>Mon, 13 Mar 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/2017-03-13/</guid>
      <description>


&lt;div id=&#34;正定半正定-正值半正值&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;正定，半正定 (正值，半正值)&lt;/h2&gt;
&lt;p&gt;對於任意的非零向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}(\neq\underline{0})\)&lt;/span&gt; ，如果2次型 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}^tA\underline{x}\)&lt;/span&gt; 始終滿足 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}^tA\underline{x} &amp;gt; 0\)&lt;/span&gt; &lt;strong&gt;注意此處無等號&lt;/strong&gt;。我們稱這個2次型爲&lt;strong&gt;正定(positive definite)&lt;/strong&gt;，&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;爲&lt;strong&gt;正定矩陣(positive definite matrix)&lt;/strong&gt;。另外，如果任意非零向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}(\neq\underline{0})\)&lt;/span&gt; 始終滿足2次型 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}^tA\underline{x} \geqslant 0\)&lt;/span&gt;， 這個2次型被叫做&lt;strong&gt;半正定(positive semi-definite)&lt;/strong&gt;，&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;爲&lt;strong&gt;半正定矩陣(positive semi-definite matrix)&lt;/strong&gt;。&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(x=\left( \begin{array}{} x_1\\ x_2\\ x_3 \end{array} \right), \ A=\left( \begin{array}{} 5 &amp;amp; 2 &amp;amp; 4\\ 2 &amp;amp; 2 &amp;amp; 3\\ 4 &amp;amp; 3 &amp;amp; 25 \end{array} \right)\)&lt;/span&gt;，2次型 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}^tA\underline{x}\)&lt;/span&gt; 是正定。因爲：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}^tA\underline{x}=5x_1^2+2x_2^2+25x_3^2\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+4x_1x_2+8x_1x_3+6x_2x_3\\\;\;\;\;\;\;\;\;\;\:=(2x_1+x_2)^2+(x_2+3x_3)^2+(x_1+4x_3)^2\\ \because \underline{x}\neq\underline{0}=\left( \begin{array}{} 0\\ 0\\ 0 \end{array} \right)\\ \therefore \underline{x}^tA\underline{x}&amp;gt;0\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(x=\left( \begin{array}{} x_1\\ x_2\\ x_3 \end{array} \right), \ A=\left( \begin{array}{} 5 &amp;amp; -6 &amp;amp; 3\\ -6 &amp;amp; 25 &amp;amp; 32\\ 3 &amp;amp; 32 &amp;amp; 73 \end{array} \right)\)&lt;/span&gt;，2次型 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}^tA\underline{x}\)&lt;/span&gt; 是半正定。因爲：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}^tA\underline{x}=5x_1^2+25x_2^2+73x_3^2\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;-12x_1x_2+6x_2x_3+64x_1x_3\\ \;\;\;\;\;\;\;\;\;\:=(2x_1-3x_3)^2+(x_1+3x_3)^2+(4x_2+8x_3)^2\\\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\because \underline{x}=\left( \begin{array}{c} 3\\ 2\\ -1 \end{array} \right)\)&lt;/span&gt; 時 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}^tA\underline{x}=0\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\therefore \underline{x}^tA\underline{x} \geqslant0\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}=\left( \begin{array}{c} x_1\\ x_2\\ \vdots\\ x_n \end{array} \right)(\neq\underline{0})\)&lt;/span&gt; 與單位矩陣 &lt;span class=&#34;math inline&#34;&gt;\(E_n\)&lt;/span&gt; 構成的2次型 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}^tE_n\underline{x}=\underline{x}^t\underline{x}=\sum\limits_{i=1}^nx_i^2&amp;gt;0\)&lt;/span&gt; 是爲正定。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}=\left( \begin{array}{c} x_1\\ x_2\\ \vdots\\ x_n \end{array} \right), \underline{\frac{1}{n}}=\left( \begin{array}{c} \frac{1}{n}\\ \frac{1}{n}\\ \vdots \\ \frac{1}{n} \end{array} \right)\)&lt;/span&gt; 已知，&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}\underline{1/n}=\sum\limits_{i=1}^nx_i\cdot \frac{1}{n}=\bar{x}\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}\)&lt;/span&gt; 的平均值)，包含了 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 個 &lt;span class=&#34;math inline&#34;&gt;\(\bar{x}\)&lt;/span&gt; 的橫向量： &lt;span class=&#34;math inline&#34;&gt;\((\bar{x}，\bar{x},\cdots,\bar{x})\)&lt;/span&gt; 展開以後成爲：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\((\bar{x}，\bar{x},\cdots,\bar{x})\\ \;\;\;\;\;\;\;\;\;\:=(x_1, x_2, \cdots, x_n)\left( \begin{array}{c} \frac{1}{n} &amp;amp; \frac{1}{n} &amp;amp; \cdots &amp;amp; \frac{1}{n}\\ \frac{1}{n} &amp;amp; \frac{1}{n} &amp;amp; \cdots &amp;amp; \frac{1}{n}\\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ \frac{1}{n} &amp;amp; \frac{1}{n} &amp;amp; \cdots &amp;amp; \frac{1}{n} \end{array} \right)\\ \;\;\;\;\;\;\;\;\;\:=\underline{x}U\)&lt;/span&gt;&lt;br&gt;
令 &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; 代表上面第二個等式中有半部分的矩陣。那麼將之從右往左乘以 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}\)&lt;/span&gt; 我們可以得到：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}^tU\underline{x}=(\bar{x},\bar{x},\cdots,\bar{x})\underline{x}=(\bar{x},\bar{x},\cdots,\bar{x})\left( \begin{array}{c} x_1\\ x_2\\ \vdots\\ x_n \end{array} \right)\\ =\sum\limits_{i=1}^n\bar{x}x_i=\bar{x}\sum\limits_{i=1}^nx_i=n\bar{x}^2\)&lt;/span&gt;&lt;br&gt;
利用上面的式子，我們可以得到，&lt;strong&gt;偏差平方和(sum of squared deviation, SS)&lt;/strong&gt;：&lt;span class=&#34;math inline&#34;&gt;\(E_n\)&lt;/span&gt; 爲 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 次單位矩陣。&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(SS=\sum\limits_{i=1}^n(x_i-\bar{x})^2\\ \;\;\;\;\:=\sum\limits_{i=1}^n(x_i^2-2x_i\bar{x}+\bar{x}^2)\\ \;\;\;\;\:=\sum\limits_{i=1}^nx_i^2-2\bar{x}\sum\limits_{i=1}^nx_i+n\bar{x}^2\\ \;\;\;\;\:=\sum\limits_{i=1}^nx_i^2-2n\bar{x}^2+n\bar{x}^2\\ \;\;\;\;\:=\sum\limits_{i=1}^nx_i^2-n\bar{x}^2\\ \;\;\;\;\:=\sum\limits_{i=1}^nx_i^2-\underline{x}^tU\underline{x}\\ \;\;\;\;\:=\underline{x}^t\underline{x}-\underline{x}^tU\underline{x}\\ \;\;\;\;\:=\underline{x}^tE_n\underline{x}-\underline{x}^tU\underline{x}\\ \;\;\;\;\:=\underline{x}^t(E_n-U)\underline{x}\\ \because when \underline{x}=\left( \begin{array}{c} x_1\\ x_2\\ \vdots\\ x_n \end{array} \right)=\left( \begin{array}{c} \bar{x}\\ \bar{x}\\ \vdots\\ \bar{x} \end{array} \right), \&amp;amp; (\bar{x}\neq0), SS=0\\ \therefore \underline{x}^t(E_n-U)\underline{x}\;是半正定2次型。\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;雙一次型&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;雙一次型&lt;/h2&gt;
&lt;p&gt;對於 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}=\left( \begin{array}{c} x_1\\ x_2\\ \vdots\\ x_m \end{array} \right), A=\left( \begin{array}{c} a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n}\\ a_{21} &amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n}\\ \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots\\ a_{m1} &amp;amp; a_{m2} &amp;amp; \cdots &amp;amp; a_{mn} \end{array} \right), \underline{y}=\left( \begin{array}{c} y_1\\ y_2\\ \vdots\\ y_n \end{array} \right)\)&lt;/span&gt; 來說，&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}^tA\underline{y}=\sum\limits_{i=1}^m\sum\limits_{j=1}^na_{ij}x_iy_j\)&lt;/span&gt; 既是 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}\)&lt;/span&gt; 的1次型，也是 &lt;span class=&#34;math inline&#34;&gt;\(\underline{y}\)&lt;/span&gt; 的1次型，所以又叫做 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}\underline{y}\)&lt;/span&gt; 的&lt;strong&gt;雙1次型(bilinear form)&lt;/strong&gt;。雙1次型是一個標量(scalar)。&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;對於 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}=\left( \begin{array}{c} x_1\\ x_2\\ x_3 \end{array} \right), B=(b_{ij})=\left( \begin{array}{c} 1 &amp;amp; 0 &amp;amp; 0\\ 0 &amp;amp; 1 &amp;amp; 1\\ 1 &amp;amp; 0 &amp;amp; 1 \end{array} \right), \underline{y}=\left( \begin{array}{c} y_1\\ y_2\\ y_3 \end{array} \right)\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}^tB\underline{y}=\sum\limits_{i=1}^3\sum\limits_{j=1}^3b_{ij}x_iy_j=(x_1+x_3, x_2, x_2+x_3)\left( \begin{array}{c} y_1\\ y_2\\ y_3 \end{array} \right)\\ \;\;\;\;\:=x_1y_1+x_2y_2+x_2y_3+x_3y_1+x_3y_3\\ \;\;\;\;\:=x_1y_1+x_2(y_2+y_3)+x_3(y_1+y_3) \; \bf (\underline{x} 的1次型)\)&lt;/span&gt; &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\;\;\;\;\;\:=y_1(x_1+x_3)+x_2y_2+(x_2+x_3)y_3 \;\bf (\underline{y} 的1次型)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;對於 &lt;span class=&#34;math inline&#34;&gt;\(\underline{l}=\left( \begin{array}{c} l_1\\ l_2\\ \end{array} \right), T=\left( \begin{array}{c} t_{11} &amp;amp; t_{12} &amp;amp; t_{13}\\ t_{21} &amp;amp; t_{22} &amp;amp; t_{23}\\ \end{array} \right), \underline{m}=\left( \begin{array}{c} m_1\\ m_2\\ m_3 \end{array} \right)\)&lt;/span&gt; &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{l}^tT\underline{m}=\sum\limits_{i=1}^2\sum\limits_{j=1}^3t_{ij}l_im_j\\ \;\;\;\;\;\;\;\;\;\;=l_1t_{11}m_1+l_1t_{12}m_2+l_1t_{13}m_3\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+l_2t_{21}m_1+l_2t_{22}m_2+l_3t_{23}m_3\\ \;\;\;\;\;\;\;\;\;\;=l_1(t_{11}m_1+t_{12}m_2+t_{13}m_3)\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+l_2(t_{21}m_1+t_{22}m_2+t_{23}m_3) \;\;(\underline{l} \textbf{的1次型})\\ \;\;\;\;\;\;\;\;\;\;=(t_{11}l_1+t_{21}l_2)m_1+(t_{12}l_1+t_{22}l_2)m_2\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+(t_{13}l_1+t_{23}l_2)m_3\;\;(\underline{m} \textbf{的1次型})\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記16</title>
      <link>https://wangcc.me/post/2017-03-11/</link>
      <pubDate>Sat, 11 Mar 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/2017-03-11/</guid>
      <description>


&lt;div id=&#34;二次型形式&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;二次型(形式)&lt;/h2&gt;
&lt;p&gt;對於 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}=\left( \begin{array}{c} x_{1}\\ x_{2}\\ \vdots\\ x_{n} \end{array} \right), A=\left( \begin{array}{c} a_{11}&amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n}\\ a_{21}&amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n}\\ \vdots&amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots\\ a_{n1}&amp;amp; a_{n2} &amp;amp; \cdots &amp;amp; a_{nn} \end{array} \right)\)&lt;/span&gt; 那麼：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}^tA\underline{x}=\sum\limits_{i=1}^n\sum\limits_{j=1}^na_{ij}x_ix_j\\ \;\;\;\;\;\;\;\;\:\:=\sum\limits_{i=1}^na_{ii}x_i^2+\mathop{\sum\limits^n\sum\limits^n}_{i \neq j}a_{ij}x_ix_j\\ \;\;\;\;\;\;\;\;\:\:=\sum\limits_{i=1}^na_{ii}x_i^2+\mathop{\sum\limits^n\sum\limits^n}_{i\ &amp;lt;\ j}(a_{ij}+a_{ji})x_ix_j\)&lt;/span&gt;&lt;br&gt;
被稱爲 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}\)&lt;/span&gt; 的同次2次式。又被叫做關於 &lt;span class=&#34;math inline&#34;&gt;\(x_1,x_2,\cdots,x_n\)&lt;/span&gt; 的&lt;strong&gt;2次型(quadratic form)&lt;/strong&gt;。特別的，當 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 爲對稱矩陣時的2次型：&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}^tA\underline{x}=\sum\limits_{i=1}^na_{ii}x_i^2+2\mathop{\sum\limits^n\sum\limits^n}_{i\ &amp;lt;\ j}a_{ij}x_ix_j\)&lt;/span&gt; 在多元變量分析中十分重要。&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(x=\left( \begin{array}{} x_1\\ x_2 \end{array} \right),\ A=\left( \begin{array}{} a_{11} &amp;amp; a_{12}\\ a_{12} &amp;amp; a_{22} \end{array} \right)\)&lt;/span&gt;, 那麼： &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}^tA\underline{x}=a_{11}x_1^2+a_{22}x_2^2+2a_{12}x_1x_2\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(x=\left( \begin{array}{} x_1\\ x_2\\ x_3 \end{array} \right),\ A=\left( \begin{array}{} a_{11} &amp;amp; a_{12} &amp;amp; a_{13}\\ a_{12} &amp;amp; a_{22} &amp;amp; a_{23}\\ a_{13} &amp;amp; a_{23} &amp;amp; a_{33} \end{array} \right)\)&lt;/span&gt;，那麼： &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}^tA\underline{x}=a_{11}x_1^2+a_{22}x_2^2+a_{33}x_3^2+2a_{12}x_1x_2\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+2a_{13}x_1x_3+2a_{23}x_2x_3\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(x=\left( \begin{array}{} x_1\\ x_2\\ x_3 \end{array} \right), \ A=\left( \begin{array}{} 3 &amp;amp; 2 &amp;amp; 4\\ 6 &amp;amp; 5 &amp;amp; 1\\ -2 &amp;amp; 5 &amp;amp; 8 \end{array} \right)非對稱矩陣\)&lt;/span&gt;，那麼：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}^tA\underline{x}=3x_1^2+5x_2^2+8x_3^2\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+2x_1x_2+4x_1x_3+6x_2x_1\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+x_2x_3-2x_3x_1+5x_3x_2\\ \;\;\;\;\;\;\;\;\;\:=3x_1^2+5x_2^2+8x_3^2\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+(2+6)x_1x_2+(4-2)x_1x_3\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+(5+1)x_2x_3\\ \;\;\;\;\;\;\;\;\;\:=3x_1^2+5x_2^2+8x_3^2\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+8x_1x_2+2x_1x_3+6z_2x_3\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;有式子 &lt;span class=&#34;math inline&#34;&gt;\(3x_1^2-5x_2^2+7x_3^2+8x_1x_2+4x_1x_3-12x_2x_3\)&lt;/span&gt; 如果要將它改寫成 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}^tA\underline{x}\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;是對稱矩陣) 的話，試求 &lt;span class=&#34;math inline&#34;&gt;\(A=\left( \begin{array}{} a_{11} &amp;amp; a_{12} &amp;amp; a_{13}\\ a_{22} &amp;amp; a_{22} &amp;amp; a_{23}\\ a_{33} &amp;amp; a_{32} &amp;amp; a_{33} \end{array} \right)=\left( \begin{array}{} a_{11} &amp;amp; a_{12} &amp;amp; a_{13}\\ a_{12} &amp;amp; a_{22} &amp;amp; a_{23}\\ a_{13} &amp;amp; a_{23} &amp;amp; a_{33} \end{array} \right)\)&lt;/span&gt; 的各個成分。&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 的對角成分：&lt;span class=&#34;math inline&#34;&gt;\(a_{11},a_{22},a_{33}\)&lt;/span&gt; 分別是 &lt;span class=&#34;math inline&#34;&gt;\(x_1^2,x_2^2,x_3^2\)&lt;/span&gt; 的系數： &lt;span class=&#34;math inline&#34;&gt;\(3,-5,7\)&lt;/span&gt;。非對角成分 &lt;span class=&#34;math inline&#34;&gt;\(a_{12}(=a_{21})\)&lt;/span&gt; 等於 &lt;span class=&#34;math inline&#34;&gt;\(x_1x_2\)&lt;/span&gt; 系數的一半：&lt;span class=&#34;math inline&#34;&gt;\(4\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(a_{13}(=a_{31})\)&lt;/span&gt; 等於 &lt;span class=&#34;math inline&#34;&gt;\(x_1x_3\)&lt;/span&gt; 系數的一半:&lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt;,
&lt;span class=&#34;math inline&#34;&gt;\(a_23(=a_{32})\)&lt;/span&gt; 等於 &lt;span class=&#34;math inline&#34;&gt;\(x_2x_3\)&lt;/span&gt; 系數的一半：&lt;span class=&#34;math inline&#34;&gt;\(-6\)&lt;/span&gt;。&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\therefore A=\left( \begin{array}{} 3 &amp;amp; 4 &amp;amp; 2\\ 4 &amp;amp; -5 &amp;amp; -6\\ 2 &amp;amp; -6 &amp;amp; 7 \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記15</title>
      <link>https://wangcc.me/post/2017-03-08/</link>
      <pubDate>Wed, 08 Mar 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/2017-03-08/</guid>
      <description>


&lt;div id=&#34;單位矩陣&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;單位矩陣&lt;/h2&gt;
&lt;p&gt;對角成分全部都是 &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; (此時我們假定有 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 個)，的對角矩陣被叫做&lt;strong&gt;單位矩陣(identity matrix, unit matrix)&lt;/strong&gt;。寫作：
&lt;span class=&#34;math inline&#34;&gt;\(\left( \begin{array}{c} 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\ 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\ 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; \vdots \\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \ddots &amp;amp; 0 \\ 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 1 \end{array} \right)=E_n=I_n\)&lt;/span&gt; 下標 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 常被省略。一般的，將 &lt;span class=&#34;math inline&#34;&gt;\(E_n\)&lt;/span&gt; 從左往右乘以 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 次正方形矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;，的結果和從右往左相乘的結果是相等的： &lt;span class=&#34;math inline&#34;&gt;\(E_nA=AE_n=A\)&lt;/span&gt;。&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;單位矩陣 &lt;span class=&#34;math inline&#34;&gt;\(E=\left( \begin{array}{c} 1 &amp;amp; 0 &amp;amp; 0 \\ 0 &amp;amp; 1 &amp;amp; 0 \\ 0 &amp;amp; 0 &amp;amp; 1 \\ \end{array} \right)\)&lt;/span&gt; 和矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A=\left( \begin{array}{c} a_1 &amp;amp; a_2 &amp;amp; a_3 \\ b_1 &amp;amp; b_2 &amp;amp; b_3 \\ c_1 &amp;amp; c_2 &amp;amp; c_3 \\ \end{array} \right)\)&lt;/span&gt; 的積爲：&lt;span class=&#34;math inline&#34;&gt;\(EA=\left( \begin{array}{c} a_1 &amp;amp; a_2 &amp;amp; a_3 \\ b_1 &amp;amp; b_2 &amp;amp; b_3 \\ c_1 &amp;amp; c_2 &amp;amp; c_3 \\ \end{array} \right)=AE=A\)&lt;/span&gt;，矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 的&lt;strong&gt;所有成分均不變。&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(E_nE_n=E_n\)&lt;/span&gt;。像這樣，自己與自己相乘，結果等於自己的矩陣，被叫做&lt;strong&gt;冪等矩陣(idempotent matrix, 冪等行列「べきとうぎょうれつ」)&lt;/strong&gt;。即，&lt;span class=&#34;math inline&#34;&gt;\(HH(=H^2)=H\)&lt;/span&gt; 成立時，&lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; 是冪等矩陣。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}=E\underline{x}, \lambda\underline{x}=\lambda E\underline{x}\)&lt;/span&gt; 此等式會在後面&lt;strong&gt;特徵值(eigenvalue, 固有値問題)&lt;/strong&gt;時使用。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://winterwang.github.io/post/2017-03-01/&#34;&gt;前一個小節&lt;/a&gt;中的對角矩陣(diagonal matrix) &lt;span class=&#34;math inline&#34;&gt;\(D^{\frac{1}{2}}\)&lt;/span&gt; 則具有這樣的性質： &lt;span class=&#34;math inline&#34;&gt;\(D^{\frac{1}{2}}D^{-\frac{1}{2}}=D^{-\frac{1}{2}}D^{\frac{1}{2}}=E_n\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;逆矩陣-inverse-matrix&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;逆矩陣 inverse matrix&lt;/h2&gt;
&lt;p&gt;如果正方形矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 存在另一個正放心矩陣 &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 使得他們滿足 &lt;span class=&#34;math inline&#34;&gt;\(AX=XA=E\)&lt;/span&gt;，即乘積爲一個單位矩陣，那麼我們說 &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 是 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 的&lt;strong&gt;逆矩陣(inverse matrix)&lt;/strong&gt;，寫作：&lt;span class=&#34;math inline&#34;&gt;\(A^{-1}\)&lt;/span&gt;。可以將上面的連等式改成：&lt;span class=&#34;math inline&#34;&gt;\(AA^{-1}=A^{-1}A=E\)&lt;/span&gt;。&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;如果矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A=\left( \begin{array}{c} a &amp;amp; b \\ c &amp;amp; d \\ \end{array} \right)\)&lt;/span&gt; 的成分滿足： &lt;span class=&#34;math inline&#34;&gt;\(ad -bc \neq 0\)&lt;/span&gt;，那麼有 &lt;span class=&#34;math inline&#34;&gt;\(A^{-1}=\frac{1}{ad-bc}\left( \begin{array}{c} d &amp;amp; -b \\ -c &amp;amp; a \\ \end{array} \right)\)&lt;/span&gt;。&lt;strong&gt;如果， &lt;span class=&#34;math inline&#34;&gt;\(ad-bc=0\)&lt;/span&gt; 那麼我們認爲 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 的逆矩陣不存在。&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;矩陣 &lt;span class=&#34;math inline&#34;&gt;\(P=\left( \begin{array}{c} \cos \theta &amp;amp; -\sin \theta \\ \sin \theta &amp;amp; \cos \theta \\ \end{array} \right)\)&lt;/span&gt; 的逆矩陣 &lt;span class=&#34;math inline&#34;&gt;\(P^{-1}=\left( \begin{array}{c} \cos \theta &amp;amp; \sin \theta \\ -\sin \theta &amp;amp; \cos \theta \\ \end{array} \right)\)&lt;/span&gt; &lt;span class=&#34;diff_alert&#34;&gt;注意此處出現了逆矩陣的逆矩陣爲元矩陣的例子。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;對稱矩陣(symmetric matrix)&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(A=\left( \begin{array}{c} 1 &amp;amp; 2 &amp;amp; 3 \\ 2 &amp;amp; 4 &amp;amp; 5 \\ 3 &amp;amp; 5 &amp;amp; 6 \\ \end{array} \right)\)&lt;/span&gt; 的逆矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A^{-1}=\left( \begin{array}{c} 1 &amp;amp; -3 &amp;amp; 2 \\ -3 &amp;amp; 3 &amp;amp; -1 \\ 2 &amp;amp; -1 &amp;amp; 0 \\ \end{array} \right)\)&lt;/span&gt; &lt;span class=&#34;diff_alert&#34;&gt;注意此處出現了對稱矩陣的逆矩陣還是對稱矩陣的例子。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A=\left( \begin{array}{c} -11 &amp;amp; 2 &amp;amp; 2 \\ -4 &amp;amp; 0 &amp;amp; 1 \\ 6 &amp;amp; -1 &amp;amp; -1 \\ \end{array} \right)\)&lt;/span&gt; 的逆矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A^{-1}=\left( \begin{array}{c} 1 &amp;amp; 0 &amp;amp; 2 \\ 2 &amp;amp; -1 &amp;amp; 3 \\ 4 &amp;amp; 1 &amp;amp; 8 \\ \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;正交矩陣-orthogonal-matrix&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;正交矩陣 orthogonal matrix&lt;/h2&gt;
&lt;p&gt;如果正方形矩陣 &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; 滿足： &lt;span class=&#34;math inline&#34;&gt;\(PP^t=P^tP=E\)&lt;/span&gt; (單位矩陣)；或者滿足 &lt;span class=&#34;math inline&#34;&gt;\(P^t=P^{-1}\)&lt;/span&gt; 時，我們說這個正方形矩陣 &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; 爲&lt;strong&gt;正交矩陣(orthogonal matrix，直交行列「ちょっこうぎょうれつ」)&lt;/strong&gt;。正交矩陣如果用列向量來表示，那麼這些組成正交矩陣的列向量被稱爲&lt;strong&gt;規範正交系(orthonomal system，正規直交系)&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;矩陣 &lt;span class=&#34;math inline&#34;&gt;\(P=\left( \begin{array}{c} \cos \theta &amp;amp; -\sin \theta \\ \sin \theta &amp;amp; \cos \theta \\ \end{array} \right)\)&lt;/span&gt; 是2次的正交矩陣。如果 &lt;span class=&#34;math inline&#34;&gt;\(\underline{p}_1=\left( \begin{array}{c} \cos \theta \\ \sin \theta \\ \end{array} \right), \; \underline{p}_2=\left( \begin{array}{c} -\sin \theta \\ \cos \theta \\ \end{array} \right)\)&lt;/span&gt;，那麼列向量的長度有：&lt;span class=&#34;math inline&#34;&gt;\(\| \underline{p}_1 \|=\| \underline{p}_2 \|=1\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(\underline{p}_1\cdot\underline{p}_2=0\)&lt;/span&gt;。因此組成矩陣 &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; 的兩個列向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{p}_1,\underline{p}_2\)&lt;/span&gt; 構成了一個規範正交系。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;矩陣 &lt;span class=&#34;math inline&#34;&gt;\(P=\left( \begin{array}{c} \frac{1}{\sqrt{3}} &amp;amp; \frac{1}{\sqrt{2}} &amp;amp; \frac{1}{\sqrt{6}} \\ \frac{1}{\sqrt{3}} &amp;amp; -\frac{1}{\sqrt{2}} &amp;amp; \frac{1}{\sqrt{6}} \\ \frac{1}{\sqrt{3}} &amp;amp; 0 &amp;amp; -\frac{2}{\sqrt{6}} \\ \end{array} \right)\)&lt;/span&gt; 是個3次正交矩陣。如果 &lt;span class=&#34;math inline&#34;&gt;\(\underline{p}_1=\left( \begin{array}{c} \frac{1}{\sqrt{3}}\\ \frac{1}{\sqrt{3}}\\ \frac{1}{\sqrt{3}}\\ \end{array} \right), \underline{p}_2=\left( \begin{array}{c} \frac{1}{\sqrt{2}} \\ -\frac{1}{\sqrt{2}} \\ 0 \\ \end{array} \right), \underline{p}_3=\left( \begin{array}{c} \frac{1}{\sqrt{6}}\\ \frac{1}{\sqrt{6}}\\ -\frac{2}{\sqrt{6}}\\ \end{array} \right)\)&lt;/span&gt; 這三個列向量構成了一個規範正交系。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;三角矩陣-triangular-matrix&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;三角矩陣 triangular matrix&lt;/h2&gt;
&lt;p&gt;主對角線的左下部分全部爲 &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; 的正方形矩陣被叫做：&lt;strong&gt;上三角矩陣(upper triangular matrix)&lt;/strong&gt;，右上部分的成分全部爲 &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; 的正方形矩陣被叫做： &lt;strong&gt;下三角矩陣(lower triangular matrix)&lt;/strong&gt;。上三角矩陣，下三角矩陣，統稱爲&lt;strong&gt;三角矩陣&lt;/strong&gt;。有時候左下部分或者右上部分就簡略的只寫一個大的 &lt;span class=&#34;math inline&#34;&gt;\(O\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;類型相同的兩個上三角矩陣的積依然是一個上三角矩陣。兩個類型相同的下三角矩陣的積也依然是一個下三角矩陣。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
上三角矩陣： \left(
\begin{array}{c}
a_{11}&amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n}\\
0     &amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n}\\
\vdots&amp;amp; \ddots &amp;amp; \ddots &amp;amp; \vdots\\
0     &amp;amp; \cdots &amp;amp; 0      &amp;amp; a_{nn}
\end{array}
\right)
=\left(
\begin{array}{c}
a_{11}&amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n}\\
      &amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n}\\
      &amp;amp;        &amp;amp; \ddots &amp;amp; \vdots\\
\Huge{0}  &amp;amp;        &amp;amp;        &amp;amp; a_{nn}
\end{array}
\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
下三角矩陣：\left(
\begin{array}{c}
a_{11}&amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\
a_{21}&amp;amp; a_{22} &amp;amp; \ddots &amp;amp; \vdots \\
\vdots&amp;amp; \cdots &amp;amp; \ddots &amp;amp; 0\\
a_{n1}&amp;amp; a_{n2} &amp;amp; \cdots   &amp;amp; a_{nn}
\end{array}
\right)=\left(
\begin{array}{c}
a_{11}&amp;amp; &amp;amp;&amp;amp;\Huge{0}  \\
a_{21}&amp;amp; a_{22} &amp;amp;  \\
\vdots&amp;amp; \cdots &amp;amp; \ddots &amp;amp;\\
a_{n1}&amp;amp; a_{n2} &amp;amp; \cdots   &amp;amp; a_{nn}
\end{array}
\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\left( \begin{array}{c} 2 &amp;amp; 1 \\ 0 &amp;amp; 8 \\ \end{array} \right), \left( \begin{array}{c} -3 &amp;amp; 0 &amp;amp; 6 \\ 0 &amp;amp; 5 &amp;amp; 2 \\ 0 &amp;amp; 0 &amp;amp; 4 \\ \end{array} \right), \left( \begin{array}{c} 5 &amp;amp; -6 &amp;amp; 3 &amp;amp; 2 \\ 0 &amp;amp; 9 &amp;amp;-2 &amp;amp; 4 \\ 0 &amp;amp; 0 &amp;amp; 3 &amp;amp; 7 \\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 \end{array} \right)\)&lt;/span&gt; 這些都是上三角矩陣。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\left( \begin{array}{c} 2 &amp;amp; 0 \\ 5 &amp;amp; 8 \\ \end{array} \right), \left( \begin{array}{c} -3 &amp;amp; 0 &amp;amp; 0 \\ 8 &amp;amp; 5 &amp;amp; 0 \\ 7 &amp;amp; 2 &amp;amp; 4 \\ \end{array} \right), \left( \begin{array}{c} 5 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\ 2 &amp;amp; 9 &amp;amp; 0 &amp;amp; 0 \\ 3 &amp;amp; 10 &amp;amp; 3 &amp;amp; 0 \\ 5 &amp;amp; 1 &amp;amp; 34 &amp;amp; 0 \end{array} \right)\)&lt;/span&gt; 這些都是下三角矩陣。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;階梯形矩陣-echelon-matrix&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;階梯形矩陣 echelon matrix&lt;/h2&gt;
&lt;p&gt;如下所示，第1行，第2行，第3行，行數增加的同時，左側的成分中 &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; 的個數跟着增加的矩陣被叫做&lt;strong&gt;階梯形矩陣(echelon matrix)&lt;/strong&gt;。 &lt;span class=&#34;math inline&#34;&gt;\(\#\)&lt;/span&gt; 表示非 &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; 的數， &lt;span class=&#34;math inline&#34;&gt;\(*\)&lt;/span&gt; 表示任意數。&lt;span class=&#34;math inline&#34;&gt;\(\#\)&lt;/span&gt; 的個數，或者說此矩陣的非零向量的個數被定義爲這個矩陣的&lt;strong&gt;階數 (rank)&lt;/strong&gt;。階梯形矩陣的階數記爲： &lt;span class=&#34;math inline&#34;&gt;\(rank(A)\)&lt;/span&gt;。零矩陣 &lt;span class=&#34;math inline&#34;&gt;\(O\)&lt;/span&gt; 的階數： &lt;span class=&#34;math inline&#34;&gt;\(rank(O)=0\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\left(
\begin{array}{c}
\# &amp;amp; * &amp;amp; * &amp;amp; * &amp;amp; * &amp;amp; * &amp;amp; * &amp;amp; *  &amp;amp; *\\
0 &amp;amp; \# &amp;amp; * &amp;amp; * &amp;amp; * &amp;amp; * &amp;amp; * &amp;amp; *  &amp;amp; * \\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \# &amp;amp; * &amp;amp; * &amp;amp; * &amp;amp; *  &amp;amp; * \\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0  &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \# &amp;amp; *  \\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0  &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0  &amp;amp; 0
\end{array}
\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(A=\left( \begin{array}{c} 2 &amp;amp; 5 &amp;amp; 6 &amp;amp; 9\\ 0 &amp;amp; 5 &amp;amp; -1 &amp;amp; 4\\ 0 &amp;amp; 0 &amp;amp; 5 &amp;amp; 0\\ \end{array} \right)， rank(A)=3\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(B=\left( \begin{array}{c} 4 &amp;amp; 0 &amp;amp; 6 &amp;amp; 0\\ 0 &amp;amp; 5 &amp;amp; 0 &amp;amp; 4\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 5\\ \end{array} \right)， rank(B)=3\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(C=\left( \begin{array}{c} 2 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\ 0 &amp;amp; 0 &amp;amp; -7 &amp;amp; 4\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; -1\\ \end{array} \right)， rank(C)=3\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(D=\left( \begin{array}{c} 4 &amp;amp; 0 &amp;amp; 6 &amp;amp; 0\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 4\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\ \end{array} \right), rank(D)=2\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(F=\left( \begin{array}{c} 0 &amp;amp; 2 &amp;amp; 0 &amp;amp; 0\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\ \end{array} \right), rank(F)=1\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(O=\left( \begin{array}{c} 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\ \end{array} \right), rank(O)=0\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記14</title>
      <link>https://wangcc.me/post/2017-03-01/</link>
      <pubDate>Wed, 01 Mar 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/2017-03-01/</guid>
      <description>


&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;updated: 2017-03-07&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;對稱矩陣&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;對稱矩陣&lt;/h2&gt;

&lt;div class=&#34;theorem&#34;&gt;
&lt;span id=&#34;thm:unnamed-chunk-1&#34; class=&#34;theorem&#34;&gt;&lt;strong&gt;Theorem 1  (symmetric matrix)  &lt;/strong&gt;&lt;/span&gt;矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 如果完全和它的轉置矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A^t\)&lt;/span&gt; 相同，即：&lt;span class=&#34;math inline&#34;&gt;\(A=A^t\)&lt;/span&gt; 成立時，這樣的正方形矩陣被稱爲&lt;strong&gt;對稱矩陣(symmetric matrix)&lt;/strong&gt;。對稱矩陣的成分是以主對角線(main diagonal)對稱的。
&lt;/div&gt;

&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(A=\left( \begin{array}{c} 4 &amp;amp; 3 &amp;amp; 2 &amp;amp; 1 \\ 3 &amp;amp; 5 &amp;amp; 6 &amp;amp; 7 \\ 2 &amp;amp; 6 &amp;amp; 8 &amp;amp; 9 \\ 1 &amp;amp; 7 &amp;amp; 9 &amp;amp; 0 \end{array} \right)\)&lt;/span&gt; 是典型的4次對稱矩陣。&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;數學&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;物理&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;化學&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;數學&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(0.72\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(0.62\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;物理&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(0.72\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(-0.55\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;化學&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(0.62\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(-0.55\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr /&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;上表是幾名學生的數學，物理，化學成績得分的相關系數。&lt;br&gt;
如果提取出數字的部分，左右用圓括號括起來，會得到這樣一個矩陣：&lt;span class=&#34;math inline&#34;&gt;\(R=\left( \begin{array}{c} 1 &amp;amp; 0.72 &amp;amp; 0.62 \\ 0.72 &amp;amp; 1 &amp;amp; -0.55 \\ 0.62 &amp;amp; -0.55 &amp;amp; 1 \\ \end{array} \right)\)&lt;/span&gt; 這樣類型的矩陣被特別的稱爲&lt;strong&gt;相關矩陣(correlation matrix)&lt;/strong&gt;。類似相關矩陣這樣的明確爲對稱矩陣的情況下，常常像下面這樣簡略的記左下或者右上部分：
&lt;span class=&#34;math display&#34;&gt;\[\left(
\begin{array}{c}
1 &amp;amp;  &amp;amp;  \\
0.72 &amp;amp; 1 &amp;amp;  \\
0.62 &amp;amp; -0.55 &amp;amp; 1 \\
\end{array}
\right)， \left(
\begin{array}{c}
1 &amp;amp; 0.72 &amp;amp; 0.62 \\
 &amp;amp; 1 &amp;amp; -0.55 \\
 &amp;amp;  &amp;amp; 1 \\
\end{array}
\right)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;下面的對稱矩陣，對角成分是&lt;strong&gt;方差(variance, 分散)&lt;/strong&gt;，非對角成分是&lt;strong&gt;協方差(covariance, 共分散)&lt;/strong&gt;，被稱爲&lt;strong&gt;方差協方差矩陣(variance-covariance matrix, 分散共分散行列)&lt;/strong&gt;。&lt;br&gt;
&lt;span class=&#34;math display&#34;&gt;\[\sum=\left(
\begin{array}{c}
\sigma_{1}^2 &amp;amp; \sigma_{2}   &amp;amp; \cdots &amp;amp; \sigma_{1n} \\
\sigma_{12}  &amp;amp; \sigma_{2}^2 &amp;amp; \cdots &amp;amp; \sigma_{2n} \\
\vdots       &amp;amp; \vdots       &amp;amp; \ddots &amp;amp; \vdots      \\
\sigma_{1n}  &amp;amp; \sigma_{2}   &amp;amp; \cdots &amp;amp; \sigma_{n}^2
\end{array}
\right), S=\left(
\begin{array}{c}
s_{1}^2 &amp;amp; s_{2}   &amp;amp; \cdots &amp;amp; s_{1n} \\
s_{12}  &amp;amp; s_{2}^2 &amp;amp; \cdots &amp;amp; s_{2n} \\
\vdots  &amp;amp; \vdots  &amp;amp; \ddots &amp;amp; \vdots  \\
s_{1n}  &amp;amp; s_{2}   &amp;amp; \cdots &amp;amp; s_{n}^2
\end{array}
\right)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;矩陣&lt;span class=&#34;math inline&#34;&gt;\(X=\left( \begin{array}{c} x_{11} &amp;amp; x_{12} &amp;amp; x_{13} \\ x_{21} &amp;amp; x_{22} &amp;amp; x_{23} \\ \end{array} \right)\)&lt;/span&gt; ，那麼，&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(XX^t=\left( \begin{array}{c} x_{11} &amp;amp; x_{12} &amp;amp; x_{13} \\ x_{21} &amp;amp; x_{22} &amp;amp; x_{23} \\ \end{array} \right)\left( \begin{array}{c} x_{11} &amp;amp; x_{12} \\ x_{12} &amp;amp; x_{22} \\ x_{13} &amp;amp; x_{13} \end{array} \right)\\ \;\;\;\;\;\;\;=\left( \begin{array}{c} x_{11}^2+x_{12}^2+x_{13}^2 &amp;amp; x_{11}x_{21}+x_{12}x_{22}+x_{13}x_{23} \\ x_{21}x_{11}+x_{22}x_{12}+x_{23}x_{13} &amp;amp; x_{21}^2+x_{22}^2+x_{23}^2 \\ \end{array} \right)\)&lt;/span&gt; 是一個對稱矩陣。&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(X^tX=\left( \begin{array}{c} x_{11}^2+x_{21}^2 &amp;amp; x_{11}x_{12}+x_{21}x_{22} &amp;amp; x_{11}x_{13}+x_{21}x_{23} \\ x_{12}x_{11}+x_{22}x_{21} &amp;amp; x_{12}^2+x_{22}^2 &amp;amp; x_{12}x_{13}+x_{22}x_{23} \\ x_{13}x_{11}+x_{23}x_{21} &amp;amp; x_{13}x_{12}+x_{23}x_{22} &amp;amp; x_{13}^2+x_{23}^2 \end{array} \right)\)&lt;/span&gt; 也是一個對稱矩陣。 &lt;br&gt;
且，他們的&lt;strong&gt;跡(trace)&lt;/strong&gt;也是一樣的，均爲 &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 各個成分的平方和：
&lt;span class=&#34;math display&#34;&gt;\[tr(XX^t)=tr(X^tX)=x_{11}^2+x_{12}^2+x_{13}^2+x_{21}^2+x_{22}^2+x_{23}^2\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;對角矩陣&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;對角矩陣&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;非對角成分(off-diagonal element)&lt;/strong&gt;均爲零 &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; 的正方形矩陣被稱爲&lt;strong&gt;對角矩陣(diagonal matrix)&lt;/strong&gt;。寫成如下形式：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\left(
\begin{array}{c}
a_{11} &amp;amp; 0  &amp;amp; 0 &amp;amp; \cdots  &amp;amp; 0\\
0 &amp;amp; a_{22}  &amp;amp; 0 &amp;amp; \cdots  &amp;amp; 0\\
0 &amp;amp; 0  &amp;amp; a_{33} &amp;amp; \cdots  &amp;amp; 0\\
0 &amp;amp; \cdots  &amp;amp; 0 &amp;amp; \ddots  &amp;amp; 0\\
0 &amp;amp; 0  &amp;amp; \cdots &amp;amp; 0       &amp;amp;  a_{nn}
\end{array}
\right)=D_n=\Delta_n\\=diag(a_{11},a_{22},a_{33},\cdots,a_{nn})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;這樣的矩陣也常把左下部分右上部分的非對角成分用一個大的 &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; 來表示：
&lt;span class=&#34;math display&#34;&gt;\[
\left(
    \begin{array}{ccccc}
    a_{11}                         \\
      &amp;amp; a_{22}            &amp;amp;   &amp;amp; \Huge0 \\
      &amp;amp;               &amp;amp; a_{33}          \\
      &amp;amp; \Huge 0       &amp;amp;   &amp;amp; \ddots           \\
      &amp;amp;               &amp;amp;   &amp;amp;      &amp;amp; a_{nn}
    \end{array}
    \right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;下面也是一個對角矩陣的例子：
&lt;span class=&#34;math display&#34;&gt;\[\left(
\begin{array}{c}
\sqrt{a_{11}} &amp;amp; 0  &amp;amp; 0 &amp;amp; \cdots  &amp;amp; 0\\
0 &amp;amp; \sqrt{a_{22}}  &amp;amp; 0 &amp;amp; \cdots  &amp;amp; 0\\
0 &amp;amp; 0  &amp;amp; \sqrt{a_{33}} &amp;amp; \cdots  &amp;amp; 0\\
0 &amp;amp; \cdots  &amp;amp; 0 &amp;amp; \ddots  &amp;amp; 0\\
0 &amp;amp; 0  &amp;amp; \cdots &amp;amp; 0       &amp;amp;  \sqrt{a_{nn}}
\end{array}
\right)=D_n^{\frac{1}{2}}=\Delta_n^{\frac{1}{2}}\\=diag(\sqrt{a_{11}},\sqrt{a_{22}},\sqrt{a_{33}},\cdots,\sqrt{a_{nn}})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;對角成分也可以是分母非零的分數：
&lt;span class=&#34;math display&#34;&gt;\[\left(
\begin{array}{c}
1/a_{11} &amp;amp; 0  &amp;amp; 0 &amp;amp; \cdots  &amp;amp; 0\\
0 &amp;amp; 1/a_{22}  &amp;amp; 0 &amp;amp; \cdots  &amp;amp; 0\\
0 &amp;amp; 0  &amp;amp; 1/a_{33} &amp;amp; \cdots  &amp;amp; 0\\
0 &amp;amp; \cdots  &amp;amp; 0 &amp;amp; \ddots  &amp;amp; 0\\
0 &amp;amp; 0  &amp;amp; \cdots &amp;amp; 0       &amp;amp;  1/a_{nn}
\end{array}
\right)=D_n^{-1}=\Delta_n^{-1}\\=diag(a_{11}^{-1},a_{22}^{-1},a_{33}^{-1},\cdots,a_{nn}^{-1})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;當然如下的例子也是對角矩陣，默認根號內爲正：
&lt;span class=&#34;math display&#34;&gt;\[\left(
\begin{array}{c}
\frac{1}{\sqrt{a_{11}}} &amp;amp; 0  &amp;amp; 0 &amp;amp; \cdots  &amp;amp; 0\\
0 &amp;amp; \frac{1}{\sqrt{a_{22}}}  &amp;amp; 0 &amp;amp; \cdots  &amp;amp; 0\\
0 &amp;amp; 0  &amp;amp; \frac{1}{\sqrt{a_{33}}} &amp;amp; \cdots  &amp;amp; 0\\
0 &amp;amp; \cdots  &amp;amp; 0 &amp;amp; \ddots  &amp;amp; 0\\
0 &amp;amp; 0  &amp;amp; \cdots &amp;amp; 0       &amp;amp;  \frac{1}{\sqrt{a_{nn}}}
\end{array}
\right)=D_n^{-\frac{1}{2}}=\Delta_n^{-\frac{1}{2}}\\=diag(\frac{1}{\sqrt{a_{11}}},\frac{1}{\sqrt{a_{22}}},\frac{1}{\sqrt{a_{33}}},\cdots,\frac{1}{\sqrt{a_{nn}}})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;當然，上述對角矩陣之間具有這樣的關系：&lt;span class=&#34;math inline&#34;&gt;\(D_n^{\frac{1}{2}}D_n^{\frac{1}{2}}=D_n\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(D_n^{-\frac{1}{2}}D_n^{-\frac{1}{2}}=D_n^{-1}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 或者向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}\)&lt;/span&gt; 與對角矩陣 &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; 從左向右乘時，&lt;span class=&#34;math inline&#34;&gt;\(DA, D\underline{x}\)&lt;/span&gt; 的第 &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; 行成分是：&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 或 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}\)&lt;/span&gt; 的第 &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; 行乘以 &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; 的第 &lt;span class=&#34;math inline&#34;&gt;\((i,i)\)&lt;/span&gt; 成分。例如：
&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(D=\left( \begin{array}{c} \lambda_1 &amp;amp; 0 &amp;amp; 0\\ 0 &amp;amp; \lambda_2 &amp;amp; 0\\ 0 &amp;amp; 0 &amp;amp; \lambda_3\\ \end{array} \right), A=\left( \begin{array}{c} a_1 &amp;amp; a_2 &amp;amp; a_3 \\ b_1 &amp;amp; b_2 &amp;amp; b_3 \\ c_1 &amp;amp; c_2 &amp;amp; c_3 \\ \end{array} \right),\\ \underline{x}=\left( \begin{array}{c} x_1\\ x_2\\ x_3\\ \end{array} \right)\)&lt;/span&gt; 時，&lt;span class=&#34;math inline&#34;&gt;\(DA=\left( \begin{array}{c} \lambda_1a_1 &amp;amp; \lambda_1a_2 &amp;amp; \lambda_1a_3 \\ \lambda_2b_1 &amp;amp; \lambda_2b_2 &amp;amp; \lambda_2b_3 \\ \lambda_3c_1 &amp;amp; \lambda_3c_2 &amp;amp; \lambda_3c_3 \\ \end{array} \right) \\ D\underline{x}=\left( \begin{array}{c} \lambda_1x_1\\ \lambda_2x_2\\ \lambda_3x_3\\ \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記13</title>
      <link>https://wangcc.me/post/2017-02-28/</link>
      <pubDate>Tue, 28 Feb 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/2017-02-28/</guid>
      <description>


&lt;div id=&#34;連立一次方程式與矩陣向量的積&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;連立一次方程式與矩陣向量的積&lt;/h2&gt;
&lt;p&gt;連立一次方程式可以改寫爲&lt;strong&gt;矩陣與向量的積形成的向量&lt;/strong&gt;的形式。特別的，以連立方程式的系數作成分的矩陣被叫做&lt;strong&gt;系數矩陣(coefficient matrix)&lt;/strong&gt;。當我們看到連立方程式，應該能立刻條件反射地聯想到其對應的&lt;strong&gt;矩陣和向量的積&lt;/strong&gt;。&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\begin{align} \left\{ \begin{array}{rr}  a_1+2a_2+3a_3 = 3\\ 2a_1+4a_2+5a_3 = 5\\ 3a_1+5a_2+6a_3 = 7 \end{array} \right. \end{align}\)&lt;/span&gt; 可以改寫成 &lt;span class=&#34;math inline&#34;&gt;\(\left( \begin{array}{c} 1 &amp;amp; 2 &amp;amp; 3 \\ 2 &amp;amp; 4 &amp;amp; 5 \\ 3 &amp;amp; 5 &amp;amp; 6 \end{array} \right)\left( \begin{array}{c} a_1\\ a_2\\ a_3 \end{array} \right)=\left( \begin{array}{c} 3\\ 5\\ 7 \end{array} \right)\)&lt;/span&gt; 的形式。&lt;br&gt;
如果把等號右邊的列向量寫到&lt;strong&gt;系數矩陣&lt;/strong&gt;的右側，形成的矩陣被叫做&lt;strong&gt;擴大系數矩陣(augmented coefficient)&lt;/strong&gt;：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\left( \begin{array}{c} 1 &amp;amp; 2 &amp;amp; 3 &amp;amp; 3 \\ 2 &amp;amp; 4 &amp;amp; 5 &amp;amp; 5 \\ 3 &amp;amp; 5 &amp;amp; 6 &amp;amp; 7 \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\begin{align} \left\{ \begin{array}{rr} a_{11}x_1+a_{12}x_2+a_{13}x_3=0\\ a_{21}x_1+a_{22}x_2+a_{23}x_3=0\\ \end{array} \right. \end{align}\)&lt;/span&gt; 可以改寫成 &lt;span class=&#34;math inline&#34;&gt;\(\left( \begin{array}{c} a_{11} &amp;amp; a_{12} &amp;amp; a_{13} \\ a_{21} &amp;amp; a_{22} &amp;amp; a_{23} \\ \end{array} \right)\left( \begin{array}{c} x_1\\ x_2\\ x_3 \end{array} \right)=\left( \begin{array}{c} 0\\ 0\\ \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\begin{align} \left\{ \begin{array}{rr} (5-\lambda)x_1+ x_2+ x_3 = 0\\  x_1+(3-\lambda)x_2+ x_3 = 0\\  x_1+ x_2+(3-\lambda)x_3 = 0 \end{array} \right. \end{align}\)&lt;/span&gt; 可以改寫爲 &lt;span class=&#34;math inline&#34;&gt;\(\left( \begin{array}{c} 5-\lambda &amp;amp; 1 &amp;amp; 1 \\ 1 &amp;amp; 3-\lambda &amp;amp; 1 \\ 1 &amp;amp; 1 &amp;amp; 3-\lambda \end{array} \right)\left( \begin{array}{c} x_1\\ x_2\\ x_3 \end{array} \right)=\left( \begin{array}{c} 0\\ 0\\ 0 \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;矩形矩陣&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;矩形矩陣&lt;/h2&gt;
&lt;p&gt;列數行數不相等的矩陣，被稱爲&lt;strong&gt;矩形矩陣(rectangular matrix)&lt;/strong&gt;。特別的行數 &lt;span class=&#34;math inline&#34;&gt;\(&amp;gt;\)&lt;/span&gt; 列數的矩陣被叫做&lt;strong&gt;垂直型矩形矩陣&lt;/strong&gt;。行數 &lt;span class=&#34;math inline&#34;&gt;\(&amp;lt;\)&lt;/span&gt; 列數的矩陣被叫做&lt;strong&gt;水平型矩形矩陣&lt;/strong&gt;。多元變量分析時，數據常常被加工稱爲垂直型矩形矩陣的形式。&lt;/p&gt;
&lt;hr /&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;個体&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;体重 &lt;span class=&#34;math inline&#34;&gt;\((kg)\)&lt;/span&gt;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;身長 &lt;span class=&#34;math inline&#34;&gt;\((cm)\)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;安倍さん&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(53\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(157\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;伊藤さん&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(67\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(172\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;植村さん&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(49\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(163\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;江川さん&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(80\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(178\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;小野さん&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(74\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(181\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr /&gt;
&lt;ul&gt;
&lt;li&gt;5人的體重和身高數據被表示爲上面的表格： &lt;br&gt;
如果只提取出表格中的數字寫成垂直型矩形矩陣： &lt;span class=&#34;math inline&#34;&gt;\(\left(  \begin{array}{c} 53 &amp;amp; 157 \\ 67 &amp;amp; 172 \\ 49 &amp;amp; 163 \\ 80 &amp;amp; 178 \\ 74 &amp;amp; 181 \\  \end{array}  \right)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;正方形矩陣&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;正方形矩陣&lt;/h2&gt;
&lt;p&gt;行數和列數相等的矩陣被稱爲&lt;strong&gt;正方形矩陣(sqare matrix)&lt;/strong&gt;。一個正方形的矩陣如果類型爲 &lt;span class=&#34;math inline&#34;&gt;\((n,n)\)&lt;/span&gt;，又被叫做是 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 次正方矩陣或者 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 次矩陣。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[A_{n\times n}= \left(
\begin{array}{c}
a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n} \\
a_{21} &amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n} \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
a_{n1} &amp;amp; a_{n2} &amp;amp; \cdots &amp;amp; a_{nn}
\end{array}
\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;從左上角往右下角方向劃一條對角線，這條對角線的名稱爲&lt;strong&gt;主對角線(main diagonal)&lt;/strong&gt;。主對角線上有的成分 &lt;span class=&#34;math inline&#34;&gt;\(a_{11},a_{22},\cdots, a_{nn}\)&lt;/span&gt;，被叫做&lt;strong&gt;對角成分(diagonal element)&lt;/strong&gt;。其餘的成分被叫做&lt;strong&gt;非對角成分(off-diagonal element)&lt;/strong&gt;。對角成分的和被叫做是該矩陣的跡(trace/spur)，寫作 &lt;span class=&#34;math inline&#34;&gt;\(tr(A)=\sum\limits_{i=1}^na_{ii}\)&lt;/span&gt;
。
&lt;span class=&#34;math inline&#34;&gt;\(A=\left( \begin{array}{c} 1 &amp;amp; 2 &amp;amp; 3 \\ 4 &amp;amp; 5 &amp;amp; 6 \\ 7 &amp;amp; 8 &amp;amp; 9 \end{array} \right)\)&lt;/span&gt; 是 &lt;span class=&#34;math inline&#34;&gt;\(3\)&lt;/span&gt; 次正方形矩陣， &lt;span class=&#34;math inline&#34;&gt;\(tr(A)=1+5+9=15\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;矩陣轉置&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;矩陣轉置&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\((m,n)\)&lt;/span&gt; 型矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A_{m\times n}=(a_{ij})\)&lt;/span&gt; 的行與列互相對調，被叫做&lt;strong&gt;轉置(transpose)&lt;/strong&gt;，形成的新 &lt;span class=&#34;math inline&#34;&gt;\((n,m)\)&lt;/span&gt; 型矩陣，被叫做 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 的&lt;strong&gt;轉置矩陣 (transposed matrix)&lt;/strong&gt; ： &lt;span class=&#34;math inline&#34;&gt;\((a_{ji})\)&lt;/span&gt; 有多種標記方式：&lt;span class=&#34;math inline&#34;&gt;\(A^t, A^\prime, A^T, ^TA\)&lt;/span&gt; 等，我們今後統一使用 &lt;span class=&#34;math inline&#34;&gt;\(A^t\)&lt;/span&gt;。轉置矩陣具有如下的性質：&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((A^t)^t=A\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((AB)^t=B^tA^t\)&lt;/span&gt; &lt;span class=&#34;diff_alert&#34;&gt;注意： 不是&lt;span class=&#34;math inline&#34;&gt;\(A^tB^t\)&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((A+B)^t=A^t+B^t\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((kA)^t=kA^t\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;爲標量 scalar)&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;練習&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;練習&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(A=\left( \begin{array}{c} 1 &amp;amp; 2 &amp;amp; 3 \\ 4 &amp;amp; 5 &amp;amp; 6 \\ 7 &amp;amp; 8 &amp;amp; 9 \end{array} \right)\)&lt;/span&gt; 的轉置矩陣爲：&lt;span class=&#34;math inline&#34;&gt;\(A^t=\left( \begin{array}{c} 1 &amp;amp; 4 &amp;amp; 7 \\ 2 &amp;amp; 5 &amp;amp; 8 \\ 3 &amp;amp; 6 &amp;amp; 9 \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(B=\left( \begin{array}{c} 1 &amp;amp; 2 &amp;amp; 3 &amp;amp; 4\\ 5 &amp;amp; 6 &amp;amp; 7 &amp;amp; 8\\ \end{array} \right)\)&lt;/span&gt; 的轉置矩陣爲：&lt;span class=&#34;math inline&#34;&gt;\(B^t=\left( \begin{array}{c} 1 &amp;amp; 5 \\ 2 &amp;amp; 6 \\ 3 &amp;amp; 7 \\ 4 &amp;amp; 8 \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記12</title>
      <link>https://wangcc.me/post/2017-02-22/</link>
      <pubDate>Wed, 22 Feb 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/2017-02-22/</guid>
      <description>


&lt;div id=&#34;矩陣乘法運算&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;矩陣乘法運算&lt;/h2&gt;
&lt;div id=&#34;矩陣乘法定義&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;矩陣乘法定義&lt;/h4&gt;

&lt;div class=&#34;theorem&#34;&gt;
&lt;span id=&#34;thm:unnamed-chunk-1&#34; class=&#34;theorem&#34;&gt;&lt;strong&gt;Theorem 1  (matrix multiplication)  &lt;/strong&gt;&lt;/span&gt;兩個矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A, B\)&lt;/span&gt; ，只有 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 的列數和 &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; 的行數相等(這種特徵又被稱爲：矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A,B\)&lt;/span&gt; &lt;strong&gt;可整合的&lt;/strong&gt;，conformable)時，才有定義：&lt;span class=&#34;math inline&#34;&gt;\(AB\)&lt;/span&gt;。&lt;span class=&#34;math inline&#34;&gt;\(AB\)&lt;/span&gt; 則爲新的矩陣，類型爲 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 的行數， &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;的列數。即：&lt;span class=&#34;math inline&#34;&gt;\(A_{k\times l}, \; B_{m\times n}\)&lt;/span&gt; 且 &lt;span class=&#34;math inline&#34;&gt;\(l=m\)&lt;/span&gt; 時才能計算乘積: &lt;span class=&#34;math inline&#34;&gt;\(AB_{k\times n}\)&lt;/span&gt;。
&lt;/div&gt;

&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(A_{2\times3}=\left( \begin{array}{c} 4 &amp;amp; 6 &amp;amp; 8\\ 2 &amp;amp; 1 &amp;amp; 3\\ \end{array} \right),\; B_{3\times2}=\left( \begin{array}{c} 0 &amp;amp; 8\\ 2 &amp;amp; -1\\ 9 &amp;amp; 4 \\ \end{array} \right)\)&lt;/span&gt; 時，&lt;br&gt;
“&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;的列數” &lt;span class=&#34;math inline&#34;&gt;\(=\)&lt;/span&gt; “&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; 的行數” &lt;span class=&#34;math inline&#34;&gt;\(= 3\)&lt;/span&gt;，因此積 &lt;span class=&#34;math inline&#34;&gt;\(AB\)&lt;/span&gt; 被定義，類型是 &lt;span class=&#34;math inline&#34;&gt;\((2,2)\)&lt;/span&gt; &lt;br&gt;
“&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;的列數” &lt;span class=&#34;math inline&#34;&gt;\(=\)&lt;/span&gt; “&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 的行數” &lt;span class=&#34;math inline&#34;&gt;\(= 2\)&lt;/span&gt;，因此積 &lt;span class=&#34;math inline&#34;&gt;\(BA\)&lt;/span&gt; 被定義，類型是 &lt;span class=&#34;math inline&#34;&gt;\((3,3)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(C_{3\times2}=\left( \begin{array}{c} 4 &amp;amp; 2 \\ 5 &amp;amp; 6 \\ 7 &amp;amp; 3 \end{array} \right),\; D_{2\times4}=\left( \begin{array}{c} 2 &amp;amp; 0 &amp;amp; 9 &amp;amp; -1 \\ 4 &amp;amp; 7 &amp;amp; 6 &amp;amp; 5 \\ \end{array} \right)\)&lt;/span&gt; 時， &lt;br&gt;
“&lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;的列數” &lt;span class=&#34;math inline&#34;&gt;\(=\)&lt;/span&gt; “&lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; 的行數” &lt;span class=&#34;math inline&#34;&gt;\(= 2\)&lt;/span&gt;，因此積 &lt;span class=&#34;math inline&#34;&gt;\(CD\)&lt;/span&gt; 被定義，類型是 &lt;span class=&#34;math inline&#34;&gt;\((3,4)\)&lt;/span&gt; &lt;br&gt;
“&lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt;的列數”&lt;span class=&#34;math inline&#34;&gt;\(= 4\)&lt;/span&gt;，“&lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; 的行數” &lt;span class=&#34;math inline&#34;&gt;\(= 3\)&lt;/span&gt;，因此積 &lt;span class=&#34;math inline&#34;&gt;\(DC\)&lt;/span&gt; 不被定義，&lt;span class=&#34;math inline&#34;&gt;\(DC\)&lt;/span&gt; 不可整合。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;矩陣的積的向量表達形式&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;矩陣的積的向量表達形式&lt;/h4&gt;
&lt;p&gt;矩陣也可以被看做是一個個相同類型（大小，方向）的向量組成。那麼當，下面&lt;span class=&#34;math inline&#34;&gt;\(A, B\)&lt;/span&gt;兩個矩陣滿足：&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 的行向量的維度，和&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;的列向量的維度相等時，&lt;span class=&#34;math inline&#34;&gt;\(AB\)&lt;/span&gt;被定義。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(A=\left( \begin{array}{c} \underline{a}_1^t\\ \underline{a}_2^t\\ \vdots\\ \underline{a}_k^t \end{array} \right), \; B=(\underline{b}_1,\underline{b}_2,\cdots,\underline{b}_n)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(AB=A(\underline{b}_1,\underline{b}_2,\cdots,\underline{b}_n)=(A\underline{b}_1,A\underline{b}_2,\cdots,A\underline{b}_n)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;或者：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(AB=\left( \begin{array}{c} \underline{a}_1^t\\ \underline{a}_2^t\\ \vdots\\ \underline{a}_k^t \end{array} \right)B=\left( \begin{array}{c} \underline{a}_1^tB\\ \underline{a}_2^tB\\ \vdots\\ \underline{a}_k^tB \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;特殊情況當&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;僅有一個行向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}^t\)&lt;/span&gt; 時，&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(AB=\underline{a}^tB=\underline{a}^t(\underline{b}_1,\underline{b}_2,\cdots,\underline{b}_n)=(\underline{a}^t\underline{b}_1,\underline{a}^t\underline{b}_2,\cdots,\underline{a}^t\underline{b}_n)\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;矩陣的積的成分&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;矩陣的積的成分&lt;/h4&gt;

&lt;div class=&#34;theorem&#34;&gt;
&lt;span id=&#34;thm:unnamed-chunk-2&#34; class=&#34;theorem&#34;&gt;&lt;strong&gt;Theorem 2  (matrix multiplication component)  &lt;/strong&gt;&lt;/span&gt;兩個矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A_{k\times l}, \; B_{m\times n}\)&lt;/span&gt; 的積有被定義時，矩陣 &lt;span class=&#34;math inline&#34;&gt;\(AB_{k\times n}\)&lt;/span&gt; 的任意成分&lt;span class=&#34;math inline&#34;&gt;\((i,j)\)&lt;/span&gt;，被定義爲：&lt;strong&gt;“&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 的第 &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; 行向量與 &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;的第 &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; 列向量的內積”&lt;/strong&gt;。
&lt;/div&gt;

&lt;p&gt;故：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(AB\)&lt;/span&gt; 的 &lt;span class=&#34;math inline&#34;&gt;\((1,1)\)&lt;/span&gt; 成分是，“&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 的第 &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; 行向量與 &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;的第 &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; 列向量的內積”&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(AB\)&lt;/span&gt; 的 &lt;span class=&#34;math inline&#34;&gt;\((1,2)\)&lt;/span&gt; 成分是，“&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 的第 &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; 行向量與 &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;的第 &lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt; 列向量的內積”&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\vdots\)&lt;/span&gt; &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(AB\)&lt;/span&gt; 的 &lt;span class=&#34;math inline&#34;&gt;\((k,n)\)&lt;/span&gt; 成分是，“&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 的第 &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; 行向量與 &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;的第 &lt;span class=&#34;math inline&#34;&gt;\(ns\)&lt;/span&gt; 列向量的內積”&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;練習&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;練習&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(A=\left( \begin{array}{c} 1 &amp;amp; 2 \\ 3 &amp;amp; 4 \\ \end{array} \right), B=\left( \begin{array}{c} 4 &amp;amp; 1 \\ 5 &amp;amp; 2 \\ \end{array} \right)\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(AB=\left( \begin{array}{c} 1\times4+2\times5 &amp;amp; 1\times1+2\times2 \\ 3\times4+4\times5 &amp;amp; 3\times1+4\times2 \\ \end{array} \right)\\ \;\;\;\;\;=\left( \begin{array}{c} 14 &amp;amp; 5 \\ 32 &amp;amp; 11 \\ \end{array} \right)\)&lt;/span&gt; &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(BA=\left( \begin{array}{c} 4\times1+1\times3 &amp;amp; 4\times2+1\times4 \\ 5\times1+2\times3 &amp;amp; 5\times2+2\times4 \\ \end{array} \right)\\ \;\;\;\;\;=\left( \begin{array}{c} 7 &amp;amp; 12 \\ 11 &amp;amp; 18 \\ \end{array} \right)\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;diff_alert&#34;&gt;注意： &lt;span class=&#34;math inline&#34;&gt;\(AB\neq BA\)&lt;/span&gt;&lt;/span&gt;
&lt;br&gt;
另外：&lt;span class=&#34;math inline&#34;&gt;\(AA=\left( \begin{array}{c} 1\times1+2\times3 &amp;amp; 1\times2+2\times4 \\ 3\times1+4\times3 &amp;amp; 3\times2+4\times4 \\ \end{array} \right)\\ \;\;\;\;=\left( \begin{array}{c} 7 &amp;amp; 10 \\ 15 &amp;amp; 22 \\ \end{array} \right)=AA^2\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(A=\left( \begin{array}{c} 1 &amp;amp; 2 \\ -2 &amp;amp; -4 \\ \end{array} \right), B=\left( \begin{array}{c} 6 &amp;amp; -2 \\ -3 &amp;amp; 1 \\ \end{array} \right)\)&lt;/span&gt; &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(AB=\left( \begin{array}{c} 1\times6+2\times(-3) &amp;amp; 1\times(-2)+2\times1 \\ (-2)\times6+(-4)\times(-3) &amp;amp; (-2)\times(-2)+(-4)\times1 \\ \end{array} \right)\\ \;\;\;\;=\left( \begin{array}{c} 0 &amp;amp; 0 \\ 0 &amp;amp; 0 \\ \end{array} \right)=\Large 0\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;diff_alert&#34;&gt;這裏出現了非零矩陣相乘爲&lt;strong&gt;零矩陣 &lt;span class=&#34;math inline&#34;&gt;\(\large 0\)&lt;/span&gt;&lt;/strong&gt;的例子。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(A_{2\times3}=\left( \begin{array}{c} a_{11} &amp;amp; a_{12} &amp;amp; a_{13}\\ a_{21} &amp;amp; a_{22} &amp;amp; a_{23}\\ \end{array} \right)=\left( \begin{array}{c} \underline{a}_1^t\\ \underline{a}_2^t\\ \end{array} \right)=(\underline{c}_1,\underline{c}_2,\underline{c}_3); \\ B_{3\times2}=\left( \begin{array}{c} b_{11} &amp;amp; b_{12}\\ b_{21} &amp;amp; b_{22}\\ b_{31} &amp;amp; b_{32}\\ \end{array} \right)=(\underline{b}_1,\underline{b}_2)=\left( \begin{array}{c} \underline{d}^t_1\\ \underline{d}^t_2\\ \underline{d}^t_3\\ \end{array} \right)\)&lt;/span&gt; &lt;br&gt;
&lt;span class=&#34;diff_alert&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(AB_{2\times2}\)&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;math inline&#34;&gt;\(=\left( \begin{array}{c} \sum\limits_{k=1}^3a_{1k}b_{k1} &amp;amp;\sum\limits_{k=1}^3a_{1k}b_{k2} \\ \sum\limits_{k=1}^3a_{2k}b_{k1} &amp;amp;\sum\limits_{k=1}^3a_{2k}b_{k2} \\ \end{array} \right)\\=\left( \begin{array}{c} \underline{a}^t_1\underline{b}_1 &amp;amp; \underline{a}^t_1\underline{b}_2 \\ \underline{a}^t_2\underline{b}_1 &amp;amp; \underline{a}^t_2\underline{b}_2 \\ \end{array} \right)\)&lt;/span&gt; &lt;br&gt;
&lt;span class=&#34;diff_alert&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(BA_{3\times3}\)&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;math inline&#34;&gt;\(=\left( \begin{array}{c} \sum\limits_{k=1}^2b_{1k}a_{k1} &amp;amp; \sum\limits_{k=1}^2b_{1k}a_{k2} &amp;amp; \sum\limits_{k=1}^2b_{1k}a_{k3} \\ \sum\limits_{k=1}^2b_{2k}a_{k1} &amp;amp; \sum\limits_{k=1}^2b_{2k}a_{k2} &amp;amp; \sum\limits_{k=1}^2b_{2k}a_{k3} \\ \sum\limits_{k=1}^2b_{3k}a_{k1} &amp;amp; \sum\limits_{k=1}^2b_{3k}a_{k2} &amp;amp; \sum\limits_{k=1}^2b_{3k}a_{k3} \end{array} \right)\\ =\left( \begin{array}{c} \underline{d}^t_1\underline{c}_1 &amp;amp; \underline{d}^t_1\underline{c}_2 &amp;amp; \underline{d}^t_1\underline{c}_3 \\ \underline{d}^t_2\underline{c}_1 &amp;amp; \underline{d}^t_2\underline{c}_2 &amp;amp; \underline{d}^t_2\underline{c}_3 \\ \underline{d}^t_3\underline{c}_1 &amp;amp; \underline{d}^t_3\underline{c}_2 &amp;amp; \underline{d}^t_3\underline{c}_3 \\ \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}=\left( \begin{array}{c} x_1\\ x_2\\ \vdots\\ x_n\\ \end{array} \right)\)&lt;/span&gt; 時， &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}\underline{x}^t=\left( \begin{array}{c} x_1^2 &amp;amp; x_1x_2 &amp;amp; \cdots &amp;amp; x_1x_n \\ x_2x_1 &amp;amp; x_2^2 &amp;amp; \cdots &amp;amp; x_2x_n \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ x_nx_1 &amp;amp; x_nx_2 &amp;amp; \cdots &amp;amp; x_n^2 \\ \end{array} \right)\)&lt;/span&gt;，&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}^t\underline{x}=\sum\limits_{i=1}^nx_i^2\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=\left( \begin{array}{c} a_1\\ a_2\\ a_3\\ \end{array} \right), \underline{b}=\left( \begin{array}{c} b_1\\ b_2\\ b_3\\ \end{array} \right), \underline{c}=\left( \begin{array}{c} c_1\\ c_2\\ c_3\\ \end{array} \right)\\ A=\left( \begin{array}{c} a_1 &amp;amp; b_1 &amp;amp; c_1 \\ a_2 &amp;amp; b_2 &amp;amp; c_2 \\ a_3 &amp;amp; b_3 &amp;amp; c_3 \\ \end{array} \right)\)&lt;/span&gt; &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}\underline{a}^t+\underline{b}\underline{b}^t+\underline{c}\underline{c}^t\\ \;\;=\left( \begin{array}{c} a_1^2 &amp;amp; a_1a_2 &amp;amp; a_1a_3 \\ a_2a_1 &amp;amp; a_2^2 &amp;amp; a_2a_3 \\ a_3a_1 &amp;amp; a_3a_2 &amp;amp; a_3^2 \\ \end{array} \right)\\ \;\;\;\;\;\;+\left( \begin{array}{c} b_1^2 &amp;amp; b_1b_2 &amp;amp; b_1b_3 \\ b_2b_1 &amp;amp; b_2^2 &amp;amp; b_2b_3 \\ b_3b_1 &amp;amp; b_3b_2 &amp;amp; b_3^2 \\ \end{array} \right)\\ \;\;\;\;\;\;+\left( \begin{array}{c} c_1^2 &amp;amp; c_1c_2 &amp;amp; c_1c_3 \\ c_2c_1 &amp;amp; c_2^2 &amp;amp; c_2c_3 \\ c_3c_1 &amp;amp; c_3c_2 &amp;amp; c_3^2 \\ \end{array} \right)\\ =\left( \begin{array}{c} a_1 &amp;amp; b_1 &amp;amp; c_1 \\ a_2 &amp;amp; b_2 &amp;amp; c_2 \\ a_3 &amp;amp; b_3 &amp;amp; c_3 \\ \end{array} \right)\left( \begin{array}{c} a_1 &amp;amp; a_2 &amp;amp; a_3 \\ b_1 &amp;amp; b_2 &amp;amp; b_3 \\ c_1 &amp;amp; c_2 &amp;amp; c_3 \\ \end{array} \right)=AA^t\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}=\left( \begin{array}{c} x_1\\ x_2\\ \vdots\\ x_n\\ \end{array} \right), \; \underline{\frac{1}{n}}=\left( \begin{array}{c} \frac{1}{n}\\ \frac{1}{n}\\ \cdots\\ \frac{1}{n}\\ \end{array} \right)\)&lt;/span&gt; 時， &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}^t\underline{\frac{1}{n}}=\sum\limits_{i=1}^nx_i\cdot\frac{1}{n}=\bar{x}\)&lt;/span&gt; &lt;span class=&#34;diff_alert&#34;&gt;(&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}\)&lt;/span&gt; 的&lt;strong&gt;平均值&lt;/strong&gt;)&lt;/span&gt;
&lt;br&gt;
將這樣的 &lt;span class=&#34;math inline&#34;&gt;\(\bar{x}\)&lt;/span&gt; 寫成 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 個的橫向量：&lt;span class=&#34;math inline&#34;&gt;\((\bar{x},\bar{x},\cdots,\bar{x})\)&lt;/span&gt; &lt;br&gt;
這個向量如果寫成展開的形式就是：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\((\bar{x},\bar{x},\cdots,\bar{x})=(\underline{x}^t\underline{\frac{1}{n}}, \underline{x}^t\underline{\frac{1}{n}}, \cdots,\underline{x}^t\underline{\frac{1}{n}})\\ \;\;\;\;\;\;=\underline{x}^t(\underline{\frac{1}{n}},\underline{\frac{1}{n}},\cdots,\underline{\frac{1}{n}})\\ \;\;\;\;\;\;=(x_1,x_2,\cdots,x_n)\left( \begin{array}{c} \frac{1}{n} &amp;amp; \frac{1}{n} &amp;amp; \cdots &amp;amp; \frac{1}{n} \\ \frac{1}{n} &amp;amp; \frac{1}{n} &amp;amp; \cdots &amp;amp; \frac{1}{n} \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ \frac{1}{n} &amp;amp; \frac{1}{n} &amp;amp; \cdots &amp;amp; \frac{1}{n} \\ \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;矩陣積的性質&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;矩陣積的性質&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;矩陣 &lt;span class=&#34;math inline&#34;&gt;\(AB\)&lt;/span&gt; 有定義時， &lt;span class=&#34;math inline&#34;&gt;\(BA\)&lt;/span&gt; 並不一定就有定義。無法整合時就沒有定義。&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(AB=BA\)&lt;/span&gt; 時，&lt;span class=&#34;math inline&#34;&gt;\(A, B\)&lt;/span&gt; 被稱爲&lt;strong&gt;可交換 commutative&lt;/strong&gt;，&lt;strong&gt;交換可能矩陣&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;當 &lt;span class=&#34;math inline&#34;&gt;\(AB, BA\)&lt;/span&gt; 都有定義時，也不一定就滿足 &lt;span class=&#34;math inline&#34;&gt;\(AB=BA\)&lt;/span&gt;。也就是說，多數情況下， &lt;span class=&#34;math inline&#34;&gt;\(AB\neq BA\)&lt;/span&gt;。爲了區分二者，&lt;span class=&#34;math inline&#34;&gt;\(AB\)&lt;/span&gt; 被稱爲 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 從右往左乘 &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; &lt;strong&gt;(postmultiplication of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; by &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;)&lt;/strong&gt;，&lt;span class=&#34;math inline&#34;&gt;\(BA\)&lt;/span&gt; 被稱爲 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 從左往右乘 &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; &lt;strong&gt;(postmultiplication of &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; by &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;)&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;相似的， &lt;span class=&#34;math inline&#34;&gt;\(AC=BC\)&lt;/span&gt; 時，應該理解爲： 等式&lt;span class=&#34;math inline&#34;&gt;\(A=B\)&lt;/span&gt;兩邊同時從右往左乘 &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(CA=CB\)&lt;/span&gt; 就是：等式&lt;span class=&#34;math inline&#34;&gt;\(A=B\)&lt;/span&gt;兩邊同時從左往右乘&lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;。&lt;/li&gt;
&lt;li&gt;即使 &lt;span class=&#34;math inline&#34;&gt;\(A\neq\Large 0\)&lt;/span&gt; 且 &lt;span class=&#34;math inline&#34;&gt;\(B\neq\Large 0\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(AB\)&lt;/span&gt; 也有可能等於 &lt;span class=&#34;math inline&#34;&gt;\(\Large 0\)&lt;/span&gt; (零矩陣)，此時我們說， &lt;span class=&#34;math inline&#34;&gt;\(A, B\)&lt;/span&gt; 是&lt;strong&gt;零因子 (zero divisor)&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記11</title>
      <link>https://wangcc.me/post/2017-02-21/</link>
      <pubDate>Tue, 21 Feb 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/2017-02-21/</guid>
      <description>


&lt;div id=&#34;矩陣的定義&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;矩陣的定義&lt;/h2&gt;

&lt;div class=&#34;theorem&#34;&gt;
&lt;p&gt;&lt;span id=&#34;thm:unnamed-chunk-1&#34; class=&#34;theorem&#34;&gt;&lt;strong&gt;Theorem 1  (matrix)  &lt;/strong&gt;&lt;/span&gt;將&lt;span class=&#34;math inline&#34;&gt;\(m\times n\)&lt;/span&gt; 個數 &lt;span class=&#34;math inline&#34;&gt;\(a_{ij} (i=1,2,\cdots,m; j=1,2,\cdots,n)\)&lt;/span&gt;, 寫成縱 &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; 行， 橫 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 列的長方形或者正方形，左右用圓括號或者方括號包含在內。我們稱之爲 &lt;span class=&#34;math inline&#34;&gt;\(m\times n\)&lt;/span&gt; &lt;strong&gt;矩陣(matrix)&lt;/strong&gt;，或者 &lt;span class=&#34;math inline&#34;&gt;\((m, n)\)&lt;/span&gt; 矩陣。 &lt;span class=&#34;math inline&#34;&gt;\(m\times n\)&lt;/span&gt; 或者 &lt;span class=&#34;math inline&#34;&gt;\((m,n)\)&lt;/span&gt; 被稱爲是這個矩陣的類型。我們常用大寫字母來標記一個矩陣，如下面的矩陣我們標記爲 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;。 如果要特別明示矩陣的類型，可以寫作 &lt;span class=&#34;math inline&#34;&gt;\(\mathop{A}_{m\times n}, \mathop{A}_{(m, n)}, \; A(m\times n)\)&lt;/span&gt;。兩個矩陣如果行數相等，列數也相等，我們稱他們爲類型相同的矩陣。構成矩陣的一個個數 &lt;span class=&#34;math inline&#34;&gt;\(a_{11},a_{12},\cdots,a_{mn}\)&lt;/span&gt; 被叫做矩陣的成分(component, element, entry)。&lt;/p&gt;
第&lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;行，第&lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;列交叉的地方的成分，&lt;span class=&#34;math inline&#34;&gt;\(a_{ij}\)&lt;/span&gt; 被叫做 &lt;span class=&#34;math inline&#34;&gt;\((i,j)\)&lt;/span&gt; 成分。矩陣有時候也會寫成 &lt;span class=&#34;math inline&#34;&gt;\(A=(a_{ij})\)&lt;/span&gt;
&lt;/div&gt;

&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\left( \begin{array}{c} a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1j} &amp;amp; \cdots &amp;amp; a_{1n}\\ a_{21} &amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2j} &amp;amp; \cdots &amp;amp; a_{2n}\\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ a_{i1} &amp;amp; a_{i2} &amp;amp; \cdots &amp;amp; a_{ij} &amp;amp; \cdots &amp;amp; a_{in}\\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ a_{m1} &amp;amp; a_{m2} &amp;amp; \cdots &amp;amp; a_{mj} &amp;amp; \cdots &amp;amp; a_{mn}\\ \end{array} \right), \\ \left[ \begin{array}{c} a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1j} &amp;amp; \cdots &amp;amp; a_{1n}\\ a_{21} &amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2j} &amp;amp; \cdots &amp;amp; a_{2n}\\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ a_{i1} &amp;amp; a_{i2} &amp;amp; \cdots &amp;amp; a_{ij} &amp;amp; \cdots &amp;amp; a_{in}\\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ a_{m1} &amp;amp; a_{m2} &amp;amp; \cdots &amp;amp; a_{mj} &amp;amp; \cdots &amp;amp; a_{mn}\\ \end{array} \right]\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;矩陣 &lt;span class=&#34;math inline&#34;&gt;\(\mathop{A}_{m\times n}\)&lt;/span&gt; 可以被看做是：&lt;br&gt;
以第&lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;行爲成分的行向量 &lt;span class=&#34;math inline&#34;&gt;\((a_{11},a_{12},\cdots,a_{1n})=\underline{b}_1^t\)&lt;/span&gt;；&lt;br&gt;
以第&lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt;行爲成分的行向量 &lt;span class=&#34;math inline&#34;&gt;\((a_{21},a_{22},\cdots,a_{2n})=\underline{b}_2^t\)&lt;/span&gt;；&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\vdots\)&lt;/span&gt;&lt;br&gt;
以第 &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; 行爲成分的行向量 &lt;span class=&#34;math inline&#34;&gt;\((a_{m1},a_{m2},\cdots,a_{mn})=\underline{b}_m^t\)&lt;/span&gt;；&lt;br&gt;
爲成分組成的列向量：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\left( \begin{array}{c} \underline{b}_1^t\\ \underline{b}_2^t\\ \vdots\\ \underline{b}_m^t\\ \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;類似的，矩陣 &lt;span class=&#34;math inline&#34;&gt;\(\mathop{A}_{m\times n}\)&lt;/span&gt; 可以被看做是：&lt;br&gt;
以第&lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;列爲成分的列向量： &lt;span class=&#34;math inline&#34;&gt;\(\left( \begin{array}{c} a_{11}\\ a_{21}\\ \vdots\\ a_{m1}\\ \end{array} \right)=\underline{c}_1\)&lt;/span&gt; &lt;br&gt;
以第&lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt;列爲成分的列向量：&lt;span class=&#34;math inline&#34;&gt;\(\left( \begin{array}{c} a_{12}\\ a_{22}\\ \vdots\\ a_{m2}\\ \end{array} \right)=\underline{c}_2\)&lt;/span&gt; &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\vdots\)&lt;/span&gt;&lt;br&gt;
以第&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;列爲成分的列向量：&lt;span class=&#34;math inline&#34;&gt;\(\left( \begin{array}{c} a_{1n}\\ a_{2n}\\ \vdots\\ a_{mn}\\ \end{array} \right)=\underline{c}_n\)&lt;/span&gt; &lt;br&gt;
爲成分組成的行向量：&lt;span class=&#34;math inline&#34;&gt;\((\underline{c}_1,\underline{c}_2,\cdots,\underline{c}_n)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;矩陣的運算和零矩陣&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;矩陣的運算，和零矩陣&lt;/h2&gt;
&lt;div id=&#34;矩陣的和與差&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;矩陣的和與差&lt;/h3&gt;

&lt;div class=&#34;theorem&#34;&gt;
&lt;span id=&#34;thm:unnamed-chunk-2&#34; class=&#34;theorem&#34;&gt;&lt;strong&gt;Theorem 2  (matrix plus or minus)  &lt;/strong&gt;&lt;/span&gt;類型(type)相同的矩陣之間的加減法運算，被定義爲各個對應成分的加減法結果作成分的矩陣。
&lt;/div&gt;

&lt;p&gt;對於&lt;span class=&#34;math inline&#34;&gt;\(A=\left( \begin{array}{c} a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n}\\ a_{21} &amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n}\\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ a_{m1} &amp;amp; a_{m2} &amp;amp; \cdots &amp;amp; a_{mn}\\ \end{array} \right),\\ B=\left( \begin{array}{c} b_{11} &amp;amp; b_{12} &amp;amp; \cdots &amp;amp; b_{1n}\\ b_{21} &amp;amp; b_{22} &amp;amp; \cdots &amp;amp; b_{2n}\\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ b_{m1} &amp;amp; b_{m2} &amp;amp; \cdots &amp;amp; b_{mn}\\ \end{array} \right)\)&lt;/span&gt; &lt;br&gt;
有：&lt;span class=&#34;math inline&#34;&gt;\(A\pm B=\left( \begin{array}{c} a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n}\\ a_{21} &amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n}\\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ a_{m1} &amp;amp; a_{m2} &amp;amp; \cdots &amp;amp; a_{mn}\\ \end{array} \right)\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\pm \left( \begin{array}{c} b_{11} &amp;amp; b_{12} &amp;amp; \cdots &amp;amp; b_{1n}\\ b_{21} &amp;amp; b_{22} &amp;amp; \cdots &amp;amp; b_{2n}\\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ b_{m1} &amp;amp; b_{m2} &amp;amp; \cdots &amp;amp; b_{mn}\\ \end{array} \right)\\ \;\;\;\;\;\;\;\;\;\;=\left( \begin{array}{c} a_{11}\pm b_{11} &amp;amp; a_{12}\pm b_{12} &amp;amp; \cdots &amp;amp; a_{1n}\pm b_{1n}\\ a_{21}\pm b_{21} &amp;amp; a_{22}\pm b_{22} &amp;amp; \cdots &amp;amp; a_{2n}\pm b_{2n}\\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ a_{m1}\pm b_{m1} &amp;amp; a_{m2}\pm b_{m2} &amp;amp; \cdots &amp;amp; a_{mn}\pm b_{mn}\\ \end{array} \right)(復号同順)\)&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(A=\left(  \begin{array}{c}  9 &amp;amp; 3 &amp;amp; 1\\  -2 &amp;amp; 5 &amp;amp; 8\\  \end{array}  \right)， B=\left(  \begin{array}{c}  4 &amp;amp; 2 &amp;amp; 1\\  3 &amp;amp; -3 &amp;amp; 5\\  \end{array}  \right)\)&lt;/span&gt; 那麼&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(A+B = \left(  \begin{array}{c}  9+4 &amp;amp; 3+2 &amp;amp; 1+1\\  -2+3 &amp;amp; 5+(-3) &amp;amp; 8+5\\  \end{array}  \right)=\left(  \begin{array}{c}  13 &amp;amp; 5 &amp;amp; 2\\  1 &amp;amp; 2 &amp;amp; 13\\  \end{array}  \right)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;用1.中的矩陣運算：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(A-B=\left(  \begin{array}{c}  9-4 &amp;amp; 3-2 &amp;amp; 1-1\\  -2-3 &amp;amp; 5-(-3) &amp;amp; 8-5\\  \end{array}  \right)=\left(  \begin{array}{c}  5 &amp;amp; 1 &amp;amp; 0\\  -5 &amp;amp; 8 &amp;amp; 3\\  \end{array}  \right)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;矩陣的相等&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;矩陣的相等&lt;/h3&gt;

&lt;div class=&#34;theorem&#34;&gt;
&lt;span id=&#34;thm:unnamed-chunk-3&#34; class=&#34;theorem&#34;&gt;&lt;strong&gt;Theorem 3  (matrix equal)  &lt;/strong&gt;&lt;/span&gt;類型相同的兩個矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A,B\)&lt;/span&gt;，如果他們對應的所有成分，一一相等，我們說這兩個矩陣是相等的。即：&lt;span class=&#34;math inline&#34;&gt;\(A=B\)&lt;/span&gt;。
&lt;/div&gt;

&lt;p&gt;對於&lt;span class=&#34;math inline&#34;&gt;\(A=\left( \begin{array}{c} a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n}\\ a_{21} &amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n}\\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ a_{m1} &amp;amp; a_{m2} &amp;amp; \cdots &amp;amp; a_{mn}\\ \end{array} \right),\\ B=\left( \begin{array}{c} b_{11} &amp;amp; b_{12} &amp;amp; \cdots &amp;amp; b_{1n}\\ b_{21} &amp;amp; b_{22} &amp;amp; \cdots &amp;amp; b_{2n}\\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ b_{m1} &amp;amp; b_{m2} &amp;amp; \cdots &amp;amp; b_{mn}\\ \end{array} \right)\)&lt;/span&gt;&lt;br&gt;
如果有：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(a_{11}=b_{11},a_{12}=b_{12},\cdots,a_{mn}=b_{mn}\)&lt;/span&gt;&lt;br&gt;
那麼 &lt;span class=&#34;math inline&#34;&gt;\(A=B\)&lt;/span&gt;。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;零矩陣&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;零矩陣&lt;/h3&gt;

&lt;div class=&#34;theorem&#34;&gt;
&lt;span id=&#34;thm:unnamed-chunk-4&#34; class=&#34;theorem&#34;&gt;&lt;strong&gt;Theorem 4  (zero matrix)  &lt;/strong&gt;&lt;/span&gt;所有的成分均爲數字 &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; 的 &lt;span class=&#34;math inline&#34;&gt;\(m\times n\)&lt;/span&gt; 矩陣，&lt;br&gt;
(共有　&lt;span class=&#34;math inline&#34;&gt;\(m\times n\)&lt;/span&gt; 個零。)&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\left( \begin{array}{c} 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0\\ 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0\\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0\\ \end{array} \right)\)&lt;/span&gt;&lt;br&gt;
被稱爲&lt;strong&gt;零矩陣(zero matrix, null matrix)&lt;/strong&gt;。寫作：&lt;span class=&#34;math inline&#34;&gt;\(\large 0, \mathop{\large 0}_{m\times n}, \mathop{\large 0}_{(m,n)}\)&lt;/span&gt;。要注意與標量的 &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; 區分。
&lt;/div&gt;

&lt;/div&gt;
&lt;div id=&#34;矩陣的標量倍數運算&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;矩陣的標量倍數運算&lt;/h3&gt;

&lt;div class=&#34;theorem&#34;&gt;
&lt;span id=&#34;thm:unnamed-chunk-5&#34; class=&#34;theorem&#34;&gt;&lt;strong&gt;Theorem 5  (scalar times)  &lt;/strong&gt;&lt;/span&gt;矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 的所有的成分，均乘以一個標量 &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;，獲得新的矩陣的過程被稱爲矩陣的標量倍數運算。 寫作 &lt;span class=&#34;math inline&#34;&gt;\(kA\)&lt;/span&gt;。
&lt;/div&gt;

&lt;p&gt;對於 &lt;span class=&#34;math inline&#34;&gt;\(A=\left( \begin{array}{c} a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n}\\ a_{21} &amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n}\\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ a_{m1} &amp;amp; a_{m2} &amp;amp; \cdots &amp;amp; a_{mn}\\ \end{array} \right)\)&lt;/span&gt;，&lt;br&gt;
有：&lt;span class=&#34;math inline&#34;&gt;\(kA = k\left( \begin{array}{c} a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n}\\ a_{21} &amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n}\\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ a_{m1} &amp;amp; a_{m2} &amp;amp; \cdots &amp;amp; a_{mn}\\ \end{array} \right)\\ =\left( \begin{array}{c} ka_{11} &amp;amp; ka_{12} &amp;amp; \cdots &amp;amp; ka_{1n}\\ ka_{21} &amp;amp; ka_{22} &amp;amp; \cdots &amp;amp; ka_{2n}\\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ ka_{m1} &amp;amp; ka_{m2} &amp;amp; \cdots &amp;amp; ka_{mn}\\ \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;特別的，當 &lt;span class=&#34;math inline&#34;&gt;\(k=-1\)&lt;/span&gt; 時， &lt;span class=&#34;math inline&#34;&gt;\((-1)A=-A\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(k=0\)&lt;/span&gt; 時，&lt;span class=&#34;math inline&#34;&gt;\(0A=\Large 0\)&lt;/span&gt;。注意 &lt;span class=&#34;math inline&#34;&gt;\(\Large 0\)&lt;/span&gt; 是與 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 類型相同的零矩陣，而非標量 &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;。&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;對 &lt;span class=&#34;math inline&#34;&gt;\(A=\left( \begin{array}{c} a_{11} &amp;amp; a_{12} &amp;amp; a_{13}\\ a_{21} &amp;amp; a_{22} &amp;amp; a_{23}\\ a_{31} &amp;amp; a_{32} &amp;amp; a_{33}\\ \end{array} \right)\)&lt;/span&gt;， &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(kA=\left( \begin{array}{c} ka_{11} &amp;amp; ka_{12} &amp;amp; ka_{13}\\ ka_{21} &amp;amp; ka_{22} &amp;amp; ka_{23}\\ ka_{31} &amp;amp; ka_{32} &amp;amp; ka_{33}\\ \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;對 &lt;span class=&#34;math inline&#34;&gt;\(B=\left( \begin{array}{c} 1 &amp;amp; -2 &amp;amp; 3\\ -4 &amp;amp; 5 &amp;amp; -6\\ \end{array} \right)\)&lt;/span&gt;，&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(7B=\left( \begin{array}{c} 7\times1 &amp;amp; 7\times(-2) &amp;amp; 7\times3\\ 7\times(-4) &amp;amp; 7\times5 &amp;amp; 7\times(-6)\\ \end{array} \right)\\ \;\;\;\;=\left( \begin{array}{c} 7 &amp;amp; -14 &amp;amp; 21\\ -28 &amp;amp; 35 &amp;amp; -42\\ \end{array} \right)\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(-B=\left( \begin{array}{c} -1 &amp;amp; 2 &amp;amp; -3\\ 4 &amp;amp; -5 &amp;amp; 6\\ \end{array} \right)\)&lt;/span&gt;；&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(0B=\left( \begin{array}{c} 0 &amp;amp; 0 &amp;amp; 0\\ 0 &amp;amp; 0 &amp;amp; 0\\ \end{array} \right)=\mathop{\large 0}_{2\times3}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記10</title>
      <link>https://wangcc.me/post/2017-02-19/</link>
      <pubDate>Mon, 20 Feb 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/2017-02-19/</guid>
      <description>


&lt;div id=&#34;向量的內積-inner-product&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;向量的內積 (inner product)&lt;/h2&gt;

&lt;div class=&#34;theorem&#34;&gt;
&lt;span id=&#34;thm:unnamed-chunk-1&#34; class=&#34;theorem&#34;&gt;&lt;strong&gt;Theorem 1  (vectors inner product)  &lt;/strong&gt;&lt;/span&gt;向量的&lt;strong&gt;內積&lt;/strong&gt;運算，僅限定於維度相同的兩個向量之間。一個向量爲橫向量寫在左側，一個向量爲列向量寫在右側，兩個向量的相對應成分一一相乘，然後將各成分乘積相加的過程，我們稱之爲內積(inner product, scalar product)運算。內積運算結果通常不會是向量，而是標量(scalar)，或正或負，或爲零。向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}\)&lt;/span&gt; 與向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{b}\)&lt;/span&gt; 的內積寫作：&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}^t\underline{b}, \underline{b}^t\underline{a}\)&lt;/span&gt; 或者寫作： &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}\cdot\underline{b}, (\underline{a},\underline{b}), &amp;lt;\underline{a},\underline{b}&amp;gt;\)&lt;/span&gt;。內積爲 &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; 的向量我們稱他們爲正交向量(orthogonal)，寫作：&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}\perp\underline{b}\)&lt;/span&gt;。
內積，與和記號: &lt;span class=&#34;math inline&#34;&gt;\(\sum\)&lt;/span&gt; 有緊密聯系。我們常常會把 &lt;span class=&#34;math inline&#34;&gt;\(\sum\)&lt;/span&gt; 式子/量寫成向量的內積形式。
&lt;/div&gt;

&lt;div id=&#34;練習&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;練習&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;列向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=\left( \begin{array}{c} a_1\\ a_2\\ a_3 \end{array} \right), \underline{b}=\left( \begin{array}{c} b_1\\ b_2\\ b_3 \end{array} \right)\)&lt;/span&gt; 的內積：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}^t\underline{b}=(a_1,a_2,a_3)\left( \begin{array}{c} b_1\\ b_2\\ b_3 \end{array} \right)=a_1b_1+a_2b_2+a_3b_3\\=\sum\limits_{i=1}^3a_ib_i=\sum\limits_{i=1}^3b_ia_i=\underline{b}^t\underline{a}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;橫向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=(a_1,a_2,a_3), \underline{b}=(b_1,b_2,b_3)\)&lt;/span&gt; 的內積：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}\underline{b}^t=(a_1,a_2,a_3)\left( \begin{array}{c} b_1\\ b_2\\ b_3 \end{array} \right)=a_1b_1+a_2b_2+a_3b_3\\=\sum\limits_{i=1}^3a_ib_i=\sum\limits_{i=1}^3b_ia_i=\underline{b}\underline{a}^t\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;完全相同的兩個列向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}=\left( \begin{array}{c} x_1\\ x_2\\ x_3 \end{array} \right),\;\underline{x}=\left( \begin{array}{c} x_1\\ x_2\\ x_3 \end{array} \right)\)&lt;/span&gt; 的內積：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}^t\underline{x}=(x_1,x_2,x_3)\left( \begin{array}{c} x_1\\ x_2\\ x_3 \end{array} \right)\\=x_1^2+x_2^2+x_3^2=\sum\limits_{i=1}^3x_i\cdot x_i=\sum\limits_{i=1}^3x_i^2\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;完全相同的兩個橫向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{y}=(y_1,y_2,y_3), \underline{y}=(y_1,y_2,y_3)\)&lt;/span&gt; 的內積：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{y}\underline{y}^t=(y_1,y_2,y_3)\left( \begin{array}{c} y_1\\ y_2\\ y_3 \end{array} \right)\\=y_1^2+y_2^2+y_3^2=\sum\limits_{i=1}^3y_i\cdot y_i=\sum\limits_{i=1}^3y_i^2\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=(2,0,-1), \underline{b}=(4,-2,8)\)&lt;/span&gt; 的內積：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}\underline{b}^t=(2,0,-1)\left( \begin{array}{c} 4\\ -2\\ 8 \end{array} \right)=2\times4+0\times(-2)+(-1)\times8=0\)&lt;/span&gt; &lt;br&gt;
因此我們稱這兩個向量正交。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{1}=\left( \begin{array}{c} 1\\ 1\\ 1 \end{array} \right), \underline{x}=\left( \begin{array}{c} x_1\\ x_2\\ x_3 \end{array} \right)\)&lt;/span&gt; 時：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{1}^t\underline{x}=1\cdot x_1+1\cdot x_2+1\cdot x_3 =\sum\limits_{i=1}^3x_i=\underline{x}^t\underline{1}\)&lt;/span&gt; &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{1}^t\underline{1}=\sum\limits_{i=1}^31\cdot 1=3\)&lt;/span&gt; &lt;br&gt;
前者的內積與後者內積的商： &lt;span class=&#34;math inline&#34;&gt;\(\frac{\underline{1}^t\underline{x}}{\underline{1}^t\underline{1}}=\frac{x_1+x_2+x_3}{3}\)&lt;/span&gt; 我們在統計學中用 &lt;span class=&#34;math inline&#34;&gt;\(\bar{x}\)&lt;/span&gt; (平均值) 來標記。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;問題-如果向量-underlinea-underlineb-有內積-請問有沒有所謂的外積-outer-product&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;問題： 如果，向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}, \underline{b}\)&lt;/span&gt; 有內積， 請問有沒有所謂的外積 (outer product) ？&lt;/h5&gt;
&lt;/div&gt;
&lt;div id=&#34;回答-有不過僅限於3維度的向量&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;回答： 有。不過，僅限於3維度的向量：&lt;/h5&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=\left( \begin{array}{c} a_1\\ a_2\\ a_3 \end{array} \right), \underline{b}=\left( \begin{array}{c} b_1\\ b_2\\ b_3 \end{array} \right)\)&lt;/span&gt; 的外積，我們用 &lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt; 來表示，寫作： &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}\times\underline{b}\)&lt;/span&gt;。 其運算被定義爲：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}\times\underline{b}=\left( \begin{array}{c} a_2b_3-a_3b_2\\ a_3b_1-a_1b_3\\ a_1b_2-a_2b_1 \end{array}\right)\)&lt;/span&gt;。與內積不同的是，外積運算的結果仍然是&lt;span class=&#34;math inline&#34;&gt;\(3\)&lt;/span&gt;維度的向量。外積有如下的性質：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}\times\underline{b}=-\underline{b}\times\underline{a}\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;向量的長度-length&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;向量的長度 (length)&lt;/h2&gt;

&lt;div class=&#34;theorem&#34;&gt;
&lt;p&gt;&lt;span id=&#34;thm:unnamed-chunk-2&#34; class=&#34;theorem&#34;&gt;&lt;strong&gt;Theorem 2  (vector length)  &lt;/strong&gt;&lt;/span&gt;向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}\)&lt;/span&gt; 的內積 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}^t\underline{a}\)&lt;/span&gt; 的平方根中，非負的量，我們稱之爲向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}\)&lt;/span&gt; 的&lt;strong&gt;長度&lt;/strong&gt;或者&lt;strong&gt;大小&lt;/strong&gt;。也就是：&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\underline{a}^t\underline{a}}\)&lt;/span&gt;。記作：&lt;span class=&#34;math inline&#34;&gt;\(\| \underline{a} \|\)&lt;/span&gt;。&lt;/p&gt;
兩個向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}, \underline{b}\)&lt;/span&gt; 類型(type：大小，維度)相同時，他們的差 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}-\underline{b}\)&lt;/span&gt; 依然是向量，這個新向量的長度爲：&lt;span class=&#34;math inline&#34;&gt;\(\| \underline{a}-\underline{b} \| = \sqrt{(\underline{a}-\underline{b})^t(\underline{a}-\underline{b})}\)&lt;/span&gt;
&lt;/div&gt;

&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}=\left( \begin{array}{c} x_1\\ x_2\\ x_3 \end{array} \right)\)&lt;/span&gt; 的長度爲： &lt;span class=&#34;math inline&#34;&gt;\(\| \underline{x} \| =\sqrt{\underline{x}^t\underline{x}}=\sqrt{x_1^2+x_2^2+x_3^2}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=(a_1,a_2,a_3)\)&lt;/span&gt; 的長度爲： &lt;span class=&#34;math inline&#34;&gt;\(\| \underline{a} \| =\sqrt{\underline{a}\underline{a}^t}=\sqrt{a_1^2+a_2^2+a_3^2}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;兩個向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}, \underline{b}\)&lt;/span&gt; 的長度和內積有這樣的關系：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(-\| \underline{a} \|\| \underline{b} \|\leqslant \underline{a}^t\underline{b}\leqslant\| \underline{a} \|\| \underline{b} \|\)&lt;/span&gt;&lt;br&gt;
&lt;strong&gt;證明&lt;/strong&gt;: 以維度爲 &lt;span class=&#34;math inline&#34;&gt;\(3\)&lt;/span&gt; 的向量爲例進行證明，其他維度的向量，證明思路類似：&lt;br&gt;
令 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=\left( \begin{array}{c} a_1\\ a_2\\ a_3 \end{array} \right), \underline{b}=\left( \begin{array}{c} b_1\\ b_2\\ b_3 \end{array} \right)\)&lt;/span&gt;， &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; 爲任意實數。平方和：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\sum\limits_{i=1}^3(a_it+b_i)^2 =(a_1t+b_1)^2+(a_2t+b_2)^2+(a_3t+b_3)^2\\ \;\;\;\;\;\;\;=(a_1^2+a_2^2+a_3^2)t^2+2(a_1b_1+a_2b_2+a_3b_3)t\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;+(b_1^2+b_2^2+b_3^2)\\ \;\;\;\;\;\;\;=\| \underline{a} \|^2t^2+2\underline{a}^t\underline{b}t+\| \underline{b} \|^2\geqslant0\)&lt;/span&gt; &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\therefore \| \underline{a} \|^2(t+\frac{\| \underline{b} \|^2}{2\| \underline{a} \|^2})^2+\| \underline{b} \|^2-\frac{(2\underline{a}^t\underline{b})^2}{4\| \underline{a} \|^2}\geqslant0\)&lt;/span&gt;&lt;br&gt;
可見這是一個&lt;strong&gt;關於 &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; 的絕對不等式&lt;/strong&gt;。因此，&lt;strong&gt;判別式&lt;/strong&gt;：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\((2\underline{a}^t\underline{b})^2-4\times\| \underline{a} \|^2\| \underline{b} \|^2\leqslant0\\ \therefore (\underline{a}^t\underline{b})^2\leqslant\| \underline{a} \|^2\| \underline{b} \|^2\\ \therefore -\| \underline{a} \|\| \underline{b} \|\leqslant \underline{a}^t\underline{b}\leqslant \| \underline{a} \|\| \underline{b} \|\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\divideontimes\)&lt;/span&gt; 兩向量內積，除以兩向量各自的長度(正)，在統計學中被成爲是&lt;strong&gt;相關系數&lt;/strong&gt;，寫作 &lt;span class=&#34;math inline&#34;&gt;\(r=\frac{\underline{a}^t\underline{b}}{\| \underline{a} \|\| \underline{b} \|}\)&lt;/span&gt;，我們從上面的不等式也可以得出， &lt;span class=&#34;math inline&#34;&gt;\(-1 \leqslant r \leqslant 1\)&lt;/span&gt; 另外，兩個向量又可以表示爲兩條射線，這兩條射線構成的角度如果爲 &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(\cos\theta=r =\frac{\underline{a}^t\underline{b}}{\| \underline{a} \|\| \underline{b} \|}\)&lt;/span&gt;。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;兩個向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}, \underline{b}\)&lt;/span&gt; 的和 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}+\underline{b}\)&lt;/span&gt; 也是一個新的向量。這三個向量之間有：&lt;span class=&#34;math inline&#34;&gt;\(\| \underline{a}+\underline{b} \|\leqslant\| \underline{a} \|+\| \underline{b} \|\)&lt;/span&gt;。這個關系被稱爲&lt;strong&gt;三角不等式&lt;/strong&gt;，或者&lt;strong&gt;三角關系&lt;/strong&gt;(triangular inequality)。&lt;br&gt;
&lt;strong&gt;證明&lt;/strong&gt;：此處亦爲了簡便起見使用維度爲 &lt;span class=&#34;math inline&#34;&gt;\(3\)&lt;/span&gt; 的向量，即，前述3.的 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}, \underline{b}\)&lt;/span&gt;：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\| \underline{a}+\underline{b}\|^2=(a_1+b_1)^2+(a_2+b_2)^2+(a_3+b_3)^2\\ \;\;\;\;\;\;\;=(a_1^2+a_2^2+a_3^3)+2(a_1b_1+a_2b_2+a_3b_3)+(b_1^2+b_2^2+b_3^2)\\ \;\;\;\;\;\;\;=\| \underline{a} \|^2+2\underline{a}^t\underline{b}+\| \underline{b} \|^2\)&lt;/span&gt;&lt;br&gt;
如果我們把前面問題3.中的不等式代入：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\| \underline{a} \|^2+2\underline{a}^t\underline{b}+\| \underline{b} \|^2\leqslant \| \underline{a} \|^2+2\| \underline{a} \|\| \underline{b} \|+\| \underline{b} \|^2\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;=(\| \underline{a} \|+\| \underline{b} \|)^2\\ \therefore \| \underline{a}+\underline{b}\|^2 \leqslant (\| \underline{a} \|+\| \underline{b} \|)^2\\ \therefore \| \underline{a}+\underline{b}\|\leqslant\| \underline{a} \|+\| \underline{b} \|\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;向量正規化-normalize&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;向量正規化 normalize&lt;/h2&gt;

&lt;div class=&#34;theorem&#34;&gt;
&lt;p&gt;&lt;span id=&#34;thm:unnamed-chunk-3&#34; class=&#34;theorem&#34;&gt;&lt;strong&gt;Theorem 3  (normalize)  &lt;/strong&gt;&lt;/span&gt;長度不爲 &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; 的任意向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}(\neq\underline{0})\)&lt;/span&gt;，如果將它轉變成長度爲 &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; 的向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{e}_{\underline{a}}\)&lt;/span&gt;。這個過程被叫做向量的正規化(normalize)。通常只要將向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}\)&lt;/span&gt; 除以他的長度 &lt;span class=&#34;math inline&#34;&gt;\(\| \underline{a} \|\)&lt;/span&gt; 即可。&lt;/p&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{e}_{\underline{a}}=\frac{\underline{a}}{\| \underline{a} \|}=\frac{1}{\| \underline{a} \|}\underline{a}\)&lt;/span&gt;
&lt;/div&gt;

&lt;p&gt;例如：&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=\left( \begin{array}{c} a_1\\ a_2\\ a_3 \end{array} \right)\)&lt;/span&gt;， 則有 &lt;span class=&#34;math inline&#34;&gt;\(\underline{e}_{\underline{a}}=\frac{1}{\sqrt{a_1^2+a_2^2+a_3^2}}\left( \begin{array}{c} a_1\\ a_2\\ a_3 \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{b}=\left( \begin{array}{c} -2\\ 1\\ 2 \end{array} \right)\)&lt;/span&gt;，則有 &lt;span class=&#34;math inline&#34;&gt;\(\underline{e}_{\underline{a}}=\frac{1}{\sqrt{(-2)^2+1^2+2^2}}\left( \begin{array}{c} -2\\ 1\\ 2 \end{array} \right)=\left( \begin{array}{c} -\frac{2}{3}\\ \frac{1}{3}\\ \frac{2}{3} \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記9</title>
      <link>https://wangcc.me/post/2017-02-18/</link>
      <pubDate>Sat, 18 Feb 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/2017-02-18/</guid>
      <description>


&lt;div id=&#34;特殊向量&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;特殊向量&lt;/h2&gt;
&lt;div id=&#34;零向量-zero-vector-null-vector&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;零向量 (zero vector, null vector)&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;全部的成分均爲&lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;的向量，我們稱之爲&lt;strong&gt;零向量(zero vector, null vector)&lt;/strong&gt;, 寫作： &lt;span class=&#34;math inline&#34;&gt;\(\underline{0}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;注意與&lt;strong&gt;標量(scalar)&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; 相區分。&lt;/li&gt;
&lt;li&gt;如果想要加注零向量的維度，我們可以在右下角加上 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;：&lt;span class=&#34;math inline&#34;&gt;\(\underline{0}_n\)&lt;/span&gt; ，意爲 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 維度的零向量。&lt;/li&gt;
&lt;li&gt;不是零向量的向量又被叫做，&lt;strong&gt;非零向量(non-zero vector, non-null vector)&lt;/strong&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;例如： 列向量：&lt;span class=&#34;math inline&#34;&gt;\(\underline{0}_3=\left( \begin{array}{c} 0\\ 0\\ 0\\ \end{array} \right)\)&lt;/span&gt;， 行向量：&lt;span class=&#34;math inline&#34;&gt;\(\underline{0}_3^t=(0,0,0)\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;向量-vector-with-all-elements-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; 向量 (vector with all elements 1)&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;當一個向量的全部成分都是數字 &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;，我們稱這個向量爲 &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; 向量。 &lt;span class=&#34;math inline&#34;&gt;\(\underline{1}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;這裏也需要注意與&lt;strong&gt;標量&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; 相區分。&lt;/li&gt;
&lt;li&gt;如果想要加注&lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;向量的維度，我們可以在右下角加上 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;：&lt;span class=&#34;math inline&#34;&gt;\(\underline{1}_n\)&lt;/span&gt; ，意爲 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 維度的&lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;向量。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;例如：列向量：&lt;span class=&#34;math inline&#34;&gt;\(\underline{1}_4=\left( \begin{array}{c} 1\\ 1\\ 1\\ 1 \end{array} \right)\)&lt;/span&gt;， 行向量：&lt;span class=&#34;math inline&#34;&gt;\(\underline{1}_4^t=(1,1,1,1)\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;第-i-基本向量&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;第 &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; 基本向量&lt;/h4&gt;

&lt;div class=&#34;theorem&#34;&gt;
&lt;span id=&#34;thm:unnamed-chunk-1&#34; class=&#34;theorem&#34;&gt;&lt;strong&gt;Theorem 1  (fundamental vector)  &lt;/strong&gt;&lt;/span&gt;&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 維度的向量，假如它的第 &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; 個成分是自然數 &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;，其他的成分全部都是 &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;， 我們稱這樣的向量爲&lt;strong&gt;第&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(\textbf{i}\)&lt;/span&gt; &lt;strong&gt;基本向量 (fundamental vector)&lt;/strong&gt;。寫作 &lt;span class=&#34;math inline&#34;&gt;\(\underline{\smash{e}}_i\)&lt;/span&gt;。
&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;平時我們較少用到一個單獨的基本向量。大多情況下我們用的是由 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 個單獨向量組成的一組向量。這個類型的向量與坐標軸的關系緊密。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;例如：維度爲4的第 &lt;span class=&#34;math inline&#34;&gt;\(1\sim4\)&lt;/span&gt; 基本向量：&lt;span class=&#34;math inline&#34;&gt;\(\underline{e}_1=\left( \begin{array}{c} 1\\ 0\\ 0\\ 0 \end{array} \right), \; \underline{e}_2=\left( \begin{array}{c} 0\\ 1\\ 0\\ 0 \end{array} \right), \; \underline{e}_3=\left( \begin{array}{c} 0\\ 0\\ 1\\ 0 \end{array} \right), \; \underline{e}_4=\left( \begin{array}{c} 0\\ 0\\ 0\\ 1 \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;單位向量-unit-vector&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;單位向量 (unit vector)&lt;/h4&gt;

&lt;div class=&#34;theorem&#34;&gt;
&lt;span id=&#34;thm:unnamed-chunk-2&#34; class=&#34;theorem&#34;&gt;&lt;strong&gt;Theorem 2  (unit vector)  &lt;/strong&gt;&lt;/span&gt;求向量的各個成分平方和的正平方根，當結果爲 &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; 時，這個向量被稱作&lt;strong&gt;單位向量(unit vector)&lt;/strong&gt;。寫作： &lt;span class=&#34;math inline&#34;&gt;\(\underline{e}\)&lt;/span&gt;。
&lt;/div&gt;

&lt;p&gt;例如： 因爲 &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{(\frac{2}{3})^2+(-\frac{1}{3})^2+(\frac{2}{3})^2}=1\)&lt;/span&gt;，所以我們稱向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{e}=\left( \begin{array}{c} \frac{2}{3}\\ -\frac{1}{3}\\ \frac{2}{3}\\ \end{array} \right)\)&lt;/span&gt; 爲&lt;strong&gt;單位向量&lt;/strong&gt;。另外，&lt;span class=&#34;math inline&#34;&gt;\((\frac{1}{\sqrt{2}},-\frac{1}{\sqrt{2}},0)^t, \; (\frac{2}{\sqrt{6}},\frac{1}{\sqrt{6}},-\frac{1}{\sqrt{6}})^t\)&lt;/span&gt;，以及前一項的&lt;strong&gt;第 &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; 基本向量&lt;/strong&gt;，都是單位向量。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;向量的計算與相等&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;向量的計算，與相等&lt;/h2&gt;
&lt;div id=&#34;向量的和與差&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;向量的和與差&lt;/h4&gt;

&lt;div class=&#34;theorem&#34;&gt;
&lt;span id=&#34;thm:unnamed-chunk-3&#34; class=&#34;theorem&#34;&gt;&lt;strong&gt;Theorem 3  (vectorplus)  &lt;/strong&gt;&lt;/span&gt;類型(type)/成分，維度相同的向量之間的加減運算定義爲：相對應的成分之間的和或差。
&lt;/div&gt;

&lt;p&gt;例如：&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=\left( \begin{array}{c} a_1\\ a_2\\ \vdots\\ a_n \end{array} \right), \; \underline{b}=\left( \begin{array}{c} b_1\\ b_2\\ \vdots\\ b_n \end{array} \right)\)&lt;/span&gt;，則有： &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}\pm\underline{b}=\left( \begin{array}{c} a_1\\ a_2\\ \vdots\\ a_n \end{array} \right)\pm\left( \begin{array}{c} b_1\\ b_2\\ \vdots\\ b_n \end{array} \right)=\left( \begin{array}{c} a_1 \pm b_1\\ a_2 \pm b_2\\ \vdots\\ a_n \pm b_n \end{array} \right)\)&lt;/span&gt; &lt;strong&gt;複号同順&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}= (a_1,a_2,\cdots,a_n), \; \underline{b} = (b_1,b_2,\cdots,b_n)\)&lt;/span&gt;，則有： &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}\pm\underline{b}=(a_1,a_2,\cdots,a_n)\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+(b_1,b_2,\cdots,b_n)\\ \;\;\;\;\;\;\;\;\;\;=(a_1 \pm b_1, a_2 \pm b_2, \cdots, a_n \pm b_n)\)&lt;/span&gt; &lt;br&gt;&lt;strong&gt;複号同順&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;練習&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;練習&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=\left( \begin{array}{c} 6\\ 7\\ 8\\ \end{array} \right),\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\underline{b}=\left( \begin{array}{c} 1\\ 3\\ 5\\ \end{array} \right)\)&lt;/span&gt; 時，&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}+\underline{b} =\left( \begin{array}{c} 6+1\\ 7+3\\ 8+5\\ \end{array} \right)=\left( \begin{array}{c} 7\\ 10\\ 13\\ \end{array} \right)\)&lt;/span&gt; &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}-\underline{b}=\left( \begin{array}{c} 6-1\\ 7-3\\ 8-5\\ \end{array} \right)=\left( \begin{array}{c} 5\\ 4\\ 3\\ \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{c}=(6,0,9), \underline{d}=(7,-3,2)\)&lt;/span&gt; 時，&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{c}+\underline{d}=(6+7,0-3,9+2)=(13,-3,11)\)&lt;/span&gt; &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{c}-\underline{d}=(6-7,0-(-3),9-2)=(-1,3,7)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;向量的標量乘法scalar-multiplication&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;向量的標量乘法(scalar multiplication)&lt;/h4&gt;

&lt;div class=&#34;theorem&#34;&gt;
&lt;span id=&#34;thm:unnamed-chunk-4&#34; class=&#34;theorem&#34;&gt;&lt;strong&gt;Theorem 4  (scalar multiplication)  &lt;/strong&gt;&lt;/span&gt;向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}\)&lt;/span&gt; 的所有成分同時乘以標量 &lt;span class=&#34;math inline&#34;&gt;\((k)\)&lt;/span&gt; 以後的向量，我們稱爲 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}\)&lt;/span&gt; 的標量 &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; 倍。寫作： &lt;span class=&#34;math inline&#34;&gt;\(k\underline{a}\)&lt;/span&gt;。特別地，當 &lt;span class=&#34;math inline&#34;&gt;\(k=1\)&lt;/span&gt; 時，&lt;span class=&#34;math inline&#34;&gt;\(1\underline{a}=\underline{a}\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(k=-1\)&lt;/span&gt; 時，&lt;span class=&#34;math inline&#34;&gt;\((-1)\underline{a}=-\underline{a}\)&lt;/span&gt;。另外 &lt;span class=&#34;math inline&#34;&gt;\(k=0\)&lt;/span&gt; 時，&lt;span class=&#34;math inline&#34;&gt;\(0\underline{a}=\underline{0}\)&lt;/span&gt;。注意此時&lt;span class=&#34;math inline&#34;&gt;\(\underline{0}\)&lt;/span&gt;是與&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}\)&lt;/span&gt;同維度的零向量。不可寫作標量的 &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;。
&lt;span class=&#34;math display&#34;&gt;\[k\underline{a}=k\left(
\begin{array}{c}
a_1\\
a_2\\
\vdots\\
a_n
\end{array}
\right)=\left(
\begin{array}{c}
ka_1\\
ka_2\\
\vdots\\
ka_n
\end{array}
\right), \\k\underline{a}=k(a_1,a_2,\cdots,a_n)=(ka_1,ka_2,\cdots,ka_n)\]&lt;/span&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;練習-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;練習&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(k=5, l=\frac{1}{9}, \underline{a}=\left( \begin{array}{c} 3\\ 2\\ -7\\ \end{array} \right)\)&lt;/span&gt; 時，&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(k\underline{a}=5\left( \begin{array}{c} 3\\ 2\\ -7\\ \end{array} \right)=\left( \begin{array}{c} 5\times3\\ 5\times2\\ 5\times(-7)\\ \end{array} \right)=\left( \begin{array}{c} 15\\ 10\\ -35\\ \end{array} \right)\)&lt;/span&gt;, &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(l\underline{a}=\frac{1}{9}\left( \begin{array}{c} 3\\ 2\\ -7\\ \end{array} \right)=\left( \begin{array}{c} \frac{1}{9}\times3\\ \frac{1}{9}\times2\\ \frac{1}{9}\times(-7)\\ \end{array} \right)=\left( \begin{array}{c} \frac{1}{3}\\ \frac{2}{9}\\ -\frac{7}{9}\\ \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=\left( \begin{array}{c} 3\\ -2\\ 4\\ \end{array} \right), \underline{b}=\left( \begin{array}{c} 1\\ 1\\ -3\\ \end{array} \right), \underline{c}=\left( \begin{array}{c} 0\\ 5\\ 2\\ \end{array} \right)\)&lt;/span&gt; 時，&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(2\underline{a}-\underline{b}+3\underline{c}=\left( \begin{array}{c} 2\times3\\ 2\times(-2)\\ 2\times4\\ \end{array} \right)-\left( \begin{array}{c} 1\\ 1\\ -3\\ \end{array} \right)\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+\left( \begin{array}{c} 3\times0\\ 3\times5\\ 3\times2\\ \end{array} \right)=\left( \begin{array}{c} 6-1+0\\ -4-1+15\\ 8-(-3)+6\\ \end{array} \right)=\left( \begin{array}{c} 5\\ 10\\ 17\\ \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;向量相等-equal&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;向量相等 equal&lt;/h4&gt;

&lt;div class=&#34;theorem&#34;&gt;
&lt;span id=&#34;thm:unnamed-chunk-5&#34; class=&#34;theorem&#34;&gt;&lt;strong&gt;Theorem 5  (vectors equal)  &lt;/strong&gt;&lt;/span&gt;類型(type)/成分，維度相同的向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}, \underline{b}\)&lt;/span&gt;，其對應成分完全一致，我們就稱 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=\underline{b}\)&lt;/span&gt;，此時有 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}-\underline{b}=\underline{0}\)&lt;/span&gt; &lt;strong&gt;零向量&lt;/strong&gt;。
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;練習-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;練習：&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=\left( \begin{array}{c} a_1\\ a_2\\ a_3\\ \end{array} \right), \underline{b}=\left( \begin{array}{c} b_1\\ b_2\\ b_3\\ \end{array} \right)\)&lt;/span&gt; 如果相等，那麼 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=\underline{b}\)&lt;/span&gt;，&lt;br&gt;即：&lt;span class=&#34;math inline&#34;&gt;\(\begin{align} \left\{ \begin{array}{ll} a_1 = b_1 \\ a_2 = b_2 \\ a_3 = b_3 \end{array} \right. \end{align}\)&lt;/span&gt; 等價於：&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}-\underline{b}=0\)&lt;/span&gt;，或者&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\begin{align} \left\{ \begin{array}{ll} a_1 - b_1 =0\\ a_2 - b_2 =0\\ a_3 - b_3 =0 \end{array} \right. \end{align}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;向量等式：&lt;span class=&#34;math inline&#34;&gt;\(\left( \begin{array}{c} a_{11}x_1+a_{12}x_2+a_{13}x_3\\ a_{21}x_1+a_{22}x_2+a_{23}x_3\\ a_{31}x_1+a_{32}x_2+a_{33}x_3\\ \end{array} \right)=\left( \begin{array}{c} b_1\\ b_2\\ b_3\\ \end{array} \right)\)&lt;/span&gt; 等價於三個等式的連立方程：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\begin{align} \left\{ \begin{array}{ll} a_{11}x_1+a_{12}x_2+a_{13}x_3= b_1\\ a_{21}x_1+a_{22}x_2+a_{23}x_3= b_2\\ a_{31}x_1+a_{32}x_2+a_{33}x_3= b_3 \end{array} \right. \end{align}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;求滿足 &lt;span class=&#34;math inline&#34;&gt;\(5\left( \begin{array}{c} a_1\\ a_2\\ a_3\\ \end{array} \right)+\left( \begin{array}{c} 2\\ 5\\ -1\\ \end{array} \right)=\left( \begin{array}{c} 12\\ 25\\ 29\\ \end{array} \right)\)&lt;/span&gt; 的向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=\left( \begin{array}{c} a_1\\ a_2\\ a_3\\ \end{array} \right)\)&lt;/span&gt;。&lt;br&gt;
解：&lt;span class=&#34;math inline&#34;&gt;\(5\left( \begin{array}{c} a_1\\ a_2\\ a_3\\ \end{array} \right)=\left( \begin{array}{c} 12\\ 25\\ 29\\ \end{array} \right)-\left( \begin{array}{c} 2\\ 5\\ -1\\ \end{array} \right)=\left( \begin{array}{c} 10\\ 20\\ 30\\ \end{array} \right)\)&lt;/span&gt;&lt;br&gt;
因此，&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=\left( \begin{array}{c} a_1\\ a_2\\ a_3\\ \end{array} \right)=\left( \begin{array}{c} 2\\ 4\\ 6\\ \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記8</title>
      <link>https://wangcc.me/post/2017-02-17/</link>
      <pubDate>Fri, 17 Feb 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/2017-02-17/</guid>
      <description>


&lt;div id=&#34;向量-vector&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;向量 vector&lt;/h2&gt;
&lt;div id=&#34;列向量-column-vector&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;列向量 column vector&lt;/h3&gt;
&lt;p&gt;在等號的右側，將數字寫成一列，左右用圓括號或者方括號包含在內的形式，被叫做列向量(column vector)：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=\left( \begin{array}{c} a_1\\ a_2\\ \vdots\\ a_i\\ \vdots\\ a_n \end{array} \right), \;\; \textbf{a}=\left[ \begin{array}{c} a_1\\ a_2\\ \vdots\\ a_i\\ \vdots\\ a_n \end{array} \right]\)&lt;/span&gt;&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;我們接下來將會繼續定義，向量的加減法，標量乘法(scalar multiplication)。把上述的向量用一個文字表示的時候，通常會記爲下劃線 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}\)&lt;/span&gt;，或者是加粗的小寫字母： &lt;span class=&#34;math inline&#34;&gt;\(\bf{a}\)&lt;/span&gt;。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;構成向量的各個數字，被命名爲&lt;strong&gt;成分(component, element, entry)&lt;/strong&gt;，從上往下第 &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; 個成分稱爲第 &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; 成分。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;成分的個數爲 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;，就被稱爲這個向量具有 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 個&lt;strong&gt;維度(次元，dimension)&lt;/strong&gt;，或者說這個向量的維度爲 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;。成分可以是數字，也可以是函數，或者式子。如果兩個列向量的維度一致，我們稱這兩個列向量的&lt;strong&gt;型(size, order)&lt;/strong&gt;,或者 &lt;strong&gt;類型(type)&lt;/strong&gt; 一致。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;成分只有一個的向量，被特別稱爲&lt;strong&gt;標量(scalar)&lt;/strong&gt;，原則上不加括號。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;將向量成分全部羅列出來，寫成上面的形式的過程，被稱爲&lt;strong&gt;成分表示&lt;/strong&gt;。在多元變量分析中，我們說到向量，多默認指的就是列向量。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{b}=\left( \begin{array}{c} 16\\ 59\\ 80\\ \end{array} \right)=\left[ \begin{array}{c} 16\\ 59\\ 80\\ \end{array} \right]=\textbf{b}\)&lt;/span&gt;&lt;br&gt;今後我們都用字母帶下劃線，圓括號包含數字的方式表示向量。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{c}=\left( \begin{array}{c} \sin t+\cos t\\ \cos t+\tan t-2\\ \tan t + \sin t\\ \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;當 &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; 爲 &lt;span class=&#34;math inline&#34;&gt;\(a_1,a_2,a_3\)&lt;/span&gt; 的函數時，寫作 &lt;span class=&#34;math inline&#34;&gt;\(F(a_1,a_2,a_3)\)&lt;/span&gt;。 以&lt;strong&gt;三個未知數的偏微分&lt;/strong&gt;爲成分的向量(梯度向量，gradient vector)，寫成下面等式左邊的形式。可以簡略寫作: &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=\left( \begin{array}{c} a_1\\ a_2\\ a_3\\ \end{array} \right)\)&lt;/span&gt;。&lt;span class=&#34;math inline&#34;&gt;\(\nabla\)&lt;/span&gt;讀作&lt;code&gt;nabla&lt;/code&gt;。&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\left( \begin{array}{c} \frac{\partial F}{\partial a_1}\\ \frac{\partial F}{\partial a_2}\\ \frac{\partial F}{\partial a_3}\\ \end{array} \right)=\frac{\partial F}{\partial \underline{a}}=\nabla_{\underline{a}}F\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;橫向量行向量-row-vector&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;橫向量(行向量) row vector&lt;/h2&gt;
&lt;p&gt;在等號的右側，將數字寫成一行，左右用圓括號或者方括號包含在內的形式，被叫做橫向量(row vector):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=(a_1,a_2,\cdots,a_j,\cdots,a_n), \; \textbf{a}=[a_1,a_2,\cdots,a_j,\cdots,a_n]\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;成分，維度，類型等的定義與列向量相同。另外注意，維度相同，但是一個是橫向量，一個是列向量的話，這兩個向量是不同類型的。&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}=(x_1,x_2,x_3)\)&lt;/span&gt;&lt;br&gt;
有時也可以不用逗號分隔成分。 寫作 &lt;span class=&#34;math inline&#34;&gt;\((x_1 \; x_2 \;x_3)\)&lt;/span&gt;。下同。&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((\frac{\partial F}{\partial x_1},\frac{\partial F}{\partial x_2},\frac{\partial F}{\partial x_3})=\frac{\partial F}{\partial \underline{x}}=\nabla_{\underline{x}F}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{u}=(\sin\theta\cos\phi, \sin\theta\cos\theta, \cos\theta)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;向量的轉置-vector-transpose&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;向量的轉置 (vector transpose)&lt;/h2&gt;
&lt;p&gt;將列向量的每個成分，按照從上到下的順序，一字橫着排開寫成橫向量。這個向量稱爲原來列向量的轉置向量(transposed vector)。反之亦然。&lt;/p&gt;
&lt;p&gt;向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}\)&lt;/span&gt; 的轉置向量，可以標記爲 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}^t,\;\underline{a}^\prime,\;^t\underline{a},\;\underline{a}^T, \;^T\underline{a}\)&lt;/span&gt; 各種形式。今後統一用 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}^t\)&lt;/span&gt;。&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=\left( \begin{array}{c} a_1\\ a_2\\ a_3\\ \end{array} \right)\)&lt;/span&gt; 的轉置向量我們會記爲：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}^t=\left( \begin{array}{c} a_1\\ a_2\\ a_3\\ \end{array} \right)^t=(a_1,a_2,a_3)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}=(x_1.x_2,x_3)\)&lt;/span&gt; 的轉置向量我們會記爲：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}^t=(x_1.x_2,x_3)^t=\left( \begin{array}{c} x_1\\ x_2\\ x_3\\ \end{array} \right)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記7</title>
      <link>https://wangcc.me/post/2017-02-16/</link>
      <pubDate>Thu, 16 Feb 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/2017-02-16/</guid>
      <description>


&lt;div id=&#34;分解平方和-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;分解平方和 1&lt;/h2&gt;
&lt;p&gt;樣本量均爲 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 的兩變量 &lt;span class=&#34;math inline&#34;&gt;\(z, \hat{z}\)&lt;/span&gt; 如下表，已知這兩個變量滿足條件：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\bar{z}=\frac{1}{n}\sum\limits_{i=1}^nz_i=\frac{1}{n}\sum\limits_{i=1}^n\hat{z}_i=\bar{\hat{z}},\)&lt;/span&gt; &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\sum\limits_{i=1}^n(z_i-\hat{z_i})(\hat{z_i}-\bar{z})=0\)&lt;/span&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;個体の番号&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;変量 &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;変量 &lt;span class=&#34;math inline&#34;&gt;\(\hat{z}\)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(z_1\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{z}_1\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(z_2\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{z}_2\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\vdots\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\vdots\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\vdots\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(z_i\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{z}_i\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\vdots\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\vdots\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\vdots\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(z_n\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{z}_n\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;此時我們有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;全平方和&lt;/strong&gt;(全変動，總平方和，總變動， &lt;strong&gt;Total sum of Squares&lt;/strong&gt;)：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(S_T=(z_i-\bar{z})^2+(z_2-\bar{z})^2+\cdots+(z_n-\bar{z})^2\\ \;\;\;\;=\sum\limits_{i=1}^n(z_i-\bar{z})^2\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;回歸平方和&lt;/strong&gt;(回歸變動，&lt;strong&gt;Regression sum of Squares&lt;/strong&gt;)&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(S_R=(\hat{z_1}-\bar{\hat{z}})^2+(\hat{z_2}-\bar{\hat{z}})^2+\cdots+(\hat{z_n}-\bar{\hat{z}})^2\\ \;\;\;\;=\sum\limits_{i=1}^n(\hat{z_i}-\bar{\hat{z}})^2=\sum\limits_{i=1}^n(\hat{z_i}-\bar{z})^2\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;殘差平方和&lt;/strong&gt;(誤差平方和，殘差變動，誤差變動，&lt;strong&gt;residual sum of Squares&lt;/strong&gt;)&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(S_e=(z_1-\hat{z_1})^2+(z_2-\hat{z_2})^2+\cdots+(z_n-\hat{z_n})^2\\ \;\;\;\;=\sum\limits_{i=1}^n(z_i-\hat{z_i})^2\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;上面三個平方和之間，有如下的關系：
&lt;span class=&#34;math display&#34; id=&#34;eq:Sumofsquares&#34;&gt;\[\begin{equation}
  S_T=S_R+S_e
  \tag{1}
  \end{equation}\]&lt;/span&gt;
&lt;br&gt;
既：全平方和等於殘差平方和與回歸平方和之和。&lt;a href=&#34;#eq:Sumofsquares&#34;&gt;(1)&lt;/a&gt;式被稱爲&lt;strong&gt;平方和的分解(decomposition of sum of squares)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;證明refeqsumofsquares式&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;證明&lt;a href=&#34;#eq:Sumofsquares&#34;&gt;(1)&lt;/a&gt;式&lt;/h5&gt;
&lt;/div&gt;
&lt;div id=&#34;解&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;解：&lt;/h5&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{equation}
\begin{split}
S_T &amp;amp; = \sum\limits_{i=1}^n(z_i-\bar{z})^2 \\
&amp;amp; = \sum\limits_{i=1}^n\left\{(z_i-\hat{z_i})+(\hat{z_i}-\bar{z})\right\}^2\\
&amp;amp; = \sum\limits_{i=1}^n\left\{(z_i-\hat{z_i})^2+(\hat{z_i}-\bar{z})^2+2(z_i-\hat{z_i})(\hat{z_i}-\bar{z})\right\}\\
&amp;amp; = \sum\limits_{i=1}^n(z_i-\hat{z_i})^2+\sum\limits_{i=1}^n(\hat{z_i}-\bar{z})^2 + 0\\
&amp;amp; = S_e + S_R
\end{split}
\end{equation}
\]&lt;/span&gt;
最後一步等式，利用了一開始給出的條件 &lt;span class=&#34;math inline&#34;&gt;\(\sum\limits_{i=1}^n(z_i-\hat{z_i})(\hat{z_i}-\bar{z})=0\)&lt;/span&gt;&lt;br&gt;
這裏的平方和分解與&lt;strong&gt;回歸分析&lt;/strong&gt;有着緊密的聯系。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;分解平方和-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;分解平方和 2&lt;/h2&gt;
&lt;p&gt;有樣本量爲 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 的變量 &lt;span class=&#34;math inline&#34;&gt;\(z_1\)&lt;/span&gt; 與樣本量爲 &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; 的變量 &lt;span class=&#34;math inline&#34;&gt;\(z_2\)&lt;/span&gt; 的數據如下表：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;変量 &lt;span class=&#34;math inline&#34;&gt;\(z_1\)&lt;/span&gt;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;変量 &lt;span class=&#34;math inline&#34;&gt;\(z_2\)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(z_{11}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(z_{12}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(z_{21}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(z_{22}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\vdots\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\vdots\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(z_{i1}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(z_{i2}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\vdots\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\vdots\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(z_{n1}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\vdots\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(z_{m2}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;此時我們有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;樣本&lt;strong&gt;平均值&lt;/strong&gt;： &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\bar{z_1}=\frac{1}{n}\sum\limits_{i=1}^nz_{i1}, \;\bar{z_2}=\frac{1}{m}\sum\limits_{i=1}^mz_{i2}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;樣本&lt;strong&gt;總平均值&lt;/strong&gt;： &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\bar{z}=\frac{1}{n+m}(\sum\limits_{i=1}^nz_{i1}+\sum\limits_{i=1}^mz_{i2})\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;全&lt;strong&gt;平方和&lt;/strong&gt; (全変動，總平方和，總變動, &lt;strong&gt;Total sum of Squares&lt;/strong&gt;)：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(S_T=\left\{(z_{11}-\bar{z})^2+(z_{21}-\bar{z})^2+\cdots+(z_{n1}-\bar{z})^2\right\}\\ \;\;\;\;\;\;\;\;+\left\{(z_{12}-\bar{z})^2+(z_{22}-\bar{z})^2+\cdots+(z_{m2}-\bar{z})^2\right\}\\ \;\;\;\;=\sum\limits_{i=1}^n(z_{i1}-\bar{z})^2+\sum\limits_{i=1}^m(z_{i2}-\bar{z})^2\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;羣內平方和&lt;/strong&gt;(組內平方和，級內平方和，羣內變動，級內變動，變量內平方和，變量內變動，&lt;strong&gt;Within-groups sum of Squares&lt;/strong&gt;)：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(S_W=\left\{(z_{11}-\bar{z_1})^2+(z_{21}-\bar{z_1})^2+\cdots+(z_{n1}-\bar{z_1})^2\right\}\\ \;\;\;\;\;\;\;\;+\left\{(z_{12}-\bar{z_2})^2+(z_{22}-\bar{z_2})^2+\cdots+(z_{m2}-\bar{z_2})^2\right\}\\ \;\;\;\;=\sum\limits_{i=1}^n(z_{i1}-\bar{z_1})^2+\sum\limits_{i=1}^m(z_{i2}-\bar{z_2})^2\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;羣間平方和&lt;/strong&gt;(組間平方和，級間平方和，羣間變動，級間變動，變量間平方和，變量間變動，&lt;strong&gt;Between-groups sum of Squares&lt;/strong&gt;)：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(S_B=\left\{(\bar{z_1}-\bar{z})^2+(\bar{z_1}-\bar{z})^2+\cdots+(\bar{z_1}-\bar{z})^2\right\}\\ \;\;\;\;\;\;\;\;+\left\{(\bar{z_2}-\bar{z})^2+(\bar{z_2}-\bar{z})^2+\cdots+(\bar{z_2}-\bar{z})^2\right\}\\ \;\;\;\;=\sum\limits_{i=1}^n(\bar{z_1}-\bar{z})^2+\sum\limits_{i=1}^m(\bar{z_2}-\bar{z})^2\\ \;\;\;\;=n(\bar{z_1}-\bar{z})^2+m(\bar{z_2}-\bar{z})^2\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;上面三個平方和之間有如下的關系：
&lt;span class=&#34;math display&#34; id=&#34;eq:Sumofsqua&#34;&gt;\[\begin{equation}
S_T=S_W+S_B
\tag{2}
\end{equation}\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;證明refeqsumofsqua式&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;證明&lt;a href=&#34;#eq:Sumofsqua&#34;&gt;(2)&lt;/a&gt;式&lt;/h5&gt;
&lt;/div&gt;
&lt;div id=&#34;解-1&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;解：&lt;/h5&gt;
&lt;p&gt;注意利用：&lt;span class=&#34;math inline&#34;&gt;\(\sum\limits_{i=1}^n(z_{i1}-\bar{z_1})(\bar{z_1}-\bar{z})=(\bar{z_1}-\bar{z})\sum\limits_{i=1}^n(z_{i1}-\bar{z_1})\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\:=(\bar{z_1}-\bar{z})(\sum\limits_{i=1}^nz_{i1}-n\bar{z_1})\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\:=(\bar{z_1}-\bar{z})(n\bar{z_1}-n\bar{z_1})=0\)&lt;/span&gt;&lt;br&gt;因此&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{equation}
\begin{split}
S_T &amp;amp; = \sum_{i=1}^n(z_{i1}-\bar{z})^2+\sum_{i=1}^m(z_{i2}-\bar{z})^2\\
&amp;amp; = \sum_{i=1}^n\left\{(z_{i1}-\bar{z_1})+(\bar{z_1}-\bar{z})\right\}^2\\
&amp;amp;\;\;\;\;\; + \sum_{i=1}^m\left\{(z_{i2}-\bar{z_2})+(\bar{z_2}-\bar{z})\right\}^2\\
&amp;amp; = \sum_{i=1}^n\left\{(z_{i1}-\bar{z_1})^2+2(z_{i1}-\bar{z_1})(\bar{z_1}-\bar{z})+(\bar{z_1}-\bar{z})^2\right\}\\
&amp;amp;\;\;\;\;\; +\sum_{i=1}^m\left\{(z_{i2}-\bar{z_2})^2+2(z_{i2}-\bar{z_2})(\bar{z_2}-\bar{z})+(\bar{z_2}-\bar{z})^2\right\}\\
&amp;amp; = \sum_{i=1}^n(z_{i1}-\bar{z_1})^2 + n(\bar{z_1}-\bar{z})^2\\
&amp;amp;\;\;\;\;\; + \sum_{i=1}^m(z_{i2}-\bar{z_2})^2 + m(\bar{z_2}-\bar{z})^2\\
&amp;amp; = S_W+S_B
\end{split}
\end{equation}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;變量的合成與加權&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;變量的合成與加權&lt;/h2&gt;
&lt;p&gt;我們說，將 &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; 個變量 &lt;span class=&#34;math inline&#34;&gt;\(x_1,x_2,\cdots,x_p\)&lt;/span&gt; 轉變成一次式：&lt;span class=&#34;math inline&#34;&gt;\(w_1x_1+w_2x_2+\cdots+w_px_p (=\hat{y})\)&lt;/span&gt; 的過程稱爲變量的合成 &lt;strong&gt;(linear combination of variables)&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(\hat{y}\)&lt;/span&gt; 被叫做&lt;strong&gt;合成變量&lt;/strong&gt;。系數 &lt;span class=&#34;math inline&#34;&gt;\(w_1,w_2,\cdots,w_p\)&lt;/span&gt; 被叫做&lt;strong&gt;權重 (weight)&lt;/strong&gt;。假如 &lt;span class=&#34;math inline&#34;&gt;\(x_1,x_2,\cdots,x_p\)&lt;/span&gt; 是 &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; 個科目的考試得分，那麼:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(w_1=w_2=\cdots=w_p=1\)&lt;/span&gt; 時，&lt;span class=&#34;math inline&#34;&gt;\(\hat{y}\)&lt;/span&gt; 意思就是 &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; 個科目的總分&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(w_1=w_2=\cdots=w_p=\frac{1}{p}\)&lt;/span&gt; 時，&lt;span class=&#34;math inline&#34;&gt;\(\hat{y}\)&lt;/span&gt; 意思就是 &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; 個科目的平均分&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;多元變量分析中，我們實質上做的許多事就是思考如何合理的決定這個&lt;strong&gt;權重&lt;/strong&gt;。&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記6</title>
      <link>https://wangcc.me/post/2017-02-15/</link>
      <pubDate>Wed, 15 Feb 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/2017-02-15/</guid>
      <description>


&lt;div id=&#34;數據的變換&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;數據的變換&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;平均值附近的偏差:
&lt;ul&gt;
&lt;li&gt;各個數值 &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; 與樣本平均值 &lt;span class=&#34;math inline&#34;&gt;\(\bar{x}\)&lt;/span&gt; 的差 &lt;span class=&#34;math display&#34;&gt;\[x_i^\prime=x_i-\bar{x} (i = 1,2,\cdots,n)\]&lt;/span&gt; &lt;br&gt;
稱爲數據 &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; 在它的平均值 &lt;span class=&#34;math inline&#34;&gt;\(\bar{x}\)&lt;/span&gt; 附近的&lt;strong&gt;偏差(deviation)&lt;/strong&gt;。通常我們說&lt;strong&gt;求偏差&lt;/strong&gt;，指的是，對數據 &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; 進行&lt;strong&gt;偏差轉換&lt;/strong&gt;。這個過程又被稱作是&lt;strong&gt;中心變換(centering)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;關於偏差，我們列舉如下兩個有特徵的的&lt;strong&gt;概括統計&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;樣本平均值：
&lt;span class=&#34;math display&#34; id=&#34;eq:samplemean&#34;&gt;\[\begin{equation}
   \bar{x}^\prime=\frac{1}{n}\sum_{i=1}^nx_i^\prime=0
   \tag{1}
   \end{equation}\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;樣本偏差平方和：
&lt;span class=&#34;math display&#34; id=&#34;eq:SSprime&#34;&gt;\[\begin{equation}
  SS^\prime=\sum_{i=1}^n(x^\prime)^2=SS
  \tag{2}
  \end{equation}\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;練習證明refeqsamplemean&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;練習：證明&lt;a href=&#34;#eq:samplemean&#34;&gt;(1)&lt;/a&gt;&lt;/h4&gt;
&lt;/div&gt;
&lt;div id=&#34;解&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;解：&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;證明&lt;a href=&#34;#eq:samplemean&#34;&gt;(1)&lt;/a&gt;&lt;/strong&gt;:
&lt;span class=&#34;math display&#34;&gt;\[\bar{x}^\prime=\frac{\sum\limits_{i=1}^n(x_i-\bar{x})}{n}\\
\;\;\;=\frac{\sum\limits_{i=1}^nx_i-n\bar{x}}{n}\\
\;\;\;=\frac{\sum\limits_{i=1}^nx_i}{n}-\bar{x}\\
\;\;\;=\bar{x}-\bar{x}=0\]&lt;/span&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;數據的標準化：
&lt;ul&gt;
&lt;li&gt;將數據 &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; 的平均值 &lt;span class=&#34;math inline&#34;&gt;\(\bar{x}\)&lt;/span&gt; 附近的偏差除以樣本標準偏差 &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; 從而獲得下面式子所表示的數據 &lt;span class=&#34;math inline&#34;&gt;\(z_i\)&lt;/span&gt; 的過程，被叫做&lt;strong&gt;數據的標準化 (standardization)&lt;/strong&gt;：
&lt;span class=&#34;math display&#34; id=&#34;eq:standardization&#34;&gt;\[\begin{equation}
 z_i=\frac{x_i-\bar{x}}{s}
 \tag{3}
 \end{equation}\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;標準化後的數據 &lt;span class=&#34;math inline&#34;&gt;\(z_i\)&lt;/span&gt; 的&lt;strong&gt;概括統計&lt;/strong&gt;有下列特徵：
&lt;ul&gt;
&lt;li&gt;樣本平均值：
&lt;span class=&#34;math display&#34; id=&#34;eq:samplemeans&#34;&gt;\[\begin{equation}
  \bar{z}=\frac{1}{n}\sum_{i=1}^nz_i=0
  \tag{4}
  \end{equation}\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;樣本方差:
&lt;span class=&#34;math display&#34; id=&#34;eq:Ssquare&#34;&gt;\[\begin{equation}
  s_{z}^2=\frac{1}{n}\sum_{i=1}^nz_i^2=1
  \tag{5}
  \end{equation}\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;由於標準化數據具有上述兩個非常顯著的特徵，均值爲 &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;，方差爲 &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;，因此我們實際分析數據過程中常常對數據進行標準化。標準化以後的數據，單位消失，變成了一組&lt;strong&gt;無名數&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\divideontimes\)&lt;/span&gt; 數據的標準化，有時你會看到被定義爲:
&lt;span class=&#34;math display&#34; id=&#34;eq:newstandardization&#34;&gt;\[\begin{equation}
 z_i=\frac{x_i-\bar{x}}{u}
 \tag{6}
 \end{equation}\]&lt;/span&gt; &lt;br&gt;
此時的不偏樣本方差爲：
&lt;span class=&#34;math display&#34; id=&#34;eq:unbsamplevar&#34;&gt;\[\begin{equation}
 u_z^2=\frac{1}{n-1}\sum_{i=1}{n}z_i^2=1
 \tag{7}
 \end{equation}\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;變量數據的概括統計&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2變量數據的概括統計：&lt;/h2&gt;
&lt;div id=&#34;樣本量同爲-n-的-2-變量-x_1x_2-的數據表示爲如下表格&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;樣本量同爲 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 的 &lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt; 變量 &lt;span class=&#34;math inline&#34;&gt;\(x_1,x_2\)&lt;/span&gt; 的數據，表示爲如下表格：&lt;/h5&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;個体の番号&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;変量 &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;変量 &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_{11}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_{12}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_{21}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_{22}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\vdots\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\vdots\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\vdots\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_{i1}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_{i2}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\vdots\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\vdots\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\vdots\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_{n1}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_{n2}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;按照變量-x_1x_2-各自的定義&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;按照變量 &lt;span class=&#34;math inline&#34;&gt;\(x_1,x_2\)&lt;/span&gt; 各自的定義：&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;樣本&lt;strong&gt;平均值&lt;/strong&gt;：&lt;span class=&#34;math inline&#34;&gt;\(\bar{x_1}=\frac{1}{n}\sum\limits_{i=1}^nx_{i1}, \; \bar{x_2}=\frac{1}{n}\sum\limits_{i=1}^nx_{i2}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;樣本&lt;strong&gt;偏差平方和&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(SS_1=\sum\limits_{i=1}^n(x_{i1}-\bar{x_1})^2, \; SS_2=\sum\limits_{i=1}^n(x_{i2}-\bar{x_2})^2\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;樣本&lt;strong&gt;方差&lt;/strong&gt;： &lt;span class=&#34;math inline&#34;&gt;\(s_1^2=\frac{SS_1}{n}, \; s_2^2=\frac{SS_2}{n}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;樣本&lt;strong&gt;標準偏差&lt;/strong&gt;： &lt;span class=&#34;math inline&#34;&gt;\(s_1=\sqrt{s_1^2}, \; s_2=\sqrt{s_2^2}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;不偏樣本方差&lt;/strong&gt;： &lt;span class=&#34;math inline&#34;&gt;\(u_1^2=\frac{SS}{n-1}, \; u_2^2=\frac{SS_2}{n-1}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;不偏樣本方差平方根&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(u_1=\sqrt{u_1^2}, \; u_2=\sqrt{u_2^2}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;對於這樣一對變量-x_1x_2-來說我們又可以追加如下的概括統計&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;對於這樣一對變量 &lt;span class=&#34;math inline&#34;&gt;\(x_1,x_2\)&lt;/span&gt; 來說，我們又可以追加如下的概括統計：&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;樣本&lt;strong&gt;總體平均值&lt;/strong&gt;： &lt;span class=&#34;math inline&#34;&gt;\(\bar{x}=\frac{1}{n+n}(\sum\limits_{i-1}^nx_{i1}+\sum\limits_{i-1}^nx_{i2})\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;樣本&lt;strong&gt;方差積和(cross-product)&lt;/strong&gt;：
&lt;span class=&#34;math display&#34; id=&#34;eq:crossproduct&#34;&gt;\[\begin{equation}
\begin{split}
S_{12} &amp;amp; = \sum_{i=1}^n(x_{i1}-\bar{x_1})\cdot(x_{i2}-\bar{x_2})\\
&amp;amp; = \sum_{i=1}^n(x_{i1}x_{i2}-\bar{x_1}x_{i2}-x_{i1}\bar{x_2}+\bar{x_1}\bar{x_2})\\
&amp;amp; = \sum_{i=1}^nx_{i1}x_{i2}-\bar{x_1}\sum_{i=1}^nx_{i2}-{\sum_{i=1}^nx_{i1}}\bar{x_2}+n\bar{x_1}\bar{x_2}\\
&amp;amp; = \sum_{i=1}^nx_{i1}x_{i2}-\bar{x_1}\cdot n\bar{x_2}-n\bar{x_1}\cdot\bar{x_2}+n\bar{x_1}\bar{x_2}\\
&amp;amp; = \sum_{i=1}^nx_{i1}x_{i2}-n\cdot\bar{x_1}\cdot\bar{x_2}\\
&amp;amp; = \sum_{i=1}^nx_{i1}x_{i2}-n\cdot\frac{\sum\limits_{i=1}^nx_{i1}}{n}\cdot\frac{\sum\limits_{i=1}^nx_{i2}}{n}\\
&amp;amp; = \sum_{i=1}^nx_{i1}x_{i2}-\frac{1}{n}(\sum_{i=1}^nx_{i1})(\sum_{i=1}^nx_{i2}) = S_{21}
\end{split}
\tag{8}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;樣本&lt;strong&gt;協方差(covariance，共分散)&lt;/strong&gt;：&lt;span class=&#34;math inline&#34;&gt;\(s_{12}=\frac{S_{12}}{n}=\frac{S_{21}}{n}=s_{21}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;樣本&lt;strong&gt;相關系數 (correlation coefficient)&lt;/strong&gt;： &lt;span class=&#34;math inline&#34;&gt;\(r_{11}=r_{22}=1\)&lt;/span&gt;
&lt;span class=&#34;math display&#34; id=&#34;eq:correlation&#34;&gt;\[\begin{equation}
\begin{split}
r_{12} &amp;amp; = \frac{S_{12}}{\sqrt{SS_1\cdot SS_2}}\\
&amp;amp; = \frac{\frac{S_{12}}{n}}{\sqrt{\frac{SS_1}{n}}\cdot\sqrt{\frac{SS_2}{n}}}\\
&amp;amp; = \frac{s_{12}}{s_1s_2}=r_{21}\\
\end{split}
\tag{9}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;此處我們再來證明一下標準化以後的數據的樣本協方差covariance和標準化以前原來的數據的樣本相關系數correlation-coefficient是相等的&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;此處我們再來證明一下，標準化以後的數據的樣本協方差(covariance)，和標準化以前原來的數據的樣本相關系數(correlation coefficient)是相等的：&lt;/h5&gt;
&lt;p&gt;假設，&lt;span class=&#34;math inline&#34;&gt;\(x_{i1}\)&lt;/span&gt; 標準化以後爲 &lt;span class=&#34;math inline&#34;&gt;\(z_{i1}=\frac{x_{i1}-\bar{x_1}}{s_1}\)&lt;/span&gt;； &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(x_{i2}\)&lt;/span&gt; 標準化以後爲 &lt;span class=&#34;math inline&#34;&gt;\(z_{i2}=\frac{x_{i2}-\bar{x_2}}{s_1}\)&lt;/span&gt;。 &lt;br&gt;
此時，&lt;span class=&#34;math inline&#34;&gt;\(z_{i1}, z_{i2}\)&lt;/span&gt; 的樣本協方差可以計算如下:
&lt;span class=&#34;math display&#34; id=&#34;eq:stcorrelation&#34;&gt;\[\begin{equation}
\begin{split}
s_{z_{12}} &amp;amp; = \frac{S_{z_{12}}}{n} \\
&amp;amp; = \frac{1}{n}\cdot\sum_{i=1}^n(z_{i1}-\bar{z_1})(z_{i2}-\bar{z_2})\\
&amp;amp; = \frac{1}{n}\sum_{i=1}^nz_{i1}z_{i2}\\
&amp;amp; = \frac{1}{n}\sum_{i=1}^n(\frac{x_{i1}-\bar{x_1}}{s_1})(\frac{x_{i2}-\bar{x_2}}{s_2})\\
&amp;amp; = \frac{S_{12}}{n}\cdot\frac{1}{s_1s_2} = \frac{s_{12}}{s_1s_2}=r_{12}
\end{split}
\tag{10}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記4</title>
      <link>https://wangcc.me/post/2017-02-12-t/</link>
      <pubDate>Sun, 12 Feb 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/2017-02-12-t/</guid>
      <description>


&lt;div id=&#34;連立方程式-simultaneous-equations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;連立方程式 (simultaneous equations)&lt;/h2&gt;
&lt;p&gt;連立方程式，將與第六章談的特徵值問題(固有値問題)有緊密聯系，此處我們一起觀察幾種不同的組合：&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;解同次連立1次方程式 &lt;span class=&#34;math inline&#34;&gt;\(\left\{ \begin{array}{ll}  (1)\;a_1+2a_2+3a_3 = 0 \\  (2)\;2a_1+4a_2+5a_3 = 0 \;\\  (3)\;3a_1+5a_2+6a_3 = 0 \\  \end{array} \right.\)&lt;/span&gt; &lt;br&gt;
由 &lt;span class=&#34;math inline&#34;&gt;\(2\times(1)-(2)\)&lt;/span&gt; 可得 &lt;span class=&#34;math inline&#34;&gt;\(a_3=0\)&lt;/span&gt; 。 代入 &lt;span class=&#34;math inline&#34;&gt;\((1),(2),(3)\)&lt;/span&gt; 式後，&lt;span class=&#34;math inline&#34;&gt;\((3)-(2)\)&lt;/span&gt; 可得 &lt;span class=&#34;math inline&#34;&gt;\(a_1=-a_2\)&lt;/span&gt; 。 代入 &lt;span class=&#34;math inline&#34;&gt;\((1)\)&lt;/span&gt; 式可得 &lt;span class=&#34;math inline&#34;&gt;\(a_2=0\)&lt;/span&gt; 。 再代入 &lt;span class=&#34;math inline&#34;&gt;\((4)\)&lt;/span&gt; 式可知 &lt;span class=&#34;math inline&#34;&gt;\(a_1=0\)&lt;/span&gt; 。最終可得 &lt;span class=&#34;math inline&#34;&gt;\(a_1=a_2=a_3=0\)&lt;/span&gt; &lt;br&gt;
其實上述問題不解自明 (trivial solution)。 那麼同次1次連立方程式 (homogeneous system) 除了自明解之外，還有別的解嗎? 我們再看下面一例。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;解 &lt;span class=&#34;math inline&#34;&gt;\(\left\{ \begin{array}{ll}  (1)\;4a_1+3a_2+6a_3 = 0 \\  (2)\;2a_1+a_2+4a_3 = 0 \;\\  (3)\;a_1+a_2+a_3 = 0 \\  \end{array} \right.\)&lt;/span&gt; &lt;br&gt;
上述方程表面上看有三個式子，實際上由於 &lt;span class=&#34;math inline&#34;&gt;\((3)=\left\{(1)-(2)\right\}\div2\)&lt;/span&gt; 只有2個有意義的方程式。如此這般，有3個未知數，卻只有兩個連立方程組，是無法求解的。如果將三個未知數中的一個例如 &lt;span class=&#34;math inline&#34;&gt;\(a_3\)&lt;/span&gt; 視爲常數(定数) (寫作：&lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; ) 即： &lt;br&gt;&lt;span class=&#34;math inline&#34;&gt;\((4)\;a_3=s\)&lt;/span&gt; &lt;br&gt;
整理方程組得到新的連立方程 &lt;span class=&#34;math inline&#34;&gt;\(\left\{ \begin{array}{ll}  (1^\prime)\;4a_1+3a_2 = -6s \\  (2^\prime)\;2a_1+a_2 = -4s \;\\  \end{array} \right.\)&lt;/span&gt; &lt;br&gt;
由 &lt;span class=&#34;math inline&#34;&gt;\((1^\prime)-2\times(2^\prime)\)&lt;/span&gt; 可得 &lt;span class=&#34;math inline&#34;&gt;\(a_2=2s\)&lt;/span&gt; 。代入 &lt;span class=&#34;math inline&#34;&gt;\((2^\prime)\)&lt;/span&gt; 可得 &lt;span class=&#34;math inline&#34;&gt;\(a_1=-3s\)&lt;/span&gt;。因此我們得到 &lt;span class=&#34;math inline&#34;&gt;\(a_1=-3s,a_2=2s,a_3=s\)&lt;/span&gt; 且 &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; 爲任意常數，故此連立方程組的解有無數組。當且僅當 &lt;span class=&#34;math inline&#34;&gt;\(s=0\)&lt;/span&gt; 時方程組有自明解， &lt;span class=&#34;math inline&#34;&gt;\(s\neq0\)&lt;/span&gt; 時此連立方程組的解爲非自明解 (non-trivial solution)。如果將其他未知數視爲常數(定数)時，求得的解會有變化嗎？&lt;br&gt;
若視 &lt;span class=&#34;math inline&#34;&gt;\(a_2=s\)&lt;/span&gt; 求解連立方程的解時，我們會獲得 &lt;span class=&#34;math inline&#34;&gt;\(a_1=-\frac{3}{2}s, a_2=s, a_3=-\frac{1}{2}s\)&lt;/span&gt;。若視 &lt;span class=&#34;math inline&#34;&gt;\(a_1=s\)&lt;/span&gt; 時，計算可得 &lt;span class=&#34;math inline&#34;&gt;\(a_1=s, a_2=-\frac{2}{3}s,a_3=-\frac{1}{3}s\)&lt;/span&gt;。&lt;br&gt;
由此可見，非自明解表面看去各不相同，但是都滿足了 &lt;span class=&#34;math inline&#34;&gt;\(a_1:a_2:a_3=-3:2:1\)&lt;/span&gt; 的本質條件。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;解 &lt;span class=&#34;math inline&#34;&gt;\(\left\{ \begin{array}{ll}  (1)\;4a_1+3a_2+6a_3 = 0 \\  (2)\;2a_1+a_2+4a_3 = 0 \;\\  (3)\;a_1^2+a_2^2+a_3^2 = 0 \\  \end{array} \right.\)&lt;/span&gt; &lt;br&gt;
上述方程組其實是將例題2.中的方程 &lt;span class=&#34;math inline&#34;&gt;\((3)\)&lt;/span&gt; 替換成了2次方程。&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(3\times(2)-(1)\)&lt;/span&gt; 可得 &lt;span class=&#34;math inline&#34;&gt;\(a_1=-3a_3\)&lt;/span&gt; &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\((1)-2\times(2)\)&lt;/span&gt; 可得 &lt;span class=&#34;math inline&#34;&gt;\(a_2=2a_3\)&lt;/span&gt; &lt;br&gt;
以上代入 &lt;span class=&#34;math inline&#34;&gt;\((3)\)&lt;/span&gt; 可得， &lt;span class=&#34;math inline&#34;&gt;\(a_3 = \pm \frac{1}{\sqrt{14}}\)&lt;/span&gt;。&lt;br&gt;
總結一下：　&lt;span class=&#34;math inline&#34;&gt;\(a_1=\mp \frac{3}{\sqrt{14}}, a_2=\pm \frac{2}{\sqrt{14}}, a_3=\pm\frac{1}{\sqrt{14}}\)&lt;/span&gt; (複号同順 double-sign corresponds)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;解 &lt;span class=&#34;math inline&#34;&gt;\(a_1+2a_2-3a_3=0\)&lt;/span&gt; &lt;br&gt;
上面的方程只有一個，並不是連立方程組，將其中兩個未知數視爲常數時就變成了只有一個未知數的方程。例如視，&lt;span class=&#34;math inline&#34;&gt;\(a_2=s, a_3=t\)&lt;/span&gt; 代入上述方程則可以得到: &lt;span class=&#34;math inline&#34;&gt;\(a_1=-2s+3t\)&lt;/span&gt;，因此，此方程的解爲： &lt;span class=&#34;math inline&#34;&gt;\(a_1=-2s+3t, a_2=s, a_3=t\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(s,t\)&lt;/span&gt; 爲任意常數，有無數組解。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;練習-解下列連立方程組&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;練習: 解下列連立方程組&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\left\{ \begin{array}{ll}  (1)\;2a_1-3a_2 = 0 \\  (2)\;-4a_1+6a_2 = 0 \;\\  \end{array} \right.\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\left\{ \begin{array}{ll}  (1)\;2a_1-3a_2 = 0 \\  (2)\;-4a_1+6a_2 = 0 \;\\  (3)\;a_1^2+a_2^2 = 0 \\  \end{array} \right.\)&lt;/span&gt; &lt;br&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;解&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;解&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\because\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\((2)=-2\times(1)\)&lt;/span&gt; 實質上方程組僅有一個方程。&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\therefore a_1=\frac{3}{2}s, a_2=s\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;只需要求解例題1 中符合方程 &lt;span class=&#34;math inline&#34;&gt;\((3)\)&lt;/span&gt; 的解即可。 &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\therefore a_1=\pm\frac{3}{\sqrt{13}}, a_2=\pm\frac{2}{\sqrt{13}}\)&lt;/span&gt; (複号同順 double-sign corresponds)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記3</title>
      <link>https://wangcc.me/post/2017-02-10/</link>
      <pubDate>Fri, 10 Feb 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/2017-02-10/</guid>
      <description>


&lt;div id=&#34;函數的最大值最小值問題&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;函數的最大值最小值問題&lt;/h2&gt;
&lt;div id=&#34;沒有制約條件的情況&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;沒有制約條件的情況&lt;/h3&gt;
&lt;p&gt;函數 &lt;span class=&#34;math inline&#34;&gt;\(F(a_1,a_2,\dots,a_i,\dots,a_n)\)&lt;/span&gt; 取最大值或者最小值時，以下的連立方程
&lt;span class=&#34;math display&#34;&gt;\[\frac{\partial F}{\partial a_1}=0,\frac{\partial F}{\partial a_2}=0，\frac{\partial F}{\partial a_3}=0, \dots,\frac{\partial F}{\partial a_i}=0, \dots, \frac{\partial F}{\partial a_n}=0\]&lt;/span&gt;
要成立&lt;strong&gt;(必要條件)&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;1.已知下列方程有最小值，求當該方程取最小值時&lt;span class=&#34;math inline&#34;&gt;\(a_1,a_2\)&lt;/span&gt;的值 &lt;span class=&#34;math display&#34;&gt;\[F(a_1,a_2)=\left\{y_1-(a_1+a_2x_1)\right\}^2+\left\{y_2-(a_1+a_2x_2)\right\}^2+\cdots+\left\{y_n-(a_1+a_2x_n)\right\}^2\\
\;\;\;\;\;\;\;\;\;\;\;\;\;\;=\sum\limits_{i=1}^n\left\{y_i-(a_1+a_2x_i)\right\}^2\\\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
\frac{\partial F}{\partial a_1}&amp;amp;=-2\left\{y_1-(a_1+a_2x_1)\right\}-2\left\{y_2-(a_1+a_2x_2)\right\}-\cdots-2\left\{y_n-(a_1+a_2x_n)\right\}\\
&amp;amp;= -2\sum_{i=1}^n\left\{y_i-(a_1+a_2x_i)\right\}=0 \Leftrightarrow  \sum_{i=1}^n\left\{y_i-(a_1+a_2x_i)\right\}=0\\
\Leftrightarrow \sum_{i=1}^ny_i &amp;amp;= a_1\cdot n+a_2\sum_{i=1}^nx_i (1)\\
\\
\frac{\partial F}{\partial a_2}&amp;amp;=-2x_1\left\{y_1-(a_1+a_2x_1)\right\}-2x_2\left\{y_2-(a_1+a_2x_2)\right\}-\cdots-2x_3\left\{y_n-(a_1+a_2x_n)\right\}\\
&amp;amp;= -2\sum_{i=1}^nx_i\left\{y_i-(a_1+a_2x_i)\right\}=0\\
\Leftrightarrow \sum_{i=1}^nx_iy_i &amp;amp;=a_1\sum_{i=1}^nx_i+a_2\sum_{i=1}^nx_i^2 (2)\\

&amp;amp;將(1)(2)連立方程求解即可。在回歸分析中，\\
&amp;amp;這個連立方程組被稱作正規方程組(Normal \;equation)
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;求下列方程取最大或者最小值時的&lt;span class=&#34;math inline&#34;&gt;\(a_1,a_2,a_3\)&lt;/span&gt;的大小：
&lt;span class=&#34;math display&#34;&gt;\[F(a_1,a_2,a_3)=a_1^2+a_1a_2+a_1a_3+a_2^2+a_2a_3+a_3^2-6a_1-3a_2-7a_3\]&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
解連立方程：\\
\frac{\partial F}{\partial a_1} &amp;amp; = 2a_1+a_2+a_3-6=0\\
\frac{\partial F}{\partial a_2} &amp;amp; = a_1+2a_2+a_3-3=0\\
\frac{\partial F}{\partial a_3} &amp;amp; = a_1+a_2+2a_3-7=0\\
答：&amp;amp; a_1=2, a_2=-1,a_3=3
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記2</title>
      <link>https://wangcc.me/post/2017-02-08/</link>
      <pubDate>Wed, 08 Feb 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/2017-02-08/</guid>
      <description>


&lt;div id=&#34;偏微分&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;偏微分&lt;/h2&gt;
&lt;div id=&#34;個變量的函數的微分&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;1個變量的函數的微分&lt;/h3&gt;
&lt;div id=&#34;公式&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;公式：&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;函數 &lt;span class=&#34;math inline&#34;&gt;\(f(a)\)&lt;/span&gt; 關於變量 &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; 的微分，被定義爲： &lt;span class=&#34;math inline&#34;&gt;\(\lim\limits_{h \to 0} \frac{f(a+h)-f(a)}{h}\)&lt;/span&gt; , 寫作 &lt;span class=&#34;math inline&#34;&gt;\(\frac{df}{da}\)&lt;/span&gt;, 具有下列性質：
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(f(a) = a^n\)&lt;/span&gt; 時， &lt;span class=&#34;math inline&#34;&gt;\(\frac{df}{da} = na^{n-1}\)&lt;/span&gt; &lt;strong&gt;重要&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{d}{da}\left\{kf(a)+lg(a)\right\}=k\frac{df}{da}+l\frac{dg}{da}\)&lt;/span&gt; &lt;strong&gt;(&lt;span class=&#34;math inline&#34;&gt;\(k,l\)&lt;/span&gt; 是常數)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{d}{da}\left\{f(a) \cdot g(a)\right\}=\frac{df}{da}g(a)+f{a}\frac{dg}{da}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{d}{da}\left\{\frac{f(a)}{g(a)}\right\}=\frac{\frac{df}{da}g(a)-f(a)\frac{dg}{da}}{\left\{g(a)\right\}^2}\)&lt;/span&gt;, 特別的有，&lt;span class=&#34;math inline&#34;&gt;\(\frac{d}{da}\left\{\frac{1}{g(a)}\right\}=-\frac{\frac{dg}{da}}{\left\{g(a)\right\}^2}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(y=f(b), b=g(a)\)&lt;/span&gt; 時， &lt;span class=&#34;math inline&#34;&gt;\(\frac{dy}{da}=\frac{dy}{db}\frac{db}{da}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;2次（2階）微分 【二階導數】:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(f(a)\)&lt;/span&gt; 關於常數 &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; 的微分 &lt;span class=&#34;math inline&#34;&gt;\(\frac{df}{da}\)&lt;/span&gt; 的二次微分表示爲： &lt;span class=&#34;math inline&#34;&gt;\(\frac{d^2f}{da^2}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;多個變量的函數的微分&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;多個變量的函數的微分&lt;/h3&gt;
&lt;div id=&#34;偏微分-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;偏微分&lt;/h4&gt;
&lt;p&gt;包含了 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 個獨立變量 &lt;span class=&#34;math inline&#34;&gt;\(a_1, a_2,a_3,\cdots,a_i,\cdots,a_n\)&lt;/span&gt;的函數，即多變量函數 &lt;span class=&#34;math inline&#34;&gt;\(F(a_1, a_2,a_3,\cdots,a_i,\cdots,a_n)\)&lt;/span&gt; 關於 &lt;span class=&#34;math inline&#34;&gt;\(a_i (i=1,2,\cdots,n)\)&lt;/span&gt; 的偏微分 &lt;em&gt;(partial differentiation)&lt;/em&gt; 的定義是，把 &lt;span class=&#34;math inline&#34;&gt;\(a_i\)&lt;/span&gt; 以外的獨立變量當做常數（定数），將函數 &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; 對變量 &lt;span class=&#34;math inline&#34;&gt;\(a_i\)&lt;/span&gt; 求微分，寫作： &lt;span class=&#34;math inline&#34;&gt;\(\frac{\partial F}{\partial a_i}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;以下爲了便於說明，以三個變量爲例。&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;函數 &lt;span class=&#34;math inline&#34;&gt;\(F(a_1,a_2,a_3)=a_1+a_2+a_3=\sum\limits_{i=1}^3a_i\)&lt;/span&gt; 對於三個獨立變量分別求偏微分：
&lt;span class=&#34;math display&#34;&gt;\[\frac{\partial F}{\partial a_1}=1，\frac{\partial F}{\partial a_2}=1， \frac{\partial F}{\partial a_3}=1\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;函數 &lt;span class=&#34;math inline&#34;&gt;\(F(a_1,a_2,a_3)=a_1b_1+a_2b_2+a_3b_3=\sum\limits_{i=1}^3a_ib_i\)&lt;/span&gt; 對於三個獨立變量分別求偏微分：
&lt;span class=&#34;math display&#34;&gt;\[\frac{\partial F}{\partial a_1}=b_1，\frac{\partial F}{\partial a_2}=b_2， \frac{\partial F}{\partial a_3}=b_3\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;函數 &lt;span class=&#34;math inline&#34;&gt;\(F(a_1,a_2,a_3)=a_1^2+a_2^2+a_3^2=a_1\cdot a_1+a_2\cdot a_2+a_3\cdot a_3\\=\sum\limits_{i=1}^3a_i^2=\sum\limits_{i=1}^3a_i\cdot a_i \;對三個變量分別求偏微分：\)&lt;/span&gt;　
&lt;span class=&#34;math display&#34;&gt;\[\frac{\partial F}{\partial a_1}=2a_1，\frac{\partial F}{\partial a_2}=2a_2， \frac{\partial F}{\partial a_3}=2a_3\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;函數 &lt;span class=&#34;math inline&#34;&gt;\(F(a_1,a_2,a_3)=\lambda_1a_1^2+\lambda_2a_2^2+\lambda_3a_3^2=a_1\cdot\lambda_1\cdot a_1+a_2\cdot\lambda_2\cdot a_2+a_3\cdot\lambda_3\cdot a_3\\=\sum\limits_{i=1}^3\lambda_ia_i^2=\sum\limits_{i=1}^3a_i\cdot\lambda_i\cdot a_i \; 對三個變量分別求偏微分：\)&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\frac{\partial F}{\partial a_1}=2\lambda_1a_1，\frac{\partial F}{\partial a_2}=2\lambda_2a_2， \frac{\partial F}{\partial a_3}=2\lambda_3a_3\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;函數 &lt;span class=&#34;math inline&#34;&gt;\(F(a_1,a_2,a_3)=(b_1-\lambda a_1)^2+(b_2-\lambda a_2)^2+(b_3-\lambda a_3)^2\\=\sum\limits_{i=1}^3(b_i-\lambda a_i)^2=\sum\limits_{i=1}^3(b_i-\lambda a_i)(b_i-\lambda a_i)\;對三個變量求偏微分：\)&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\frac{\partial F}{\partial a_1}=-2\lambda(b_1-\lambda a_1)，\frac{\partial F}{\partial a_2}=-2\lambda(b_2-\lambda a_2)， \frac{\partial F}{\partial a_3}=-2\lambda(b_3-\lambda a_3)\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;函數 &lt;span class=&#34;math inline&#34;&gt;\(F = a_{11}x_1y_1 + a_{12}x_1y_2 + a_{13}x_1y_3 \\ \;\;\;\;\;\;+a_{21}x_2y_1+a_{22}x_2y_2+a_{23}x_2y_3\\ \;\;\;\;\;\;+a_{31}x_3y_1+a_{32}x_3y_2+a_{33}x_3y_3\\ \;\;\;=\sum\limits_{i=1}^3\sum\limits_{i=1}^3a_{ij}x_iy_j\;對三個變量求偏微分：\)&lt;/span&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(F(x_1,x_2,x_3)\)&lt;/span&gt;, 即視爲 &lt;span class=&#34;math inline&#34;&gt;\(x_1,x_2,x_3\)&lt;/span&gt; 的函數的時候：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
 \frac{\partial F}{\partial x_1}=a_{11}y_1+a_{12}y_2+a_{13}y_3=\sum_{j=1}^3a_{1j}y_j \\
 \frac{\partial F}{\partial x_2}=a_{21}y_1+a_{22}y_2+a_{23}y_3=\sum_{j=1}^3a_{2j}y_j \\
 \frac{\partial F}{\partial x_3}=a_{31}y_1+a_{32}y_2+a_{33}y_3=\sum_{j=1}^3a_{3j}y_j \\
 將上面三個式子總結一下就是: \\
 \frac{\partial F}{\partial x_i}=\sum_{j=1}^3a_{ij}y_j (i=1,2,3)
 \]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(F(y_1,y_2,y_3)\)&lt;/span&gt;, 即視爲 &lt;span class=&#34;math inline&#34;&gt;\(y_1,y_2,y_3\)&lt;/span&gt; 的函數的時候：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
 \frac{\partial F}{\partial y_1}=a_{11}x_1+a_{21}x_2+a_{31}x_3=\sum_{i=1}^3a_{i1}x_i \\
 \frac{\partial F}{\partial y_2}=a_{12}x_1+a_{22}x_2+a_{32}x_3=\sum_{i=1}^3a_{i2}x_i \\
 \frac{\partial F}{\partial y_3}=a_{13}x_1+a_{32}x_2+a_{33}x_3=\sum_{i=1}^3a_{i3}x_i \\
 將上面三個式子總結一下就是: \\
 \frac{\partial F}{\partial x_i}=\sum_{i=1}^3a_{ij}x_i (j=1,2,3)
 \]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;函數 &lt;span class=&#34;math inline&#34;&gt;\(F(x_1,x_2,x_3)=a_{11}x_1x_1+a_{12}x_1x_2+a_{13}x_1x_3 \\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+a_{12}x_2x_1+a_{12}x_2x_2+a_{23}x_2x_3\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+a_{13}x_3x_1+a_{23}x_3x_2+a_{33}x_3x_3\\ \;\;\;\;\;\;\;\;\;\;\;\;=a_{11}x_1^2+2a_{12}x_1x_2+2a_{13}x_1x_3+a_{22}x_2^2+2a_{23}x_2x_3+a_{33}x_3^2\\ \;\;\;\;\;\;\;\;\;\;\;\;=\sum\limits_{i=1}^3a_{ii}x_i^2+2\mathop{\sum\limits^3\sum\limits^3}\limits_{i&amp;lt;j}a_{ij}x_ix_j\\ \;\;\;\;\;\;\;\;\;\;\;\;==\sum\limits_{i=1}^3x_ia_{ii}x_i+2\mathop{\sum\limits^3\sum\limits^3}\limits_{i&amp;lt;j}x_ia_{ij}x_j\;對三個變量求偏微分：\)&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\frac{\partial F}{\partial x_1}=2(a_{11}x_1+a_{12}x_2+a_{13}x_3)\\
\frac{\partial F}{\partial x_2}=2(a_{12}x_1+a_{22}x_2+a_{23}x_3)\\
\frac{\partial F}{\partial x_3}=2(a_{13}x_1+a_{23}x_2+a_{33}x_3)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;函數 &lt;span class=&#34;math inline&#34;&gt;\(F(x_1,x_2,x_3)=a_{11}x_1x_1+a_{12}x_1x_2+a_{13}x_1x_3 \\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+a_{12}x_2x_1+a_{12}x_2x_2+a_{23}x_2x_3\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+a_{13}x_3x_1+a_{23}x_3x_2+a_{33}x_3x_3\\ G(x_1,x_2,x_3)=b_{11}x_1x_1+b_{12}x_1x_2+b_{13}x_1x_3 \\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+b_{12}x_2x_1+b_{12}x_2x_2+b_{23}x_2x_3\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+b_{13}x_3x_1+b_{23}x_3x_2+b_{33}x_3x_3\\\;對三個變量求偏微分：\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\frac{\partial}{\partial x_1}(\frac{F}{G})=\frac{\frac{\partial F}{\partial x_1}G(x_1,x_2,x_3)-F(x_1,x_2,x_3)\frac{\partial G}{\partial x_1}}{\left\{G(x_1,x_2,x_3)\right\}^2}\\
=2\cdot \frac{(a_{11}x_1+a_{12}x_2+a_{13}x_3)\cdot G(x_1,x_2,x_3)-F(x_1,x_2,x_3)\cdot (b_{11}x_1+b_{12}x_2+b_{13}x_3)}{\left\{G(x_1,x_2,x_3)\right\}^2}\\
\\
\frac{\partial}{\partial x_2}(\frac{F}{G})=\frac{\frac{\partial F}{\partial x_2}G(x_1,x_2,x_3)-F(x_1,x_2,x_3)\frac{\partial G}{\partial x_2}}{\left\{G(x_1,x_2,x_3)\right\}^2}\\
=2\cdot \frac{(a_{12}x_1+a_{22}x_2+a_{23}x_3)\cdot G(x_1,x_2,x_3)-F(x_1,x_2,x_3)\cdot (b_{12}x_1+b_{22}x_2+b_{23}x_3)}{\left\{G(x_1,x_2,x_3)\right\}^2}\\
\frac{\partial}{\partial x_3}(\frac{F}{G})=\frac{\frac{\partial F}{\partial x_3}G(x_1,x_2,x_3)-F(x_1,x_2,x_3)\frac{\partial G}{\partial x_3}}{\left\{G(x_1,x_2,x_3)\right\}^2}\\
=2\cdot \frac{(a_{13}x_1+a_{23}x_2+a_{33}x_3)\cdot G(x_1,x_2,x_3)-F(x_1,x_2,x_3)\cdot (b_{13}x_1+b_{23}x_2+b_{33}x_3)}{\left\{G(x_1,x_2,x_3)\right\}^2}\\
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;次2階偏微分-二階導數&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;2次（2階）偏微分 【二階導數】:&lt;/h4&gt;
&lt;p&gt;函數 &lt;span class=&#34;math inline&#34;&gt;\(F(a_1,a_2,\cdots,a_n)\)&lt;/span&gt; 對 &lt;span class=&#34;math inline&#34;&gt;\(a_i\)&lt;/span&gt; 取偏微分 &lt;span class=&#34;math inline&#34;&gt;\(\frac{\partial F}{\partial a_i}\)&lt;/span&gt; 時，記作 &lt;span class=&#34;math inline&#34;&gt;\(\frac{\partial^2 F}{\partial a_i^2}\)&lt;/span&gt; ; 取變量 &lt;span class=&#34;math inline&#34;&gt;\(a_j\)&lt;/span&gt; 的偏微分時記作 &lt;span class=&#34;math inline&#34;&gt;\(\frac{\partial^2 F}{\partial a_i\partial a_j}\)&lt;/span&gt; 或者 &lt;span class=&#34;math inline&#34;&gt;\(\frac{\partial^2 F}{\partial a_j\partial a_i}\)&lt;/span&gt;。 這些都被稱爲是函數 &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; 的2次（2階）偏微分。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記1</title>
      <link>https://wangcc.me/post/2017-02-06/</link>
      <pubDate>Mon, 06 Feb 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/2017-02-06/</guid>
      <description>


&lt;div id=&#34;和記號sum&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;和記號&lt;span class=&#34;math inline&#34;&gt;\(\sum\)&lt;/span&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sum\)&lt;/span&gt; 的性質 (1)
&lt;strong&gt;下標(添字)&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(x_1 + x_2 + x_3 + \dots + x_n\)&lt;/span&gt; 記作如下:&lt;span class=&#34;math display&#34;&gt;\[\sum_{i=1}^{n}x_i\]&lt;/span&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^{n}x_i\)&lt;/span&gt; 中的&lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; 稱爲&lt;code&gt;dummy index&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;可以簡略寫爲：&lt;span class=&#34;math inline&#34;&gt;\(\sum x\)&lt;/span&gt; 或者 &lt;span class=&#34;math inline&#34;&gt;\(\sum_1 x_i\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\sum x_i\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sum\)&lt;/span&gt; 的性質 (2)
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:1&#34;&gt;\[\begin{equation}
\sum_{i=1}^{n}(ax_i + by_i)= a\sum_{i=1}^{n}x_i + b\sum_{i=1}^{n}y_i \tag{1}
\end{equation}\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^{n}ax_i = a\sum_{i=1}^{n}x_i\)&lt;/span&gt; &lt;code&gt;常數(定数)可以提前&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^{n}a = na\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^{n}1 = n\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^{n}(ax_i+b) = a\sum_{i=1}^{n}x_i + nb\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;公式&lt;a href=&#34;#eq:1&#34;&gt;(1)&lt;/a&gt;的應用:
&lt;span class=&#34;math display&#34;&gt;\[
  \begin{aligned}
  \sum_{i=1}^{n}(ax_i -by_i)^2 &amp;amp;= \sum_{i=1}^{n}(a^2x_i^2 - 2abx_iy_i + b^2y_i^2) \\
   &amp;amp;= \sum_{i=1}^{n}a^2x_i^2 -\sum_{i=1}^{n}2abx_iy_i + \sum_{i=1}^{n}b^2y_i^2 \\
   &amp;amp;= a^2\sum_{i=1}^{n}x_i^2 - 2ab\sum_{i=1}^{n}x_iy_i + b^2\sum_{i=1}^{n}y_i^2
  \end{aligned}
  \]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;但是，乘法或平方有如下性質，計算方差(分散)或者相關系數時需要注意：&lt;span class=&#34;math display&#34;&gt;\[\sum_{i=1}^{n}x_i^2 \neq (\sum_{i=1}^{n}x_i)^2\]&lt;/span&gt; 以及 &lt;span class=&#34;math display&#34;&gt;\[\sum_{i=1}^{n}x_iy_i \neq (\sum_{i=1}^{n}x_i)(\sum_{i=1}^{n}y_i)\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;自然數的冪運算之和(冪[べき]乗の和)的公式:
&lt;span class=&#34;math display&#34;&gt;\[
  \begin{aligned}
  1+2+3+\dots+n &amp;amp;= \sum_{t=1}^{n}t = \frac{n(n+1)}{2}\\
  1^2+2^2+3^2+\dots+n^2 &amp;amp;= \sum_{t=1}^{n}t^2 = \frac{n(n+1)(2n+1)}{6} \\
  1^3+2^3+3^3+\dots+n^3 &amp;amp;= \sum_{t=1}^{n}t^3 = {\frac{n(n+1)}{2}}^2 \\
  1^4+2^4+3^4+\dots+n^4 &amp;amp;= \sum_{t=1}^{n}t^4 = \frac{n(n+1)(2n+1)(3n^2+3n-1)}{30}
  \end{aligned}
  \]&lt;/span&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    上面的公式將會應用在時間序列分析，斯皮尔曼等级相关系数(スピアマンの順位相関係数)的定義公式的推導。
  &lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;sum式子變形成普通計算式&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sum\)&lt;/span&gt;式子變形成普通計算式：&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math display&#34;&gt;\[\sum_{i=1}^{n}f_{ij} = f_{i1} + f_{i2} + f_{i3} + \dots + f_{in}\]&lt;/span&gt; 此式子也常寫作&lt;span class=&#34;math inline&#34;&gt;\(f_{i+}\)&lt;/span&gt;, 或者&lt;span class=&#34;math inline&#34;&gt;\(f_{i\cdot}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math display&#34;&gt;\[\sum_{i=1}^{m}f_{ij} = f_{1i} + f_{2i} + f_{3i} + \dots + f_{mj}\]&lt;/span&gt; 此式子也常寫作&lt;span class=&#34;math inline&#34;&gt;\(f_{+j}\)&lt;/span&gt;, 或者&lt;span class=&#34;math inline&#34;&gt;\(f_{\cdot j}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math display&#34;&gt;\[\sum_{i=1}^{2}\sum_{j=1}^{3}x_{ij} = \sum_{i=1}^{2}(\sum_{i=j}^{3}x_{ij}) = \sum_{i=1}^{2}(x_{i1} + x_{i2} + x_{i3}) = (x_{11} + x_{12} + x_{13}) + (x_{21} + x_{22} + x_{23})\]&lt;/span&gt; 此式子也可以寫作&lt;span class=&#34;math inline&#34;&gt;\(x_{++}\)&lt;/span&gt;, 或者&lt;span class=&#34;math inline&#34;&gt;\(x_{\cdot\cdot}\)&lt;/span&gt;。另外，中間的式子如果是&lt;span class=&#34;math inline&#34;&gt;\(\sum_{j=1}^{3}(\sum_{i=1}^{2}x_{ij})\)&lt;/span&gt;也可以成立，過程如下：&lt;span class=&#34;math display&#34;&gt;\[\sum_{j=1}^{3}(\sum_{i=1}^{2}x_{ij})=\sum_{j=1}^{3}(x_{1j} + x_{2j}) = (x_{11} + x_{21}) + (x_{12} + x_{22}) + (x_{13} + x_{23})\]&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math display&#34;&gt;\[
  \begin{aligned}
  \sum_{i=1}^2\sum_{j=1}^2a_{ij}x_ix_j &amp;amp;= \sum_{i=1}^2(\sum_{j=1}^2a_{ij}x_ix_j) \\
  &amp;amp;= \sum_{i=1}^2({\sum_{j=1}^2a_{ij}x_j)x_i} \\
  &amp;amp;= \sum_{i=1}^2(a_{i1}x_1 + a_{i2}x_2)x_i \\
  &amp;amp;= (a_{11}x_1 + a_{12}x_2)x_1 + (a_{21}x_1 + a_{22}x_2)x_2 \\
  &amp;amp;= a_{11}x_1^2 + (a_{12} + a_{21})x_1x_2 + a_{22}x_2^2
  \end{aligned}
  \]&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math display&#34;&gt;\[
  \begin{aligned}
  \sum_{k=1}^3\left\{(\sum_{i=1}^2b_ix_{ik})(\sum_{j=1}^2b_jx_{jk})\right\} &amp;amp;= \sum_{k=1}^3\left\{(b_1x_{1k} + b_2x_{2k})(b_1x_{1k} + b_2x_{2k})\right\} \\
  &amp;amp;= \sum_{k=1}^3(b_1x_{1k} + b_2x_{2k})^2 \\
  &amp;amp;= (b_1x_{11} + b_2x_{21})^2 + (b_1x_{12} + b_2x_{22})^2 + (b_1x_{13} + b_2x_{23})^2
  \end{aligned}
  \]&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathop{\sum\limits^3\sum\limits^3}\limits_{i&amp;lt;j}e_{ij}\)&lt;/span&gt; 會變成怎樣的式子呢？ 滿足 &lt;span class=&#34;math inline&#34;&gt;\(i&amp;lt;j (i = 1,2,3; j = 1,2,3)\)&lt;/span&gt; 條件的 &lt;span class=&#34;math inline&#34;&gt;\(i,j\)&lt;/span&gt;, 有且僅有 &lt;span class=&#34;math inline&#34;&gt;\((1,2),(1,3),(2,3)\)&lt;/span&gt;,故 &lt;span class=&#34;math inline&#34;&gt;\(\mathop{\sum\limits^3\sum\limits^3}\limits_{i&amp;lt;j}e_{ij} = e_{12} + e_{13} + e_{23}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;
&lt;ol start=&#34;7&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;那麼&lt;span class=&#34;math inline&#34;&gt;\(\mathop{\sum\limits^3\sum\limits^3}\limits_{i\neq j}e_{ij}\)&lt;/span&gt;又會變成怎樣的式子呢？ 滿足 &lt;span class=&#34;math inline&#34;&gt;\(i\neq j (i = 1,2,3; j = 1,2,3)\)&lt;/span&gt; 的 &lt;span class=&#34;math inline&#34;&gt;\((i,j)\)&lt;/span&gt; 有6種組合:&lt;span class=&#34;math inline&#34;&gt;\((1,2),(2,1),(1,3),(3,1),(2,3),(3,2)\)&lt;/span&gt;, 故&lt;span class=&#34;math inline&#34;&gt;\(\mathop{\sum\limits^3\sum\limits^3}\limits_{i\neq j}e_{ij} = e_{12} + e_{21} + e_{13} + e_{31} + e_{23} + e_{32}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;加法算式變形爲sum&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;加法算式變形爲&lt;span class=&#34;math inline&#34;&gt;\(\sum\)&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;在多元變量分析(多変量解析)中，與前項相比，加法算式變形成爲&lt;span class=&#34;math inline&#34;&gt;\(\sum\)&lt;/span&gt;式子更加重要。也就是說，以前項計算爲例的話，作爲答案的計算式如果放在題幹，反向求解&lt;span class=&#34;math inline&#34;&gt;\(\sum\)&lt;/span&gt;式的過程更加常用。簡單練習一下吧：&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;將計算式&lt;span class=&#34;math inline&#34;&gt;\(a_1x_1^2 + a_2x_2^2 + a_3x_3^2 + a_4x_4^2 + a_5x_5^2\)&lt;/span&gt;改寫成&lt;span class=&#34;math inline&#34;&gt;\(\sum\)&lt;/span&gt;式：
&lt;ul&gt;
&lt;li&gt;先寫下：&lt;span class=&#34;math inline&#34;&gt;\(\sum\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;再寫各個單元的共通部分&lt;span class=&#34;math inline&#34;&gt;\((a,x^2)\)&lt;/span&gt;： &lt;span class=&#34;math inline&#34;&gt;\(\sum ax^2\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;各單元不同的僅爲下標： &lt;span class=&#34;math inline&#34;&gt;\(\sum a_ix_i^2\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;注意到下標的變化規律爲&lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;到&lt;span class=&#34;math inline&#34;&gt;\(5\)&lt;/span&gt;之間的整數，故在&lt;span class=&#34;math inline&#34;&gt;\(\sum\)&lt;/span&gt;符號的上部寫上&lt;span class=&#34;math inline&#34;&gt;\(5\)&lt;/span&gt;，下部寫上&lt;span class=&#34;math inline&#34;&gt;\(i=1\)&lt;/span&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\sum\limits_{i=1}^5a_ix_i^2\)&lt;/span&gt; (答)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;將計算式&lt;span class=&#34;math inline&#34;&gt;\(f_2(x_2 - \bar{x})^2 + f_3(x_3 - \bar{x})^2 + f_4(x_4 - \bar{x})^2 + f_5(x_5 - \bar{x})^2\)&lt;/span&gt;改寫成&lt;span class=&#34;math inline&#34;&gt;\(\sum\)&lt;/span&gt;式：
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sum\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sum f(x-\bar{x})^2\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sum f_i(x_i - \bar{x})^2\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sum\limits_{i=2}^5f_i(x_i - \bar{x})^2\)&lt;/span&gt; (答)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;提問&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;提問：&lt;/h2&gt;
&lt;div id=&#34;我們現在了解了可以使用簡單的sum符號來表達復雜有規律的加法算式請問有沒有相應的記號來代表乘法&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;我們現在了解了可以使用簡單的&lt;span class=&#34;math inline&#34;&gt;\(\sum\)&lt;/span&gt;符號來表達復雜有規律的加法算式。請問有沒有相應的記號來代表乘法？&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;當然有用希臘字母pi的大寫pi來表示例如-x_1x_2x_3x_4x_5-prod_i15x_i-x_1x_2x_3dots-x_n-prod_i1nx_i&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;當然有。用希臘字母&lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt;的大寫&lt;span class=&#34;math inline&#34;&gt;\(\Pi\)&lt;/span&gt;來表示。例如： &lt;span class=&#34;math display&#34;&gt;\[x_1x_2x_3x_4x_5 = \prod_{i=1}^5x_i\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[x_1x_2x_3\dots x_n = \prod_{i=1}^nx_i\]&lt;/span&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;乘法記號可以證明下面的等式成立-prod_i1nx_i2-prod_i1nx_i2-logprod_i1nx_i-prod_i1nlog-x_i&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;乘法記號可以證明下面的等式成立： &lt;span class=&#34;math display&#34;&gt;\[\prod_{i=1}^nx_i^2 = (\prod_{i=1}^nx_i)^2\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[\log(\prod_{i=1}^nx_i) = \prod_{i=1}^n\log x_i\]&lt;/span&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
