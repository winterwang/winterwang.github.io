<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>statistics | Chaochen Wang - Be Ambitious</title>
    <link>https://wangcc.me/categories/statistics/</link>
      <atom:link href="https://wangcc.me/categories/statistics/index.xml" rel="self" type="application/rss+xml" />
    <description>statistics</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2017-2025 Chaochen Wang | 王超辰</copyright><lastBuildDate>Fri, 16 Aug 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://wangcc.me/img/icon-192.png</url>
      <title>statistics</title>
      <link>https://wangcc.me/categories/statistics/</link>
    </image>
    
    <item>
      <title>等級線性回歸模型的 Rstan 貝葉斯實現</title>
      <link>https://wangcc.me/post/multilevel-model-rstan/</link>
      <pubDate>Fri, 16 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/multilevel-model-rstan/</guid>
      <description>
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#多層等級線性回歸模型混合效應模型-multilevelmixed-effect-regression-model&#34;&gt;多層(等級)線性回歸模型/混合效應模型 multilevel/mixed effect regression model&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#適用於等級線性回歸模型的數據&#34;&gt;適用於等級線性回歸模型的數據&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#確認數據分佈&#34;&gt;確認數據分佈&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#如果不考慮組間公司間差異&#34;&gt;如果不考慮組間(公司間)差異&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#如果要考慮組間差異&#34;&gt;如果要考慮組間差異&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#等級線性回歸的貝葉斯實現&#34;&gt;等級線性回歸的貝葉斯實現&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#模型機制-mechanism&#34;&gt;模型機制 mechanism&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;多層等級線性回歸模型混合效應模型-multilevelmixed-effect-regression-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;多層(等級)線性回歸模型/混合效應模型 multilevel/mixed effect regression model&lt;/h2&gt;
&lt;p&gt;關於等級線性回歸的基本知識和概念，請參考&lt;a href=&#34;https://wangcc.me/LSHTMlearningnote/Hierarchical.html&#34;&gt;讀書筆記58-60章節&lt;/a&gt;。簡單來說，等級線性回歸通過給數據內部可能存在或者已知存在的結構或者層級增加隨機截距或者隨機斜率的方式來輔助解釋組間差異和組內的差異。&lt;/p&gt;
&lt;div id=&#34;適用於等級線性回歸模型的數據&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;適用於等級線性回歸模型的數據&lt;/h3&gt;
&lt;p&gt;本章節使用的數據是四家大公司40名社員的年齡和年收入數據：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d &amp;lt;- read.csv(file=&amp;#39;../../static/files/data-salary-2.txt&amp;#39;)
d&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     X   Y KID
## 1   7 457   1
## 2  10 482   1
## 3  16 518   1
## 4  25 535   1
## 5   5 427   1
## 6  25 603   1
## 7  26 610   1
## 8  18 484   1
## 9  17 508   1
## 10  1 380   1
## 11  5 453   1
## 12  4 391   1
## 13 19 559   1
## 14 10 453   1
## 15 21 517   1
## 16 12 553   2
## 17 17 653   2
## 18 22 763   2
## 19  9 538   2
## 20 18 708   2
## 21 21 740   2
## 22  6 437   2
## 23 15 646   2
## 24  4 422   2
## 25  7 444   2
## 26 10 504   2
## 27  2 376   2
## 28 15 522   3
## 29 27 623   3
## 30 14 515   3
## 31 18 542   3
## 32 20 529   3
## 33 18 540   3
## 34 11 411   3
## 35 26 666   3
## 36 22 641   3
## 37 25 592   3
## 38 28 722   4
## 39 24 726   4
## 40 22 728   4&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;X&lt;/code&gt;: 社員年齡減去23獲得的數據（23歲是大部分人大學畢業入職時的年齡）&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Y&lt;/code&gt;: 年收入（萬日元）&lt;/li&gt;
&lt;li&gt;&lt;code&gt;KID&lt;/code&gt;: 公司編號&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我們認爲，年收入 &lt;code&gt;Y&lt;/code&gt;，是基本平均年收入和隨機誤差（服從均值爲零，方差是 &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; 的正態分佈）之和。且基本平均年收入和年齡成正比（年功序列型企業）。但是呢，因爲不同的公司入職時的基本收入可能不同，且可能隨着年齡增加而增長薪水的速度可能也不一樣。那麼由於不同公司所造成的差異，可以被認爲是組間差異。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;確認數據分佈&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;確認數據分佈&lt;/h3&gt;
&lt;p&gt;這次分析的目的是要瞭解「每個公司&lt;code&gt;KID&lt;/code&gt;內隨着年齡的增加而增長的薪水幅度是多少」，那麼我們要在結果報告中體現的就是每家公司的基本年收入，新入職時的年收入，以及隨着年齡增長而上升的薪水的事後分佈。&lt;/p&gt;
&lt;p&gt;我們先來看把四家公司職員放在一起時的整體圖形：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)

d$KID &amp;lt;- as.factor(d$KID)
res_lm &amp;lt;- lm(Y ~ X, data=d)
coef &amp;lt;- as.numeric(res_lm$coefficients)

p &amp;lt;- ggplot(d, aes(X, Y, shape=KID))
p &amp;lt;- p + theme_bw(base_size=18)
p &amp;lt;- p + geom_abline(intercept=coef[1], slope=coef[2], size=2, alpha=0.3)
p &amp;lt;- p + geom_point(size=2)
p &amp;lt;- p + scale_shape_manual(values=c(16, 2, 4, 9))
p &amp;lt;- p + labs(x=&amp;#39;X (age-23)&amp;#39;, y=&amp;#39;Y (10,000 Yen/year)&amp;#39;)
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:fig8-1&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-08-16-multilevel-model-rstan_files/figure-html/fig8-1-1.png&#34; alt=&#34;年齡和年收入的散點圖，不同點的形狀代表不同的公司編號。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: 年齡和年收入的散點圖，不同點的形狀代表不同的公司編號。
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;從總體的散點圖 &lt;a href=&#34;#fig:fig8-1&#34;&gt;1&lt;/a&gt; 來看，似乎年收入確實是隨着年齡增長而呈現直線增加的趨勢。但是公司編號 &lt;code&gt;KID = 4&lt;/code&gt; 的三名社員薪水似乎是在同一水平的並無明顯變化。這一點可以把四家公司社員的數據分開來看更加清晰:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- ggplot(d, aes(X, Y, shape=KID))
p &amp;lt;- p + theme_bw(base_size=20)
p &amp;lt;- p + geom_abline(intercept=coef[1], slope=coef[2], size=2, alpha=0.3)
p &amp;lt;- p + facet_wrap(~KID)
p &amp;lt;- p + geom_line(stat=&amp;#39;smooth&amp;#39;, method=&amp;#39;lm&amp;#39;, se=FALSE, size=1, color=&amp;#39;black&amp;#39;, linetype=&amp;#39;31&amp;#39;, alpha=0.8)
p &amp;lt;- p + geom_point(size=3)
p &amp;lt;- p + scale_shape_manual(values=c(16, 2, 4, 9))
p &amp;lt;- p + labs(x=&amp;#39;X (age-23)&amp;#39;, y=&amp;#39;Y (10,000 Yen/year)&amp;#39;)
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:fig8-2&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-08-16-multilevel-model-rstan_files/figure-html/fig8-2-1.png&#34; alt=&#34;年齡和年收入的散點圖，不同的公司在四個平面中展示,黑色點線是每家公司數據單獨使用線性回歸時獲得的直線。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: 年齡和年收入的散點圖，不同的公司在四個平面中展示,黑色點線是每家公司數據單獨使用線性回歸時獲得的直線。
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;如果不考慮組間公司間差異&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;如果不考慮組間(公司間)差異&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;模型的數學描述&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
              Y[n] &amp;amp; = y_{\text{base}}[n] + \varepsilon[n] &amp;amp; n = 1, \dots, N \\
y_{\text{base}}[n] &amp;amp; = a + bX[n]                           &amp;amp; n = 1, \dots, N \\
    \varepsilon[n] &amp;amp; \sim \text{Normal}(0, \sigma_Y^2)     &amp;amp; n = 1, \dots, N \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;當然，如果你想，模型可以直接簡化成：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y[n] \sim \text{Normal}(a + bX[n], \sigma^2_Y) \;\;\;\;\;\; n = 1, \dots, N \\
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;上述簡化版的模型，翻譯成Stan語言如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data {
  int N;
  real X[N];
  real Y[N];
}

parameters{
  real a;
  real b;
  real&amp;lt;lower = 0&amp;gt; s_Y;
}

model {
  for (n in 1 : N)
  Y[n] = normal(a + b * X[n], s_Y);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;下面的 R 代碼用來實現對上面 Stan 模型的擬合:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rstan)
d &amp;lt;- read.csv(file=&amp;#39;../../static/files/data-salary-2.txt&amp;#39;)
d$KID &amp;lt;- as.factor(d$KID)

data &amp;lt;- list(N=nrow(d), X=d$X, Y=d$Y)
fit &amp;lt;- stan(file=&amp;#39;stanfiles/model8-1.stan&amp;#39;, data=data, seed=1234)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &amp;#39;model8-1&amp;#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 1.3e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.076557 seconds (Warm-up)
## Chain 1:                0.043822 seconds (Sampling)
## Chain 1:                0.120379 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &amp;#39;model8-1&amp;#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 5e-06 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.080947 seconds (Warm-up)
## Chain 2:                0.043589 seconds (Sampling)
## Chain 2:                0.124536 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &amp;#39;model8-1&amp;#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 3e-06 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.077026 seconds (Warm-up)
## Chain 3:                0.043674 seconds (Sampling)
## Chain 3:                0.1207 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &amp;#39;model8-1&amp;#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 5e-06 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.082446 seconds (Warm-up)
## Chain 4:                0.035819 seconds (Sampling)
## Chain 4:                0.118265 seconds (Total)
## Chain 4:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Stan model: model8-1.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##         mean se_mean    sd    2.5%     25%     50%     75%   97.5% n_eff Rhat
## a     376.15    0.63 24.41  328.86  359.80  376.18  392.66  423.88  1483    1
## b      11.05    0.04  1.41    8.27   10.12   11.02   12.02   13.78  1527    1
## s_Y    68.42    0.21  8.26   54.41   62.59   67.64   73.53   86.73  1538    1
## lp__ -184.12    0.03  1.31 -187.60 -184.70 -183.76 -183.18 -182.66  1391    1
## 
## Samples were drawn using NUTS(diag_e) at Mon May 18 16:58:23 2020.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;現在有更加方便的 &lt;code&gt;rstanarm&lt;/code&gt; 包可以幫助我們省去寫 Stan 模型的過程：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rstanarm)

rstanarm_results = stan_glm(Y ~ X, data=d, iter=2000, warmup=1000, cores=4)
summary(rstanarm_results, probs=c(.025, .975), digits=3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Model Info:
##  function:     stan_glm
##  family:       gaussian [identity]
##  formula:      Y ~ X
##  algorithm:    sampling
##  sample:       4000 (posterior sample size)
##  priors:       see help(&amp;#39;prior_summary&amp;#39;)
##  observations: 40
##  predictors:   2
## 
## Estimates:
##               mean    sd      2.5%    97.5%
## (Intercept) 376.371  24.615 328.532 423.842
## X            11.030   1.401   8.387  13.793
## sigma        68.159   8.136  54.474  86.401
## 
## Fit Diagnostics:
##            mean    sd      2.5%    97.5%
## mean_PPD 548.026  15.100 517.682 577.179
## 
## The mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help(&amp;#39;summary.stanreg&amp;#39;)).
## 
## MCMC diagnostics
##               mcse  Rhat  n_eff
## (Intercept)   0.397 1.000 3836 
## X             0.022 1.001 3940 
## sigma         0.146 1.000 3112 
## mean_PPD      0.245 1.000 3795 
## log-posterior 0.029 1.001 1788 
## 
## For each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;可以看到強制不同公司社員的年收入來自同一個正態分佈時，方差顯得非常的大。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;如果要考慮組間差異&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;如果要考慮組間差異&lt;/h3&gt;
&lt;p&gt;我們認爲每家公司社員新入職時的起點薪水不同(截距不同-隨機截距)，進入公司之後隨年齡增加的薪水幅度也不同(斜率不同-隨機斜率)。因此，用 &lt;span class=&#34;math inline&#34;&gt;\(a[1]\sim a[K], K = 1, 2, 3, 4\)&lt;/span&gt; 表示每家公司的截距，用 &lt;span class=&#34;math inline&#34;&gt;\(b[1] \sim b[K], K = 1, 2, 3, 4\)&lt;/span&gt; 表示每家公司薪水上升的斜率。那麼每家公司的薪水年齡線性回歸模型可以寫作是 &lt;span class=&#34;math inline&#34;&gt;\(a[K] + b[K] X, K = 1, 2, 3, 4\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;模型數學描述&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y[n] \sim \text{Normal}(a[\text{KID[n]}] + b[\text{KID}[n]] X[n], \sigma^2_Y) \\ n = 1, \dots, N
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;上述模型的 Stan 譯文如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data {
  int N;
  int K;
  real X[N];
  real Y[N];
  int&amp;lt;lower = 1, upper = K&amp;gt; KID[N];
}

parameters {
  real a[K];
  real b[K];
  real&amp;lt;lower = 0&amp;gt; s_Y; 
}

model {
  for (n in 1:N)
  Y[n] ~ normal(a[KID[n]] + b[KID[n]] * X[n], s_Y);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;下面的 R 代碼用來實現上面貝葉斯多組不同截距不同斜率線性回歸模型的擬合:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rstan)
d &amp;lt;- read.csv(file=&amp;#39;../../static/files/data-salary-2.txt&amp;#39;)

data &amp;lt;- list(N=nrow(d), X=d$X, Y=d$Y, KID = d$KID, K = 4)
fit &amp;lt;- stan(file=&amp;#39;stanfiles/model8-2.stan&amp;#39;, data=data, seed=1234)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &amp;#39;model8-2&amp;#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 1.7e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.17 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.450551 seconds (Warm-up)
## Chain 1:                0.267262 seconds (Sampling)
## Chain 1:                0.717813 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &amp;#39;model8-2&amp;#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 5e-06 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.414798 seconds (Warm-up)
## Chain 2:                0.262094 seconds (Sampling)
## Chain 2:                0.676892 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &amp;#39;model8-2&amp;#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 5e-06 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.440107 seconds (Warm-up)
## Chain 3:                0.288008 seconds (Sampling)
## Chain 3:                0.728115 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &amp;#39;model8-2&amp;#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 5e-06 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.447923 seconds (Warm-up)
## Chain 4:                0.259137 seconds (Sampling)
## Chain 4:                0.70706 seconds (Total)
## Chain 4:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Stan model: model8-2.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##         mean se_mean     sd    2.5%     25%     50%     75%   97.5% n_eff Rhat
## a[1]  387.16    0.28  14.18  359.00  377.67  387.06  396.34  415.21  2560    1
## a[2]  328.45    0.32  16.66  295.74  317.60  328.68  339.52  361.24  2649    1
## a[3]  314.06    0.66  34.62  246.26  290.04  314.33  337.91  381.39  2725    1
## a[4]  751.09    3.10 157.77  440.06  643.83  752.64  857.32 1063.97  2598    1
## b[1]    7.51    0.02   0.87    5.79    6.94    7.51    8.10    9.19  2381    1
## b[2]   19.88    0.02   1.23   17.51   19.07   19.88   20.70   22.32  2545    1
## b[3]   12.45    0.03   1.69    9.16   11.28   12.45   13.60   15.75  2661    1
## b[4]   -1.04    0.12   6.36  -13.54   -5.34   -1.06    3.39   11.43  2616    1
## s_Y    27.32    0.07   3.57   21.48   24.77   26.95   29.43   35.38  2858    1
## lp__ -148.07    0.07   2.38 -153.72 -149.41 -147.68 -146.36 -144.57  1311    1
## 
## Samples were drawn using NUTS(diag_e) at Mon May 18 16:59:02 2020.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;等級線性回歸的貝葉斯實現&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;等級線性回歸的貝葉斯實現&lt;/h2&gt;
&lt;div id=&#34;模型機制-mechanism&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;模型機制 mechanism&lt;/h3&gt;
&lt;p&gt;如果我們認爲每家公司的起點薪水 &lt;span class=&#34;math inline&#34;&gt;\(a[k]\)&lt;/span&gt; 服從正態分佈，且該正態分佈的平均值是全體公司的起點薪水的均值 &lt;span class=&#34;math inline&#34;&gt;\(a_\mu\)&lt;/span&gt;，方差是 &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_a\)&lt;/span&gt;。類似地，假設每家公司內隨着年齡增長而增加薪水的幅度 &lt;span class=&#34;math inline&#34;&gt;\(b[k]\)&lt;/span&gt; 也服從某個正態分佈，均值和方差分別是 &lt;span class=&#34;math inline&#34;&gt;\(b_\mu, \sigma^2_b\)&lt;/span&gt;。這樣我們就不僅僅是允許了各家公司的薪水年齡回歸直線擁有不同的斜率和截距，還對這些隨機斜率和截距的前概率分佈進行了設定。&lt;/p&gt;
&lt;p&gt;此時，隨機效應模型的數學表達式就可以寫成下面這樣:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
Y[n] &amp;amp;\sim \text{Normal}(a[\text{KID[n]}] + b[\text{KID}[n]] X[n], \sigma^2_Y) &amp;amp; n = 1, \dots, N \\
a[k] &amp;amp;= a_\mu + a_\varepsilon[k]   &amp;amp; k = 1, \dots, K \\
a_\varepsilon[k] &amp;amp; \sim \text{Normal}(0, \sigma^2_a) &amp;amp; k = 1, \dots, K \\
b[k] &amp;amp; = b_\mu + b_\varepsilon[k]  &amp;amp; k = 1, \dots, K \\
b_\varepsilon[k] &amp;amp;\sim \text{Normal}(0, \sigma^2_b) &amp;amp; k = 1, \dots, K
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;使用 &lt;code&gt;rstanarm&lt;/code&gt; 包可以使用下面的代碼實現&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rstanarm)
M1_stanlmer &amp;lt;- stan_lmer(formula = Y ~ X  + (X | KID), 
                            data = d,
                            seed = 1234)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &amp;#39;continuous&amp;#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0.000116 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.16 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 7.72361 seconds (Warm-up)
## Chain 1:                3.71292 seconds (Sampling)
## Chain 1:                11.4365 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &amp;#39;continuous&amp;#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 2.5e-05 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.25 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 5.67096 seconds (Warm-up)
## Chain 2:                2.07953 seconds (Sampling)
## Chain 2:                7.75049 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &amp;#39;continuous&amp;#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 4.2e-05 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.42 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 7.81475 seconds (Warm-up)
## Chain 3:                3.88969 seconds (Sampling)
## Chain 3:                11.7044 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &amp;#39;continuous&amp;#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 3.7e-05 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.37 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 11.2964 seconds (Warm-up)
## Chain 4:                2.51584 seconds (Sampling)
## Chain 4:                13.8122 seconds (Total)
## Chain 4:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(M1_stanlmer, digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## stan_lmer
##  family:       gaussian [identity]
##  formula:      Y ~ X + (X | KID)
##  observations: 40
## ------
##             Median MAD_SD
## (Intercept) 358.76  15.31
## X            12.71   3.11
## 
## Auxiliary parameter(s):
##       Median MAD_SD
## sigma 29.93   3.79 
## 
## Error terms:
##  Groups   Name        Std.Dev. Corr 
##  KID      (Intercept) 24.36         
##           X            9.26    -0.15
##  Residual             30.32         
## Num. levels: KID 4 
## 
## ------
## * For help interpreting the printed output see ?print.stanreg
## * For info on the priors used see ?prior_summary.stanreg&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(M1_stanlmer, 
        pars = c(&amp;quot;(Intercept)&amp;quot;, &amp;quot;X&amp;quot;,&amp;quot;sigma&amp;quot;, 
                 &amp;quot;Sigma[KID:(Intercept),(Intercept)]&amp;quot;,
                 &amp;quot;Sigma[KID:X,(Intercept)]&amp;quot;, &amp;quot;Sigma[KID:X,X]&amp;quot;),
        probs = c(0.025, 0.975),
        digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Model Info:
##  function:     stan_lmer
##  family:       gaussian [identity]
##  formula:      Y ~ X + (X | KID)
##  algorithm:    sampling
##  sample:       4000 (posterior sample size)
##  priors:       see help(&amp;#39;prior_summary&amp;#39;)
##  observations: 40
##  groups:       KID (4)
## 
## Estimates:
##                                      mean    sd      2.5%    97.5%
## (Intercept)                         357.39   18.40  312.29  390.14
## X                                    12.40    4.06    2.50   19.81
## sigma                                30.32    3.91   23.68   38.98
## Sigma[KID:(Intercept),(Intercept)]  593.52 1322.20    1.72 3794.51
## Sigma[KID:X,(Intercept)]            -34.16  170.47 -375.20  302.16
## Sigma[KID:X,X]                       85.70  170.48    6.74  481.62
## 
## MCMC diagnostics
##                                    mcse  Rhat  n_eff
## (Intercept)                         1.20  1.01  233 
## X                                   0.25  1.02  267 
## sigma                               0.08  1.00 2545 
## Sigma[KID:(Intercept),(Intercept)] 65.01  1.01  414 
## Sigma[KID:X,(Intercept)]            8.90  1.01  367 
## Sigma[KID:X,X]                     10.07  1.01  287 
## 
## For each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;和非貝葉斯版本的概率論隨機效應線性回歸模型的結果相對比一下：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lme4)
M1 &amp;lt;- lmer(formula = Y ~ X  + (X | KID), 
           data = d, 
           REML = TRUE)
summary(M1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by REML [&amp;#39;lmerMod&amp;#39;]
## Formula: Y ~ X + (X | KID)
##    Data: d
## 
## REML criterion at convergence: 387.2
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.3697 -0.5184 -0.0355  0.7635  1.8788 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev. Corr 
##  KID      (Intercept) 504.00   22.450        
##           X            28.54    5.342   -1.00
##  Residual             833.94   28.878        
## Number of obs: 40, groups:  KID, 4
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)  358.207     15.385  23.283
## X             13.067      2.741   4.767
## 
## Correlation of Fixed Effects:
##   (Intr)
## X -0.848
## convergence code: 0
## boundary (singular) fit: see ?isSingular&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>泊松回歸模型的貝葉斯Stan實現</title>
      <link>https://wangcc.me/post/poisson-stan/</link>
      <pubDate>Thu, 11 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/poisson-stan/</guid>
      <description>
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#分析目的數據和選擇-poisson-回歸模型的原因&#34;&gt;分析目的，數據，和選擇 Poisson 回歸模型的原因&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#想象模型機制&#34;&gt;想象模型機制&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#寫下數學模型表達式&#34;&gt;寫下數學模型表達式&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#把數學模型翻譯成-stan-模型代碼&#34;&gt;把數學模型翻譯成 Stan 模型代碼&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#運行結果的解釋&#34;&gt;運行結果的解釋&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;分析目的數據和選擇-poisson-回歸模型的原因&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;分析目的，數據，和選擇 Poisson 回歸模型的原因&lt;/h1&gt;
&lt;p&gt;我們這裏使用&lt;a href=&#34;https://wangcc.me/post/logistic-rstan/&#34;&gt;之前擬合貝葉斯邏輯回歸模型&lt;/a&gt;時使用的相同的數據來展示如何跑貝葉斯泊松回歸模型。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d &amp;lt;- read.table(&amp;quot;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap05/input/data-attendance-2.txt&amp;quot;, sep = &amp;quot;,&amp;quot;, header = T)
head(d)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   PersonID A Score  M  Y
## 1        1 0    69 43 38
## 2        2 1   145 56 40
## 3        3 0   125 32 24
## 4        4 1    86 45 33
## 5        5 1   158 33 23
## 6        6 0   133 61 60&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;PersonID&lt;/code&gt;: 是學生的編號；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;A&lt;/code&gt;, &lt;code&gt;Score&lt;/code&gt;: 用來預測出勤率的兩個預測變量，分別是表示是否喜歡打工的 &lt;code&gt;A&lt;/code&gt;，和表示對學習本身是否喜歡的評分 (滿分200)；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;M&lt;/code&gt;: 過去三個月內，該名學生一共需要上課的總課時數；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Y&lt;/code&gt;: 過去三個月內，該名學生實際上出勤的課時數。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;這一次我們希望通過分析泊松回歸來回答「&lt;code&gt;A&lt;/code&gt; 和 &lt;code&gt;Score&lt;/code&gt; 對總課時數 &lt;code&gt;M&lt;/code&gt; 具體有多大的影響？」這個問題。&lt;a href=&#34;https://wangcc.me/post/logistic-rstan/&#34;&gt;之前擬合貝葉斯邏輯回歸模型&lt;/a&gt;時，使用的結果變量是 &lt;code&gt;Y&lt;/code&gt;，也就是實際出勤課時數。但是本小節我們用 &lt;code&gt;M&lt;/code&gt; 作爲結果變量。因爲總課時數是學生自己選課時的結果，也就是說學生本身的態度（是否喜歡打工，是否熱愛學習），可能本身左右了他/她到底會選多少課。背景知識假設是：喜歡多去打工的學生，選課可能態度消極，總課時數從開始可能就選的少。那麼像總選課時數這樣的非負（計數型）離散變量作爲結果變量的時候，&lt;strong&gt;泊松回歸模型是我們的第一選擇。&lt;/strong&gt;&lt;/p&gt;
&lt;div id=&#34;想象模型機制&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;想象模型機制&lt;/h2&gt;
&lt;p&gt;如果使用&lt;a href=&#34;https://wangcc.me/post/rstan-wonderful-r3/&#34;&gt;上上節介紹的多重線性回歸模型&lt;/a&gt;，那麼模型的預測變量的分佈便可能取到負數，這樣就不符合實際情況下“總選課時數”是非負（計數型）離散變量這一事實。這就需要把預測變量 &lt;code&gt;A&lt;/code&gt; 和 &lt;code&gt;Score&lt;/code&gt; 相加的線性模型 &lt;span class=&#34;math inline&#34;&gt;\((b_1 + b_2A + b_3Score)\)&lt;/span&gt;，通過數學轉換限制在非負數範圍。假設平均總課時數是 &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;，我們認爲它服從均值是 &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; 的泊松分佈。關於泊松分佈的詳細知識，期望值和方差的推導可以參考&lt;a href=&#34;https://wangcc.me/LSHTMlearningnote/poisson.html&#34;&gt;學習筆記&lt;/a&gt;。另外，非貝葉斯版本的一般性傳統泊松回歸模型可以參照學習筆記的&lt;a href=&#34;https://wangcc.me/LSHTMlearningnote/poisson-regression.html&#34;&gt;廣義線性回歸的泊松回歸模型章節&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;對泊松回歸模型略有瞭解的話應該很自然地想到，把結果變量限制在非負數範圍的標準鏈接方程是 &lt;span class=&#34;math inline&#34;&gt;\(\log(\lambda)\)&lt;/span&gt;，或者在 Stan 模型中，我們更自然地把線性模型部分寫在指數模型中: &lt;span class=&#34;math inline&#34;&gt;\(\exp(b_1 + b_2A + b_3Score)\)&lt;/span&gt;。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;寫下數學模型表達式&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;寫下數學模型表達式&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\lambda[n] &amp;amp; = \exp(b_1 + b_2A[n] + b_3Score[n]) &amp;amp; n = 1, \dots, N \\
M[n]       &amp;amp; \sim \text{Poisson}(\lambda[n])     &amp;amp; n = 1, \dots, N
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;，是該數據中學生的人數；&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;，是每名學生的標籤/編號（下標）；&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(b_1, b_2, b_3\)&lt;/span&gt; 是我們感興趣的參數。&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;把數學模型翻譯成-stan-模型代碼&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;把數學模型翻譯成 Stan 模型代碼&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;data {
  int N; 
  int&amp;lt;lower=0, upper=1&amp;gt; A[N]; 
  real&amp;lt;lower=0, upper=1&amp;gt; Score[N]; 
  int&amp;lt;lower=0&amp;gt; M[N];
}

parameters {
  real b[3]; 
}

transformed parameters {
  real lambda[N];
  for (n in 1:N) {
    lambda[n] = exp(b[1] + b[2]*A[n] + b[3]*Score[n]);
  }
}

model {
  for (n in 1:N) {
    M[n] ~ poisson(lambda[n]); 
  }
}

generated quantities {
  int m_pred[N]; 
  for (n in 1:N) {
    m_pred[n] = poisson_rng(M[n], q[n]);
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;值得一提的是，在 Stan 中，提供了 &lt;code&gt;poisson_log(x)&lt;/code&gt; 分佈函數，其實它等價於使用 &lt;code&gt;poisson(exp(x))&lt;/code&gt;。除了更加接近我們熟悉的泊松回歸模型的數學表達式，避免了 &lt;code&gt;exp&lt;/code&gt; 指數運算，計算結果穩定。於是我們還可以把上面的模型修改成：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data {
  int N; 
  int&amp;lt;lower=0, upper=1&amp;gt; A[N]; 
  real&amp;lt;lower=0, upper=1&amp;gt; Score[N]; 
  int&amp;lt;lower=0&amp;gt; M[N];
}

parameters {
  real b[3]; 
}

transformed parameters {
  real lambda[N];
  for (n in 1:N) {
    lambda[n] = b[1] + b[2]*A[n] + b[3]*Score[n]；
  }
}

model {
  for (n in 1:N) {
    M[n] ~ poisson_log(lambda[n]); 
  }
}

generated quantities {
  int m_pred[N]; 
  for (n in 1:N) {
    m_pred[n] = poisson_log_rng(M[n], q[n]);
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;運行它的代碼如下：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rstan)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: StanHeaders&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggplot2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## rstan (Version 2.19.3, GitRev: 2e1f913d3ca3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## For execution on a local, multicore CPU with excess RAM we recommend calling
## options(mc.cores = parallel::detectCores()).
## To avoid recompilation of unchanged Stan programs, we recommend calling
## rstan_options(auto_write = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data &amp;lt;- list(N=nrow(d), A=d$A, Score=d$Score/200, M=d$M)
# fit &amp;lt;- stan(file=&amp;#39;model/model5-6.stan&amp;#39;, data=data, seed=1234)
fit &amp;lt;- stan(file=&amp;#39;stanfiles/model5-6b.stan&amp;#39;, data=data, seed=1234, pars = c(&amp;quot;b&amp;quot;, &amp;quot;lambda&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &amp;#39;model5-6b&amp;#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 2e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.2 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.113057 seconds (Warm-up)
## Chain 1:                0.111411 seconds (Sampling)
## Chain 1:                0.224468 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &amp;#39;model5-6b&amp;#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 7e-06 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.107243 seconds (Warm-up)
## Chain 2:                0.125844 seconds (Sampling)
## Chain 2:                0.233087 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &amp;#39;model5-6b&amp;#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 6e-06 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.111618 seconds (Warm-up)
## Chain 3:                0.122729 seconds (Sampling)
## Chain 3:                0.234347 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &amp;#39;model5-6b&amp;#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 7e-06 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.116266 seconds (Warm-up)
## Chain 4:                0.114301 seconds (Sampling)
## Chain 4:                0.230567 seconds (Total)
## Chain 4:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Stan model: model5-6b.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##               mean se_mean   sd    2.5%     25%     50%     75%   97.5% n_eff
## b[1]          3.58    0.00 0.09    3.41    3.52    3.58    3.64    3.77  1323
## b[2]          0.26    0.00 0.04    0.18    0.23    0.26    0.29    0.34  1775
## b[3]          0.29    0.00 0.14    0.00    0.19    0.28    0.39    0.57  1332
## lambda[1]     3.68    0.00 0.05    3.59    3.65    3.68    3.71    3.77  1470
## lambda[2]     4.05    0.00 0.03    3.98    4.03    4.05    4.07    4.11  2285
## lambda[3]     3.76    0.00 0.03    3.70    3.74    3.76    3.78    3.81  2639
## lambda[4]     3.97    0.00 0.04    3.89    3.94    3.97    3.99    4.04  1687
## lambda[5]     4.07    0.00 0.04    3.99    4.04    4.07    4.10    4.14  1976
## lambda[6]     3.77    0.00 0.03    3.71    3.75    3.77    3.79    3.83  2604
## lambda[7]     3.74    0.00 0.03    3.68    3.72    3.74    3.76    3.79  2297
## lambda[8]     4.05    0.00 0.03    3.98    4.03    4.05    4.08    4.12  2232
## lambda[9]     3.79    0.00 0.03    3.72    3.77    3.79    3.81    3.85  2312
## lambda[10]    3.79    0.00 0.03    3.72    3.77    3.79    3.81    3.85  2337
## lambda[11]    4.04    0.00 0.03    3.98    4.02    4.04    4.07    4.11  2392
## lambda[12]    3.78    0.00 0.03    3.72    3.76    3.78    3.80    3.83  2530
## lambda[13]    4.01    0.00 0.03    3.95    3.99    4.01    4.03    4.07  2518
## lambda[14]    3.74    0.00 0.03    3.68    3.72    3.74    3.76    3.79  2297
## lambda[15]    3.74    0.00 0.03    3.68    3.72    3.74    3.76    3.79  2230
## lambda[16]    3.98    0.00 0.03    3.92    3.96    3.99    4.01    4.05  1959
## lambda[17]    3.74    0.00 0.03    3.69    3.72    3.74    3.76    3.80  2396
## lambda[18]    3.70    0.00 0.04    3.62    3.67    3.70    3.72    3.77  1587
## lambda[19]    3.84    0.00 0.05    3.74    3.81    3.84    3.88    3.95  1687
## lambda[20]    4.07    0.00 0.04    3.99    4.04    4.07    4.09    4.14  2017
## lambda[21]    3.97    0.00 0.04    3.89    3.94    3.97    3.99    4.04  1687
## lambda[22]    4.00    0.00 0.03    3.94    3.98    4.00    4.02    4.06  2226
## lambda[23]    3.99    0.00 0.03    3.93    3.97    4.00    4.02    4.06  2164
## lambda[24]    4.05    0.00 0.03    3.98    4.02    4.05    4.07    4.11  2339
## lambda[25]    4.01    0.00 0.03    3.95    3.99    4.01    4.03    4.07  2493
## lambda[26]    4.03    0.00 0.03    3.97    4.01    4.03    4.05    4.08  2626
## lambda[27]    3.75    0.00 0.03    3.69    3.73    3.75    3.77    3.80  2540
## lambda[28]    3.75    0.00 0.03    3.69    3.73    3.75    3.77    3.80  2540
## lambda[29]    3.81    0.00 0.04    3.73    3.78    3.81    3.84    3.89  1979
## lambda[30]    3.74    0.00 0.03    3.69    3.72    3.74    3.76    3.80  2363
## lambda[31]    4.08    0.00 0.04    3.99    4.05    4.08    4.11    4.16  1853
## lambda[32]    3.95    0.00 0.05    3.85    3.92    3.95    3.98    4.04  1544
## lambda[33]    4.04    0.00 0.03    3.98    4.02    4.04    4.06    4.10  2469
## lambda[34]    4.02    0.00 0.03    3.96    4.00    4.02    4.04    4.08  2616
## lambda[35]    3.76    0.00 0.03    3.70    3.74    3.76    3.78    3.81  2645
## lambda[36]    3.77    0.00 0.03    3.71    3.75    3.77    3.79    3.82  2630
## lambda[37]    3.99    0.00 0.03    3.93    3.97    3.99    4.01    4.06  2134
## lambda[38]    3.74    0.00 0.03    3.68    3.72    3.74    3.76    3.79  2263
## lambda[39]    3.71    0.00 0.04    3.64    3.68    3.71    3.73    3.78  1689
## lambda[40]    3.71    0.00 0.04    3.63    3.68    3.71    3.73    3.78  1672
## lambda[41]    3.76    0.00 0.03    3.71    3.75    3.76    3.78    3.82  2645
## lambda[42]    3.77    0.00 0.03    3.71    3.75    3.77    3.79    3.83  2604
## lambda[43]    3.75    0.00 0.03    3.70    3.74    3.75    3.77    3.81  2602
## lambda[44]    3.79    0.00 0.03    3.72    3.77    3.79    3.82    3.86  2264
## lambda[45]    3.84    0.00 0.05    3.74    3.81    3.84    3.88    3.94  1711
## lambda[46]    3.73    0.00 0.03    3.67    3.71    3.73    3.75    3.79  2100
## lambda[47]    3.65    0.00 0.06    3.54    3.61    3.65    3.69    3.77  1390
## lambda[48]    3.79    0.00 0.03    3.73    3.77    3.79    3.82    3.86  2216
## lambda[49]    3.72    0.00 0.03    3.66    3.70    3.72    3.74    3.78  1911
## lambda[50]    3.98    0.00 0.03    3.92    3.96    3.99    4.01    4.05  1959
## lp__       6896.60    0.03 1.18 6893.47 6896.04 6896.91 6897.47 6897.96  1368
##            Rhat
## b[1]          1
## b[2]          1
## b[3]          1
## lambda[1]     1
## lambda[2]     1
## lambda[3]     1
## lambda[4]     1
## lambda[5]     1
## lambda[6]     1
## lambda[7]     1
## lambda[8]     1
## lambda[9]     1
## lambda[10]    1
## lambda[11]    1
## lambda[12]    1
## lambda[13]    1
## lambda[14]    1
## lambda[15]    1
## lambda[16]    1
## lambda[17]    1
## lambda[18]    1
## lambda[19]    1
## lambda[20]    1
## lambda[21]    1
## lambda[22]    1
## lambda[23]    1
## lambda[24]    1
## lambda[25]    1
## lambda[26]    1
## lambda[27]    1
## lambda[28]    1
## lambda[29]    1
## lambda[30]    1
## lambda[31]    1
## lambda[32]    1
## lambda[33]    1
## lambda[34]    1
## lambda[35]    1
## lambda[36]    1
## lambda[37]    1
## lambda[38]    1
## lambda[39]    1
## lambda[40]    1
## lambda[41]    1
## lambda[42]    1
## lambda[43]    1
## lambda[44]    1
## lambda[45]    1
## lambda[46]    1
## lambda[47]    1
## lambda[48]    1
## lambda[49]    1
## lambda[50]    1
## lp__          1
## 
## Samples were drawn using NUTS(diag_e) at Mon May 18 16:57:44 2020.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;運行結果的解釋&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;運行結果的解釋&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;...{省略}...
              mean se_mean   sd    2.5%     25%     50%     75%   97.5% n_eff Rhat
b[1]          3.58    0.00 0.09    3.38    3.51    3.58    3.64    3.76  1373    1
b[2]          0.26    0.00 0.04    0.18    0.24    0.26    0.29    0.35  1797    1
b[3]          0.29    0.00 0.15    0.00    0.20    0.29    0.39    0.59  1422    1
lambda[1]     3.68    0.00 0.05    3.58    3.65    3.68    3.71    3.77  1510    1
...{省略}...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;我們把計算獲得的事後概率分佈均值放入前面寫下的數學表達式:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\lambda[n] &amp;amp; = \exp(3.58 + 0.26A[n] + 0.29Score[n]/200) &amp;amp; n = 1, \dots, N \\
M[n]       &amp;amp; \sim \text{Poisson}(\lambda[n])     &amp;amp; n = 1, \dots, N
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;例如說，&lt;code&gt;Score = 150&lt;/code&gt; 和 &lt;code&gt;Score = 50&lt;/code&gt; 的兩名學生，如果對打工喜好態度相同的話，他們之間選課的總課時數之比爲：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\frac{M_\text{Score = 150}}{M_\text{Score = 50}} &amp;amp; = \frac{\exp(3.58 + 0.26A + 0.29\times\frac{150}{200})}{\exp(3.58 + 0.26A + 0.29\times\frac{50}{200})} \\ 
&amp;amp; = \exp(0.29\times\frac{150-50}{200}) \approx 1.16
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;也就是熱愛學習分數 &lt;code&gt;Score&lt;/code&gt; 達到150的人和只有50的人相比，選課總課時數平均多 16%。相似地，喜歡打工 &lt;code&gt;A = 1&lt;/code&gt; 的學生和不喜歡打工 &lt;code&gt;A = 0&lt;/code&gt; 的學生選課總課時數之比爲 &lt;span class=&#34;math inline&#34;&gt;\(\exp(0.26)\approx1.30\)&lt;/span&gt;。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Rstan Wonderful R-(5)</title>
      <link>https://wangcc.me/post/logistic-rstan2/</link>
      <pubDate>Sun, 03 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/logistic-rstan2/</guid>
      <description>
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#另一種形式的貝葉斯邏輯回歸&#34;&gt;另一種形式的貝葉斯邏輯回歸&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#分析的目的&#34;&gt;分析的目的&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#確認數據分佈&#34;&gt;確認數據分佈&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#思考數據模型&#34;&gt;思考數據模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#寫下-stan-模型代碼&#34;&gt;寫下 Stan 模型代碼&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#檢查模型參數的收斂情況&#34;&gt;檢查模型參數的收斂情況&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#檢查模型的擬合情況&#34;&gt;檢查模型的擬合情況&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;另一種形式的貝葉斯邏輯回歸&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;另一種形式的貝葉斯邏輯回歸&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://wangcc.me/post/logistic-rstan/&#34;&gt;前面一節&lt;/a&gt;使用的數據是以學生爲單位，將每名學生的實際課時數和實際出勤數進行了彙總之後的總結性數據，本章我們來看看相同數據的另一種形式。由於分析中有人建議說，天氣狀況對出勤率也是有較大的影響的，所以希望在&lt;a href=&#34;https://wangcc.me/post/logistic-rstan/&#34;&gt;前一節&lt;/a&gt;已有的邏輯回歸模型中增加對天氣狀況的調整。那麼這時候需要使用的就是彙總之前的數據，也就是要是用實際記錄了每名學生每一次課時的出勤與否的原始數據。值得注意的是，這時候&lt;a href=&#34;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap05/input/data-attendance-3.txt&#34;&gt;原始數據&lt;/a&gt;中每名學生的記錄有許多行，因爲每行記錄的是該名學生每次上課時的天氣狀況和他/她是否出勤(0,1)的結果。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d &amp;lt;- read.table(&amp;quot;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap05/input/data-attendance-3.txt&amp;quot;, 
                     sep = &amp;quot;,&amp;quot;, header = T)
head(d, 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    PersonID A Score Weather Y
## 1         1 0    69       B 1
## 2         1 0    69       A 1
## 3         1 0    69       C 1
## 4         1 0    69       A 1
## 5         1 0    69       B 1
## 6         1 0    69       B 1
## 7         1 0    69       C 0
## 8         1 0    69       B 1
## 9         1 0    69       A 1
## 10        1 0    69       A 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Weather\)&lt;/span&gt;，天氣數據 (&lt;code&gt;A&lt;/code&gt; = 晴天，&lt;code&gt;B&lt;/code&gt; = 多雲，&lt;code&gt;C&lt;/code&gt; = 下雨)；&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;，該次課時學生是否出勤 (&lt;code&gt;0&lt;/code&gt; = 缺勤，&lt;code&gt;1&lt;/code&gt; = 出勤)。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其他的數據和前一節中使用的數據相同。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;分析的目的&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;分析的目的&lt;/h1&gt;
&lt;p&gt;本次數據分析的目的依然是瞭解幾個預測變量，天氣，是否喜歡打工，是否熱愛學習，對學生出勤率的影響。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;確認數據分佈&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;確認數據分佈&lt;/h1&gt;
&lt;p&gt;你可以用先進的 &lt;code&gt;tidyverse&lt;/code&gt; 進行簡單的數據彙總，看看天氣狀況不同時實際出勤率是否有差別:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
d %&amp;gt;% 
  group_by(Weather, Y) %&amp;gt;% 
  summarise (n= n()) %&amp;gt;%
  mutate(rel.freq = paste0(round(100 * n/sum(n), 2), &amp;quot;%&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 4
## # Groups:   Weather [3]
##   Weather     Y     n rel.freq
##   &amp;lt;fct&amp;gt;   &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;   
## 1 A           0   306 24.31%  
## 2 A           1   953 75.69%  
## 3 B           0   230 31.51%  
## 4 B           1   500 68.49%  
## 5 C           0   138 33.91%  
## 6 C           1   269 66.09%&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果你不想學習 &lt;code&gt;tidyverse&lt;/code&gt;，也可以用下面的方法獲得類似的效果，&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;aggregate(Y ~ Weather, data = d, FUN = table)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Weather Y.0 Y.1
## 1       A 306 953
## 2       B 230 500
## 3       C 138 269&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;無論是哪種方法，我們都能大概猜出，天氣是晴天的時候 (&lt;code&gt;Weather = A&lt;/code&gt;)，出勤率相對較高。&lt;/p&gt;
&lt;p&gt;在作者的原著中，&lt;a href=&#34;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap05/run-model5-5.R&#34;&gt;使用的是給分類型變量強制賦予連續值的方法&lt;/a&gt;，這點確實有點噁心，爲了正常的模型，我們需要把天氣轉換成爲更加常見的啞變量 (dummy variable) 如下:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d &amp;lt;- fastDummies::dummy_cols(d, select_columns = &amp;quot;Weather&amp;quot;)
head(d)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   PersonID A Score Weather Y Weather_A Weather_B Weather_C
## 1        1 0    69       B 1         0         1         0
## 2        1 0    69       A 1         1         0         0
## 3        1 0    69       C 1         0         0         1
## 4        1 0    69       A 1         1         0         0
## 5        1 0    69       B 1         0         1         0
## 6        1 0    69       B 1         0         1         0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;思考數據模型&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;思考數據模型&lt;/h1&gt;
&lt;p&gt;我們設想的數學模型應該是這樣子的:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\text{logit}(q[i]) &amp;amp; = b_{1} + b_{2}A_{i} + b_{3}\text{Score}_{i} + b_{4}\text{WeatherB} + b_{5}\text{WeatherC} \\ 
\text{where} &amp;amp; \\ 
&amp;amp; \text{ WeatherB} = 0, \text{ WeatherC} = 0 \text{ indicates weather = A} \\ 
&amp;amp; \text{ WeatherB} = 1, \text{ WeatherC} = 0 \text{ indicates weather = B} \\ 
&amp;amp; \text{ WeatherB} = 0, \text{ WeatherC} = 1 \text{ indicates weather = C} \\
Y[i] &amp;amp;\sim \text{Bernulli}(q[i])
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;寫下-stan-模型代碼&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;寫下 Stan 模型代碼&lt;/h1&gt;
&lt;p&gt;下面是相應的 Stan 模型:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data {
  int I;
  int&amp;lt;lower=0, upper=1&amp;gt; A[I];
  real&amp;lt;lower=0, upper=1&amp;gt; Score[I];
  int&amp;lt;lower=0, upper=1&amp;gt; W_B[I];
  int&amp;lt;lower=0, upper=1&amp;gt; W_C[I];
  int&amp;lt;lower=0, upper=1&amp;gt; Y[I];
}

// The parameters accepted by the model. 
parameters {
  real b[5];
}

// The model to be estimated. 
model {
   for (i in 1:I)
    Y[i] ~ bernoulli_logit(b[1] + b[2]*A[i] + b[3]*Score[i] + b[4]*W_B[i] + b[5]*W_C[i]);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;和跑它們的 R 代碼&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rstan)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: StanHeaders&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## rstan (Version 2.19.3, GitRev: 2e1f913d3ca3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## For execution on a local, multicore CPU with excess RAM we recommend calling
## options(mc.cores = parallel::detectCores()).
## To avoid recompilation of unchanged Stan programs, we recommend calling
## rstan_options(auto_write = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;rstan&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:tidyr&amp;#39;:
## 
##     extract&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data &amp;lt;- list(I=nrow(d), A=d$A, Score=d$Score/200, 
             W_A=d$Weather_A, W_B = d$Weather_B, W_C = d$Weather_C, 
             Y=d$Y)
fit1 &amp;lt;- stan(file=&amp;#39;stanfiles/myex4.stan&amp;#39;, data=data, pars=c(&amp;#39;b&amp;#39;, &amp;quot;OR1&amp;quot;, &amp;quot;OR2&amp;quot;, &amp;quot;OR3&amp;quot;, &amp;quot;OR4&amp;quot;, &amp;quot;q&amp;quot;), seed=1234)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &amp;#39;myex4&amp;#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0.000761 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 7.61 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 7.48792 seconds (Warm-up)
## Chain 1:                7.76994 seconds (Sampling)
## Chain 1:                15.2579 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &amp;#39;myex4&amp;#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 0.000538 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 5.38 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 7.6327 seconds (Warm-up)
## Chain 2:                8.47747 seconds (Sampling)
## Chain 2:                16.1102 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &amp;#39;myex4&amp;#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 0.000533 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 5.33 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 7.62369 seconds (Warm-up)
## Chain 3:                8.71986 seconds (Sampling)
## Chain 3:                16.3436 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &amp;#39;myex4&amp;#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 0.000367 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 3.67 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 7.33882 seconds (Warm-up)
## Chain 4:                8.12735 seconds (Sampling)
## Chain 4:                15.4662 seconds (Total)
## Chain 4:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Stan model: myex4.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##             mean se_mean   sd     2.5%      25%      50%      75%    97.5%
## b[1]        0.27    0.01 0.23    -0.19     0.11     0.26     0.43     0.73
## b[2]       -0.63    0.00 0.09    -0.81    -0.69    -0.63    -0.57    -0.45
## b[3]        1.97    0.01 0.37     1.23     1.72     1.97     2.21     2.72
## b[4]       -0.38    0.00 0.10    -0.58    -0.45    -0.38    -0.30    -0.18
## b[5]       -0.50    0.00 0.13    -0.74    -0.59    -0.50    -0.41    -0.25
## OR1         0.54    0.00 0.05     0.45     0.50     0.53     0.57     0.64
## OR2         7.65    0.07 2.96     3.42     5.60     7.15     9.10    15.15
## OR3         0.69    0.00 0.07     0.56     0.64     0.68     0.74     0.84
## OR4         0.61    0.00 0.08     0.48     0.56     0.61     0.66     0.78
## q[1]        0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[2]        0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[3]        0.61    0.00 0.04     0.54     0.59     0.61     0.63     0.68
## q[4]        0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[5]        0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[6]        0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[7]        0.61    0.00 0.04     0.54     0.59     0.61     0.63     0.68
## q[8]        0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[9]        0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[10]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[11]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[12]       0.61    0.00 0.04     0.54     0.59     0.61     0.63     0.68
## q[13]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[14]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[15]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[16]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[17]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[18]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[19]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[20]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[21]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[22]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[23]       0.61    0.00 0.04     0.54     0.59     0.61     0.63     0.68
## q[24]       0.61    0.00 0.04     0.54     0.59     0.61     0.63     0.68
## q[25]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[26]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[27]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[28]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[29]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[30]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[31]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[32]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[33]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[34]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[35]       0.61    0.00 0.04     0.54     0.59     0.61     0.63     0.68
## q[36]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[37]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[38]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[39]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[40]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[41]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[42]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[43]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[44]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[45]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[46]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[47]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[48]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[49]       0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.71
## q[50]       0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.71
## q[51]       0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.71
## q[52]       0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.71
## q[53]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[54]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[55]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[56]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[57]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[58]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[59]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[60]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[61]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[62]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[63]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[64]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[65]       0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.71
## q[66]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[67]       0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.71
## q[68]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[69]       0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.71
## q[70]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[71]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[72]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[73]       0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.71
## q[74]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[75]       0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.71
## q[76]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[77]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[78]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[79]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[80]       0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.71
## q[81]       0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.71
## q[82]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[83]       0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.71
## q[84]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[85]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[86]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[87]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[88]       0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.71
## q[89]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[90]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[91]       0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.71
## q[92]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[93]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[94]       0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.71
## q[95]       0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.71
## q[96]       0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.71
## q[97]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[98]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[99]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[100]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[101]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[102]      0.73    0.00 0.02     0.68     0.71     0.73     0.75     0.77
## q[103]      0.75    0.00 0.02     0.72     0.74     0.75     0.76     0.79
## q[104]      0.75    0.00 0.02     0.72     0.74     0.75     0.76     0.79
## q[105]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[106]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[107]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[108]      0.73    0.00 0.02     0.68     0.71     0.73     0.75     0.77
## q[109]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[110]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[111]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[112]      0.75    0.00 0.02     0.72     0.74     0.75     0.76     0.79
## q[113]      0.75    0.00 0.02     0.72     0.74     0.75     0.76     0.79
## q[114]      0.73    0.00 0.02     0.68     0.71     0.73     0.75     0.77
## q[115]      0.75    0.00 0.02     0.72     0.74     0.75     0.76     0.79
## q[116]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[117]      0.73    0.00 0.02     0.68     0.71     0.73     0.75     0.77
## q[118]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[119]      0.73    0.00 0.02     0.68     0.71     0.73     0.75     0.77
## q[120]      0.75    0.00 0.02     0.72     0.74     0.75     0.76     0.79
## q[121]      0.75    0.00 0.02     0.72     0.74     0.75     0.76     0.79
## q[122]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[123]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[124]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[125]      0.75    0.00 0.02     0.72     0.74     0.75     0.76     0.79
## q[126]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[127]      0.75    0.00 0.02     0.72     0.74     0.75     0.76     0.79
## q[128]      0.75    0.00 0.02     0.72     0.74     0.75     0.76     0.79
## q[129]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[130]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[131]      0.73    0.00 0.02     0.68     0.71     0.73     0.75     0.77
## q[132]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[133]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[134]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[135]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[136]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[137]      0.50    0.00 0.03     0.43     0.47     0.50     0.52     0.56
## q[138]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[139]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[140]      0.50    0.00 0.03     0.43     0.47     0.50     0.52     0.56
## q[141]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[142]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[143]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[144]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[145]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[146]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[147]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[148]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[149]      0.50    0.00 0.03     0.43     0.47     0.50     0.52     0.56
## q[150]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[151]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[152]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[153]      0.50    0.00 0.03     0.43     0.47     0.50     0.52     0.56
## q[154]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[155]      0.50    0.00 0.03     0.43     0.47     0.50     0.52     0.56
## q[156]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[157]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[158]      0.50    0.00 0.03     0.43     0.47     0.50     0.52     0.56
## q[159]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[160]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[161]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[162]      0.50    0.00 0.03     0.43     0.47     0.50     0.52     0.56
## q[163]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[164]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[165]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[166]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[167]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[168]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[169]      0.50    0.00 0.03     0.43     0.47     0.50     0.52     0.56
## q[170]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[171]      0.50    0.00 0.03     0.43     0.47     0.50     0.52     0.56
## q[172]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[173]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[174]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[175]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[176]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[177]      0.67    0.00 0.03     0.61     0.65     0.67     0.69     0.72
## q[178]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[179]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[180]      0.67    0.00 0.03     0.61     0.65     0.67     0.69     0.72
## q[181]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[182]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[183]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[184]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[185]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[186]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[187]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[188]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[189]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[190]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[191]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[192]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[193]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[194]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[195]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[196]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[197]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[198]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[199]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[200]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[201]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[202]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[203]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[204]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[205]      0.67    0.00 0.03     0.61     0.65     0.67     0.69     0.72
## q[206]      0.67    0.00 0.03     0.61     0.65     0.67     0.69     0.72
## q[207]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[208]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[209]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[210]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[211]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[212]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[213]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[214]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[215]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[216]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[217]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[218]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[219]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[220]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[221]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[222]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[223]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[224]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[225]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[226]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[227]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[228]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[229]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[230]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[231]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[232]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[233]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[234]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[235]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[236]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[237]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[238]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[239]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[240]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[241]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[242]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[243]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[244]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[245]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[246]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[247]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[248]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[249]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[250]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[251]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[252]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[253]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[254]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[255]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[256]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[257]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[258]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[259]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[260]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[261]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[262]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[263]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[264]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[265]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[266]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[267]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[268]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[269]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[270]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[271]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[272]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[273]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[274]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[275]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[276]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[277]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[278]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[279]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[280]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[281]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[282]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[283]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[284]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[285]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[286]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[287]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[288]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[289]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[290]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[291]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[292]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[293]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[294]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[295]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[296]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[297]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[298]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[299]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[300]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[301]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[302]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[303]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[304]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[305]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[306]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[307]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[308]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[309]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[310]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[311]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[312]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[313]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[314]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[315]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[316]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[317]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[318]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[319]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[320]      0.67    0.00 0.02     0.62     0.65     0.67     0.69     0.71
## q[321]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[322]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[323]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[324]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[325]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[326]      0.67    0.00 0.02     0.62     0.65     0.67     0.69     0.71
## q[327]      0.67    0.00 0.02     0.62     0.65     0.67     0.69     0.71
## q[328]      0.67    0.00 0.02     0.62     0.65     0.67     0.69     0.71
## q[329]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[330]      0.67    0.00 0.02     0.62     0.65     0.67     0.69     0.71
## q[331]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[332]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[333]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[334]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[335]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[336]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[337]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[338]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[339]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[340]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[341]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[342]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[343]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[344]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[345]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[346]      0.67    0.00 0.02     0.62     0.65     0.67     0.69     0.71
## q[347]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[348]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[349]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[350]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[351]      0.67    0.00 0.02     0.62     0.65     0.67     0.69     0.71
## q[352]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[353]      0.67    0.00 0.02     0.62     0.65     0.67     0.69     0.71
## q[354]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[355]      0.67    0.00 0.02     0.62     0.65     0.67     0.69     0.71
## q[356]      0.67    0.00 0.02     0.62     0.65     0.67     0.69     0.71
## q[357]      0.67    0.00 0.02     0.62     0.65     0.67     0.69     0.71
## q[358]      0.67    0.00 0.02     0.62     0.65     0.67     0.69     0.71
## q[359]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[360]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[361]      0.67    0.00 0.02     0.62     0.65     0.67     0.69     0.71
## q[362]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[363]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[364]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[365]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[366]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[367]      0.67    0.00 0.02     0.62     0.65     0.67     0.69     0.71
## q[368]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[369]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[370]      0.67    0.00 0.02     0.62     0.65     0.67     0.69     0.71
## q[371]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[372]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[373]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[374]      0.67    0.00 0.02     0.62     0.65     0.67     0.69     0.71
## q[375]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[376]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[377]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[378]      0.67    0.00 0.02     0.62     0.65     0.67     0.69     0.71
## q[379]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[380]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[381]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[382]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[383]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[384]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[385]      0.67    0.00 0.02     0.62     0.65     0.67     0.69     0.71
## q[386]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[387]      0.67    0.00 0.02     0.62     0.65     0.67     0.69     0.71
## q[388]      0.67    0.00 0.02     0.62     0.65     0.67     0.69     0.71
## q[389]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[390]      0.67    0.00 0.02     0.62     0.65     0.67     0.69     0.71
## q[391]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[392]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[393]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[394]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[395]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[396]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[397]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[398]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[399]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[400]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[401]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[402]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[403]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[404]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[405]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[406]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[407]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[408]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[409]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[410]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[411]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[412]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[413]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[414]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[415]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[416]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[417]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[418]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[419]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[420]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[421]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[422]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[423]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[424]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[425]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[426]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[427]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[428]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[429]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[430]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[431]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[432]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[433]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[434]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[435]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[436]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[437]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[438]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[439]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[440]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[441]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[442]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[443]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[444]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[445]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[446]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[447]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[448]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[449]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[450]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[451]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[452]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[453]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[454]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[455]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[456]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[457]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[458]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[459]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[460]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[461]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[462]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[463]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[464]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[465]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[466]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[467]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[468]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[469]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[470]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[471]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[472]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[473]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[474]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[475]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[476]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[477]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[478]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[479]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[480]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[481]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[482]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[483]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[484]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[485]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[486]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[487]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[488]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[489]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[490]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[491]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[492]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[493]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[494]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[495]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[496]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[497]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[498]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[499]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[500]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[501]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[502]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[503]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[504]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[505]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[506]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[507]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[508]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[509]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[510]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[511]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[512]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[513]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[514]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[515]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[516]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[517]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[518]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[519]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[520]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[521]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[522]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[523]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[524]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[525]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[526]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[527]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[528]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[529]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[530]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[531]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[532]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[533]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[534]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[535]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[536]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[537]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[538]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[539]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[540]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[541]      0.66    0.00 0.02     0.61     0.64     0.66     0.67     0.70
## q[542]      0.66    0.00 0.02     0.61     0.64     0.66     0.67     0.70
## q[543]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[544]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[545]      0.63    0.00 0.03     0.57     0.61     0.63     0.65     0.68
## q[546]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[547]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[548]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[549]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[550]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[551]      0.66    0.00 0.02     0.61     0.64     0.66     0.67     0.70
## q[552]      0.63    0.00 0.03     0.57     0.61     0.63     0.65     0.68
## q[553]      0.66    0.00 0.02     0.61     0.64     0.66     0.67     0.70
## q[554]      0.66    0.00 0.02     0.61     0.64     0.66     0.67     0.70
## q[555]      0.63    0.00 0.03     0.57     0.61     0.63     0.65     0.68
## q[556]      0.63    0.00 0.03     0.57     0.61     0.63     0.65     0.68
## q[557]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[558]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[559]      0.63    0.00 0.03     0.57     0.61     0.63     0.65     0.68
## q[560]      0.66    0.00 0.02     0.61     0.64     0.66     0.67     0.70
## q[561]      0.63    0.00 0.03     0.57     0.61     0.63     0.65     0.68
## q[562]      0.66    0.00 0.02     0.61     0.64     0.66     0.67     0.70
## q[563]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[564]      0.66    0.00 0.02     0.61     0.64     0.66     0.67     0.70
## q[565]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[566]      0.63    0.00 0.03     0.57     0.61     0.63     0.65     0.68
## q[567]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[568]      0.66    0.00 0.02     0.61     0.64     0.66     0.67     0.70
## q[569]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[570]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[571]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[572]      0.83    0.00 0.01     0.81     0.82     0.83     0.84     0.86
## q[573]      0.83    0.00 0.01     0.81     0.82     0.83     0.84     0.86
## q[574]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[575]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[576]      0.75    0.00 0.02     0.71     0.74     0.75     0.77     0.79
## q[577]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[578]      0.83    0.00 0.01     0.81     0.82     0.83     0.84     0.86
## q[579]      0.83    0.00 0.01     0.81     0.82     0.83     0.84     0.86
## q[580]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[581]      0.75    0.00 0.02     0.71     0.74     0.75     0.77     0.79
## q[582]      0.83    0.00 0.01     0.81     0.82     0.83     0.84     0.86
## q[583]      0.83    0.00 0.01     0.81     0.82     0.83     0.84     0.86
## q[584]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[585]      0.83    0.00 0.01     0.81     0.82     0.83     0.84     0.86
## q[586]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[587]      0.83    0.00 0.01     0.81     0.82     0.83     0.84     0.86
## q[588]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[589]      0.83    0.00 0.01     0.81     0.82     0.83     0.84     0.86
## q[590]      0.83    0.00 0.01     0.81     0.82     0.83     0.84     0.86
## q[591]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[592]      0.83    0.00 0.01     0.81     0.82     0.83     0.84     0.86
## q[593]      0.83    0.00 0.01     0.81     0.82     0.83     0.84     0.86
## q[594]      0.75    0.00 0.02     0.71     0.74     0.75     0.77     0.79
## q[595]      0.75    0.00 0.02     0.71     0.74     0.75     0.77     0.79
## q[596]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[597]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[598]      0.83    0.00 0.01     0.81     0.82     0.83     0.84     0.86
## q[599]      0.75    0.00 0.02     0.71     0.74     0.75     0.77     0.79
## q[600]      0.83    0.00 0.01     0.81     0.82     0.83     0.84     0.86
## q[601]      0.83    0.00 0.01     0.81     0.82     0.83     0.84     0.86
## q[602]      0.83    0.00 0.01     0.81     0.82     0.83     0.84     0.86
## q[603]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[604]      0.75    0.00 0.02     0.71     0.74     0.75     0.77     0.79
## q[605]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[606]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[607]      0.83    0.00 0.01     0.81     0.82     0.83     0.84     0.86
## q[608]      0.83    0.00 0.01     0.81     0.82     0.83     0.84     0.86
## q[609]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[610]      0.75    0.00 0.02     0.71     0.74     0.75     0.77     0.79
## q[611]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[612]      0.83    0.00 0.01     0.81     0.82     0.83     0.84     0.86
## q[613]      0.83    0.00 0.01     0.81     0.82     0.83     0.84     0.86
## q[614]      0.75    0.00 0.02     0.71     0.74     0.75     0.77     0.79
## q[615]      0.75    0.00 0.02     0.71     0.74     0.75     0.77     0.79
## q[616]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[617]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[618]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[619]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[620]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[621]      0.57    0.00 0.03     0.52     0.56     0.57     0.59     0.63
## q[622]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[623]      0.57    0.00 0.03     0.52     0.56     0.57     0.59     0.63
## q[624]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[625]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[626]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[627]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[628]      0.57    0.00 0.03     0.52     0.56     0.57     0.59     0.63
## q[629]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[630]      0.57    0.00 0.03     0.52     0.56     0.57     0.59     0.63
## q[631]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[632]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[633]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[634]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[635]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[636]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[637]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[638]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[639]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[640]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[641]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[642]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[643]      0.57    0.00 0.03     0.52     0.56     0.57     0.59     0.63
## q[644]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[645]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[646]      0.57    0.00 0.03     0.52     0.56     0.57     0.59     0.63
## q[647]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[648]      0.57    0.00 0.03     0.52     0.56     0.57     0.59     0.63
## q[649]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[650]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[651]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[652]      0.57    0.00 0.03     0.52     0.56     0.57     0.59     0.63
## q[653]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[654]      0.57    0.00 0.03     0.52     0.56     0.57     0.59     0.63
## q[655]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[656]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[657]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[658]      0.57    0.00 0.03     0.52     0.56     0.57     0.59     0.63
## q[659]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[660]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[661]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[662]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[663]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[664]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[665]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[666]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[667]      0.57    0.00 0.03     0.52     0.56     0.57     0.59     0.63
## q[668]      0.57    0.00 0.03     0.52     0.56     0.57     0.59     0.63
## q[669]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[670]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[671]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[672]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[673]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[674]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[675]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[676]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[677]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[678]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[679]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[680]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[681]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[682]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[683]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[684]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[685]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[686]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[687]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[688]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[689]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[690]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[691]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[692]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[693]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[694]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[695]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[696]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[697]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[698]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[699]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[700]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[701]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[702]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[703]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[704]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[705]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[706]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[707]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[708]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[709]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[710]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[711]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[712]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[713]      0.72    0.00 0.02     0.69     0.71     0.72     0.73     0.76
## q[714]      0.72    0.00 0.02     0.69     0.71     0.72     0.73     0.76
## q[715]      0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.74
## q[716]      0.72    0.00 0.02     0.69     0.71     0.72     0.73     0.76
## q[717]      0.72    0.00 0.02     0.69     0.71     0.72     0.73     0.76
## q[718]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[719]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[720]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[721]      0.72    0.00 0.02     0.69     0.71     0.72     0.73     0.76
## q[722]      0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.74
## q[723]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[724]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[725]      0.72    0.00 0.02     0.69     0.71     0.72     0.73     0.76
## q[726]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[727]      0.72    0.00 0.02     0.69     0.71     0.72     0.73     0.76
## q[728]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[729]      0.72    0.00 0.02     0.69     0.71     0.72     0.73     0.76
## q[730]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[731]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[732]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[733]      0.72    0.00 0.02     0.69     0.71     0.72     0.73     0.76
## q[734]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[735]      0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.74
## q[736]      0.72    0.00 0.02     0.69     0.71     0.72     0.73     0.76
## q[737]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[738]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[739]      0.72    0.00 0.02     0.69     0.71     0.72     0.73     0.76
## q[740]      0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.74
## q[741]      0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.74
## q[742]      0.72    0.00 0.02     0.69     0.71     0.72     0.73     0.76
## q[743]      0.72    0.00 0.02     0.69     0.71     0.72     0.73     0.76
## q[744]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[745]      0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.74
## q[746]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[747]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[748]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[749]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[750]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[751]      0.72    0.00 0.02     0.69     0.71     0.72     0.73     0.76
## q[752]      0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.74
## q[753]      0.72    0.00 0.02     0.69     0.71     0.72     0.73     0.76
## q[754]      0.72    0.00 0.02     0.69     0.71     0.72     0.73     0.76
## q[755]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[756]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[757]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[758]      0.72    0.00 0.02     0.69     0.71     0.72     0.73     0.76
## q[759]      0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.74
## q[760]      0.72    0.00 0.02     0.69     0.71     0.72     0.73     0.76
## q[761]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[762]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[763]      0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.74
## q[764]      0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.74
## q[765]      0.56    0.00 0.03     0.51     0.54     0.56     0.57     0.61
## q[766]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[767]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[768]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[769]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[770]      0.56    0.00 0.03     0.51     0.54     0.56     0.57     0.61
## q[771]      0.56    0.00 0.03     0.51     0.54     0.56     0.57     0.61
## q[772]      0.56    0.00 0.03     0.51     0.54     0.56     0.57     0.61
## q[773]      0.56    0.00 0.03     0.51     0.54     0.56     0.57     0.61
## q[774]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[775]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[776]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[777]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[778]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[779]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[780]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[781]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[782]      0.56    0.00 0.03     0.51     0.54     0.56     0.57     0.61
## q[783]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[784]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[785]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[786]      0.56    0.00 0.03     0.51     0.54     0.56     0.57     0.61
## q[787]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[788]      0.56    0.00 0.03     0.51     0.54     0.56     0.57     0.61
## q[789]      0.56    0.00 0.03     0.51     0.54     0.56     0.57     0.61
## q[790]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[791]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[792]      0.56    0.00 0.03     0.51     0.54     0.56     0.57     0.61
## q[793]      0.56    0.00 0.03     0.51     0.54     0.56     0.57     0.61
## q[794]      0.56    0.00 0.03     0.51     0.54     0.56     0.57     0.61
## q[795]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[796]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[797]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[798]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[799]      0.56    0.00 0.03     0.51     0.54     0.56     0.57     0.61
## q[800]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[801]      0.56    0.00 0.03     0.51     0.54     0.56     0.57     0.61
## q[802]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[803]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[804]      0.56    0.00 0.03     0.51     0.54     0.56     0.57     0.61
## q[805]      0.56    0.00 0.03     0.51     0.54     0.56     0.57     0.61
## q[806]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[807]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[808]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[809]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[810]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[811]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[812]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[813]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[814]      0.56    0.00 0.03     0.51     0.54     0.56     0.57     0.61
## q[815]      0.56    0.00 0.03     0.51     0.54     0.56     0.57     0.61
## q[816]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[817]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[818]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[819]      0.56    0.00 0.03     0.51     0.54     0.56     0.57     0.61
## q[820]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[821]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[822]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[823]      0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[824]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[825]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[826]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[827]      0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.77
## q[828]      0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.77
## q[829]      0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.77
## q[830]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[831]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[832]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[833]      0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.77
## q[834]      0.71    0.00 0.02     0.66     0.69     0.71     0.72     0.75
## q[835]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[836]      0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.77
## q[837]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[838]      0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.77
## q[839]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[840]      0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.77
## q[841]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[842]      0.71    0.00 0.02     0.66     0.69     0.71     0.72     0.75
## q[843]      0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.77
## q[844]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[845]      0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.77
## q[846]      0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.77
## q[847]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[848]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[849]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[850]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[851]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[852]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[853]      0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.77
## q[854]      0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.77
## q[855]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[856]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[857]      0.71    0.00 0.02     0.66     0.69     0.71     0.72     0.75
## q[858]      0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.77
## q[859]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[860]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[861]      0.71    0.00 0.02     0.66     0.69     0.71     0.72     0.75
## q[862]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.72
## q[863]      0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.78
## q[864]      0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.78
## q[865]      0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.78
## q[866]      0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.78
## q[867]      0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.78
## q[868]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.72
## q[869]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.72
## q[870]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.72
## q[871]      0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.70
## q[872]      0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.78
## q[873]      0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.78
## q[874]      0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.78
## q[875]      0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.78
## q[876]      0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.78
## q[877]      0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.78
## q[878]      0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.70
## q[879]      0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.78
## q[880]      0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.78
## q[881]      0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.78
## q[882]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.72
## q[883]      0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.78
## q[884]      0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.78
## q[885]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.72
## q[886]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.72
## q[887]      0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.70
## q[888]      0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.78
## q[889]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.72
## q[890]      0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.78
## q[891]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.72
## q[892]      0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.78
## q[893]      0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.78
## q[894]      0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.78
## q[895]      0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.78
## q[896]      0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.78
## q[897]      0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.70
## q[898]      0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.78
## q[899]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.72
## q[900]      0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.78
## q[901]      0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.78
## q[902]      0.67    0.00 0.02     0.62     0.65     0.67     0.68     0.72
## q[903]      0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.78
## q[904]      0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.78
## q[905]      0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.70
## q[906]      0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.78
## q[907]      0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.92
## q[908]      0.83    0.00 0.02     0.78     0.81     0.83     0.85     0.87
## q[909]      0.85    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[910]      0.85    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[911]      0.83    0.00 0.02     0.78     0.81     0.83     0.85     0.87
## q[912]      0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.92
## q[913]      0.85    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[914]      0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.92
## q[915]      0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.92
## q[916]      0.85    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[917]      0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.92
## q[918]      0.85    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[919]      0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.92
## q[920]      0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.92
## q[921]      0.85    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[922]      0.85    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[923]      0.83    0.00 0.02     0.78     0.81     0.83     0.85     0.87
## q[924]      0.85    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[925]      0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.92
## q[926]      0.85    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[927]      0.85    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[928]      0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.92
## q[929]      0.85    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[930]      0.83    0.00 0.02     0.78     0.81     0.83     0.85     0.87
## q[931]      0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.92
## q[932]      0.83    0.00 0.02     0.78     0.81     0.83     0.85     0.87
## q[933]      0.85    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[934]      0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.92
## q[935]      0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.92
## q[936]      0.85    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[937]      0.85    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[938]      0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.92
## q[939]      0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.92
## q[940]      0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.92
## q[941]      0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.92
## q[942]      0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.92
## q[943]      0.85    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[944]      0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.92
## q[945]      0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.92
## q[946]      0.83    0.00 0.02     0.78     0.81     0.83     0.85     0.87
## q[947]      0.83    0.00 0.02     0.78     0.81     0.83     0.85     0.87
## q[948]      0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.92
## q[949]      0.85    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[950]      0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.92
## q[951]      0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.92
## q[952]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[953]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[954]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[955]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[956]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[957]      0.69    0.00 0.02     0.64     0.67     0.69     0.71     0.73
## q[958]      0.69    0.00 0.02     0.64     0.67     0.69     0.71     0.73
## q[959]      0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[960]      0.69    0.00 0.02     0.64     0.67     0.69     0.71     0.73
## q[961]      0.69    0.00 0.02     0.64     0.67     0.69     0.71     0.73
## q[962]      0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[963]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[964]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[965]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[966]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[967]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[968]      0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[969]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[970]      0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[971]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[972]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[973]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[974]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[975]      0.69    0.00 0.02     0.64     0.67     0.69     0.71     0.73
## q[976]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[977]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[978]      0.69    0.00 0.02     0.64     0.67     0.69     0.71     0.73
## q[979]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[980]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[981]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[982]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[983]      0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[984]      0.69    0.00 0.02     0.64     0.67     0.69     0.71     0.73
## q[985]      0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[986]      0.69    0.00 0.02     0.64     0.67     0.69     0.71     0.73
## q[987]      0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[988]      0.69    0.00 0.02     0.64     0.67     0.69     0.71     0.73
## q[989]      0.69    0.00 0.02     0.64     0.67     0.69     0.71     0.73
## q[990]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[991]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[992]      0.69    0.00 0.02     0.64     0.67     0.69     0.71     0.73
## q[993]      0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[994]      0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[995]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[996]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[997]      0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[998]      0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[999]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1000]     0.69    0.00 0.02     0.64     0.67     0.69     0.71     0.73
## q[1001]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1002]     0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[1003]     0.69    0.00 0.02     0.64     0.67     0.69     0.71     0.73
## q[1004]     0.69    0.00 0.02     0.64     0.67     0.69     0.71     0.73
## q[1005]     0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[1006]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1007]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1008]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1009]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1010]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1011]     0.69    0.00 0.02     0.64     0.67     0.69     0.71     0.73
## q[1012]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1013]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1014]     0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[1015]     0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[1016]     0.69    0.00 0.02     0.64     0.67     0.69     0.71     0.73
## q[1017]     0.69    0.00 0.02     0.64     0.67     0.69     0.71     0.73
## q[1018]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1019]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1020]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1021]     0.69    0.00 0.02     0.64     0.67     0.69     0.71     0.73
## q[1022]     0.69    0.00 0.02     0.64     0.67     0.69     0.71     0.73
## q[1023]     0.69    0.00 0.02     0.64     0.67     0.69     0.71     0.73
## q[1024]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1025]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1026]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1027]     0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[1028]     0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[1029]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1030]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1031]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1032]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1033]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1034]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1035]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1036]     0.50    0.00 0.03     0.43     0.47     0.50     0.52     0.56
## q[1037]     0.50    0.00 0.03     0.43     0.47     0.50     0.52     0.56
## q[1038]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1039]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1040]     0.50    0.00 0.03     0.43     0.47     0.50     0.52     0.56
## q[1041]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1042]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1043]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1044]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1045]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1046]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1047]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1048]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1049]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1050]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1051]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1052]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1053]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1054]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1055]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1056]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1057]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1058]     0.50    0.00 0.03     0.43     0.47     0.50     0.52     0.56
## q[1059]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1060]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1061]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1062]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1063]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1064]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1065]     0.50    0.00 0.03     0.43     0.47     0.50     0.52     0.56
## q[1066]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1067]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1068]     0.50    0.00 0.03     0.43     0.47     0.50     0.52     0.56
## q[1069]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1070]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1071]     0.50    0.00 0.03     0.43     0.47     0.50     0.52     0.56
## q[1072]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1073]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1074]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1075]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1076]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1077]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1078]     0.50    0.00 0.03     0.43     0.47     0.50     0.52     0.56
## q[1079]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1080]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1081]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1082]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1083]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1084]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1085]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1086]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1087]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1088]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1089]     0.50    0.00 0.03     0.43     0.47     0.50     0.52     0.56
## q[1090]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1091]     0.50    0.00 0.03     0.43     0.47     0.50     0.52     0.56
## q[1092]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1093]     0.50    0.00 0.03     0.43     0.47     0.50     0.52     0.56
## q[1094]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1095]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1096]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1097]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1098]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1099]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1100]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1101]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1102]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1103]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1104]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1105]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1106]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1107]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1108]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1109]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1110]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1111]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1112]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1113]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1114]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1115]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1116]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1117]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1118]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1119]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1120]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1121]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1122]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1123]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1124]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1125]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1126]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1127]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1128]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1129]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1130]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1131]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1132]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1133]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1134]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1135]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1136]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1137]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1138]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1139]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1140]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1141]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1142]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1143]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1144]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1145]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1146]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1147]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1148]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1149]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1150]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1151]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1152]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1153]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1154]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1155]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1156]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1157]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1158]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1159]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1160]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1161]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1162]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1163]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1164]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1165]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1166]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1167]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1168]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1169]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.63
## q[1170]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1171]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1172]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1173]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1174]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.70
## q[1175]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1176]     0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[1177]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1178]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1179]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.60
## q[1180]     0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[1181]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.60
## q[1182]     0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[1183]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.60
## q[1184]     0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[1185]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1186]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1187]     0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[1188]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1189]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1190]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1191]     0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[1192]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1193]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1194]     0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[1195]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1196]     0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[1197]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1198]     0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[1199]     0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[1200]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1201]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1202]     0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[1203]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1204]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.60
## q[1205]     0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[1206]     0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[1207]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1208]     0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[1209]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1210]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1211]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1212]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.60
## q[1213]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.60
## q[1214]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1215]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.60
## q[1216]     0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[1217]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.60
## q[1218]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1219]     0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[1220]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1221]     0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[1222]     0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[1223]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1224]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.60
## q[1225]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1226]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1227]     0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[1228]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1229]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1230]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.60
## q[1231]     0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[1232]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1233]     0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[1234]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.60
## q[1235]     0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[1236]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1237]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1238]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.60
## q[1239]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.77
## q[1240]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.77
## q[1241]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.77
## q[1242]     0.63    0.00 0.03     0.58     0.61     0.63     0.65     0.69
## q[1243]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.77
## q[1244]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.77
## q[1245]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.77
## q[1246]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1247]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1248]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1249]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1250]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1251]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.77
## q[1252]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.77
## q[1253]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.77
## q[1254]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.77
## q[1255]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.77
## q[1256]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.77
## q[1257]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1258]     0.63    0.00 0.03     0.58     0.61     0.63     0.65     0.69
## q[1259]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.77
## q[1260]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.77
## q[1261]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.77
## q[1262]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1263]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1264]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.77
## q[1265]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1266]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.77
## q[1267]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1268]     0.63    0.00 0.03     0.58     0.61     0.63     0.65     0.69
## q[1269]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1270]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1271]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.77
## q[1272]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1273]     0.63    0.00 0.03     0.58     0.61     0.63     0.65     0.69
## q[1274]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.77
## q[1275]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1276]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.77
## q[1277]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1278]     0.63    0.00 0.03     0.58     0.61     0.63     0.65     0.69
## q[1279]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.77
## q[1280]     0.63    0.00 0.03     0.58     0.61     0.63     0.65     0.69
## q[1281]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1282]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1283]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1284]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.77
## q[1285]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.77
## q[1286]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.77
## q[1287]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.77
## q[1288]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.77
## q[1289]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.77
## q[1290]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1291]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.77
## q[1292]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.77
## q[1293]     0.63    0.00 0.03     0.58     0.61     0.63     0.65     0.69
## q[1294]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.77
## q[1295]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1296]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.77
## q[1297]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.77
## q[1298]     0.63    0.00 0.03     0.58     0.61     0.63     0.65     0.69
## q[1299]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1300]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1301]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.77
## q[1302]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.77
## q[1303]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.77
## q[1304]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.77
## q[1305]     0.63    0.00 0.03     0.58     0.61     0.63     0.65     0.69
## q[1306]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.77
## q[1307]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1308]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1309]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1310]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1311]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1312]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1313]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1314]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1315]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1316]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1317]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1318]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1319]     0.57    0.00 0.03     0.51     0.55     0.57     0.59     0.63
## q[1320]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1321]     0.57    0.00 0.03     0.51     0.55     0.57     0.59     0.63
## q[1322]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1323]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1324]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1325]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1326]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1327]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1328]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1329]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1330]     0.57    0.00 0.03     0.51     0.55     0.57     0.59     0.63
## q[1331]     0.57    0.00 0.03     0.51     0.55     0.57     0.59     0.63
## q[1332]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1333]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1334]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1335]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1336]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1337]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1338]     0.57    0.00 0.03     0.51     0.55     0.57     0.59     0.63
## q[1339]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1340]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1341]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1342]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1343]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1344]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1345]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1346]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1347]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1348]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1349]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1350]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1351]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1352]     0.57    0.00 0.03     0.51     0.55     0.57     0.59     0.63
## q[1353]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1354]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1355]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1356]     0.57    0.00 0.03     0.51     0.55     0.57     0.59     0.63
## q[1357]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1358]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1359]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1360]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1361]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1362]     0.63    0.00 0.02     0.58     0.61     0.63     0.64     0.67
## q[1363]     0.60    0.00 0.03     0.54     0.58     0.60     0.62     0.65
## q[1364]     0.63    0.00 0.02     0.58     0.61     0.63     0.64     0.67
## q[1365]     0.63    0.00 0.02     0.58     0.61     0.63     0.64     0.67
## q[1366]     0.60    0.00 0.03     0.54     0.58     0.60     0.62     0.65
## q[1367]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1368]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1369]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1370]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1371]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1372]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1373]     0.63    0.00 0.02     0.58     0.61     0.63     0.64     0.67
## q[1374]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1375]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1376]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1377]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1378]     0.63    0.00 0.02     0.58     0.61     0.63     0.64     0.67
## q[1379]     0.60    0.00 0.03     0.54     0.58     0.60     0.62     0.65
## q[1380]     0.63    0.00 0.02     0.58     0.61     0.63     0.64     0.67
## q[1381]     0.63    0.00 0.02     0.58     0.61     0.63     0.64     0.67
## q[1382]     0.63    0.00 0.02     0.58     0.61     0.63     0.64     0.67
## q[1383]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1384]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1385]     0.63    0.00 0.02     0.58     0.61     0.63     0.64     0.67
## q[1386]     0.60    0.00 0.03     0.54     0.58     0.60     0.62     0.65
## q[1387]     0.60    0.00 0.03     0.54     0.58     0.60     0.62     0.65
## q[1388]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1389]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1390]     0.63    0.00 0.02     0.58     0.61     0.63     0.64     0.67
## q[1391]     0.60    0.00 0.03     0.54     0.58     0.60     0.62     0.65
## q[1392]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1393]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1394]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1395]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1396]     0.60    0.00 0.03     0.54     0.58     0.60     0.62     0.65
## q[1397]     0.60    0.00 0.03     0.54     0.58     0.60     0.62     0.65
## q[1398]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1399]     0.60    0.00 0.03     0.54     0.58     0.60     0.62     0.65
## q[1400]     0.63    0.00 0.02     0.58     0.61     0.63     0.64     0.67
## q[1401]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1402]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1403]     0.63    0.00 0.02     0.58     0.61     0.63     0.64     0.67
## q[1404]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1405]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1406]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1407]     0.60    0.00 0.03     0.54     0.58     0.60     0.62     0.65
## q[1408]     0.72    0.00 0.02     0.67     0.70     0.72     0.73     0.76
## q[1409]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1410]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1411]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[1412]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[1413]     0.72    0.00 0.02     0.67     0.70     0.72     0.73     0.76
## q[1414]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1415]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1416]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1417]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1418]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1419]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[1420]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[1421]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[1422]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1423]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[1424]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[1425]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1426]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1427]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1428]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1429]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1430]     0.72    0.00 0.02     0.67     0.70     0.72     0.73     0.76
## q[1431]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1432]     0.72    0.00 0.02     0.67     0.70     0.72     0.73     0.76
## q[1433]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1434]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[1435]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1436]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1437]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1438]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[1439]     0.72    0.00 0.02     0.67     0.70     0.72     0.73     0.76
## q[1440]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1441]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[1442]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1443]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[1444]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1445]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[1446]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1447]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1448]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[1449]     0.72    0.00 0.02     0.67     0.70     0.72     0.73     0.76
## q[1450]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[1451]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1452]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[1453]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[1454]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[1455]     0.72    0.00 0.02     0.67     0.70     0.72     0.73     0.76
## q[1456]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1457]     0.72    0.00 0.02     0.67     0.70     0.72     0.73     0.76
## q[1458]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1459]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1460]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[1461]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[1462]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1463]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1464]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1465]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1466]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1467]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[1468]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1469]     0.72    0.00 0.02     0.67     0.70     0.72     0.73     0.76
## q[1470]     0.72    0.00 0.02     0.67     0.70     0.72     0.73     0.76
## q[1471]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1472]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1473]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1474]     0.86    0.00 0.01     0.84     0.85     0.86     0.87     0.89
## q[1475]     0.81    0.00 0.02     0.77     0.80     0.81     0.83     0.85
## q[1476]     0.79    0.00 0.02     0.75     0.78     0.79     0.81     0.83
## q[1477]     0.86    0.00 0.01     0.84     0.85     0.86     0.87     0.89
## q[1478]     0.86    0.00 0.01     0.84     0.85     0.86     0.87     0.89
## q[1479]     0.81    0.00 0.02     0.77     0.80     0.81     0.83     0.85
## q[1480]     0.86    0.00 0.01     0.84     0.85     0.86     0.87     0.89
## q[1481]     0.86    0.00 0.01     0.84     0.85     0.86     0.87     0.89
## q[1482]     0.86    0.00 0.01     0.84     0.85     0.86     0.87     0.89
## q[1483]     0.79    0.00 0.02     0.75     0.78     0.79     0.81     0.83
## q[1484]     0.79    0.00 0.02     0.75     0.78     0.79     0.81     0.83
## q[1485]     0.81    0.00 0.02     0.77     0.80     0.81     0.83     0.85
## q[1486]     0.79    0.00 0.02     0.75     0.78     0.79     0.81     0.83
## q[1487]     0.79    0.00 0.02     0.75     0.78     0.79     0.81     0.83
## q[1488]     0.81    0.00 0.02     0.77     0.80     0.81     0.83     0.85
## q[1489]     0.86    0.00 0.01     0.84     0.85     0.86     0.87     0.89
## q[1490]     0.81    0.00 0.02     0.77     0.80     0.81     0.83     0.85
## q[1491]     0.79    0.00 0.02     0.75     0.78     0.79     0.81     0.83
## q[1492]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1493]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1494]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1495]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1496]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1497]     0.71    0.00 0.02     0.66     0.69     0.71     0.72     0.75
## q[1498]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1499]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1500]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1501]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1502]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1503]     0.71    0.00 0.02     0.66     0.69     0.71     0.72     0.75
## q[1504]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1505]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1506]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1507]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1508]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1509]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1510]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1511]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1512]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1513]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1514]     0.71    0.00 0.02     0.66     0.69     0.71     0.72     0.75
## q[1515]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1516]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1517]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1518]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1519]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1520]     0.71    0.00 0.02     0.66     0.69     0.71     0.72     0.75
## q[1521]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1522]     0.71    0.00 0.02     0.66     0.69     0.71     0.72     0.75
## q[1523]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1524]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1525]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1526]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1527]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1528]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1529]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1530]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1531]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1532]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1533]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1534]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1535]     0.71    0.00 0.02     0.66     0.69     0.71     0.72     0.75
## q[1536]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1537]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1538]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1539]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1540]     0.71    0.00 0.02     0.66     0.69     0.71     0.72     0.75
## q[1541]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.81
## q[1542]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.81
## q[1543]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.81
## q[1544]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1545]     0.71    0.00 0.03     0.66     0.69     0.71     0.72     0.75
## q[1546]     0.71    0.00 0.03     0.66     0.69     0.71     0.72     0.75
## q[1547]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1548]     0.71    0.00 0.03     0.66     0.69     0.71     0.72     0.75
## q[1549]     0.71    0.00 0.03     0.66     0.69     0.71     0.72     0.75
## q[1550]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1551]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.81
## q[1552]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.81
## q[1553]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.81
## q[1554]     0.71    0.00 0.03     0.66     0.69     0.71     0.72     0.75
## q[1555]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1556]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.81
## q[1557]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.81
## q[1558]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.81
## q[1559]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.81
## q[1560]     0.71    0.00 0.03     0.66     0.69     0.71     0.72     0.75
## q[1561]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.81
## q[1562]     0.71    0.00 0.03     0.66     0.69     0.71     0.72     0.75
## q[1563]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.81
## q[1564]     0.71    0.00 0.03     0.66     0.69     0.71     0.72     0.75
## q[1565]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.81
## q[1566]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.81
## q[1567]     0.71    0.00 0.03     0.66     0.69     0.71     0.72     0.75
## q[1568]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.81
## q[1569]     0.71    0.00 0.03     0.66     0.69     0.71     0.72     0.75
## q[1570]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.81
## q[1571]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1572]     0.71    0.00 0.03     0.66     0.69     0.71     0.72     0.75
## q[1573]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.81
## q[1574]     0.71    0.00 0.03     0.66     0.69     0.71     0.72     0.75
## q[1575]     0.71    0.00 0.03     0.66     0.69     0.71     0.72     0.75
## q[1576]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.81
## q[1577]     0.71    0.00 0.03     0.66     0.69     0.71     0.72     0.75
## q[1578]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.81
## q[1579]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1580]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1581]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.81
## q[1582]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1583]     0.71    0.00 0.03     0.66     0.69     0.71     0.72     0.75
## q[1584]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1585]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.81
## q[1586]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.81
## q[1587]     0.71    0.00 0.03     0.66     0.69     0.71     0.72     0.75
## q[1588]     0.71    0.00 0.03     0.66     0.69     0.71     0.72     0.75
## q[1589]     0.71    0.00 0.03     0.66     0.69     0.71     0.72     0.75
## q[1590]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.81
## q[1591]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1592]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.81
## q[1593]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.81
## q[1594]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.81
## q[1595]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.81
## q[1596]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.81
## q[1597]     0.71    0.00 0.03     0.66     0.69     0.71     0.72     0.75
## q[1598]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.81
## q[1599]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.81
## q[1600]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1601]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1602]     0.71    0.00 0.03     0.66     0.69     0.71     0.72     0.75
## q[1603]     0.71    0.00 0.03     0.66     0.69     0.71     0.72     0.75
## q[1604]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.81
## q[1605]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.81
## q[1606]     0.71    0.00 0.03     0.66     0.69     0.71     0.72     0.75
## q[1607]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1608]     0.71    0.00 0.03     0.66     0.69     0.71     0.72     0.75
## q[1609]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.81
## q[1610]     0.71    0.00 0.03     0.66     0.69     0.71     0.72     0.75
## q[1611]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.81
## q[1612]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.81
## q[1613]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.81
## q[1614]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1615]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1616]     0.59    0.00 0.03     0.53     0.57     0.59     0.61     0.65
## q[1617]     0.59    0.00 0.03     0.53     0.57     0.59     0.61     0.65
## q[1618]     0.50    0.00 0.03     0.44     0.47     0.50     0.52     0.56
## q[1619]     0.59    0.00 0.03     0.53     0.57     0.59     0.61     0.65
## q[1620]     0.59    0.00 0.03     0.53     0.57     0.59     0.61     0.65
## q[1621]     0.47    0.00 0.04     0.40     0.44     0.47     0.49     0.54
## q[1622]     0.50    0.00 0.03     0.44     0.47     0.50     0.52     0.56
## q[1623]     0.59    0.00 0.03     0.53     0.57     0.59     0.61     0.65
## q[1624]     0.50    0.00 0.03     0.44     0.47     0.50     0.52     0.56
## q[1625]     0.59    0.00 0.03     0.53     0.57     0.59     0.61     0.65
## q[1626]     0.59    0.00 0.03     0.53     0.57     0.59     0.61     0.65
## q[1627]     0.59    0.00 0.03     0.53     0.57     0.59     0.61     0.65
## q[1628]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[1629]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[1630]     0.62    0.00 0.03     0.57     0.60     0.62     0.64     0.67
## q[1631]     0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.76
## q[1632]     0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.76
## q[1633]     0.62    0.00 0.03     0.57     0.60     0.62     0.64     0.67
## q[1634]     0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.76
## q[1635]     0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.76
## q[1636]     0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.76
## q[1637]     0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.76
## q[1638]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[1639]     0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.76
## q[1640]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[1641]     0.62    0.00 0.03     0.57     0.60     0.62     0.64     0.67
## q[1642]     0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.76
## q[1643]     0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.76
## q[1644]     0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.76
## q[1645]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[1646]     0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.76
## q[1647]     0.62    0.00 0.03     0.57     0.60     0.62     0.64     0.67
## q[1648]     0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.76
## q[1649]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[1650]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[1651]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1652]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1653]     0.59    0.00 0.03     0.53     0.57     0.59     0.61     0.64
## q[1654]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1655]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1656]     0.59    0.00 0.03     0.53     0.57     0.59     0.61     0.64
## q[1657]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1658]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1659]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1660]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1661]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1662]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1663]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1664]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1665]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1666]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1667]     0.59    0.00 0.03     0.53     0.57     0.59     0.61     0.64
## q[1668]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1669]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1670]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1671]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1672]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1673]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1674]     0.59    0.00 0.03     0.53     0.57     0.59     0.61     0.64
## q[1675]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1676]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1677]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1678]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1679]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1680]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1681]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1682]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1683]     0.59    0.00 0.03     0.53     0.57     0.59     0.61     0.64
## q[1684]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1685]     0.59    0.00 0.03     0.53     0.57     0.59     0.61     0.64
## q[1686]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1687]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1688]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1689]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1690]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1691]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1692]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1693]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1694]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1695]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1696]     0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[1697]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1698]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1699]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1700]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1701]     0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[1702]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1703]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1704]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1705]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1706]     0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[1707]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1708]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1709]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1710]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1711]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1712]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1713]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1714]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1715]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1716]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1717]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1718]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1719]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1720]     0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[1721]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1722]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1723]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1724]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1725]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1726]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1727]     0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[1728]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1729]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1730]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1731]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1732]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1733]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1734]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1735]     0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[1736]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1737]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1738]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1739]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1740]     0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[1741]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1742]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1743]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1744]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1745]     0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[1746]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1747]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1748]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1749]     0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.78
## q[1750]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1751]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1752]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1753]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1754]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1755]     0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.78
## q[1756]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1757]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1758]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1759]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1760]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1761]     0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.78
## q[1762]     0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.78
## q[1763]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1764]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1765]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1766]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1767]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1768]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1769]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1770]     0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.78
## q[1771]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1772]     0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.78
## q[1773]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1774]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1775]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1776]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1777]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1778]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1779]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1780]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1781]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1782]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1783]     0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.78
## q[1784]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1785]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1786]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1787]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1788]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1789]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1790]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1791]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1792]     0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.78
## q[1793]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1794]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1795]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1796]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1797]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1798]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1799]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1800]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1801]     0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.78
## q[1802]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1803]     0.66    0.00 0.02     0.62     0.65     0.66     0.67     0.70
## q[1804]     0.66    0.00 0.02     0.62     0.65     0.66     0.67     0.70
## q[1805]     0.66    0.00 0.02     0.62     0.65     0.66     0.67     0.70
## q[1806]     0.66    0.00 0.02     0.62     0.65     0.66     0.67     0.70
## q[1807]     0.66    0.00 0.02     0.62     0.65     0.66     0.67     0.70
## q[1808]     0.66    0.00 0.02     0.62     0.65     0.66     0.67     0.70
## q[1809]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1810]     0.54    0.00 0.03     0.48     0.52     0.54     0.56     0.60
## q[1811]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1812]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1813]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1814]     0.54    0.00 0.03     0.48     0.52     0.54     0.56     0.60
## q[1815]     0.54    0.00 0.03     0.48     0.52     0.54     0.56     0.60
## q[1816]     0.66    0.00 0.02     0.62     0.65     0.66     0.67     0.70
## q[1817]     0.66    0.00 0.02     0.62     0.65     0.66     0.67     0.70
## q[1818]     0.66    0.00 0.02     0.62     0.65     0.66     0.67     0.70
## q[1819]     0.66    0.00 0.02     0.62     0.65     0.66     0.67     0.70
## q[1820]     0.66    0.00 0.02     0.62     0.65     0.66     0.67     0.70
## q[1821]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1822]     0.66    0.00 0.02     0.62     0.65     0.66     0.67     0.70
## q[1823]     0.66    0.00 0.02     0.62     0.65     0.66     0.67     0.70
## q[1824]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1825]     0.66    0.00 0.02     0.62     0.65     0.66     0.67     0.70
## q[1826]     0.66    0.00 0.02     0.62     0.65     0.66     0.67     0.70
## q[1827]     0.66    0.00 0.02     0.62     0.65     0.66     0.67     0.70
## q[1828]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1829]     0.66    0.00 0.02     0.62     0.65     0.66     0.67     0.70
## q[1830]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1831]     0.66    0.00 0.02     0.62     0.65     0.66     0.67     0.70
## q[1832]     0.66    0.00 0.02     0.62     0.65     0.66     0.67     0.70
## q[1833]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1834]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1835]     0.66    0.00 0.02     0.62     0.65     0.66     0.67     0.70
## q[1836]     0.66    0.00 0.02     0.62     0.65     0.66     0.67     0.70
## q[1837]     0.66    0.00 0.02     0.62     0.65     0.66     0.67     0.70
## q[1838]     0.66    0.00 0.02     0.62     0.65     0.66     0.67     0.70
## q[1839]     0.54    0.00 0.03     0.48     0.52     0.54     0.56     0.60
## q[1840]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1841]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1842]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1843]     0.66    0.00 0.02     0.62     0.65     0.66     0.67     0.70
## q[1844]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1845]     0.54    0.00 0.03     0.48     0.52     0.54     0.56     0.60
## q[1846]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1847]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1848]     0.66    0.00 0.02     0.62     0.65     0.66     0.67     0.70
## q[1849]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1850]     0.66    0.00 0.02     0.62     0.65     0.66     0.67     0.70
## q[1851]     0.66    0.00 0.02     0.62     0.65     0.66     0.67     0.70
## q[1852]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1853]     0.54    0.00 0.03     0.48     0.52     0.54     0.56     0.60
## q[1854]     0.54    0.00 0.03     0.48     0.52     0.54     0.56     0.60
## q[1855]     0.66    0.00 0.02     0.62     0.65     0.66     0.67     0.70
## q[1856]     0.54    0.00 0.03     0.48     0.52     0.54     0.56     0.60
## q[1857]     0.54    0.00 0.03     0.48     0.52     0.54     0.56     0.60
## q[1858]     0.66    0.00 0.02     0.62     0.65     0.66     0.67     0.70
## q[1859]     0.66    0.00 0.02     0.62     0.65     0.66     0.67     0.70
## q[1860]     0.66    0.00 0.02     0.62     0.65     0.66     0.67     0.70
## q[1861]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1862]     0.66    0.00 0.02     0.62     0.65     0.66     0.67     0.70
## q[1863]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1864]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1865]     0.66    0.00 0.02     0.62     0.65     0.66     0.67     0.70
## q[1866]     0.54    0.00 0.03     0.48     0.52     0.54     0.56     0.60
## q[1867]     0.66    0.00 0.02     0.62     0.65     0.66     0.67     0.70
## q[1868]     0.66    0.00 0.02     0.62     0.65     0.66     0.67     0.70
## q[1869]     0.66    0.00 0.02     0.62     0.65     0.66     0.67     0.70
## q[1870]     0.66    0.00 0.02     0.62     0.65     0.66     0.67     0.70
## q[1871]     0.66    0.00 0.02     0.62     0.65     0.66     0.67     0.70
## q[1872]     0.66    0.00 0.02     0.62     0.65     0.66     0.67     0.70
## q[1873]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1874]     0.66    0.00 0.02     0.62     0.65     0.66     0.67     0.70
## q[1875]     0.54    0.00 0.03     0.48     0.52     0.54     0.56     0.60
## q[1876]     0.54    0.00 0.03     0.48     0.52     0.54     0.56     0.60
## q[1877]     0.66    0.00 0.02     0.62     0.65     0.66     0.67     0.70
## q[1878]     0.54    0.00 0.03     0.48     0.52     0.54     0.56     0.60
## q[1879]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1880]     0.66    0.00 0.02     0.62     0.65     0.66     0.67     0.70
## q[1881]     0.66    0.00 0.02     0.62     0.65     0.66     0.67     0.70
## q[1882]     0.57    0.00 0.02     0.52     0.56     0.57     0.59     0.62
## q[1883]     0.54    0.00 0.03     0.48     0.52     0.54     0.56     0.60
## q[1884]     0.66    0.00 0.02     0.62     0.65     0.66     0.67     0.70
## q[1885]     0.66    0.00 0.02     0.62     0.65     0.66     0.67     0.70
## q[1886]     0.66    0.00 0.02     0.62     0.65     0.66     0.67     0.70
## q[1887]     0.66    0.00 0.02     0.62     0.65     0.66     0.67     0.70
## q[1888]     0.66    0.00 0.02     0.62     0.65     0.66     0.67     0.70
## q[1889]     0.54    0.00 0.03     0.48     0.52     0.54     0.56     0.60
## q[1890]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1891]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1892]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1893]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1894]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1895]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1896]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1897]     0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.75
## q[1898]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1899]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1900]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1901]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1902]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1903]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1904]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1905]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1906]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1907]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1908]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1909]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1910]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1911]     0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.75
## q[1912]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1913]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1914]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1915]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1916]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1917]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1918]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1919]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1920]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1921]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1922]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1923]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1924]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1925]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1926]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1927]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1928]     0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.75
## q[1929]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1930]     0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.75
## q[1931]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1932]     0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.75
## q[1933]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1934]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1935]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1936]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1937]     0.68    0.00 0.02     0.64     0.67     0.68     0.70     0.73
## q[1938]     0.65    0.00 0.03     0.60     0.64     0.65     0.68     0.71
## q[1939]     0.65    0.00 0.03     0.60     0.64     0.65     0.68     0.71
## q[1940]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1941]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1942]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1943]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1944]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1945]     0.68    0.00 0.02     0.64     0.67     0.68     0.70     0.73
## q[1946]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1947]     0.68    0.00 0.02     0.64     0.67     0.68     0.70     0.73
## q[1948]     0.68    0.00 0.02     0.64     0.67     0.68     0.70     0.73
## q[1949]     0.68    0.00 0.02     0.64     0.67     0.68     0.70     0.73
## q[1950]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1951]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1952]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1953]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1954]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1955]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1956]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1957]     0.65    0.00 0.03     0.60     0.64     0.65     0.68     0.71
## q[1958]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1959]     0.68    0.00 0.02     0.63     0.66     0.68     0.70     0.72
## q[1960]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1961]     0.65    0.00 0.03     0.59     0.63     0.65     0.67     0.71
## q[1962]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1963]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1964]     0.68    0.00 0.02     0.63     0.66     0.68     0.70     0.72
## q[1965]     0.65    0.00 0.03     0.59     0.63     0.65     0.67     0.71
## q[1966]     0.68    0.00 0.02     0.63     0.66     0.68     0.70     0.72
## q[1967]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1968]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1969]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1970]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1971]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1972]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1973]     0.68    0.00 0.02     0.63     0.66     0.68     0.70     0.72
## q[1974]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1975]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1976]     0.68    0.00 0.02     0.63     0.66     0.68     0.70     0.72
## q[1977]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1978]     0.68    0.00 0.02     0.63     0.66     0.68     0.70     0.72
## q[1979]     0.65    0.00 0.03     0.59     0.63     0.65     0.67     0.71
## q[1980]     0.68    0.00 0.02     0.63     0.66     0.68     0.70     0.72
## q[1981]     0.68    0.00 0.02     0.63     0.66     0.68     0.70     0.72
## q[1982]     0.68    0.00 0.02     0.63     0.66     0.68     0.70     0.72
## q[1983]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1984]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1985]     0.65    0.00 0.03     0.59     0.63     0.65     0.67     0.71
## q[1986]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1987]     0.65    0.00 0.03     0.59     0.63     0.65     0.67     0.71
## q[1988]     0.68    0.00 0.02     0.63     0.66     0.68     0.70     0.72
## q[1989]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1990]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1991]     0.68    0.00 0.02     0.63     0.66     0.68     0.70     0.72
## q[1992]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1993]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1994]     0.65    0.00 0.03     0.59     0.63     0.65     0.67     0.71
## q[1995]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1996]     0.68    0.00 0.02     0.63     0.66     0.68     0.70     0.72
## q[1997]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1998]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1999]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[2000]     0.76    0.00 0.02     0.73     0.75     0.76     0.77     0.79
## q[2001]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2002]     0.74    0.00 0.02     0.69     0.72     0.74     0.75     0.78
## q[2003]     0.76    0.00 0.02     0.73     0.75     0.76     0.77     0.79
## q[2004]     0.76    0.00 0.02     0.73     0.75     0.76     0.77     0.79
## q[2005]     0.74    0.00 0.02     0.69     0.72     0.74     0.75     0.78
## q[2006]     0.76    0.00 0.02     0.73     0.75     0.76     0.77     0.79
## q[2007]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2008]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2009]     0.74    0.00 0.02     0.69     0.72     0.74     0.75     0.78
## q[2010]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2011]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2012]     0.76    0.00 0.02     0.73     0.75     0.76     0.77     0.79
## q[2013]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2014]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2015]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2016]     0.76    0.00 0.02     0.73     0.75     0.76     0.77     0.79
## q[2017]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2018]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2019]     0.76    0.00 0.02     0.73     0.75     0.76     0.77     0.79
## q[2020]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2021]     0.76    0.00 0.02     0.73     0.75     0.76     0.77     0.79
## q[2022]     0.76    0.00 0.02     0.73     0.75     0.76     0.77     0.79
## q[2023]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2024]     0.76    0.00 0.02     0.73     0.75     0.76     0.77     0.79
## q[2025]     0.74    0.00 0.02     0.69     0.72     0.74     0.75     0.78
## q[2026]     0.76    0.00 0.02     0.73     0.75     0.76     0.77     0.79
## q[2027]     0.76    0.00 0.02     0.73     0.75     0.76     0.77     0.79
## q[2028]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2029]     0.76    0.00 0.02     0.73     0.75     0.76     0.77     0.79
## q[2030]     0.74    0.00 0.02     0.69     0.72     0.74     0.75     0.78
## q[2031]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2032]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2033]     0.74    0.00 0.02     0.69     0.72     0.74     0.75     0.78
## q[2034]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2035]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2036]     0.74    0.00 0.02     0.69     0.72     0.74     0.75     0.78
## q[2037]     0.76    0.00 0.02     0.73     0.75     0.76     0.77     0.79
## q[2038]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2039]     0.76    0.00 0.02     0.73     0.75     0.76     0.77     0.79
## q[2040]     0.76    0.00 0.02     0.73     0.75     0.76     0.77     0.79
## q[2041]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2042]     0.74    0.00 0.02     0.69     0.72     0.74     0.75     0.78
## q[2043]     0.76    0.00 0.02     0.73     0.75     0.76     0.77     0.79
## q[2044]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2045]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2046]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2047]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2048]     0.76    0.00 0.02     0.73     0.75     0.76     0.77     0.79
## q[2049]     0.76    0.00 0.02     0.73     0.75     0.76     0.77     0.79
## q[2050]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2051]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2052]     0.76    0.00 0.02     0.73     0.75     0.76     0.77     0.79
## q[2053]     0.74    0.00 0.02     0.69     0.72     0.74     0.75     0.78
## q[2054]     0.76    0.00 0.02     0.73     0.75     0.76     0.77     0.79
## q[2055]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2056]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2057]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2058]     0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[2059]     0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[2060]     0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[2061]     0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[2062]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2063]     0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[2064]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2065]     0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[2066]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2067]     0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[2068]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2069]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2070]     0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[2071]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2072]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2073]     0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[2074]     0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[2075]     0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[2076]     0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[2077]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2078]     0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[2079]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2080]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2081]     0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[2082]     0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[2083]     0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[2084]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2085]     0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[2086]     0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[2087]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2088]     0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[2089]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2090]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2091]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2092]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2093]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2094]     0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.77
## q[2095]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2096]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2097]     0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.77
## q[2098]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2099]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2100]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2101]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2102]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2103]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2104]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2105]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2106]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2107]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2108]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2109]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2110]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2111]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2112]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2113]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2114]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2115]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2116]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2117]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2118]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2119]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2120]     0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.77
## q[2121]     0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.77
## q[2122]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2123]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2124]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2125]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2126]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2127]     0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.77
## q[2128]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2129]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2130]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2131]     0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.77
## q[2132]     0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.77
## q[2133]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2134]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2135]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2136]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2137]     0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.77
## q[2138]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2139]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2140]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2141]     0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.77
## q[2142]     0.79    0.00 0.02     0.76     0.78     0.79     0.80     0.83
## q[2143]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2144]     0.77    0.00 0.02     0.73     0.76     0.77     0.79     0.81
## q[2145]     0.77    0.00 0.02     0.73     0.76     0.77     0.79     0.81
## q[2146]     0.79    0.00 0.02     0.76     0.78     0.79     0.80     0.83
## q[2147]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2148]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2149]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2150]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2151]     0.79    0.00 0.02     0.76     0.78     0.79     0.80     0.83
## q[2152]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2153]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2154]     0.79    0.00 0.02     0.76     0.78     0.79     0.80     0.83
## q[2155]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2156]     0.79    0.00 0.02     0.76     0.78     0.79     0.80     0.83
## q[2157]     0.77    0.00 0.02     0.73     0.76     0.77     0.79     0.81
## q[2158]     0.79    0.00 0.02     0.76     0.78     0.79     0.80     0.83
## q[2159]     0.79    0.00 0.02     0.76     0.78     0.79     0.80     0.83
## q[2160]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2161]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2162]     0.77    0.00 0.02     0.73     0.76     0.77     0.79     0.81
## q[2163]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2164]     0.77    0.00 0.02     0.73     0.76     0.77     0.79     0.81
## q[2165]     0.79    0.00 0.02     0.76     0.78     0.79     0.80     0.83
## q[2166]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2167]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2168]     0.79    0.00 0.02     0.76     0.78     0.79     0.80     0.83
## q[2169]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2170]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2171]     0.79    0.00 0.02     0.76     0.78     0.79     0.80     0.83
## q[2172]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2173]     0.89    0.00 0.01     0.85     0.88     0.89     0.90     0.91
## q[2174]     0.89    0.00 0.01     0.85     0.88     0.89     0.90     0.91
## q[2175]     0.84    0.00 0.02     0.80     0.83     0.84     0.86     0.88
## q[2176]     0.82    0.00 0.02     0.77     0.81     0.83     0.84     0.87
## q[2177]     0.89    0.00 0.01     0.85     0.88     0.89     0.90     0.91
## q[2178]     0.89    0.00 0.01     0.85     0.88     0.89     0.90     0.91
## q[2179]     0.89    0.00 0.01     0.85     0.88     0.89     0.90     0.91
## q[2180]     0.89    0.00 0.01     0.85     0.88     0.89     0.90     0.91
## q[2181]     0.82    0.00 0.02     0.77     0.81     0.83     0.84     0.87
## q[2182]     0.84    0.00 0.02     0.80     0.83     0.84     0.86     0.88
## q[2183]     0.84    0.00 0.02     0.80     0.83     0.84     0.86     0.88
## q[2184]     0.84    0.00 0.02     0.80     0.83     0.84     0.86     0.88
## q[2185]     0.89    0.00 0.01     0.85     0.88     0.89     0.90     0.91
## q[2186]     0.84    0.00 0.02     0.80     0.83     0.84     0.86     0.88
## q[2187]     0.89    0.00 0.01     0.85     0.88     0.89     0.90     0.91
## q[2188]     0.89    0.00 0.01     0.85     0.88     0.89     0.90     0.91
## q[2189]     0.89    0.00 0.01     0.85     0.88     0.89     0.90     0.91
## q[2190]     0.89    0.00 0.01     0.85     0.88     0.89     0.90     0.91
## q[2191]     0.89    0.00 0.01     0.85     0.88     0.89     0.90     0.91
## q[2192]     0.89    0.00 0.01     0.85     0.88     0.89     0.90     0.91
## q[2193]     0.82    0.00 0.02     0.77     0.81     0.83     0.84     0.87
## q[2194]     0.89    0.00 0.01     0.85     0.88     0.89     0.90     0.91
## q[2195]     0.89    0.00 0.01     0.85     0.88     0.89     0.90     0.91
## q[2196]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2197]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2198]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2199]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2200]     0.71    0.00 0.02     0.68     0.70     0.71     0.73     0.75
## q[2201]     0.69    0.00 0.03     0.64     0.67     0.69     0.71     0.74
## q[2202]     0.71    0.00 0.02     0.68     0.70     0.71     0.73     0.75
## q[2203]     0.69    0.00 0.03     0.64     0.67     0.69     0.71     0.74
## q[2204]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2205]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2206]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2207]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2208]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2209]     0.71    0.00 0.02     0.68     0.70     0.71     0.73     0.75
## q[2210]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2211]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2212]     0.71    0.00 0.02     0.68     0.70     0.71     0.73     0.75
## q[2213]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2214]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2215]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2216]     0.69    0.00 0.03     0.64     0.67     0.69     0.71     0.74
## q[2217]     0.71    0.00 0.02     0.68     0.70     0.71     0.73     0.75
## q[2218]     0.71    0.00 0.02     0.68     0.70     0.71     0.73     0.75
## q[2219]     0.71    0.00 0.02     0.68     0.70     0.71     0.73     0.75
## q[2220]     0.71    0.00 0.02     0.68     0.70     0.71     0.73     0.75
## q[2221]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2222]     0.69    0.00 0.03     0.64     0.67     0.69     0.71     0.74
## q[2223]     0.69    0.00 0.03     0.64     0.67     0.69     0.71     0.74
## q[2224]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2225]     0.69    0.00 0.03     0.64     0.67     0.69     0.71     0.74
## q[2226]     0.69    0.00 0.03     0.64     0.67     0.69     0.71     0.74
## q[2227]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2228]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2229]     0.71    0.00 0.02     0.68     0.70     0.71     0.73     0.75
## q[2230]     0.71    0.00 0.02     0.68     0.70     0.71     0.73     0.75
## q[2231]     0.69    0.00 0.03     0.64     0.67     0.69     0.71     0.74
## q[2232]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2233]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2234]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2235]     0.69    0.00 0.03     0.64     0.67     0.69     0.71     0.74
## q[2236]     0.69    0.00 0.03     0.64     0.67     0.69     0.71     0.74
## q[2237]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2238]     0.69    0.00 0.03     0.64     0.67     0.69     0.71     0.74
## q[2239]     0.71    0.00 0.02     0.68     0.70     0.71     0.73     0.75
## q[2240]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2241]     0.71    0.00 0.02     0.68     0.70     0.71     0.73     0.75
## q[2242]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2243]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2244]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2245]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2246]     0.69    0.00 0.03     0.64     0.67     0.69     0.71     0.74
## q[2247]     0.57    0.00 0.04     0.48     0.54     0.57     0.60     0.65
## q[2248]     0.60    0.00 0.04     0.53     0.57     0.60     0.62     0.67
## q[2249]     0.60    0.00 0.04     0.53     0.57     0.60     0.62     0.67
## q[2250]     0.60    0.00 0.04     0.53     0.57     0.60     0.62     0.67
## q[2251]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[2252]     0.60    0.00 0.04     0.53     0.57     0.60     0.62     0.67
## q[2253]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[2254]     0.60    0.00 0.04     0.53     0.57     0.60     0.62     0.67
## q[2255]     0.60    0.00 0.04     0.53     0.57     0.60     0.62     0.67
## q[2256]     0.60    0.00 0.04     0.53     0.57     0.60     0.62     0.67
## q[2257]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[2258]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[2259]     0.60    0.00 0.04     0.53     0.57     0.60     0.62     0.67
## q[2260]     0.60    0.00 0.04     0.53     0.57     0.60     0.62     0.67
## q[2261]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[2262]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[2263]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[2264]     0.60    0.00 0.04     0.53     0.57     0.60     0.62     0.67
## q[2265]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[2266]     0.57    0.00 0.04     0.48     0.54     0.57     0.60     0.65
## q[2267]     0.60    0.00 0.04     0.53     0.57     0.60     0.62     0.67
## q[2268]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[2269]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2270]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2271]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2272]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2273]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2274]     0.78    0.00 0.02     0.73     0.76     0.78     0.79     0.82
## q[2275]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2276]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2277]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2278]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2279]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2280]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2281]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2282]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2283]     0.78    0.00 0.02     0.73     0.76     0.78     0.79     0.82
## q[2284]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2285]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2286]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2287]     0.78    0.00 0.02     0.73     0.76     0.78     0.79     0.82
## q[2288]     0.78    0.00 0.02     0.73     0.76     0.78     0.79     0.82
## q[2289]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2290]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2291]     0.78    0.00 0.02     0.73     0.76     0.78     0.79     0.82
## q[2292]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2293]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2294]     0.78    0.00 0.02     0.73     0.76     0.78     0.79     0.82
## q[2295]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2296]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2297]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2298]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2299]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2300]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2301]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2302]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2303]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2304]     0.78    0.00 0.02     0.73     0.76     0.78     0.79     0.82
## q[2305]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2306]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.88
## q[2307]     0.70    0.00 0.02     0.66     0.69     0.70     0.72     0.74
## q[2308]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.73
## q[2309]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.80
## q[2310]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.80
## q[2311]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.73
## q[2312]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.80
## q[2313]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.80
## q[2314]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.80
## q[2315]     0.70    0.00 0.02     0.66     0.69     0.70     0.72     0.74
## q[2316]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.73
## q[2317]     0.70    0.00 0.02     0.66     0.69     0.70     0.72     0.74
## q[2318]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.80
## q[2319]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.80
## q[2320]     0.70    0.00 0.02     0.66     0.69     0.70     0.72     0.74
## q[2321]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.80
## q[2322]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.73
## q[2323]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.80
## q[2324]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.80
## q[2325]     0.70    0.00 0.02     0.66     0.69     0.70     0.72     0.74
## q[2326]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.80
## q[2327]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.73
## q[2328]     0.70    0.00 0.02     0.66     0.69     0.70     0.72     0.74
## q[2329]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.80
## q[2330]     0.56    0.00 0.03     0.51     0.54     0.56     0.57     0.61
## q[2331]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2332]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2333]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2334]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2335]     0.56    0.00 0.03     0.51     0.54     0.56     0.57     0.61
## q[2336]     0.56    0.00 0.03     0.51     0.54     0.56     0.57     0.61
## q[2337]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[2338]     0.56    0.00 0.03     0.51     0.54     0.56     0.57     0.61
## q[2339]     0.56    0.00 0.03     0.51     0.54     0.56     0.57     0.61
## q[2340]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[2341]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2342]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2343]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2344]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2345]     0.56    0.00 0.03     0.51     0.54     0.56     0.57     0.61
## q[2346]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[2347]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2348]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2349]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2350]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2351]     0.56    0.00 0.03     0.51     0.54     0.56     0.57     0.61
## q[2352]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2353]     0.56    0.00 0.03     0.51     0.54     0.56     0.57     0.61
## q[2354]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2355]     0.56    0.00 0.03     0.51     0.54     0.56     0.57     0.61
## q[2356]     0.56    0.00 0.03     0.51     0.54     0.56     0.57     0.61
## q[2357]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2358]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2359]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2360]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2361]     0.56    0.00 0.03     0.51     0.54     0.56     0.57     0.61
## q[2362]     0.56    0.00 0.03     0.51     0.54     0.56     0.57     0.61
## q[2363]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2364]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[2365]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2366]     0.56    0.00 0.03     0.51     0.54     0.56     0.57     0.61
## q[2367]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2368]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2369]     0.56    0.00 0.03     0.51     0.54     0.56     0.57     0.61
## q[2370]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[2371]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[2372]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2373]     0.56    0.00 0.03     0.51     0.54     0.56     0.57     0.61
## q[2374]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2375]     0.56    0.00 0.03     0.51     0.54     0.56     0.57     0.61
## q[2376]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2377]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[2378]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2379]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2380]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2381]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2382]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2383]     0.56    0.00 0.03     0.51     0.54     0.56     0.57     0.61
## q[2384]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[2385]     0.56    0.00 0.03     0.51     0.54     0.56     0.57     0.61
## q[2386]     0.56    0.00 0.03     0.51     0.54     0.56     0.57     0.61
## q[2387]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2388]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2389]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2390]     0.56    0.00 0.03     0.51     0.54     0.56     0.57     0.61
## q[2391]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[2392]     0.56    0.00 0.03     0.51     0.54     0.56     0.57     0.61
## q[2393]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2394]     0.65    0.00 0.02     0.61     0.63     0.65     0.66     0.69
## q[2395]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[2396]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## lp__    -1379.61    0.04 1.55 -1383.60 -1380.40 -1379.31 -1378.45 -1377.57
##         n_eff Rhat
## b[1]     1975    1
## b[2]     3064    1
## b[3]     2053    1
## b[4]     2756    1
## b[5]     2948    1
## OR1      3081    1
## OR2      2053    1
## OR3      2713    1
## OR4      2979    1
## q[1]     2488    1
## q[2]     2122    1
## q[3]     2522    1
## q[4]     2122    1
## q[5]     2488    1
## q[6]     2488    1
## q[7]     2522    1
## q[8]     2488    1
## q[9]     2122    1
## q[10]    2122    1
## q[11]    2122    1
## q[12]    2522    1
## q[13]    2122    1
## q[14]    2122    1
## q[15]    2122    1
## q[16]    2122    1
## q[17]    2122    1
## q[18]    2488    1
## q[19]    2122    1
## q[20]    2122    1
## q[21]    2488    1
## q[22]    2488    1
## q[23]    2522    1
## q[24]    2522    1
## q[25]    2122    1
## q[26]    2122    1
## q[27]    2488    1
## q[28]    2122    1
## q[29]    2488    1
## q[30]    2122    1
## q[31]    2488    1
## q[32]    2122    1
## q[33]    2488    1
## q[34]    2122    1
## q[35]    2522    1
## q[36]    2122    1
## q[37]    2122    1
## q[38]    2122    1
## q[39]    2488    1
## q[40]    2122    1
## q[41]    2122    1
## q[42]    2488    1
## q[43]    2488    1
## q[44]    2961    1
## q[45]    2961    1
## q[46]    2961    1
## q[47]    3069    1
## q[48]    2961    1
## q[49]    2653    1
## q[50]    2653    1
## q[51]    2653    1
## q[52]    2653    1
## q[53]    2961    1
## q[54]    2961    1
## q[55]    2961    1
## q[56]    2961    1
## q[57]    2961    1
## q[58]    3069    1
## q[59]    2961    1
## q[60]    3069    1
## q[61]    2961    1
## q[62]    2961    1
## q[63]    2961    1
## q[64]    2961    1
## q[65]    2653    1
## q[66]    2961    1
## q[67]    2653    1
## q[68]    3069    1
## q[69]    2653    1
## q[70]    3069    1
## q[71]    3069    1
## q[72]    2961    1
## q[73]    2653    1
## q[74]    2961    1
## q[75]    2653    1
## q[76]    3069    1
## q[77]    2961    1
## q[78]    2961    1
## q[79]    3069    1
## q[80]    2653    1
## q[81]    2653    1
## q[82]    3069    1
## q[83]    2653    1
## q[84]    2961    1
## q[85]    2961    1
## q[86]    2961    1
## q[87]    2961    1
## q[88]    2653    1
## q[89]    2961    1
## q[90]    2961    1
## q[91]    2653    1
## q[92]    2961    1
## q[93]    2961    1
## q[94]    2653    1
## q[95]    2653    1
## q[96]    2653    1
## q[97]    2961    1
## q[98]    2961    1
## q[99]    3069    1
## q[100]   3197    1
## q[101]   3197    1
## q[102]   3594    1
## q[103]   3256    1
## q[104]   3256    1
## q[105]   3197    1
## q[106]   3197    1
## q[107]   3197    1
## q[108]   3594    1
## q[109]   3197    1
## q[110]   3197    1
## q[111]   3197    1
## q[112]   3256    1
## q[113]   3256    1
## q[114]   3594    1
## q[115]   3256    1
## q[116]   3197    1
## q[117]   3594    1
## q[118]   3197    1
## q[119]   3594    1
## q[120]   3256    1
## q[121]   3256    1
## q[122]   3197    1
## q[123]   3197    1
## q[124]   3197    1
## q[125]   3256    1
## q[126]   3197    1
## q[127]   3256    1
## q[128]   3256    1
## q[129]   3197    1
## q[130]   3197    1
## q[131]   3594    1
## q[132]   2570    1
## q[133]   2570    1
## q[134]   2570    1
## q[135]   2974    1
## q[136]   2974    1
## q[137]   2772    1
## q[138]   2570    1
## q[139]   2570    1
## q[140]   2772    1
## q[141]   2570    1
## q[142]   2570    1
## q[143]   2570    1
## q[144]   2570    1
## q[145]   2570    1
## q[146]   2974    1
## q[147]   2570    1
## q[148]   2570    1
## q[149]   2772    1
## q[150]   2974    1
## q[151]   2974    1
## q[152]   2974    1
## q[153]   2772    1
## q[154]   2974    1
## q[155]   2772    1
## q[156]   2570    1
## q[157]   2570    1
## q[158]   2772    1
## q[159]   2570    1
## q[160]   2974    1
## q[161]   2570    1
## q[162]   2772    1
## q[163]   2974    1
## q[164]   2570    1
## q[165]   2570    1
## q[166]   2974    1
## q[167]   2570    1
## q[168]   2570    1
## q[169]   2772    1
## q[170]   2570    1
## q[171]   2772    1
## q[172]   2974    1
## q[173]   2570    1
## q[174]   2570    1
## q[175]   2570    1
## q[176]   2570    1
## q[177]   2879    1
## q[178]   2383    1
## q[179]   2383    1
## q[180]   2879    1
## q[181]   2383    1
## q[182]   2708    1
## q[183]   2708    1
## q[184]   2383    1
## q[185]   2708    1
## q[186]   2708    1
## q[187]   2383    1
## q[188]   2383    1
## q[189]   2383    1
## q[190]   2708    1
## q[191]   2383    1
## q[192]   2708    1
## q[193]   2383    1
## q[194]   2383    1
## q[195]   2708    1
## q[196]   2708    1
## q[197]   2383    1
## q[198]   2708    1
## q[199]   2708    1
## q[200]   2708    1
## q[201]   2708    1
## q[202]   2708    1
## q[203]   2383    1
## q[204]   2708    1
## q[205]   2879    1
## q[206]   2879    1
## q[207]   2708    1
## q[208]   2383    1
## q[209]   2708    1
## q[210]   3235    1
## q[211]   3235    1
## q[212]   3235    1
## q[213]   3570    1
## q[214]   3235    1
## q[215]   3198    1
## q[216]   3198    1
## q[217]   3570    1
## q[218]   3198    1
## q[219]   3198    1
## q[220]   3570    1
## q[221]   3235    1
## q[222]   3235    1
## q[223]   3235    1
## q[224]   3570    1
## q[225]   3235    1
## q[226]   3235    1
## q[227]   3235    1
## q[228]   3235    1
## q[229]   3235    1
## q[230]   3198    1
## q[231]   3235    1
## q[232]   3235    1
## q[233]   3235    1
## q[234]   3198    1
## q[235]   3235    1
## q[236]   3570    1
## q[237]   3198    1
## q[238]   3235    1
## q[239]   3198    1
## q[240]   3198    1
## q[241]   3235    1
## q[242]   3235    1
## q[243]   3198    1
## q[244]   3570    1
## q[245]   3198    1
## q[246]   3570    1
## q[247]   3235    1
## q[248]   3235    1
## q[249]   3198    1
## q[250]   3570    1
## q[251]   3235    1
## q[252]   3235    1
## q[253]   3235    1
## q[254]   3235    1
## q[255]   3235    1
## q[256]   3235    1
## q[257]   3570    1
## q[258]   3570    1
## q[259]   3198    1
## q[260]   3198    1
## q[261]   3235    1
## q[262]   3235    1
## q[263]   3235    1
## q[264]   3198    1
## q[265]   3198    1
## q[266]   3235    1
## q[267]   3198    1
## q[268]   3235    1
## q[269]   3570    1
## q[270]   3570    1
## q[271]   2886    1
## q[272]   3190    1
## q[273]   3190    1
## q[274]   3444    1
## q[275]   2886    1
## q[276]   2886    1
## q[277]   3444    1
## q[278]   2886    1
## q[279]   3190    1
## q[280]   2886    1
## q[281]   2886    1
## q[282]   3190    1
## q[283]   2886    1
## q[284]   3190    1
## q[285]   2886    1
## q[286]   2886    1
## q[287]   3190    1
## q[288]   3444    1
## q[289]   3190    1
## q[290]   2886    1
## q[291]   3190    1
## q[292]   3190    1
## q[293]   3444    1
## q[294]   3190    1
## q[295]   3444    1
## q[296]   2886    1
## q[297]   2886    1
## q[298]   3444    1
## q[299]   2886    1
## q[300]   3190    1
## q[301]   2886    1
## q[302]   3190    1
## q[303]   3444    1
## q[304]   3190    1
## q[305]   2886    1
## q[306]   2886    1
## q[307]   2886    1
## q[308]   2886    1
## q[309]   2886    1
## q[310]   3190    1
## q[311]   2886    1
## q[312]   3190    1
## q[313]   2886    1
## q[314]   3444    1
## q[315]   3190    1
## q[316]   3444    1
## q[317]   2886    1
## q[318]   2886    1
## q[319]   2886    1
## q[320]   2604    1
## q[321]   2905    1
## q[322]   2905    1
## q[323]   3042    1
## q[324]   2905    1
## q[325]   2905    1
## q[326]   2604    1
## q[327]   2604    1
## q[328]   2604    1
## q[329]   3042    1
## q[330]   2604    1
## q[331]   3042    1
## q[332]   2905    1
## q[333]   2905    1
## q[334]   2905    1
## q[335]   2905    1
## q[336]   3042    1
## q[337]   2905    1
## q[338]   2905    1
## q[339]   2905    1
## q[340]   2905    1
## q[341]   3042    1
## q[342]   2905    1
## q[343]   2905    1
## q[344]   2905    1
## q[345]   2905    1
## q[346]   2604    1
## q[347]   2905    1
## q[348]   2905    1
## q[349]   2905    1
## q[350]   2905    1
## q[351]   2604    1
## q[352]   2905    1
## q[353]   2604    1
## q[354]   3042    1
## q[355]   2604    1
## q[356]   2604    1
## q[357]   2604    1
## q[358]   2604    1
## q[359]   3042    1
## q[360]   2905    1
## q[361]   2604    1
## q[362]   2905    1
## q[363]   3042    1
## q[364]   2905    1
## q[365]   2905    1
## q[366]   3042    1
## q[367]   2604    1
## q[368]   2905    1
## q[369]   2905    1
## q[370]   2604    1
## q[371]   2905    1
## q[372]   2905    1
## q[373]   3042    1
## q[374]   2604    1
## q[375]   2905    1
## q[376]   2905    1
## q[377]   2905    1
## q[378]   2604    1
## q[379]   2905    1
## q[380]   2905    1
## q[381]   2905    1
## q[382]   3042    1
## q[383]   2905    1
## q[384]   3042    1
## q[385]   2604    1
## q[386]   2905    1
## q[387]   2604    1
## q[388]   2604    1
## q[389]   2905    1
## q[390]   2604    1
## q[391]   2905    1
## q[392]   2905    1
## q[393]   2905    1
## q[394]   3042    1
## q[395]   2905    1
## q[396]   2944    1
## q[397]   3106    1
## q[398]   3106    1
## q[399]   3398    1
## q[400]   3106    1
## q[401]   3106    1
## q[402]   3106    1
## q[403]   2944    1
## q[404]   2944    1
## q[405]   2944    1
## q[406]   2944    1
## q[407]   3398    1
## q[408]   2944    1
## q[409]   3398    1
## q[410]   3106    1
## q[411]   3106    1
## q[412]   3106    1
## q[413]   3106    1
## q[414]   3106    1
## q[415]   3106    1
## q[416]   2944    1
## q[417]   3398    1
## q[418]   3106    1
## q[419]   3106    1
## q[420]   3106    1
## q[421]   3106    1
## q[422]   3106    1
## q[423]   2944    1
## q[424]   3106    1
## q[425]   3106    1
## q[426]   2944    1
## q[427]   2944    1
## q[428]   3106    1
## q[429]   3106    1
## q[430]   2944    1
## q[431]   2944    1
## q[432]   2944    1
## q[433]   2944    1
## q[434]   3106    1
## q[435]   3398    1
## q[436]   2944    1
## q[437]   3106    1
## q[438]   2944    1
## q[439]   3106    1
## q[440]   2944    1
## q[441]   3106    1
## q[442]   2944    1
## q[443]   2944    1
## q[444]   3106    1
## q[445]   3106    1
## q[446]   3106    1
## q[447]   2944    1
## q[448]   3106    1
## q[449]   3106    1
## q[450]   3106    1
## q[451]   3106    1
## q[452]   3106    1
## q[453]   3106    1
## q[454]   3106    1
## q[455]   3106    1
## q[456]   3106    1
## q[457]   2944    1
## q[458]   3106    1
## q[459]   3398    1
## q[460]   3106    1
## q[461]   3398    1
## q[462]   2944    1
## q[463]   3106    1
## q[464]   3106    1
## q[465]   3398    1
## q[466]   2944    1
## q[467]   3106    1
## q[468]   2944    1
## q[469]   3106    1
## q[470]   3106    1
## q[471]   3106    1
## q[472]   3398    1
## q[473]   3106    1
## q[474]   2964    1
## q[475]   3121    1
## q[476]   3121    1
## q[477]   3121    1
## q[478]   2964    1
## q[479]   2964    1
## q[480]   3415    1
## q[481]   3415    1
## q[482]   3415    1
## q[483]   3121    1
## q[484]   3121    1
## q[485]   3121    1
## q[486]   3121    1
## q[487]   3415    1
## q[488]   3121    1
## q[489]   3121    1
## q[490]   3121    1
## q[491]   2964    1
## q[492]   3121    1
## q[493]   3121    1
## q[494]   3121    1
## q[495]   3121    1
## q[496]   2964    1
## q[497]   3121    1
## q[498]   2964    1
## q[499]   3121    1
## q[500]   3121    1
## q[501]   3121    1
## q[502]   3415    1
## q[503]   2964    1
## q[504]   2964    1
## q[505]   2964    1
## q[506]   2964    1
## q[507]   3121    1
## q[508]   3121    1
## q[509]   3415    1
## q[510]   3415    1
## q[511]   3121    1
## q[512]   3415    1
## q[513]   3415    1
## q[514]   3121    1
## q[515]   3121    1
## q[516]   3121    1
## q[517]   3121    1
## q[518]   2964    1
## q[519]   2964    1
## q[520]   3415    1
## q[521]   3121    1
## q[522]   3121    1
## q[523]   3121    1
## q[524]   3121    1
## q[525]   3415    1
## q[526]   3415    1
## q[527]   2964    1
## q[528]   2964    1
## q[529]   3121    1
## q[530]   3121    1
## q[531]   2964    1
## q[532]   2964    1
## q[533]   3121    1
## q[534]   3121    1
## q[535]   3121    1
## q[536]   3415    1
## q[537]   3415    1
## q[538]   3076    1
## q[539]   3076    1
## q[540]   3076    1
## q[541]   2757    1
## q[542]   2757    1
## q[543]   3076    1
## q[544]   3076    1
## q[545]   3118    1
## q[546]   3076    1
## q[547]   3076    1
## q[548]   3076    1
## q[549]   3076    1
## q[550]   3076    1
## q[551]   2757    1
## q[552]   3118    1
## q[553]   2757    1
## q[554]   2757    1
## q[555]   3118    1
## q[556]   3118    1
## q[557]   3076    1
## q[558]   3076    1
## q[559]   3118    1
## q[560]   2757    1
## q[561]   3118    1
## q[562]   2757    1
## q[563]   3076    1
## q[564]   2757    1
## q[565]   3076    1
## q[566]   3118    1
## q[567]   3076    1
## q[568]   2757    1
## q[569]   3076    1
## q[570]   3076    1
## q[571]   3076    1
## q[572]   3214    1
## q[573]   3214    1
## q[574]   3124    1
## q[575]   3124    1
## q[576]   3531    1
## q[577]   3124    1
## q[578]   3214    1
## q[579]   3214    1
## q[580]   3124    1
## q[581]   3531    1
## q[582]   3214    1
## q[583]   3214    1
## q[584]   3124    1
## q[585]   3214    1
## q[586]   3124    1
## q[587]   3214    1
## q[588]   3124    1
## q[589]   3214    1
## q[590]   3214    1
## q[591]   3124    1
## q[592]   3214    1
## q[593]   3214    1
## q[594]   3531    1
## q[595]   3531    1
## q[596]   3124    1
## q[597]   3124    1
## q[598]   3214    1
## q[599]   3531    1
## q[600]   3214    1
## q[601]   3214    1
## q[602]   3214    1
## q[603]   3124    1
## q[604]   3531    1
## q[605]   3124    1
## q[606]   3124    1
## q[607]   3214    1
## q[608]   3214    1
## q[609]   3124    1
## q[610]   3531    1
## q[611]   3124    1
## q[612]   3214    1
## q[613]   3214    1
## q[614]   3531    1
## q[615]   3531    1
## q[616]   3323    1
## q[617]   3240    1
## q[618]   3240    1
## q[619]   3323    1
## q[620]   3323    1
## q[621]   3157    1
## q[622]   3323    1
## q[623]   3157    1
## q[624]   3240    1
## q[625]   3240    1
## q[626]   3240    1
## q[627]   3240    1
## q[628]   3157    1
## q[629]   3240    1
## q[630]   3157    1
## q[631]   3240    1
## q[632]   3240    1
## q[633]   3240    1
## q[634]   3240    1
## q[635]   3240    1
## q[636]   3240    1
## q[637]   3323    1
## q[638]   3323    1
## q[639]   3240    1
## q[640]   3240    1
## q[641]   3240    1
## q[642]   3323    1
## q[643]   3157    1
## q[644]   3240    1
## q[645]   3240    1
## q[646]   3157    1
## q[647]   3240    1
## q[648]   3157    1
## q[649]   3240    1
## q[650]   3323    1
## q[651]   3240    1
## q[652]   3157    1
## q[653]   3323    1
## q[654]   3157    1
## q[655]   3240    1
## q[656]   3240    1
## q[657]   3323    1
## q[658]   3157    1
## q[659]   3323    1
## q[660]   3323    1
## q[661]   3240    1
## q[662]   3240    1
## q[663]   3323    1
## q[664]   3323    1
## q[665]   3323    1
## q[666]   3240    1
## q[667]   3157    1
## q[668]   3157    1
## q[669]   3190    1
## q[670]   2886    1
## q[671]   3190    1
## q[672]   3444    1
## q[673]   3444    1
## q[674]   3444    1
## q[675]   2886    1
## q[676]   2886    1
## q[677]   2886    1
## q[678]   2886    1
## q[679]   2886    1
## q[680]   2886    1
## q[681]   3190    1
## q[682]   2886    1
## q[683]   3190    1
## q[684]   2886    1
## q[685]   2886    1
## q[686]   2886    1
## q[687]   3190    1
## q[688]   3190    1
## q[689]   3190    1
## q[690]   2886    1
## q[691]   2886    1
## q[692]   3444    1
## q[693]   3444    1
## q[694]   2886    1
## q[695]   2886    1
## q[696]   2886    1
## q[697]   2886    1
## q[698]   3190    1
## q[699]   3444    1
## q[700]   2886    1
## q[701]   2886    1
## q[702]   3444    1
## q[703]   3444    1
## q[704]   3190    1
## q[705]   2886    1
## q[706]   3190    1
## q[707]   2886    1
## q[708]   3444    1
## q[709]   2829    1
## q[710]   2829    1
## q[711]   2829    1
## q[712]   2829    1
## q[713]   3166    1
## q[714]   3166    1
## q[715]   3407    1
## q[716]   3166    1
## q[717]   3166    1
## q[718]   2829    1
## q[719]   2829    1
## q[720]   2829    1
## q[721]   3166    1
## q[722]   3407    1
## q[723]   2829    1
## q[724]   2829    1
## q[725]   3166    1
## q[726]   2829    1
## q[727]   3166    1
## q[728]   2829    1
## q[729]   3166    1
## q[730]   2829    1
## q[731]   2829    1
## q[732]   2829    1
## q[733]   3166    1
## q[734]   2829    1
## q[735]   3407    1
## q[736]   3166    1
## q[737]   2829    1
## q[738]   2829    1
## q[739]   3166    1
## q[740]   3407    1
## q[741]   3407    1
## q[742]   3166    1
## q[743]   3166    1
## q[744]   2829    1
## q[745]   3407    1
## q[746]   2829    1
## q[747]   2829    1
## q[748]   2829    1
## q[749]   2829    1
## q[750]   2829    1
## q[751]   3166    1
## q[752]   3407    1
## q[753]   3166    1
## q[754]   3166    1
## q[755]   2829    1
## q[756]   2829    1
## q[757]   2829    1
## q[758]   3166    1
## q[759]   3407    1
## q[760]   3166    1
## q[761]   2829    1
## q[762]   2829    1
## q[763]   3407    1
## q[764]   3407    1
## q[765]   3219    1
## q[766]   2829    1
## q[767]   2829    1
## q[768]   2829    1
## q[769]   2829    1
## q[770]   3219    1
## q[771]   3219    1
## q[772]   3219    1
## q[773]   3219    1
## q[774]   2971    1
## q[775]   2829    1
## q[776]   2829    1
## q[777]   2829    1
## q[778]   2971    1
## q[779]   2829    1
## q[780]   2829    1
## q[781]   2829    1
## q[782]   3219    1
## q[783]   2829    1
## q[784]   2829    1
## q[785]   2829    1
## q[786]   3219    1
## q[787]   2829    1
## q[788]   3219    1
## q[789]   3219    1
## q[790]   2829    1
## q[791]   2829    1
## q[792]   3219    1
## q[793]   3219    1
## q[794]   3219    1
## q[795]   2829    1
## q[796]   2971    1
## q[797]   2971    1
## q[798]   2829    1
## q[799]   3219    1
## q[800]   2829    1
## q[801]   3219    1
## q[802]   2829    1
## q[803]   2829    1
## q[804]   3219    1
## q[805]   3219    1
## q[806]   2829    1
## q[807]   2971    1
## q[808]   2829    1
## q[809]   2829    1
## q[810]   2829    1
## q[811]   2829    1
## q[812]   2829    1
## q[813]   2829    1
## q[814]   3219    1
## q[815]   3219    1
## q[816]   2971    1
## q[817]   2829    1
## q[818]   2829    1
## q[819]   3219    1
## q[820]   2971    1
## q[821]   2829    1
## q[822]   2829    1
## q[823]   2829    1
## q[824]   2970    1
## q[825]   2970    1
## q[826]   2970    1
## q[827]   3221    1
## q[828]   3221    1
## q[829]   3221    1
## q[830]   2970    1
## q[831]   2970    1
## q[832]   2970    1
## q[833]   3221    1
## q[834]   3494    1
## q[835]   2970    1
## q[836]   3221    1
## q[837]   2970    1
## q[838]   3221    1
## q[839]   2970    1
## q[840]   3221    1
## q[841]   2970    1
## q[842]   3494    1
## q[843]   3221    1
## q[844]   2970    1
## q[845]   3221    1
## q[846]   3221    1
## q[847]   2970    1
## q[848]   2970    1
## q[849]   2970    1
## q[850]   2970    1
## q[851]   2970    1
## q[852]   2970    1
## q[853]   3221    1
## q[854]   3221    1
## q[855]   2970    1
## q[856]   2970    1
## q[857]   3494    1
## q[858]   3221    1
## q[859]   2970    1
## q[860]   2970    1
## q[861]   3494    1
## q[862]   2680    1
## q[863]   2246    1
## q[864]   2246    1
## q[865]   2246    1
## q[866]   2246    1
## q[867]   2246    1
## q[868]   2680    1
## q[869]   2680    1
## q[870]   2680    1
## q[871]   2740    1
## q[872]   2246    1
## q[873]   2246    1
## q[874]   2246    1
## q[875]   2246    1
## q[876]   2246    1
## q[877]   2246    1
## q[878]   2740    1
## q[879]   2246    1
## q[880]   2246    1
## q[881]   2246    1
## q[882]   2680    1
## q[883]   2246    1
## q[884]   2246    1
## q[885]   2680    1
## q[886]   2680    1
## q[887]   2740    1
## q[888]   2246    1
## q[889]   2680    1
## q[890]   2246    1
## q[891]   2680    1
## q[892]   2246    1
## q[893]   2246    1
## q[894]   2246    1
## q[895]   2246    1
## q[896]   2246    1
## q[897]   2740    1
## q[898]   2246    1
## q[899]   2680    1
## q[900]   2246    1
## q[901]   2246    1
## q[902]   2680    1
## q[903]   2246    1
## q[904]   2246    1
## q[905]   2740    1
## q[906]   2246    1
## q[907]   2582    1
## q[908]   2780    1
## q[909]   2417    1
## q[910]   2417    1
## q[911]   2780    1
## q[912]   2582    1
## q[913]   2417    1
## q[914]   2582    1
## q[915]   2582    1
## q[916]   2417    1
## q[917]   2582    1
## q[918]   2417    1
## q[919]   2582    1
## q[920]   2582    1
## q[921]   2417    1
## q[922]   2417    1
## q[923]   2780    1
## q[924]   2417    1
## q[925]   2582    1
## q[926]   2417    1
## q[927]   2417    1
## q[928]   2582    1
## q[929]   2417    1
## q[930]   2780    1
## q[931]   2582    1
## q[932]   2780    1
## q[933]   2417    1
## q[934]   2582    1
## q[935]   2582    1
## q[936]   2417    1
## q[937]   2417    1
## q[938]   2582    1
## q[939]   2582    1
## q[940]   2582    1
## q[941]   2582    1
## q[942]   2582    1
## q[943]   2417    1
## q[944]   2582    1
## q[945]   2582    1
## q[946]   2780    1
## q[947]   2780    1
## q[948]   2582    1
## q[949]   2417    1
## q[950]   2582    1
## q[951]   2582    1
## q[952]   2746    1
## q[953]   2746    1
## q[954]   2746    1
## q[955]   2746    1
## q[956]   2746    1
## q[957]   2416    1
## q[958]   2416    1
## q[959]   2909    1
## q[960]   2416    1
## q[961]   2416    1
## q[962]   2909    1
## q[963]   2746    1
## q[964]   2746    1
## q[965]   2746    1
## q[966]   2746    1
## q[967]   2746    1
## q[968]   2909    1
## q[969]   2746    1
## q[970]   2909    1
## q[971]   2746    1
## q[972]   2746    1
## q[973]   2746    1
## q[974]   2746    1
## q[975]   2416    1
## q[976]   2746    1
## q[977]   2746    1
## q[978]   2416    1
## q[979]   2746    1
## q[980]   2746    1
## q[981]   2746    1
## q[982]   2746    1
## q[983]   2909    1
## q[984]   2416    1
## q[985]   2909    1
## q[986]   2416    1
## q[987]   2909    1
## q[988]   2416    1
## q[989]   2416    1
## q[990]   2746    1
## q[991]   2746    1
## q[992]   2416    1
## q[993]   2909    1
## q[994]   2909    1
## q[995]   2746    1
## q[996]   2746    1
## q[997]   2909    1
## q[998]   2909    1
## q[999]   2746    1
## q[1000]  2416    1
## q[1001]  2746    1
## q[1002]  2909    1
## q[1003]  2416    1
## q[1004]  2416    1
## q[1005]  2909    1
## q[1006]  2746    1
## q[1007]  2746    1
## q[1008]  2746    1
## q[1009]  2746    1
## q[1010]  2746    1
## q[1011]  2416    1
## q[1012]  2746    1
## q[1013]  2746    1
## q[1014]  2909    1
## q[1015]  2909    1
## q[1016]  2416    1
## q[1017]  2416    1
## q[1018]  2746    1
## q[1019]  2746    1
## q[1020]  2746    1
## q[1021]  2416    1
## q[1022]  2416    1
## q[1023]  2416    1
## q[1024]  2746    1
## q[1025]  2746    1
## q[1026]  2746    1
## q[1027]  2909    1
## q[1028]  2909    1
## q[1029]  2974    1
## q[1030]  2570    1
## q[1031]  2570    1
## q[1032]  2570    1
## q[1033]  2974    1
## q[1034]  2974    1
## q[1035]  2974    1
## q[1036]  2772    1
## q[1037]  2772    1
## q[1038]  2570    1
## q[1039]  2570    1
## q[1040]  2772    1
## q[1041]  2570    1
## q[1042]  2570    1
## q[1043]  2570    1
## q[1044]  2570    1
## q[1045]  2974    1
## q[1046]  2570    1
## q[1047]  2570    1
## q[1048]  2974    1
## q[1049]  2570    1
## q[1050]  2570    1
## q[1051]  2570    1
## q[1052]  2974    1
## q[1053]  2570    1
## q[1054]  2570    1
## q[1055]  2974    1
## q[1056]  2974    1
## q[1057]  2570    1
## q[1058]  2772    1
## q[1059]  2974    1
## q[1060]  2974    1
## q[1061]  2974    1
## q[1062]  2570    1
## q[1063]  2974    1
## q[1064]  2974    1
## q[1065]  2772    1
## q[1066]  2974    1
## q[1067]  2570    1
## q[1068]  2772    1
## q[1069]  2570    1
## q[1070]  2570    1
## q[1071]  2772    1
## q[1072]  2570    1
## q[1073]  2570    1
## q[1074]  2974    1
## q[1075]  2570    1
## q[1076]  2974    1
## q[1077]  2570    1
## q[1078]  2772    1
## q[1079]  2974    1
## q[1080]  2570    1
## q[1081]  2570    1
## q[1082]  2570    1
## q[1083]  2570    1
## q[1084]  2570    1
## q[1085]  2974    1
## q[1086]  2570    1
## q[1087]  2974    1
## q[1088]  2570    1
## q[1089]  2772    1
## q[1090]  2570    1
## q[1091]  2772    1
## q[1092]  2974    1
## q[1093]  2772    1
## q[1094]  2570    1
## q[1095]  2570    1
## q[1096]  2570    1
## q[1097]  2570    1
## q[1098]  2570    1
## q[1099]  3043    1
## q[1100]  3043    1
## q[1101]  3043    1
## q[1102]  3043    1
## q[1103]  3043    1
## q[1104]  3352    1
## q[1105]  3084    1
## q[1106]  3352    1
## q[1107]  3352    1
## q[1108]  3352    1
## q[1109]  3352    1
## q[1110]  3084    1
## q[1111]  3043    1
## q[1112]  3043    1
## q[1113]  3043    1
## q[1114]  3084    1
## q[1115]  3043    1
## q[1116]  3043    1
## q[1117]  3352    1
## q[1118]  3043    1
## q[1119]  3043    1
## q[1120]  3043    1
## q[1121]  3352    1
## q[1122]  3043    1
## q[1123]  3352    1
## q[1124]  3043    1
## q[1125]  3043    1
## q[1126]  3352    1
## q[1127]  3043    1
## q[1128]  3043    1
## q[1129]  3043    1
## q[1130]  3352    1
## q[1131]  3352    1
## q[1132]  3043    1
## q[1133]  3084    1
## q[1134]  3352    1
## q[1135]  3084    1
## q[1136]  3352    1
## q[1137]  3352    1
## q[1138]  3043    1
## q[1139]  3043    1
## q[1140]  3352    1
## q[1141]  3084    1
## q[1142]  3043    1
## q[1143]  3084    1
## q[1144]  3043    1
## q[1145]  3352    1
## q[1146]  3043    1
## q[1147]  3352    1
## q[1148]  3084    1
## q[1149]  3352    1
## q[1150]  3043    1
## q[1151]  3084    1
## q[1152]  3043    1
## q[1153]  3043    1
## q[1154]  3043    1
## q[1155]  3043    1
## q[1156]  3043    1
## q[1157]  3043    1
## q[1158]  3352    1
## q[1159]  3043    1
## q[1160]  3352    1
## q[1161]  3084    1
## q[1162]  3084    1
## q[1163]  3043    1
## q[1164]  3084    1
## q[1165]  3352    1
## q[1166]  3043    1
## q[1167]  3043    1
## q[1168]  3352    1
## q[1169]  3352    1
## q[1170]  3084    1
## q[1171]  3043    1
## q[1172]  3043    1
## q[1173]  3043    1
## q[1174]  3043    1
## q[1175]  3084    1
## q[1176]  3334    1
## q[1177]  2995    1
## q[1178]  2995    1
## q[1179]  3068    1
## q[1180]  3334    1
## q[1181]  3068    1
## q[1182]  3334    1
## q[1183]  3068    1
## q[1184]  3334    1
## q[1185]  2995    1
## q[1186]  2995    1
## q[1187]  3334    1
## q[1188]  2995    1
## q[1189]  2995    1
## q[1190]  2995    1
## q[1191]  3334    1
## q[1192]  2995    1
## q[1193]  2995    1
## q[1194]  3334    1
## q[1195]  2995    1
## q[1196]  3334    1
## q[1197]  2995    1
## q[1198]  3334    1
## q[1199]  3334    1
## q[1200]  2995    1
## q[1201]  2995    1
## q[1202]  3334    1
## q[1203]  2995    1
## q[1204]  3068    1
## q[1205]  3334    1
## q[1206]  3334    1
## q[1207]  2995    1
## q[1208]  3334    1
## q[1209]  2995    1
## q[1210]  2995    1
## q[1211]  2995    1
## q[1212]  3068    1
## q[1213]  3068    1
## q[1214]  2995    1
## q[1215]  3068    1
## q[1216]  3334    1
## q[1217]  3068    1
## q[1218]  2995    1
## q[1219]  3334    1
## q[1220]  2995    1
## q[1221]  3334    1
## q[1222]  3334    1
## q[1223]  2995    1
## q[1224]  3068    1
## q[1225]  2995    1
## q[1226]  2995    1
## q[1227]  3334    1
## q[1228]  2995    1
## q[1229]  2995    1
## q[1230]  3068    1
## q[1231]  3334    1
## q[1232]  2995    1
## q[1233]  3334    1
## q[1234]  3068    1
## q[1235]  3334    1
## q[1236]  2995    1
## q[1237]  2995    1
## q[1238]  3068    1
## q[1239]  3019    1
## q[1240]  3019    1
## q[1241]  3019    1
## q[1242]  3094    1
## q[1243]  3019    1
## q[1244]  3019    1
## q[1245]  3019    1
## q[1246]  2704    1
## q[1247]  2704    1
## q[1248]  2704    1
## q[1249]  2704    1
## q[1250]  2704    1
## q[1251]  3019    1
## q[1252]  3019    1
## q[1253]  3019    1
## q[1254]  3019    1
## q[1255]  3019    1
## q[1256]  3019    1
## q[1257]  2704    1
## q[1258]  3094    1
## q[1259]  3019    1
## q[1260]  3019    1
## q[1261]  3019    1
## q[1262]  2704    1
## q[1263]  2704    1
## q[1264]  3019    1
## q[1265]  2704    1
## q[1266]  3019    1
## q[1267]  2704    1
## q[1268]  3094    1
## q[1269]  2704    1
## q[1270]  2704    1
## q[1271]  3019    1
## q[1272]  2704    1
## q[1273]  3094    1
## q[1274]  3019    1
## q[1275]  2704    1
## q[1276]  3019    1
## q[1277]  2704    1
## q[1278]  3094    1
## q[1279]  3019    1
## q[1280]  3094    1
## q[1281]  2704    1
## q[1282]  2704    1
## q[1283]  2704    1
## q[1284]  3019    1
## q[1285]  3019    1
## q[1286]  3019    1
## q[1287]  3019    1
## q[1288]  3019    1
## q[1289]  3019    1
## q[1290]  2704    1
## q[1291]  3019    1
## q[1292]  3019    1
## q[1293]  3094    1
## q[1294]  3019    1
## q[1295]  2704    1
## q[1296]  3019    1
## q[1297]  3019    1
## q[1298]  3094    1
## q[1299]  2704    1
## q[1300]  2704    1
## q[1301]  3019    1
## q[1302]  3019    1
## q[1303]  3019    1
## q[1304]  3019    1
## q[1305]  3094    1
## q[1306]  3019    1
## q[1307]  3225    1
## q[1308]  3225    1
## q[1309]  3225    1
## q[1310]  3335    1
## q[1311]  3335    1
## q[1312]  3335    1
## q[1313]  3335    1
## q[1314]  3225    1
## q[1315]  3225    1
## q[1316]  3225    1
## q[1317]  3225    1
## q[1318]  3225    1
## q[1319]  3148    1
## q[1320]  3335    1
## q[1321]  3148    1
## q[1322]  3225    1
## q[1323]  3225    1
## q[1324]  3335    1
## q[1325]  3225    1
## q[1326]  3335    1
## q[1327]  3225    1
## q[1328]  3335    1
## q[1329]  3225    1
## q[1330]  3148    1
## q[1331]  3148    1
## q[1332]  3335    1
## q[1333]  3225    1
## q[1334]  3335    1
## q[1335]  3225    1
## q[1336]  3335    1
## q[1337]  3335    1
## q[1338]  3148    1
## q[1339]  3225    1
## q[1340]  3225    1
## q[1341]  3225    1
## q[1342]  3225    1
## q[1343]  3225    1
## q[1344]  3225    1
## q[1345]  3225    1
## q[1346]  3335    1
## q[1347]  3335    1
## q[1348]  3335    1
## q[1349]  3225    1
## q[1350]  3225    1
## q[1351]  3335    1
## q[1352]  3148    1
## q[1353]  3335    1
## q[1354]  3225    1
## q[1355]  3225    1
## q[1356]  3148    1
## q[1357]  3278    1
## q[1358]  3278    1
## q[1359]  3278    1
## q[1360]  3278    1
## q[1361]  3278    1
## q[1362]  3121    1
## q[1363]  3200    1
## q[1364]  3121    1
## q[1365]  3121    1
## q[1366]  3200    1
## q[1367]  3278    1
## q[1368]  3278    1
## q[1369]  3278    1
## q[1370]  3278    1
## q[1371]  3278    1
## q[1372]  3278    1
## q[1373]  3121    1
## q[1374]  3278    1
## q[1375]  3278    1
## q[1376]  3278    1
## q[1377]  3278    1
## q[1378]  3121    1
## q[1379]  3200    1
## q[1380]  3121    1
## q[1381]  3121    1
## q[1382]  3121    1
## q[1383]  3278    1
## q[1384]  3278    1
## q[1385]  3121    1
## q[1386]  3200    1
## q[1387]  3200    1
## q[1388]  3278    1
## q[1389]  3278    1
## q[1390]  3121    1
## q[1391]  3200    1
## q[1392]  3278    1
## q[1393]  3278    1
## q[1394]  3278    1
## q[1395]  3278    1
## q[1396]  3200    1
## q[1397]  3200    1
## q[1398]  3278    1
## q[1399]  3200    1
## q[1400]  3121    1
## q[1401]  3278    1
## q[1402]  3278    1
## q[1403]  3121    1
## q[1404]  3278    1
## q[1405]  3278    1
## q[1406]  3278    1
## q[1407]  3200    1
## q[1408]  3558    1
## q[1409]  3093    1
## q[1410]  3093    1
## q[1411]  3253    1
## q[1412]  3253    1
## q[1413]  3558    1
## q[1414]  3093    1
## q[1415]  3093    1
## q[1416]  3093    1
## q[1417]  3093    1
## q[1418]  3093    1
## q[1419]  3253    1
## q[1420]  3253    1
## q[1421]  3253    1
## q[1422]  3093    1
## q[1423]  3253    1
## q[1424]  3253    1
## q[1425]  3093    1
## q[1426]  3093    1
## q[1427]  3093    1
## q[1428]  3093    1
## q[1429]  3093    1
## q[1430]  3558    1
## q[1431]  3093    1
## q[1432]  3558    1
## q[1433]  3093    1
## q[1434]  3253    1
## q[1435]  3093    1
## q[1436]  3093    1
## q[1437]  3093    1
## q[1438]  3253    1
## q[1439]  3558    1
## q[1440]  3093    1
## q[1441]  3253    1
## q[1442]  3093    1
## q[1443]  3253    1
## q[1444]  3093    1
## q[1445]  3253    1
## q[1446]  3093    1
## q[1447]  3093    1
## q[1448]  3253    1
## q[1449]  3558    1
## q[1450]  3253    1
## q[1451]  3093    1
## q[1452]  3253    1
## q[1453]  3253    1
## q[1454]  3253    1
## q[1455]  3558    1
## q[1456]  3093    1
## q[1457]  3558    1
## q[1458]  3093    1
## q[1459]  3093    1
## q[1460]  3253    1
## q[1461]  3253    1
## q[1462]  3093    1
## q[1463]  3093    1
## q[1464]  3093    1
## q[1465]  3093    1
## q[1466]  3093    1
## q[1467]  3253    1
## q[1468]  3093    1
## q[1469]  3558    1
## q[1470]  3558    1
## q[1471]  3093    1
## q[1472]  3093    1
## q[1473]  3093    1
## q[1474]  2870    1
## q[1475]  2680    1
## q[1476]  3130    1
## q[1477]  2870    1
## q[1478]  2870    1
## q[1479]  2680    1
## q[1480]  2870    1
## q[1481]  2870    1
## q[1482]  2870    1
## q[1483]  3130    1
## q[1484]  3130    1
## q[1485]  2680    1
## q[1486]  3130    1
## q[1487]  3130    1
## q[1488]  2680    1
## q[1489]  2870    1
## q[1490]  2680    1
## q[1491]  3130    1
## q[1492]  3211    1
## q[1493]  2942    1
## q[1494]  2942    1
## q[1495]  3211    1
## q[1496]  3211    1
## q[1497]  3479    1
## q[1498]  2942    1
## q[1499]  2942    1
## q[1500]  2942    1
## q[1501]  2942    1
## q[1502]  3211    1
## q[1503]  3479    1
## q[1504]  2942    1
## q[1505]  2942    1
## q[1506]  3211    1
## q[1507]  2942    1
## q[1508]  2942    1
## q[1509]  3211    1
## q[1510]  2942    1
## q[1511]  3211    1
## q[1512]  3211    1
## q[1513]  2942    1
## q[1514]  3479    1
## q[1515]  3211    1
## q[1516]  3211    1
## q[1517]  2942    1
## q[1518]  3211    1
## q[1519]  2942    1
## q[1520]  3479    1
## q[1521]  2942    1
## q[1522]  3479    1
## q[1523]  2942    1
## q[1524]  3211    1
## q[1525]  2942    1
## q[1526]  3211    1
## q[1527]  2942    1
## q[1528]  2942    1
## q[1529]  2942    1
## q[1530]  2942    1
## q[1531]  3211    1
## q[1532]  2942    1
## q[1533]  3211    1
## q[1534]  2942    1
## q[1535]  3479    1
## q[1536]  3211    1
## q[1537]  2942    1
## q[1538]  2942    1
## q[1539]  2942    1
## q[1540]  3479    1
## q[1541]  2593    1
## q[1542]  2593    1
## q[1543]  2593    1
## q[1544]  2775    1
## q[1545]  2330    1
## q[1546]  2330    1
## q[1547]  2775    1
## q[1548]  2330    1
## q[1549]  2330    1
## q[1550]  2775    1
## q[1551]  2593    1
## q[1552]  2593    1
## q[1553]  2593    1
## q[1554]  2330    1
## q[1555]  2775    1
## q[1556]  2593    1
## q[1557]  2593    1
## q[1558]  2593    1
## q[1559]  2593    1
## q[1560]  2330    1
## q[1561]  2593    1
## q[1562]  2330    1
## q[1563]  2593    1
## q[1564]  2330    1
## q[1565]  2593    1
## q[1566]  2593    1
## q[1567]  2330    1
## q[1568]  2593    1
## q[1569]  2330    1
## q[1570]  2593    1
## q[1571]  2775    1
## q[1572]  2330    1
## q[1573]  2593    1
## q[1574]  2330    1
## q[1575]  2330    1
## q[1576]  2593    1
## q[1577]  2330    1
## q[1578]  2593    1
## q[1579]  2775    1
## q[1580]  2775    1
## q[1581]  2593    1
## q[1582]  2775    1
## q[1583]  2330    1
## q[1584]  2775    1
## q[1585]  2593    1
## q[1586]  2593    1
## q[1587]  2330    1
## q[1588]  2330    1
## q[1589]  2330    1
## q[1590]  2593    1
## q[1591]  2775    1
## q[1592]  2593    1
## q[1593]  2593    1
## q[1594]  2593    1
## q[1595]  2593    1
## q[1596]  2593    1
## q[1597]  2330    1
## q[1598]  2593    1
## q[1599]  2593    1
## q[1600]  2775    1
## q[1601]  2775    1
## q[1602]  2330    1
## q[1603]  2330    1
## q[1604]  2593    1
## q[1605]  2593    1
## q[1606]  2330    1
## q[1607]  2775    1
## q[1608]  2330    1
## q[1609]  2593    1
## q[1610]  2330    1
## q[1611]  2593    1
## q[1612]  2593    1
## q[1613]  2593    1
## q[1614]  2775    1
## q[1615]  2775    1
## q[1616]  2404    1
## q[1617]  2404    1
## q[1618]  2772    1
## q[1619]  2404    1
## q[1620]  2404    1
## q[1621]  2615    1
## q[1622]  2772    1
## q[1623]  2404    1
## q[1624]  2772    1
## q[1625]  2404    1
## q[1626]  2404    1
## q[1627]  2404    1
## q[1628]  2840    1
## q[1629]  2840    1
## q[1630]  3149    1
## q[1631]  3141    1
## q[1632]  3141    1
## q[1633]  3149    1
## q[1634]  3141    1
## q[1635]  3141    1
## q[1636]  3141    1
## q[1637]  3141    1
## q[1638]  2840    1
## q[1639]  3141    1
## q[1640]  2840    1
## q[1641]  3149    1
## q[1642]  3141    1
## q[1643]  3141    1
## q[1644]  3141    1
## q[1645]  2840    1
## q[1646]  3141    1
## q[1647]  3149    1
## q[1648]  3141    1
## q[1649]  2840    1
## q[1650]  2840    1
## q[1651]  3218    1
## q[1652]  3218    1
## q[1653]  3194    1
## q[1654]  3287    1
## q[1655]  3287    1
## q[1656]  3194    1
## q[1657]  3218    1
## q[1658]  3287    1
## q[1659]  3287    1
## q[1660]  3218    1
## q[1661]  3287    1
## q[1662]  3287    1
## q[1663]  3218    1
## q[1664]  3218    1
## q[1665]  3287    1
## q[1666]  3218    1
## q[1667]  3194    1
## q[1668]  3218    1
## q[1669]  3287    1
## q[1670]  3287    1
## q[1671]  3218    1
## q[1672]  3287    1
## q[1673]  3218    1
## q[1674]  3194    1
## q[1675]  3287    1
## q[1676]  3287    1
## q[1677]  3287    1
## q[1678]  3287    1
## q[1679]  3287    1
## q[1680]  3218    1
## q[1681]  3287    1
## q[1682]  3218    1
## q[1683]  3194    1
## q[1684]  3218    1
## q[1685]  3194    1
## q[1686]  3287    1
## q[1687]  3287    1
## q[1688]  3253    1
## q[1689]  3208    1
## q[1690]  3208    1
## q[1691]  3208    1
## q[1692]  3253    1
## q[1693]  3253    1
## q[1694]  3253    1
## q[1695]  3253    1
## q[1696]  3595    1
## q[1697]  3208    1
## q[1698]  3208    1
## q[1699]  3208    1
## q[1700]  3208    1
## q[1701]  3595    1
## q[1702]  3208    1
## q[1703]  3208    1
## q[1704]  3208    1
## q[1705]  3253    1
## q[1706]  3595    1
## q[1707]  3208    1
## q[1708]  3208    1
## q[1709]  3208    1
## q[1710]  3208    1
## q[1711]  3253    1
## q[1712]  3208    1
## q[1713]  3253    1
## q[1714]  3253    1
## q[1715]  3208    1
## q[1716]  3253    1
## q[1717]  3253    1
## q[1718]  3253    1
## q[1719]  3208    1
## q[1720]  3595    1
## q[1721]  3208    1
## q[1722]  3208    1
## q[1723]  3208    1
## q[1724]  3253    1
## q[1725]  3253    1
## q[1726]  3208    1
## q[1727]  3595    1
## q[1728]  3208    1
## q[1729]  3208    1
## q[1730]  3208    1
## q[1731]  3208    1
## q[1732]  3208    1
## q[1733]  3253    1
## q[1734]  3253    1
## q[1735]  3595    1
## q[1736]  3208    1
## q[1737]  3253    1
## q[1738]  3208    1
## q[1739]  3253    1
## q[1740]  3595    1
## q[1741]  3253    1
## q[1742]  3208    1
## q[1743]  3208    1
## q[1744]  3208    1
## q[1745]  3595    1
## q[1746]  3208    1
## q[1747]  3236    1
## q[1748]  3236    1
## q[1749]  3583    1
## q[1750]  3236    1
## q[1751]  3223    1
## q[1752]  3223    1
## q[1753]  3223    1
## q[1754]  3223    1
## q[1755]  3583    1
## q[1756]  3236    1
## q[1757]  3236    1
## q[1758]  3236    1
## q[1759]  3236    1
## q[1760]  3236    1
## q[1761]  3583    1
## q[1762]  3583    1
## q[1763]  3236    1
## q[1764]  3236    1
## q[1765]  3236    1
## q[1766]  3236    1
## q[1767]  3236    1
## q[1768]  3236    1
## q[1769]  3223    1
## q[1770]  3583    1
## q[1771]  3223    1
## q[1772]  3583    1
## q[1773]  3236    1
## q[1774]  3223    1
## q[1775]  3223    1
## q[1776]  3236    1
## q[1777]  3223    1
## q[1778]  3236    1
## q[1779]  3223    1
## q[1780]  3236    1
## q[1781]  3223    1
## q[1782]  3236    1
## q[1783]  3583    1
## q[1784]  3236    1
## q[1785]  3236    1
## q[1786]  3236    1
## q[1787]  3236    1
## q[1788]  3236    1
## q[1789]  3223    1
## q[1790]  3236    1
## q[1791]  3236    1
## q[1792]  3583    1
## q[1793]  3223    1
## q[1794]  3236    1
## q[1795]  3236    1
## q[1796]  3223    1
## q[1797]  3223    1
## q[1798]  3236    1
## q[1799]  3223    1
## q[1800]  3236    1
## q[1801]  3583    1
## q[1802]  3320    1
## q[1803]  2971    1
## q[1804]  2971    1
## q[1805]  2971    1
## q[1806]  2971    1
## q[1807]  2971    1
## q[1808]  2971    1
## q[1809]  3320    1
## q[1810]  3059    1
## q[1811]  3320    1
## q[1812]  3320    1
## q[1813]  3320    1
## q[1814]  3059    1
## q[1815]  3059    1
## q[1816]  2971    1
## q[1817]  2971    1
## q[1818]  2971    1
## q[1819]  2971    1
## q[1820]  2971    1
## q[1821]  3320    1
## q[1822]  2971    1
## q[1823]  2971    1
## q[1824]  3320    1
## q[1825]  2971    1
## q[1826]  2971    1
## q[1827]  2971    1
## q[1828]  3320    1
## q[1829]  2971    1
## q[1830]  3320    1
## q[1831]  2971    1
## q[1832]  2971    1
## q[1833]  3320    1
## q[1834]  3320    1
## q[1835]  2971    1
## q[1836]  2971    1
## q[1837]  2971    1
## q[1838]  2971    1
## q[1839]  3059    1
## q[1840]  3320    1
## q[1841]  3320    1
## q[1842]  3320    1
## q[1843]  2971    1
## q[1844]  3320    1
## q[1845]  3059    1
## q[1846]  3320    1
## q[1847]  3320    1
## q[1848]  2971    1
## q[1849]  3320    1
## q[1850]  2971    1
## q[1851]  2971    1
## q[1852]  3320    1
## q[1853]  3059    1
## q[1854]  3059    1
## q[1855]  2971    1
## q[1856]  3059    1
## q[1857]  3059    1
## q[1858]  2971    1
## q[1859]  2971    1
## q[1860]  2971    1
## q[1861]  3320    1
## q[1862]  2971    1
## q[1863]  3320    1
## q[1864]  3320    1
## q[1865]  2971    1
## q[1866]  3059    1
## q[1867]  2971    1
## q[1868]  2971    1
## q[1869]  2971    1
## q[1870]  2971    1
## q[1871]  2971    1
## q[1872]  2971    1
## q[1873]  3320    1
## q[1874]  2971    1
## q[1875]  3059    1
## q[1876]  3059    1
## q[1877]  2971    1
## q[1878]  3059    1
## q[1879]  3320    1
## q[1880]  2971    1
## q[1881]  2971    1
## q[1882]  3320    1
## q[1883]  3059    1
## q[1884]  2971    1
## q[1885]  2971    1
## q[1886]  2971    1
## q[1887]  2971    1
## q[1888]  2971    1
## q[1889]  3059    1
## q[1890]  2858    1
## q[1891]  2858    1
## q[1892]  2858    1
## q[1893]  2858    1
## q[1894]  3178    1
## q[1895]  3178    1
## q[1896]  3178    1
## q[1897]  3426    1
## q[1898]  2858    1
## q[1899]  2858    1
## q[1900]  2858    1
## q[1901]  3178    1
## q[1902]  2858    1
## q[1903]  3178    1
## q[1904]  2858    1
## q[1905]  2858    1
## q[1906]  3178    1
## q[1907]  2858    1
## q[1908]  3178    1
## q[1909]  3178    1
## q[1910]  2858    1
## q[1911]  3426    1
## q[1912]  3178    1
## q[1913]  3178    1
## q[1914]  3178    1
## q[1915]  2858    1
## q[1916]  3178    1
## q[1917]  2858    1
## q[1918]  2858    1
## q[1919]  3178    1
## q[1920]  2858    1
## q[1921]  2858    1
## q[1922]  2858    1
## q[1923]  2858    1
## q[1924]  2858    1
## q[1925]  2858    1
## q[1926]  2858    1
## q[1927]  3178    1
## q[1928]  3426    1
## q[1929]  2858    1
## q[1930]  3426    1
## q[1931]  2858    1
## q[1932]  3426    1
## q[1933]  2858    1
## q[1934]  2858    1
## q[1935]  2858    1
## q[1936]  2858    1
## q[1937]  2814    1
## q[1938]  2892    1
## q[1939]  2892    1
## q[1940]  2350    1
## q[1941]  2350    1
## q[1942]  2350    1
## q[1943]  2350    1
## q[1944]  2350    1
## q[1945]  2814    1
## q[1946]  2350    1
## q[1947]  2814    1
## q[1948]  2814    1
## q[1949]  2814    1
## q[1950]  2350    1
## q[1951]  2350    1
## q[1952]  2350    1
## q[1953]  2350    1
## q[1954]  2350    1
## q[1955]  2350    1
## q[1956]  2350    1
## q[1957]  2892    1
## q[1958]  2350    1
## q[1959]  2793    1
## q[1960]  2333    1
## q[1961]  2869    1
## q[1962]  2333    1
## q[1963]  2333    1
## q[1964]  2793    1
## q[1965]  2869    1
## q[1966]  2793    1
## q[1967]  2333    1
## q[1968]  2333    1
## q[1969]  2333    1
## q[1970]  2333    1
## q[1971]  2333    1
## q[1972]  2333    1
## q[1973]  2793    1
## q[1974]  2333    1
## q[1975]  2333    1
## q[1976]  2793    1
## q[1977]  2333    1
## q[1978]  2793    1
## q[1979]  2869    1
## q[1980]  2793    1
## q[1981]  2793    1
## q[1982]  2793    1
## q[1983]  2333    1
## q[1984]  2333    1
## q[1985]  2869    1
## q[1986]  2333    1
## q[1987]  2869    1
## q[1988]  2793    1
## q[1989]  2333    1
## q[1990]  2333    1
## q[1991]  2793    1
## q[1992]  2333    1
## q[1993]  2333    1
## q[1994]  2869    1
## q[1995]  2333    1
## q[1996]  2793    1
## q[1997]  2333    1
## q[1998]  2333    1
## q[1999]  2333    1
## q[2000]  3238    1
## q[2001]  3230    1
## q[2002]  3592    1
## q[2003]  3238    1
## q[2004]  3238    1
## q[2005]  3592    1
## q[2006]  3238    1
## q[2007]  3230    1
## q[2008]  3230    1
## q[2009]  3592    1
## q[2010]  3230    1
## q[2011]  3230    1
## q[2012]  3238    1
## q[2013]  3230    1
## q[2014]  3230    1
## q[2015]  3230    1
## q[2016]  3238    1
## q[2017]  3230    1
## q[2018]  3230    1
## q[2019]  3238    1
## q[2020]  3230    1
## q[2021]  3238    1
## q[2022]  3238    1
## q[2023]  3230    1
## q[2024]  3238    1
## q[2025]  3592    1
## q[2026]  3238    1
## q[2027]  3238    1
## q[2028]  3230    1
## q[2029]  3238    1
## q[2030]  3592    1
## q[2031]  3230    1
## q[2032]  3230    1
## q[2033]  3592    1
## q[2034]  3230    1
## q[2035]  3230    1
## q[2036]  3592    1
## q[2037]  3238    1
## q[2038]  3230    1
## q[2039]  3238    1
## q[2040]  3238    1
## q[2041]  3230    1
## q[2042]  3592    1
## q[2043]  3238    1
## q[2044]  3230    1
## q[2045]  3230    1
## q[2046]  3230    1
## q[2047]  3230    1
## q[2048]  3238    1
## q[2049]  3238    1
## q[2050]  3230    1
## q[2051]  3230    1
## q[2052]  3238    1
## q[2053]  3592    1
## q[2054]  3238    1
## q[2055]  3230    1
## q[2056]  3230    1
## q[2057]  3235    1
## q[2058]  3198    1
## q[2059]  3570    1
## q[2060]  3198    1
## q[2061]  3198    1
## q[2062]  3235    1
## q[2063]  3198    1
## q[2064]  3235    1
## q[2065]  3198    1
## q[2066]  3235    1
## q[2067]  3198    1
## q[2068]  3235    1
## q[2069]  3235    1
## q[2070]  3198    1
## q[2071]  3235    1
## q[2072]  3235    1
## q[2073]  3570    1
## q[2074]  3570    1
## q[2075]  3198    1
## q[2076]  3198    1
## q[2077]  3235    1
## q[2078]  3570    1
## q[2079]  3235    1
## q[2080]  3235    1
## q[2081]  3198    1
## q[2082]  3570    1
## q[2083]  3198    1
## q[2084]  3235    1
## q[2085]  3198    1
## q[2086]  3570    1
## q[2087]  3235    1
## q[2088]  3570    1
## q[2089]  3260    1
## q[2090]  3152    1
## q[2091]  3152    1
## q[2092]  3152    1
## q[2093]  3260    1
## q[2094]  3582    1
## q[2095]  3260    1
## q[2096]  3260    1
## q[2097]  3582    1
## q[2098]  3152    1
## q[2099]  3152    1
## q[2100]  3152    1
## q[2101]  3260    1
## q[2102]  3152    1
## q[2103]  3152    1
## q[2104]  3152    1
## q[2105]  3260    1
## q[2106]  3152    1
## q[2107]  3260    1
## q[2108]  3152    1
## q[2109]  3260    1
## q[2110]  3260    1
## q[2111]  3152    1
## q[2112]  3152    1
## q[2113]  3152    1
## q[2114]  3260    1
## q[2115]  3260    1
## q[2116]  3260    1
## q[2117]  3152    1
## q[2118]  3152    1
## q[2119]  3152    1
## q[2120]  3582    1
## q[2121]  3582    1
## q[2122]  3152    1
## q[2123]  3260    1
## q[2124]  3152    1
## q[2125]  3260    1
## q[2126]  3152    1
## q[2127]  3582    1
## q[2128]  3152    1
## q[2129]  3152    1
## q[2130]  3260    1
## q[2131]  3582    1
## q[2132]  3582    1
## q[2133]  3152    1
## q[2134]  3260    1
## q[2135]  3152    1
## q[2136]  3260    1
## q[2137]  3582    1
## q[2138]  3152    1
## q[2139]  3152    1
## q[2140]  3152    1
## q[2141]  3582    1
## q[2142]  2905    1
## q[2143]  3075    1
## q[2144]  3364    1
## q[2145]  3364    1
## q[2146]  2905    1
## q[2147]  3075    1
## q[2148]  3075    1
## q[2149]  3075    1
## q[2150]  3075    1
## q[2151]  2905    1
## q[2152]  3075    1
## q[2153]  3075    1
## q[2154]  2905    1
## q[2155]  3075    1
## q[2156]  2905    1
## q[2157]  3364    1
## q[2158]  2905    1
## q[2159]  2905    1
## q[2160]  3075    1
## q[2161]  3075    1
## q[2162]  3364    1
## q[2163]  3075    1
## q[2164]  3364    1
## q[2165]  2905    1
## q[2166]  3075    1
## q[2167]  3075    1
## q[2168]  2905    1
## q[2169]  3075    1
## q[2170]  3075    1
## q[2171]  2905    1
## q[2172]  3075    1
## q[2173]  2610    1
## q[2174]  2610    1
## q[2175]  2441    1
## q[2176]  2816    1
## q[2177]  2610    1
## q[2178]  2610    1
## q[2179]  2610    1
## q[2180]  2610    1
## q[2181]  2816    1
## q[2182]  2441    1
## q[2183]  2441    1
## q[2184]  2441    1
## q[2185]  2610    1
## q[2186]  2441    1
## q[2187]  2610    1
## q[2188]  2610    1
## q[2189]  2610    1
## q[2190]  2610    1
## q[2191]  2610    1
## q[2192]  2610    1
## q[2193]  2816    1
## q[2194]  2610    1
## q[2195]  2610    1
## q[2196]  2716    1
## q[2197]  2716    1
## q[2198]  2716    1
## q[2199]  2716    1
## q[2200]  3109    1
## q[2201]  3314    1
## q[2202]  3109    1
## q[2203]  3314    1
## q[2204]  2716    1
## q[2205]  2716    1
## q[2206]  2716    1
## q[2207]  2716    1
## q[2208]  2716    1
## q[2209]  3109    1
## q[2210]  2716    1
## q[2211]  2716    1
## q[2212]  3109    1
## q[2213]  2716    1
## q[2214]  2716    1
## q[2215]  2716    1
## q[2216]  3314    1
## q[2217]  3109    1
## q[2218]  3109    1
## q[2219]  3109    1
## q[2220]  3109    1
## q[2221]  2716    1
## q[2222]  3314    1
## q[2223]  3314    1
## q[2224]  2716    1
## q[2225]  3314    1
## q[2226]  3314    1
## q[2227]  2716    1
## q[2228]  2716    1
## q[2229]  3109    1
## q[2230]  3109    1
## q[2231]  3314    1
## q[2232]  2716    1
## q[2233]  2716    1
## q[2234]  2716    1
## q[2235]  3314    1
## q[2236]  3314    1
## q[2237]  2716    1
## q[2238]  3314    1
## q[2239]  3109    1
## q[2240]  2716    1
## q[2241]  3109    1
## q[2242]  2716    1
## q[2243]  2716    1
## q[2244]  2716    1
## q[2245]  2716    1
## q[2246]  3314    1
## q[2247]  2331    1
## q[2248]  2323    1
## q[2249]  2323    1
## q[2250]  2323    1
## q[2251]  2038    1
## q[2252]  2323    1
## q[2253]  2038    1
## q[2254]  2323    1
## q[2255]  2323    1
## q[2256]  2323    1
## q[2257]  2038    1
## q[2258]  2038    1
## q[2259]  2323    1
## q[2260]  2323    1
## q[2261]  2038    1
## q[2262]  2038    1
## q[2263]  2038    1
## q[2264]  2323    1
## q[2265]  2038    1
## q[2266]  2331    1
## q[2267]  2323    1
## q[2268]  2038    1
## q[2269]  3044    1
## q[2270]  2867    1
## q[2271]  2867    1
## q[2272]  3044    1
## q[2273]  3044    1
## q[2274]  3328    1
## q[2275]  3044    1
## q[2276]  2867    1
## q[2277]  3044    1
## q[2278]  3044    1
## q[2279]  2867    1
## q[2280]  2867    1
## q[2281]  3044    1
## q[2282]  2867    1
## q[2283]  3328    1
## q[2284]  2867    1
## q[2285]  3044    1
## q[2286]  2867    1
## q[2287]  3328    1
## q[2288]  3328    1
## q[2289]  3044    1
## q[2290]  3044    1
## q[2291]  3328    1
## q[2292]  2867    1
## q[2293]  2867    1
## q[2294]  3328    1
## q[2295]  2867    1
## q[2296]  3044    1
## q[2297]  3044    1
## q[2298]  3044    1
## q[2299]  3044    1
## q[2300]  2867    1
## q[2301]  2867    1
## q[2302]  3044    1
## q[2303]  2867    1
## q[2304]  3328    1
## q[2305]  3044    1
## q[2306]  3044    1
## q[2307]  3014    1
## q[2308]  3148    1
## q[2309]  2558    1
## q[2310]  2558    1
## q[2311]  3148    1
## q[2312]  2558    1
## q[2313]  2558    1
## q[2314]  2558    1
## q[2315]  3014    1
## q[2316]  3148    1
## q[2317]  3014    1
## q[2318]  2558    1
## q[2319]  2558    1
## q[2320]  3014    1
## q[2321]  2558    1
## q[2322]  3148    1
## q[2323]  2558    1
## q[2324]  2558    1
## q[2325]  3014    1
## q[2326]  2558    1
## q[2327]  3148    1
## q[2328]  3014    1
## q[2329]  2558    1
## q[2330]  3219    1
## q[2331]  2829    1
## q[2332]  2829    1
## q[2333]  2829    1
## q[2334]  2829    1
## q[2335]  3219    1
## q[2336]  3219    1
## q[2337]  2971    1
## q[2338]  3219    1
## q[2339]  3219    1
## q[2340]  2971    1
## q[2341]  2829    1
## q[2342]  2829    1
## q[2343]  2829    1
## q[2344]  2829    1
## q[2345]  3219    1
## q[2346]  2971    1
## q[2347]  2829    1
## q[2348]  2829    1
## q[2349]  2829    1
## q[2350]  2829    1
## q[2351]  3219    1
## q[2352]  2829    1
## q[2353]  3219    1
## q[2354]  2829    1
## q[2355]  3219    1
## q[2356]  3219    1
## q[2357]  2829    1
## q[2358]  2829    1
## q[2359]  2829    1
## q[2360]  2829    1
## q[2361]  3219    1
## q[2362]  3219    1
## q[2363]  2829    1
## q[2364]  2971    1
## q[2365]  2829    1
## q[2366]  3219    1
## q[2367]  2829    1
## q[2368]  2829    1
## q[2369]  3219    1
## q[2370]  2971    1
## q[2371]  2971    1
## q[2372]  2829    1
## q[2373]  3219    1
## q[2374]  2829    1
## q[2375]  3219    1
## q[2376]  2829    1
## q[2377]  2971    1
## q[2378]  2829    1
## q[2379]  2829    1
## q[2380]  2829    1
## q[2381]  2829    1
## q[2382]  2829    1
## q[2383]  3219    1
## q[2384]  2971    1
## q[2385]  3219    1
## q[2386]  3219    1
## q[2387]  2829    1
## q[2388]  2829    1
## q[2389]  2829    1
## q[2390]  3219    1
## q[2391]  2971    1
## q[2392]  3219    1
## q[2393]  2829    1
## q[2394]  2829    1
## q[2395]  2971    1
## q[2396]  2971    1
## lp__     1724    1
## 
## Samples were drawn using NUTS(diag_e) at Mon May 18 16:55:44 2020.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;檢查模型參數的收斂情況&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;檢查模型參數的收斂情況&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(bayesplot)

color_scheme_set(&amp;quot;mix-brightblue-gray&amp;quot;)

posterior5.5 &amp;lt;- rstan::extract(fit1, inc_warmup = TRUE, permuted = FALSE)

p &amp;lt;- mcmc_trace(posterior5.5, n_warmup = 0, pars = c(&amp;quot;b[1]&amp;quot;, &amp;quot;b[2]&amp;quot;, &amp;quot;b[3]&amp;quot;,   &amp;quot;lp__&amp;quot;), facet_args = list(nrow = 2, labeller = label_parsed))

p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:chapter5-5&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-27-logistic2-rstan_files/figure-html/chapter5-5-1.png&#34; alt=&#34;用 bayesplot包數繪製的模型5-5的MCMC鏈式軌跡圖 (trace plot)。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: 用 bayesplot包數繪製的模型5-5的MCMC鏈式軌跡圖 (trace plot)。
&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- mcmc_acf_bar(posterior5.5, pars = c(&amp;quot;b[1]&amp;quot;, &amp;quot;b[2]&amp;quot;, &amp;quot;b[3]&amp;quot;,   &amp;quot;lp__&amp;quot;))
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:chapter5-5-acf&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-27-logistic2-rstan_files/figure-html/chapter5-5-acf-1.png&#34; alt=&#34;用 bayesplot包數繪製的事後樣本自相關圖(autocorrelation)。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: 用 bayesplot包數繪製的事後樣本自相關圖(autocorrelation)。
&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- mcmc_dens_overlay(posterior5.5, pars = c(&amp;quot;b[1]&amp;quot;, &amp;quot;b[2]&amp;quot;, &amp;quot;b[3]&amp;quot;, &amp;quot;OR1&amp;quot;,  &amp;quot;OR2&amp;quot;, &amp;quot;lp__&amp;quot;), color_chains = T)
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step5-5-density&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-27-logistic2-rstan_files/figure-html/step5-5-density-1.png&#34; alt=&#34;用 bayesplot包數繪製的事後樣本密度分佈圖。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: 用 bayesplot包數繪製的事後樣本密度分佈圖。
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;檢查模型的擬合情況&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;檢查模型的擬合情況&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ms &amp;lt;- rstan::extract(fit1)
set.seed(123)
logistic &amp;lt;- function(x) 1/(1+exp(-x))
X &amp;lt;- 30:200
q_qua &amp;lt;- logistic(t(sapply(1:length(X), function(i) {
  q_mcmc &amp;lt;- ms$b[,1] + ms$b[,3]*X[i]/200
  quantile(q_mcmc, probs=c(0.1, 0.5, 0.9))
})))
d_est &amp;lt;- data.frame(X, q_qua)
colnames(d_est) &amp;lt;- c(&amp;#39;X&amp;#39;, &amp;#39;p10&amp;#39;, &amp;#39;p50&amp;#39;, &amp;#39;p90&amp;#39;)
d$A &amp;lt;- as.factor(d$A)

p &amp;lt;- ggplot(d_est, aes(x=X, y=p50))
p &amp;lt;- p + theme_bw(base_size=18)
p &amp;lt;- p + geom_ribbon(aes(ymin=p10, ymax=p90), fill=&amp;#39;black&amp;#39;, alpha=2/6)
p &amp;lt;- p + geom_line(size=1)
p &amp;lt;- p + geom_point(data=subset(d, A==0 &amp;amp; Weather==&amp;#39;A&amp;#39;), aes(x=Score, y=Y, color=A),
  position=position_jitter(w=0, h=0.1), size=1)
p &amp;lt;- p + labs(x=&amp;#39;Score&amp;#39;, y=&amp;#39;q&amp;#39;)
p &amp;lt;- p + scale_color_manual(values=c(&amp;#39;black&amp;#39;))
p &amp;lt;- p + scale_y_continuous(breaks=seq(0, 1, 0.2))
p &amp;lt;- p + xlim(30, 200)
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:validity-of-model&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-27-logistic2-rstan_files/figure-html/validity-of-model-1.png&#34; alt=&#34;不喜歡打工，且天氣晴天的情況下，分數在 30-200 點之間的學生的出勤概率 q 和它的 80% 可信區間範圍。圖中黑色實線是預測概率的事後分佈的中央值，灰色帶是可信區間範圍。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4: 不喜歡打工，且天氣晴天的情況下，分數在 30-200 點之間的學生的出勤概率 q 和它的 80% 可信區間範圍。圖中黑色實線是預測概率的事後分佈的中央值，灰色帶是可信區間範圍。
&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# ggsave(file=&amp;#39;output/fig5-9.png&amp;#39;, plot=p, dpi=300, w=4.5, h=3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;圖&lt;a href=&#34;#fig:validity-of-model&#34;&gt;4&lt;/a&gt;試圖把分數範圍在 30-200 之間的學生中，通過模型計算獲得的，在天氣晴朗，且不愛打工的孩子們的事後出勤概率的預測值(黑色實線)，和它的事後概率80%可信區間，以及對應的實際觀測值的結果(黑點)。但是，當預測變量越來越多，模型結果的可視化變得越來越困難。下面我們介紹兩種常見的評價邏輯回歸擬合結果的可視化圖。&lt;/p&gt;
&lt;p&gt;首先是圖 &lt;a href=&#34;#fig:validity-of-model1&#34;&gt;5&lt;/a&gt; 顯示的事後出勤概率，和實際觀察出勤結果之間的關係圖。在這個圖中，橫軸是 &lt;span class=&#34;math inline&#34;&gt;\(q[i]\)&lt;/span&gt; 的事後分佈的中央值(每名學生都有自己的事後出勤概率預測，它的中央值)，縱軸是該名學生實際是否在該次課上出勤的觀察結果。如果模型擬合的理想的話，那麼在 &lt;span class=&#34;math inline&#34;&gt;\(Y=0\)&lt;/span&gt;，也就是圖中的下半部分，大多數的預測點應該靠近概率較低的部分(也就是靠近左側)，同時，&lt;span class=&#34;math inline&#34;&gt;\(y = 1\)&lt;/span&gt; 的部分預測概率應該大多數在靠近左側的部分。此圖其實提示我們該模型的擬合效果不理想。不能明顯地將出勤與不出勤較爲準確地區分開來。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)
ms &amp;lt;- rstan::extract(fit1)
d_qua &amp;lt;- t(apply(ms$q, 2, quantile, prob=c(0.1, 0.5, 0.9)))
colnames(d_qua) &amp;lt;- c(&amp;#39;p10&amp;#39;, &amp;#39;p50&amp;#39;, &amp;#39;p90&amp;#39;)
d_qua &amp;lt;- data.frame(d, d_qua)
d_qua$Y &amp;lt;- as.factor(d_qua$Y)
d_qua$A &amp;lt;- as.factor(d_qua$A)

p &amp;lt;- ggplot(data=d_qua, aes(x=Y, y=p50))
p &amp;lt;- p + theme_bw(base_size=18)
p &amp;lt;- p + coord_flip()
p &amp;lt;- p + geom_violin(trim=FALSE, size=1.5, color=&amp;#39;grey80&amp;#39;)
p &amp;lt;- p + geom_point(aes(color=A), position=position_jitter(w=0.4, h=0), size=1)
p &amp;lt;- p + scale_color_manual(values=c(&amp;#39;grey5&amp;#39;, &amp;#39;grey50&amp;#39;))
p &amp;lt;- p + labs(x=&amp;#39;Y&amp;#39;, y=&amp;#39;q&amp;#39;)
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:validity-of-model1&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-27-logistic2-rstan_files/figure-html/validity-of-model1-1.png&#34; alt=&#34;把計算獲得的事後概率的中央值作爲每名學生是否出勤的概率預測(x 軸)，和實際觀察的出勤結果(y 軸)，繪製的散點圖。其中，灰色的小提琴圖其實是根據獲得的事後概率的中央值的概率密度曲線，繪製的上下對稱的圖形，形似小提琴。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 5: 把計算獲得的事後概率的中央值作爲每名學生是否出勤的概率預測(x 軸)，和實際觀察的出勤結果(y 軸)，繪製的散點圖。其中，灰色的小提琴圖其實是根據獲得的事後概率的中央值的概率密度曲線，繪製的上下對稱的圖形，形似小提琴。
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Rstan Wonderful R-(4)</title>
      <link>https://wangcc.me/post/logistic-rstan/</link>
      <pubDate>Sun, 27 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/logistic-rstan/</guid>
      <description>
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#邏輯回歸模型的-rstan-貝葉斯實現&#34;&gt;邏輯回歸模型的 Rstan 貝葉斯實現&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#確定分析目的&#34;&gt;確定分析目的&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#確認數據分佈&#34;&gt;確認數據分佈&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#寫下數學模型表達式&#34;&gt;寫下數學模型表達式&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#確認收斂效果&#34;&gt;確認收斂效果&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;邏輯回歸模型的-rstan-貝葉斯實現&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;邏輯回歸模型的 Rstan 貝葉斯實現&lt;/h1&gt;
&lt;p&gt;本小節使用的&lt;a href=&#34;https://raw.githubusercontent.com/MatsuuraKentaro/RStanBook/master/chap05/input/data-attendance-2.txt&#34;&gt;數據&lt;/a&gt;，和前一節的出勤率數據很類似:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d &amp;lt;- read.table(&amp;quot;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap05/input/data-attendance-2.txt&amp;quot;, 
                     sep = &amp;quot;,&amp;quot;, header = T)
head(d)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   PersonID A Score  M  Y
## 1        1 0    69 43 38
## 2        2 1   145 56 40
## 3        3 0   125 32 24
## 4        4 1    86 45 33
## 5        5 1   158 33 23
## 6        6 0   133 61 60&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;PersonID&lt;/code&gt;: 是學生的編號；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;A&lt;/code&gt;, &lt;code&gt;Score&lt;/code&gt;: 和之前一樣用來預測出勤率的兩個預測變量，分別是表示是否喜歡打工的 &lt;code&gt;A&lt;/code&gt;，和表示對學習本身是否喜歡的評分 (滿分200)；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;M&lt;/code&gt;: 過去三個月內，該名學生一共需要上課的總課時數；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Y&lt;/code&gt;: 過去三個月內，該名學生實際上出勤的課時數。&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;確定分析目的&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;確定分析目的&lt;/h1&gt;
&lt;p&gt;需要回答的問題依然是，&lt;span class=&#34;math inline&#34;&gt;\(A, Score\)&lt;/span&gt; 分別在多大程度上預測學生的出勤率？另外，我們希望知道的是，當需要修的課時數固定的事後，這兩個預測變量能準確提供 &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; 的多少信息？&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;確認數據分佈&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;確認數據分佈&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(GGally)

set.seed(1)
d &amp;lt;- d[, -1]
# d &amp;lt;- read.csv(file=&amp;#39;input/data-attendance-2.txt&amp;#39;)[,-1]
d$A &amp;lt;- as.factor(d$A)
d &amp;lt;- transform(d, ratio=Y/M)
N_col &amp;lt;- ncol(d)
ggp &amp;lt;- ggpairs(d, upper=&amp;#39;blank&amp;#39;, diag=&amp;#39;blank&amp;#39;, lower=&amp;#39;blank&amp;#39;)

for(i in 1:N_col) {
  x &amp;lt;- d[,i]
  p &amp;lt;- ggplot(data.frame(x, A=d$A), aes(x))
  p &amp;lt;- p + theme_bw(base_size=14)
  p &amp;lt;- p + theme(axis.text.x=element_text(angle=40, vjust=1, hjust=1))
  if (class(x) == &amp;#39;factor&amp;#39;) {
    p &amp;lt;- p + geom_bar(aes(fill=A), color=&amp;#39;grey20&amp;#39;)
  } else {
    bw &amp;lt;- (max(x)-min(x))/10
    p &amp;lt;- p + geom_histogram(aes(fill=A), color=&amp;#39;grey20&amp;#39;, binwidth=bw)
    p &amp;lt;- p + geom_line(eval(bquote(aes(y=..count..*.(bw)))), stat=&amp;#39;density&amp;#39;)
  }
  p &amp;lt;- p + geom_label(data=data.frame(x=-Inf, y=Inf, label=colnames(d)[i]), aes(x=x, y=y, label=label), hjust=0, vjust=1)
  p &amp;lt;- p + scale_fill_manual(values=alpha(c(&amp;#39;white&amp;#39;, &amp;#39;grey40&amp;#39;), 0.5))
  ggp &amp;lt;- putPlot(ggp, p, i, i)
}

zcolat &amp;lt;- seq(-1, 1, length=81)
zcolre &amp;lt;- c(zcolat[1:40]+1, rev(zcolat[41:81]))

for(i in 1:(N_col-1)) {
  for(j in (i+1):N_col) {
    x &amp;lt;- as.numeric(d[,i])
    y &amp;lt;- as.numeric(d[,j])
    r &amp;lt;- cor(x, y, method=&amp;#39;spearman&amp;#39;, use=&amp;#39;pairwise.complete.obs&amp;#39;)
    zcol &amp;lt;- lattice::level.colors(r, at=zcolat, col.regions=grey(zcolre))
    textcol &amp;lt;- ifelse(abs(r) &amp;lt; 0.4, &amp;#39;grey20&amp;#39;, &amp;#39;white&amp;#39;)
    ell &amp;lt;- ellipse::ellipse(r, level=0.95, type=&amp;#39;l&amp;#39;, npoints=50, scale=c(.2, .2), centre=c(.5, .5))
    p &amp;lt;- ggplot(data.frame(ell), aes(x=x, y=y))
    p &amp;lt;- p + theme_bw() + theme(
      plot.background=element_blank(),
      panel.grid.major=element_blank(), panel.grid.minor=element_blank(),
      panel.border=element_blank(), axis.ticks=element_blank()
    )
    p &amp;lt;- p + geom_polygon(fill=zcol, color=zcol)
    p &amp;lt;- p + geom_text(data=NULL, x=.5, y=.5, label=100*round(r, 2), size=6, col=textcol)
    ggp &amp;lt;- putPlot(ggp, p, i, j)
  }
}

for(j in 1:(N_col-1)) {
  for(i in (j+1):N_col) {
    x &amp;lt;- d[,j]
    y &amp;lt;- d[,i]
    p &amp;lt;- ggplot(data.frame(x, y, gr=d$A), aes(x=x, y=y, fill=gr, shape=gr))
    p &amp;lt;- p + theme_bw(base_size=14)
    p &amp;lt;- p + theme(axis.text.x=element_text(angle=40, vjust=1, hjust=1))
    if (class(x) == &amp;#39;factor&amp;#39;) {
      p &amp;lt;- p + geom_boxplot(aes(group=x), alpha=3/6, outlier.size=0, fill=&amp;#39;white&amp;#39;)
      p &amp;lt;- p + geom_point(position=position_jitter(w=0.4, h=0), size=2)
    } else {
      p &amp;lt;- p + geom_point(size=2)
    }
    p &amp;lt;- p + scale_shape_manual(values=c(21, 24))
    p &amp;lt;- p + scale_fill_manual(values=alpha(c(&amp;#39;white&amp;#39;, &amp;#39;grey40&amp;#39;), 0.5))
    ggp &amp;lt;- putPlot(ggp, p, i, j)
  }
}

ggp&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step1&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-27-logistic-rstan_files/figure-html/step1-1.png&#34; alt=&#34;三個變量的分佈觀察圖，相比之前增加了 $ratio = Y/M$ 列。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: 三個變量的分佈觀察圖，相比之前增加了 &lt;span class=&#34;math inline&#34;&gt;\(ratio = Y/M\)&lt;/span&gt; 列。
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;從圖 &lt;a href=&#34;#fig:step1&#34;&gt;1&lt;/a&gt; 還可以看出，由於總課時數越多，學生實際出勤的課時數也會越多所以 &lt;span class=&#34;math inline&#34;&gt;\(M, Y\)&lt;/span&gt; 兩者之間理應有很強的正相關。另外可能可以推測的是 &lt;span class=&#34;math inline&#34;&gt;\(Ratio\)&lt;/span&gt; 和是否愛學習的分數之間大概有可能有正相關，和是否喜歡打工之間大概可能有負相關。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;寫下數學模型表達式&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;寫下數學模型表達式&lt;/h1&gt;
&lt;p&gt;在 Stan 的語法中，使用的是反邏輯函數 (inverse logit): &lt;code&gt;inv_logit&lt;/code&gt; 來描述下面的邏輯回歸模型 5-4。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{array}{l}
q[n] = \text{inv_logit}(b_1 + b_2 A[n] + b_3Score[n]) &amp;amp; n = 1, 2, \dots, N \\
Y[n] \sim \text{Binomial}(M[n], q[n])                 &amp;amp; n = 1, 2, \dots, N \\
\end{array}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;上面的數學模型，可以被翻譯成下面的 Stan 語言:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data {
  int N; 
  int&amp;lt;lower=0, upper=1&amp;gt; A[N]; 
  real&amp;lt;lower=0, upper=1&amp;gt; Score[N]; 
  int&amp;lt;lower=0&amp;gt; M[N];
  int&amp;lt;lower=0&amp;gt; Y[N];
}

parameters {
  real b1; 
  real b2; 
  real b3;
}

transformed parameters {
  real q[N];
  for (n in 1:N) {
    q[n] = inv_logit(b1 + b2*A[n] + b3*Score[n]);
  }
}

model {
  for (n in 1:N) {
    Y[n] ~ binomial(M[n], q[n]); 
  }
}

generated quantities {
  real y_pred[N]; 
  for (n in 1:N) {
    y_pred[n] = binomial_rng(M[n], q[n]);
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;下面的 R 代碼用來實現對上面 Stan 模型的擬合:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rstan)
d &amp;lt;- read.csv(file=&amp;#39;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap05/input/data-attendance-2.txt&amp;#39;, header = T)
data &amp;lt;- list(N=nrow(d), A=d$A, Score=d$Score/200, M=d$M, Y=d$Y)
fit &amp;lt;- stan(file=&amp;#39;stanfiles/model5-4.stan&amp;#39;, data=data, seed=1234)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &amp;#39;model5-4&amp;#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 1.9e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.19 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.148218 seconds (Warm-up)
## Chain 1:                0.146931 seconds (Sampling)
## Chain 1:                0.295149 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &amp;#39;model5-4&amp;#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 1e-05 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.140054 seconds (Warm-up)
## Chain 2:                0.148312 seconds (Sampling)
## Chain 2:                0.288366 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &amp;#39;model5-4&amp;#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 9e-06 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.151245 seconds (Warm-up)
## Chain 3:                0.135369 seconds (Sampling)
## Chain 3:                0.286614 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &amp;#39;model5-4&amp;#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 8e-06 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.144055 seconds (Warm-up)
## Chain 4:                0.144297 seconds (Sampling)
## Chain 4:                0.288352 seconds (Total)
## Chain 4:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Stan model: model5-4.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##                mean se_mean   sd     2.5%      25%      50%      75%    97.5%
## b1             0.09    0.01 0.23    -0.33    -0.08     0.08     0.25     0.53
## b2            -0.62    0.00 0.10    -0.82    -0.69    -0.62    -0.56    -0.44
## b3             1.91    0.01 0.37     1.19     1.66     1.91     2.17     2.61
## q[1]           0.68    0.00 0.02     0.63     0.66     0.68     0.70     0.73
## q[2]           0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[3]           0.78    0.00 0.01     0.76     0.77     0.78     0.79     0.81
## q[4]           0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.61
## q[5]           0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[6]           0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[7]           0.76    0.00 0.01     0.73     0.75     0.76     0.77     0.78
## q[8]           0.70    0.00 0.02     0.67     0.69     0.70     0.72     0.74
## q[9]           0.81    0.00 0.01     0.79     0.81     0.82     0.82     0.84
## q[10]          0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[11]          0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[12]          0.80    0.00 0.01     0.78     0.79     0.80     0.81     0.83
## q[13]          0.64    0.00 0.02     0.61     0.63     0.64     0.65     0.67
## q[14]          0.76    0.00 0.01     0.73     0.75     0.76     0.77     0.78
## q[15]          0.76    0.00 0.01     0.73     0.75     0.76     0.76     0.78
## q[16]          0.60    0.00 0.02     0.56     0.59     0.60     0.61     0.64
## q[17]          0.76    0.00 0.01     0.74     0.76     0.76     0.77     0.79
## q[18]          0.70    0.00 0.02     0.67     0.69     0.70     0.72     0.74
## q[19]          0.86    0.00 0.02     0.83     0.85     0.86     0.88     0.89
## q[20]          0.72    0.00 0.02     0.69     0.71     0.72     0.73     0.76
## q[21]          0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.61
## q[22]          0.62    0.00 0.02     0.59     0.61     0.62     0.63     0.65
## q[23]          0.62    0.00 0.02     0.58     0.61     0.62     0.63     0.65
## q[24]          0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[25]          0.64    0.00 0.02     0.61     0.63     0.64     0.65     0.67
## q[26]          0.67    0.00 0.01     0.64     0.66     0.67     0.68     0.69
## q[27]          0.77    0.00 0.01     0.75     0.76     0.77     0.78     0.80
## q[28]          0.77    0.00 0.01     0.75     0.76     0.77     0.78     0.80
## q[29]          0.84    0.00 0.01     0.81     0.83     0.84     0.85     0.86
## q[30]          0.76    0.00 0.01     0.74     0.75     0.76     0.77     0.79
## q[31]          0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.78
## q[32]          0.54    0.00 0.03     0.49     0.52     0.54     0.56     0.60
## q[33]          0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[34]          0.66    0.00 0.01     0.63     0.65     0.66     0.67     0.69
## q[35]          0.78    0.00 0.01     0.76     0.78     0.78     0.79     0.81
## q[36]          0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[37]          0.62    0.00 0.02     0.58     0.60     0.62     0.63     0.65
## q[38]          0.76    0.00 0.01     0.73     0.75     0.76     0.77     0.78
## q[39]          0.72    0.00 0.02     0.69     0.71     0.72     0.73     0.75
## q[40]          0.72    0.00 0.02     0.68     0.70     0.72     0.73     0.75
## q[41]          0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.81
## q[42]          0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[43]          0.78    0.00 0.01     0.75     0.77     0.78     0.79     0.80
## q[44]          0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[45]          0.86    0.00 0.02     0.83     0.85     0.86     0.87     0.89
## q[46]          0.75    0.00 0.01     0.72     0.74     0.75     0.76     0.78
## q[47]          0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.70
## q[48]          0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.85
## q[49]          0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.77
## q[50]          0.60    0.00 0.02     0.56     0.59     0.60     0.61     0.64
## y_pred[1]     29.07    0.06 3.26    23.00    27.00    29.00    31.00    35.00
## y_pred[2]     39.29    0.06 3.57    32.00    37.00    39.00    42.00    46.00
## y_pred[3]     25.03    0.04 2.38    20.00    23.00    25.00    27.00    29.00
## y_pred[4]     25.78    0.06 3.48    19.00    23.00    26.00    28.00    32.00
## y_pred[5]     23.85    0.04 2.62    19.00    22.00    24.00    26.00    29.00
## y_pred[6]     48.57    0.05 3.17    42.00    47.00    49.00    51.00    54.00
## y_pred[7]     37.24    0.05 3.03    31.00    35.00    37.00    39.00    43.00
## y_pred[8]     53.52    0.07 4.13    45.00    51.00    54.00    56.00    61.00
## y_pred[9]     63.60    0.06 3.57    56.00    61.00    64.00    66.00    70.00
## y_pred[10]    52.03    0.05 3.21    46.00    50.00    52.00    54.00    58.00
## y_pred[11]    23.56    0.05 2.73    18.00    22.00    24.00    25.00    29.00
## y_pred[12]    35.20    0.04 2.67    30.00    33.00    35.00    37.00    40.00
## y_pred[13]    34.12    0.06 3.61    27.00    32.00    34.00    37.00    41.00
## y_pred[14]    30.40    0.04 2.79    25.00    29.00    30.50    32.00    36.00
## y_pred[15]    42.26    0.05 3.28    36.00    40.00    42.00    45.00    48.00
## y_pred[16]    35.48    0.06 3.84    28.00    33.00    36.00    38.00    43.00
## y_pred[17]    29.07    0.04 2.72    23.00    27.00    29.00    31.00    34.00
## y_pred[18]    31.72    0.05 3.20    25.00    30.00    32.00    34.00    38.00
## y_pred[19]    38.83    0.04 2.41    34.00    37.00    39.00    41.00    43.00
## y_pred[20]    55.53    0.07 4.16    47.00    53.00    56.00    58.00    63.00
## y_pred[21]    40.00    0.07 4.41    31.00    37.00    40.00    43.00    49.00
## y_pred[22]    47.90    0.07 4.38    39.00    45.00    48.00    51.00    56.00
## y_pred[23]    38.95    0.06 4.00    31.00    36.00    39.00    42.00    46.00
## y_pred[24]    47.35    0.06 3.91    39.00    45.00    47.00    50.00    55.00
## y_pred[25]    32.06    0.05 3.41    25.00    30.00    32.00    34.00    39.00
## y_pred[26]    34.00    0.06 3.48    27.00    32.00    34.00    36.00    41.00
## y_pred[27]    22.38    0.04 2.33    17.00    21.00    22.00    24.00    27.00
## y_pred[28]    28.58    0.04 2.66    23.00    27.00    29.00    30.00    34.00
## y_pred[29]    15.06    0.03 1.56    12.00    14.00    15.00    16.00    18.00
## y_pred[30]    37.36    0.05 3.04    31.00    35.00    37.00    39.00    43.00
## y_pred[31]    55.39    0.07 4.07    47.00    53.00    56.00    58.00    63.00
## y_pred[32]     6.48    0.03 1.72     3.00     5.00     6.00     8.00    10.00
## y_pred[33]    15.74    0.03 2.23    11.00    14.00    16.00    17.00    20.00
## y_pred[34]    24.36    0.05 2.90    18.98    22.00    24.00    26.00    30.00
## y_pred[35]    46.30    0.05 3.26    40.00    44.00    46.00    49.00    52.00
## y_pred[36]    43.46    0.05 3.05    37.00    41.00    44.00    46.00    49.00
## y_pred[37]    54.13    0.08 4.80    45.00    51.00    54.00    57.00    63.00
## y_pred[38]    35.67    0.05 3.05    29.00    34.00    36.00    38.00    41.00
## y_pred[39]    15.85    0.04 2.13    11.00    14.00    16.00    17.00    20.00
## y_pred[40]    29.35    0.05 2.99    23.00    27.00    29.00    31.00    35.00
## y_pred[41]    45.02    0.05 3.19    39.00    43.00    45.00    47.00    51.00
## y_pred[42]    25.48    0.04 2.30    21.00    24.00    26.00    27.00    30.00
## y_pred[43]    41.32    0.05 3.11    35.00    39.00    41.00    43.00    47.00
## y_pred[44]    25.32    0.04 2.25    21.00    24.00    25.00    27.00    29.00
## y_pred[45]    19.79    0.03 1.74    16.00    19.00    20.00    21.00    23.00
## y_pred[46]    38.19    0.05 3.23    31.00    36.00    38.00    40.00    44.00
## y_pred[47]    14.04    0.04 2.40     9.00    12.00    14.00    16.00    19.00
## y_pred[48]    31.19    0.04 2.42    26.00    30.00    31.00    33.00    36.00
## y_pred[49]    16.95    0.03 2.12    12.00    16.00    17.00    18.00    21.00
## y_pred[50]    40.32    0.07 4.27    32.00    37.00    40.00    43.00    48.03
## lp__       -1389.42    0.04 1.21 -1392.48 -1390.00 -1389.12 -1388.51 -1387.97
##            n_eff Rhat
## b1          1238 1.01
## b2          1892 1.00
## b3          1161 1.01
## q[1]        1504 1.01
## q[2]        2126 1.00
## q[3]        2604 1.00
## q[4]        1454 1.00
## q[5]        1799 1.00
## q[6]        2443 1.00
## q[7]        2449 1.00
## q[8]        2065 1.00
## q[9]        1992 1.00
## q[10]       2036 1.00
## q[11]       2258 1.00
## q[12]       2332 1.00
## q[13]       2512 1.00
## q[14]       2449 1.00
## q[15]       2389 1.00
## q[16]       1727 1.00
## q[17]       2528 1.00
## q[18]       1667 1.00
## q[19]       1350 1.01
## q[20]       1839 1.00
## q[21]       1454 1.00
## q[22]       2083 1.00
## q[23]       1989 1.00
## q[24]       2191 1.00
## q[25]       2480 1.00
## q[26]       2618 1.00
## q[27]       2611 1.00
## q[28]       2611 1.00
## q[29]       1588 1.00
## q[30]       2504 1.00
## q[31]       1684 1.00
## q[32]       1329 1.01
## q[33]       2360 1.00
## q[34]       2628 1.00
## q[35]       2591 1.00
## q[36]       2494 1.00
## q[37]       1945 1.00
## q[38]       2419 1.00
## q[39]       1808 1.00
## q[40]       1785 1.00
## q[41]       2539 1.00
## q[42]       2443 1.00
## q[43]       2622 1.00
## q[44]       1910 1.00
## q[45]       1369 1.00
## q[46]       2260 1.00
## q[47]       1389 1.01
## q[48]       1840 1.00
## q[49]       2073 1.00
## q[50]       1727 1.00
## y_pred[1]   3373 1.00
## y_pred[2]   3405 1.00
## y_pred[3]   4073 1.00
## y_pred[4]   3876 1.00
## y_pred[5]   3753 1.00
## y_pred[6]   4028 1.00
## y_pred[7]   3672 1.00
## y_pred[8]   3782 1.00
## y_pred[9]   3937 1.00
## y_pred[10]  3629 1.00
## y_pred[11]  3662 1.00
## y_pred[12]  3836 1.00
## y_pred[13]  3918 1.00
## y_pred[14]  3986 1.00
## y_pred[15]  3819 1.00
## y_pred[16]  3824 1.00
## y_pred[17]  4008 1.00
## y_pred[18]  3713 1.00
## y_pred[19]  3338 1.00
## y_pred[20]  3292 1.00
## y_pred[21]  3463 1.00
## y_pred[22]  3943 1.00
## y_pred[23]  3884 1.00
## y_pred[24]  3653 1.00
## y_pred[25]  4050 1.00
## y_pred[26]  3863 1.00
## y_pred[27]  3952 1.00
## y_pred[28]  3686 1.00
## y_pred[29]  3824 1.00
## y_pred[30]  3953 1.00
## y_pred[31]  3577 1.00
## y_pred[32]  3957 1.00
## y_pred[33]  4101 1.00
## y_pred[34]  3888 1.00
## y_pred[35]  3968 1.00
## y_pred[36]  3967 1.00
## y_pred[37]  3709 1.00
## y_pred[38]  3809 1.00
## y_pred[39]  3665 1.00
## y_pred[40]  3418 1.00
## y_pred[41]  3991 1.00
## y_pred[42]  4050 1.00
## y_pred[43]  4147 1.00
## y_pred[44]  3604 1.00
## y_pred[45]  3786 1.00
## y_pred[46]  3640 1.00
## y_pred[47]  2938 1.00
## y_pred[48]  3762 1.00
## y_pred[49]  4067 1.00
## y_pred[50]  3501 1.00
## lp__        1107 1.00
## 
## Samples were drawn using NUTS(diag_e) at Mon May 18 16:53:57 2020.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;把獲得的參數事後樣本的均值代入上面的數學模型中可得:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{array}{l}
q[n] = \text{inv_logit}(0.09 - 0.62 A[n] + 1.90Score[n]) &amp;amp; n = 1, 2, \dots, N \\
Y[n] \sim \text{Binomial}(M[n], q[n])                 &amp;amp; n = 1, 2, \dots, N \\
\end{array}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;確認收斂效果&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;確認收斂效果&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(bayesplot)

color_scheme_set(&amp;quot;mix-brightblue-gray&amp;quot;)

posterior2 &amp;lt;- rstan::extract(fit, inc_warmup = TRUE, permuted = FALSE)

p &amp;lt;- mcmc_trace(posterior2, n_warmup = 0, pars = c(&amp;quot;b1&amp;quot;, &amp;quot;b2&amp;quot;, &amp;quot;b3&amp;quot;, &amp;quot;lp__&amp;quot;),
                facet_args = list(nrow = 2, labeller = label_parsed))
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step53&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-27-logistic-rstan_files/figure-html/step53-1.png&#34; alt=&#34;用 bayesplot包數繪製的模型5-3的MCMC鏈式軌跡圖 (trace plot)。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: 用 bayesplot包數繪製的模型5-3的MCMC鏈式軌跡圖 (trace plot)。
&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ms &amp;lt;- rstan::extract(fit)

d_qua &amp;lt;- t(apply(ms$y_pred, 2, quantile, prob=c(0.1, 0.5, 0.9)))
colnames(d_qua) &amp;lt;- c(&amp;#39;p10&amp;#39;, &amp;#39;p50&amp;#39;, &amp;#39;p90&amp;#39;)
d_qua &amp;lt;- data.frame(d, d_qua)
d_qua$A &amp;lt;- as.factor(d_qua$A)

p &amp;lt;- ggplot(data=d_qua, aes(x=Y, y=p50, ymin=p10, ymax=p90, shape=A, fill=A))
p &amp;lt;- p + theme_bw(base_size=18) + theme(legend.key.height=grid::unit(2.5,&amp;#39;line&amp;#39;))
p &amp;lt;- p + coord_fixed(ratio=1, xlim=c(5, 70), ylim=c(5, 70))
p &amp;lt;- p + geom_pointrange(size=0.8, color=&amp;#39;grey5&amp;#39;)
p &amp;lt;- p + geom_abline(aes(slope=1, intercept=0), color=&amp;#39;black&amp;#39;, alpha=3/5, linetype=&amp;#39;31&amp;#39;)
p &amp;lt;- p + scale_shape_manual(values=c(21, 24))
p &amp;lt;- p + scale_fill_manual(values=c(&amp;#39;white&amp;#39;, &amp;#39;grey70&amp;#39;))
p &amp;lt;- p + labs(x=&amp;#39;Observed&amp;#39;, y=&amp;#39;Predicted&amp;#39;)
p &amp;lt;- p + scale_x_continuous(breaks=seq(from=0, to=70, by=20))
p &amp;lt;- p + scale_y_continuous(breaks=seq(from=0, to=70, by=20))
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:fig58&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-27-logistic-rstan_files/figure-html/fig58-1.png&#34; alt=&#34;觀測值(x)，和預測值(y)的散點圖，以及預測值的80%預測區間。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: 觀測值(x)，和預測值(y)的散點圖，以及預測值的80%預測區間。
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Rstan Wonderful R-(3)</title>
      <link>https://wangcc.me/post/rstan-wonderful-r3/</link>
      <pubDate>Tue, 22 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/rstan-wonderful-r3/</guid>
      <description>
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#多重回歸-multiple-regression&#34;&gt;多重回歸 multiple regression&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#step-1.-確認數據分佈&#34;&gt;Step 1. 確認數據分佈&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-2.-寫下數學模型&#34;&gt;Step 2. 寫下數學模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-3.-看圖確認模型擬合狀況&#34;&gt;Step 3. 看圖確認模型擬合狀況&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-4.-mcmc-樣本的散點圖矩陣&#34;&gt;Step 4. MCMC 樣本的散點圖矩陣&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;多重回歸-multiple-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;多重回歸 multiple regression&lt;/h1&gt;
&lt;p&gt;本章使用的數據，大學生出勤記錄也是&lt;a href=&#34;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap05/input/data-attendance-1.txt&#34;&gt;架空的數據&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;有大學記錄了50名大學生的出勤狀況：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;A,Score,Y
0,69,0.286
1,145,0.196
0,125,0.261
1,86,0.109
1,158,0.23
0,133,0.35
0,111,0.33
1,147,0.194
0,146,0.413
0,145,0.36
1,141,0.225
0,137,0.423
1,118,0.186
0,111,0.287
...
0,99,0.268
1,99,0.234&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;: 是學生大學二年級時進行的問卷調查時回答是否喜歡打零工的結果（0:不喜歡打工；1:喜歡打工）&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Score\)&lt;/span&gt;: 是大學二年級時進行的問卷調查時計算的該學生對學習是否感興趣的數值評分(200分滿分，分數越高，該學生越熱愛學習)&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;: 是該學生一年內的出勤率&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在本次分析範例中，把&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;出勤率當作是連續型結果變量，我們來用Stan實施多重回歸分析，回答學生喜歡打零工與否，和學生對學習的熱情程度兩個變量能解釋多少出勤率。&lt;/p&gt;
&lt;div id=&#34;step-1.-確認數據分佈&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 1. 確認數據分佈&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# The following figure codes come from the authors website: 
# https://github.com/MatsuuraKentaro/RStanBook/blob/master/chap05/fig5-1.R
library(ggplot2)
library(GGally)

set.seed(123)
d &amp;lt;- read.csv(file=&amp;#39;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap05/input/data-attendance-1.txt&amp;#39;, header = T)
d$A &amp;lt;- as.factor(d$A)

N_col &amp;lt;- ncol(d)
ggp &amp;lt;- ggpairs(d, upper=&amp;#39;blank&amp;#39;, diag=&amp;#39;blank&amp;#39;, lower=&amp;#39;blank&amp;#39;)

for(i in 1:N_col) {
  x &amp;lt;- d[,i]
  p &amp;lt;- ggplot(data.frame(x, A=d$A), aes(x))
  p &amp;lt;- p + theme_bw(base_size=14)
  p &amp;lt;- p + theme(axis.text.x=element_text(angle=40, vjust=1, hjust=1))
  if (class(x) == &amp;#39;factor&amp;#39;) {
    p &amp;lt;- p + geom_bar(aes(fill=A), color=&amp;#39;grey5&amp;#39;)
  } else {
    bw &amp;lt;- (max(x)-min(x))/10
    p &amp;lt;- p + geom_histogram(binwidth=bw, aes(fill=A), color=&amp;#39;grey5&amp;#39;) #繪製柱狀圖
    p &amp;lt;- p + geom_line(eval(bquote(aes(y=..count..*.(bw)))), stat=&amp;#39;density&amp;#39;) #添加概率密度曲線
  }
  p &amp;lt;- p + geom_label(data=data.frame(x=-Inf, y=Inf, label=colnames(d)[i]), aes(x=x, y=y, label=label), hjust=0, vjust=1)
  p &amp;lt;- p + scale_fill_manual(values=alpha(c(&amp;#39;white&amp;#39;, &amp;#39;grey40&amp;#39;), 0.5))
  ggp &amp;lt;- putPlot(ggp, p, i, i)
}

zcolat &amp;lt;- seq(-1, 1, length=81)
zcolre &amp;lt;- c(zcolat[1:40]+1, rev(zcolat[41:81]))

for(i in 1:(N_col-1)) {
  for(j in (i+1):N_col) {
    x &amp;lt;- as.numeric(d[,i])
    y &amp;lt;- as.numeric(d[,j])
    r &amp;lt;- cor(x, y, method=&amp;#39;spearman&amp;#39;, use=&amp;#39;pairwise.complete.obs&amp;#39;)
    zcol &amp;lt;- lattice::level.colors(r, at=zcolat, col.regions=grey(zcolre))
    textcol &amp;lt;- ifelse(abs(r) &amp;lt; 0.4, &amp;#39;grey20&amp;#39;, &amp;#39;white&amp;#39;)
    ell &amp;lt;- ellipse::ellipse(r, level=0.95, type=&amp;#39;l&amp;#39;, npoints=50, scale=c(.2, .2), centre=c(.5, .5))
    p &amp;lt;- ggplot(data.frame(ell), aes(x=x, y=y))
    p &amp;lt;- p + theme_bw() + theme(
      plot.background=element_blank(),
      panel.grid.major=element_blank(), panel.grid.minor=element_blank(),
      panel.border=element_blank(), axis.ticks=element_blank()
    )
    p &amp;lt;- p + geom_polygon(fill=zcol, color=zcol)
    p &amp;lt;- p + geom_text(data=NULL, x=.5, y=.5, label=100*round(r, 2), size=6, col=textcol)
    ggp &amp;lt;- putPlot(ggp, p, i, j)
  }
}

for(j in 1:(N_col-1)) {
  for(i in (j+1):N_col) {
    x &amp;lt;- d[,j]
    y &amp;lt;- d[,i]
    p &amp;lt;- ggplot(data.frame(x, y, gr=d$A), aes(x=x, y=y, fill=gr, shape=gr))
    p &amp;lt;- p + theme_bw(base_size=14)
    p &amp;lt;- p + theme(axis.text.x=element_text(angle=40, vjust=1, hjust=1))
    if (class(x) == &amp;#39;factor&amp;#39;) {
      p &amp;lt;- p + geom_boxplot(aes(group=x), alpha=3/6, outlier.size=0, fill=&amp;#39;white&amp;#39;)
      p &amp;lt;- p + geom_point(position=position_jitter(w=0.4, h=0), size=2)
    } else {
      p &amp;lt;- p + geom_point(size=2)
    }
    p &amp;lt;- p + scale_shape_manual(values=c(21, 24))
    p &amp;lt;- p + scale_fill_manual(values=alpha(c(&amp;#39;white&amp;#39;, &amp;#39;grey40&amp;#39;), 0.5))
    ggp &amp;lt;- putPlot(ggp, p, i, j)
  }
}

ggp&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step1&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-22-rstan-wonderful-r-multi-lm_files/figure-html/step1-1.png&#34; alt=&#34;三個變量的分佈觀察圖，對角線上是三個變量各自的柱狀圖 (histogram) 和計算獲得的概率密度函數曲線；左下角三個圖是三個變量的箱式圖和散點圖；右上角三個圖是三個變量兩兩計算獲得的 Spearman 秩相關乘以100之後的數值。對角線上及左下角三個圖中數據點和形狀的不同分別表示學生喜歡(三角形)和不喜歡(圓形)打工。右上角表示秩相關的數值越接近0，顏色越白圖形越接近圓形，相關係數的絕對值越接近1，則顏色越深，橢圓越細長。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: 三個變量的分佈觀察圖，對角線上是三個變量各自的柱狀圖 (histogram) 和計算獲得的概率密度函數曲線；左下角三個圖是三個變量的箱式圖和散點圖；右上角三個圖是三個變量兩兩計算獲得的 Spearman 秩相關乘以100之後的數值。對角線上及左下角三個圖中數據點和形狀的不同分別表示學生喜歡(三角形)和不喜歡(圓形)打工。右上角表示秩相關的數值越接近0，顏色越白圖形越接近圓形，相關係數的絕對值越接近1，則顏色越深，橢圓越細長。
&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# png(file=&amp;#39;output/fig5-1.png&amp;#39;, w=1600, h=1600, res=300)
# print(ggp, left=0.3, bottom=0.3)
# dev.off()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;step-2.-寫下數學模型&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 2. 寫下數學模型&lt;/h2&gt;
&lt;p&gt;Model can be written as (Model5-1):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{array}{l}
Y[n]        = b_1 + b_2A[n] + b_3Sore[n] + \varepsilon [n]&amp;amp;  n = 1,2,\dots,N \\
\varepsilon[n] \sim \text{Normal}(0, \sigma) &amp;amp; n = 1,2,\dots,N \\ 
\end{array}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; 表示學生的人數，&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;則是學生編號的下標；&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(b_1\)&lt;/span&gt; 是回歸直線的截距；&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(b_2\)&lt;/span&gt; 是&lt;span class=&#34;math inline&#34;&gt;\(Score\)&lt;/span&gt;保持不變時，&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;從&lt;span class=&#34;math inline&#34;&gt;\(0\rightarrow 1\)&lt;/span&gt;時出勤率的變化(增加，或者減少)；&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(b_3\)&lt;/span&gt; 是&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;保持不變時，&lt;span class=&#34;math inline&#34;&gt;\(Score\)&lt;/span&gt;增加一個單位時出勤率的變化(增加，或者減少)。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Model can also be written as (Model5-2):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{array}{l}
Y[n]       \sim \text{Normal}(b_1 + b_2A[n] + b_3Score[n], \sigma) &amp;amp;  n = 1,2,\dots,N \\
\end{array}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;如果認爲&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(Score\)&lt;/span&gt;所能預測的出勤率有一個基礎的均值 &lt;span class=&#34;math inline&#34;&gt;\(\mu[n]\)&lt;/span&gt;，剩下的每名學生的出勤率服從這個均值和標準差爲 &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; 的正態分佈，那麼模型又可以繼續改寫成爲下面的 Model 5-3:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{array}{l}
\mu[n]        = b_1 + b_2A[n] + b_3Sore[n] &amp;amp;  n = 1,2,\dots,N \\
Y[n] \sim \text{Normal}(\mu[n], \sigma) &amp;amp; n = 1,2,\dots,N \\ 
\end{array}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;下面的 Stan 模型是按照 Model 5-3 寫的，它的模型參數有四個，&lt;span class=&#34;math inline&#34;&gt;\(b_1, b_2, b_3, \sigma\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(\mu[n]\)&lt;/span&gt;通過 &lt;code&gt;transformed parameter&lt;/code&gt; 計算獲得:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data {
  int N; 
  int&amp;lt;lower=0, upper=1&amp;gt; A[N];
  real&amp;lt;lower=0, upper=1&amp;gt; Score[N];
  real&amp;lt;lower=0, upper=1&amp;gt; Y[N];
}

parameters {
  real b1; 
  real b2;
  real b3;
  real&amp;lt;lower=0&amp;gt; sigma;
}

transformed parameters {
  real mu[N];
  for (n in 1:N) {
    mu[n] = b1 + b2*A[n] + b3*Score[n];
  }
}

model {
  for (n in 1:N) {
    Y[n] ~ normal(mu[n], sigma);
  }
}

generated quantities {
  real y_pred[N];
  for (n in 1:N) {
    y_pred[n] = normal_rng(mu[n], sigma);
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;下面的 R 代碼用來實現對上面 Stan 模型的擬合:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rstan)
d &amp;lt;- read.csv(file=&amp;#39;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap05/input/data-attendance-1.txt&amp;#39;, header = T)
data &amp;lt;- list(N=nrow(d), A=d$A, Score=d$Score/200, Y=d$Y)
fit &amp;lt;- stan(file=&amp;#39;stanfiles/model5-3.stan&amp;#39;, data=data, seed=1234)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &amp;#39;model5-3&amp;#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 1.8e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.120626 seconds (Warm-up)
## Chain 1:                0.137671 seconds (Sampling)
## Chain 1:                0.258297 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &amp;#39;model5-3&amp;#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 6e-06 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.11864 seconds (Warm-up)
## Chain 2:                0.14427 seconds (Sampling)
## Chain 2:                0.26291 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &amp;#39;model5-3&amp;#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 7e-06 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.125027 seconds (Warm-up)
## Chain 3:                0.122722 seconds (Sampling)
## Chain 3:                0.247749 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &amp;#39;model5-3&amp;#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 7e-06 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.129317 seconds (Warm-up)
## Chain 4:                0.13154 seconds (Sampling)
## Chain 4:                0.260857 seconds (Total)
## Chain 4:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Stan model: model5-3.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##              mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat
## b1           0.12    0.00 0.03   0.06   0.10   0.12   0.15   0.19  1803    1
## b2          -0.14    0.00 0.01  -0.17  -0.15  -0.14  -0.13  -0.11  2486    1
## b3           0.32    0.00 0.05   0.22   0.29   0.33   0.36   0.43  1751    1
## sigma        0.05    0.00 0.01   0.04   0.05   0.05   0.05   0.06  2328    1
## mu[1]        0.24    0.00 0.02   0.20   0.22   0.24   0.25   0.27  2014    1
## mu[2]        0.22    0.00 0.01   0.19   0.21   0.22   0.22   0.24  2589    1
## mu[3]        0.33    0.00 0.01   0.31   0.32   0.33   0.33   0.35  3512    1
## mu[4]        0.12    0.00 0.02   0.09   0.11   0.12   0.13   0.15  2338    1
## mu[5]        0.24    0.00 0.02   0.21   0.23   0.24   0.25   0.27  2333    1
## mu[6]        0.34    0.00 0.01   0.32   0.33   0.34   0.35   0.36  3369    1
## mu[7]        0.30    0.00 0.01   0.28   0.30   0.30   0.31   0.32  3110    1
## mu[8]        0.22    0.00 0.01   0.19   0.21   0.22   0.23   0.24  2544    1
## mu[9]        0.36    0.00 0.01   0.34   0.35   0.36   0.37   0.38  2891    1
## mu[10]       0.36    0.00 0.01   0.34   0.35   0.36   0.37   0.38  2926    1
## mu[11]       0.21    0.00 0.01   0.18   0.20   0.21   0.22   0.23  2683    1
## mu[12]       0.35    0.00 0.01   0.33   0.34   0.35   0.35   0.37  3228    1
## mu[13]       0.17    0.00 0.01   0.15   0.16   0.17   0.18   0.19  2964    1
## mu[14]       0.30    0.00 0.01   0.28   0.30   0.30   0.31   0.32  3110    1
## mu[15]       0.30    0.00 0.01   0.28   0.29   0.30   0.31   0.32  3017    1
## mu[16]       0.14    0.00 0.01   0.12   0.13   0.14   0.15   0.17  2577    1
## mu[17]       0.31    0.00 0.01   0.29   0.30   0.31   0.31   0.33  3244    1
## mu[18]       0.26    0.00 0.01   0.23   0.25   0.26   0.27   0.28  2168    1
## mu[19]       0.42    0.00 0.02   0.38   0.41   0.42   0.44   0.46  2180    1
## mu[20]       0.23    0.00 0.01   0.20   0.22   0.23   0.24   0.26  2367    1
## mu[21]       0.12    0.00 0.02   0.09   0.11   0.12   0.13   0.15  2338    1
## mu[22]       0.16    0.00 0.01   0.13   0.15   0.16   0.16   0.18  2789    1
## mu[23]       0.15    0.00 0.01   0.13   0.14   0.15   0.16   0.18  2743    1
## mu[24]       0.21    0.00 0.01   0.19   0.20   0.21   0.22   0.24  2635    1
## mu[25]       0.17    0.00 0.01   0.15   0.16   0.17   0.18   0.19  2953    1
## mu[26]       0.19    0.00 0.01   0.16   0.18   0.19   0.20   0.21  2961    1
## mu[27]       0.32    0.00 0.01   0.30   0.31   0.32   0.32   0.34  3426    1
## mu[28]       0.32    0.00 0.01   0.30   0.31   0.32   0.32   0.34  3426    1
## mu[29]       0.38    0.00 0.01   0.36   0.38   0.39   0.39   0.41  2484    1
## mu[30]       0.31    0.00 0.01   0.29   0.30   0.31   0.31   0.33  3200    1
## mu[31]       0.25    0.00 0.02   0.22   0.24   0.25   0.26   0.28  2232    1
## mu[32]       0.10    0.00 0.02   0.07   0.09   0.10   0.11   0.14  2185    1
## mu[33]       0.20    0.00 0.01   0.18   0.20   0.20   0.21   0.23  2754    1
## mu[34]       0.18    0.00 0.01   0.16   0.17   0.18   0.19   0.20  2989    1
## mu[35]       0.33    0.00 0.01   0.31   0.32   0.33   0.33   0.35  3509    1
## mu[36]       0.34    0.00 0.01   0.32   0.33   0.34   0.34   0.36  3427    1
## mu[37]       0.15    0.00 0.01   0.13   0.14   0.15   0.16   0.18  2719    1
## mu[38]       0.30    0.00 0.01   0.28   0.30   0.30   0.31   0.32  3064    1
## mu[39]       0.27    0.00 0.01   0.24   0.26   0.27   0.28   0.29  2306    1
## mu[40]       0.27    0.00 0.01   0.24   0.26   0.27   0.27   0.29  2283    1
## mu[41]       0.33    0.00 0.01   0.31   0.33   0.33   0.34   0.35  3472    1
## mu[42]       0.34    0.00 0.01   0.32   0.33   0.34   0.35   0.36  3369    1
## mu[43]       0.32    0.00 0.01   0.30   0.32   0.32   0.33   0.34  3491    1
## mu[44]       0.36    0.00 0.01   0.34   0.36   0.36   0.37   0.39  2823    1
## mu[45]       0.42    0.00 0.02   0.38   0.41   0.42   0.43   0.45  2204    1
## mu[46]       0.29    0.00 0.01   0.27   0.29   0.29   0.30   0.31  2838    1
## mu[47]       0.21    0.00 0.02   0.17   0.19   0.21   0.22   0.25  1910    1
## mu[48]       0.37    0.00 0.01   0.34   0.36   0.37   0.38   0.39  2759    1
## mu[49]       0.28    0.00 0.01   0.26   0.28   0.28   0.29   0.31  2603    1
## mu[50]       0.14    0.00 0.01   0.12   0.13   0.14   0.15   0.17  2577    1
## y_pred[1]    0.23    0.00 0.05   0.13   0.20   0.23   0.27   0.34  3730    1
## y_pred[2]    0.22    0.00 0.05   0.11   0.18   0.22   0.25   0.32  3977    1
## y_pred[3]    0.32    0.00 0.05   0.23   0.29   0.32   0.36   0.43  3856    1
## y_pred[4]    0.12    0.00 0.05   0.01   0.08   0.12   0.15   0.22  3896    1
## y_pred[5]    0.24    0.00 0.05   0.13   0.20   0.24   0.27   0.34  3804    1
## y_pred[6]    0.34    0.00 0.05   0.24   0.31   0.34   0.38   0.44  3865    1
## y_pred[7]    0.30    0.00 0.05   0.20   0.27   0.30   0.34   0.41  3757    1
## y_pred[8]    0.22    0.00 0.05   0.11   0.18   0.22   0.25   0.33  3784    1
## y_pred[9]    0.36    0.00 0.05   0.26   0.33   0.36   0.40   0.47  3939    1
## y_pred[10]   0.36    0.00 0.05   0.25   0.32   0.36   0.39   0.46  3743    1
## y_pred[11]   0.21    0.00 0.05   0.10   0.17   0.21   0.25   0.32  4113    1
## y_pred[12]   0.35    0.00 0.05   0.24   0.31   0.35   0.38   0.45  4149    1
## y_pred[13]   0.17    0.00 0.05   0.07   0.14   0.17   0.21   0.28  3806    1
## y_pred[14]   0.30    0.00 0.05   0.20   0.27   0.30   0.34   0.41  3944    1
## y_pred[15]   0.30    0.00 0.05   0.20   0.27   0.30   0.34   0.41  3930    1
## y_pred[16]   0.14    0.00 0.05   0.03   0.11   0.14   0.18   0.24  3833    1
## y_pred[17]   0.31    0.00 0.05   0.21   0.27   0.31   0.34   0.41  3981    1
## y_pred[18]   0.26    0.00 0.05   0.15   0.22   0.26   0.29   0.37  3718    1
## y_pred[19]   0.43    0.00 0.06   0.32   0.39   0.43   0.46   0.53  3690    1
## y_pred[20]   0.23    0.00 0.05   0.13   0.20   0.23   0.27   0.34  2926    1
## y_pred[21]   0.12    0.00 0.05   0.01   0.08   0.12   0.15   0.23  3751    1
## y_pred[22]   0.16    0.00 0.05   0.05   0.12   0.16   0.19   0.26  3993    1
## y_pred[23]   0.15    0.00 0.05   0.04   0.11   0.15   0.19   0.26  3990    1
## y_pred[24]   0.21    0.00 0.05   0.11   0.17   0.21   0.25   0.32  3766    1
## y_pred[25]   0.17    0.00 0.05   0.06   0.13   0.17   0.21   0.27  3800    1
## y_pred[26]   0.19    0.00 0.05   0.08   0.15   0.19   0.22   0.30  3781    1
## y_pred[27]   0.32    0.00 0.05   0.22   0.28   0.32   0.35   0.42  4073    1
## y_pred[28]   0.32    0.00 0.05   0.21   0.28   0.32   0.35   0.42  4155    1
## y_pred[29]   0.38    0.00 0.05   0.28   0.35   0.38   0.42   0.49  3869    1
## y_pred[30]   0.31    0.00 0.05   0.20   0.27   0.31   0.34   0.41  3708    1
## y_pred[31]   0.25    0.00 0.06   0.14   0.21   0.25   0.29   0.35  4005    1
## y_pred[32]   0.10    0.00 0.06  -0.01   0.06   0.10   0.14   0.21  3479    1
## y_pred[33]   0.20    0.00 0.05   0.10   0.17   0.20   0.24   0.31  3656    1
## y_pred[34]   0.18    0.00 0.05   0.08   0.15   0.18   0.22   0.28  4111    1
## y_pred[35]   0.33    0.00 0.05   0.22   0.29   0.33   0.36   0.43  3866    1
## y_pred[36]   0.34    0.00 0.05   0.23   0.30   0.34   0.37   0.44  4043    1
## y_pred[37]   0.15    0.00 0.05   0.04   0.12   0.15   0.19   0.25  3830    1
## y_pred[38]   0.30    0.00 0.05   0.20   0.27   0.30   0.34   0.41  3814    1
## y_pred[39]   0.27    0.00 0.05   0.16   0.23   0.27   0.30   0.37  3792    1
## y_pred[40]   0.27    0.00 0.05   0.16   0.23   0.27   0.30   0.37  3793    1
## y_pred[41]   0.33    0.00 0.05   0.23   0.30   0.33   0.37   0.44  3748    1
## y_pred[42]   0.34    0.00 0.05   0.23   0.30   0.34   0.38   0.44  3811    1
## y_pred[43]   0.32    0.00 0.05   0.22   0.29   0.32   0.35   0.42  4268    1
## y_pred[44]   0.36    0.00 0.05   0.26   0.33   0.36   0.40   0.47  4015    1
## y_pred[45]   0.42    0.00 0.05   0.31   0.38   0.42   0.45   0.53  3658    1
## y_pred[46]   0.29    0.00 0.05   0.19   0.26   0.29   0.33   0.40  3993    1
## y_pred[47]   0.21    0.00 0.06   0.10   0.17   0.21   0.24   0.32  3671    1
## y_pred[48]   0.37    0.00 0.05   0.26   0.33   0.37   0.40   0.47  3988    1
## y_pred[49]   0.28    0.00 0.05   0.18   0.25   0.28   0.32   0.39  3596    1
## y_pred[50]   0.14    0.00 0.05   0.03   0.10   0.14   0.18   0.24  4027    1
## lp__       120.85    0.04 1.43 117.36 120.12 121.19 121.90 122.68  1610    1
## 
## Samples were drawn using NUTS(diag_e) at Mon May 18 16:52:55 2020.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;上述代碼中值得注意的是我們對 &lt;span class=&#34;math inline&#34;&gt;\(Score\)&lt;/span&gt; 進行了全部除以 &lt;span class=&#34;math inline&#34;&gt;\(200\)&lt;/span&gt; 的數據縮放調整 (scaling)。這樣有助於我們的模型在進行 MCMC 計算時加速其達到收斂時所需要的時間。&lt;/p&gt;
&lt;p&gt;把計算獲得的事後模型參數平均值代入模型 Model 5-3:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{array}{l}
\mu[n]        = 0.12 - 0.14A[n] + 0.32Sore[n] &amp;amp;  n = 1,2,\dots,N \\
Y[n] \sim \text{Normal}(\mu[n], 0.05) &amp;amp; n = 1,2,\dots,N \\ 
\end{array}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;從輸出的結果報告來看，所有的 &lt;code&gt;Rhat&lt;/code&gt; 都小於1.1，可以認爲採樣已經達到收斂效果，再來確認一下軌跡圖：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(bayesplot)

color_scheme_set(&amp;quot;mix-brightblue-gray&amp;quot;)

posterior2 &amp;lt;- rstan::extract(fit, inc_warmup = TRUE, permuted = FALSE)

p &amp;lt;- mcmc_trace(posterior2, n_warmup = 0, pars = c(&amp;quot;b1&amp;quot;, &amp;quot;b2&amp;quot;, &amp;quot;b3&amp;quot;, &amp;quot;sigma&amp;quot;, &amp;quot;lp__&amp;quot;),
                facet_args = list(nrow = 2, labeller = label_parsed))
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step53&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-22-rstan-wonderful-r-multi-lm_files/figure-html/step53-1.png&#34; alt=&#34;用 bayesplot包數繪製的模型5-3的MCMC鏈式軌跡圖 (trace plot)。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: 用 bayesplot包數繪製的模型5-3的MCMC鏈式軌跡圖 (trace plot)。
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;收斂效果很不錯，下面來解釋回歸係數的事後均值的涵義：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;b3&lt;/code&gt;的事後均值是&lt;span class=&#34;math inline&#34;&gt;\(0.32\)&lt;/span&gt;，所以，&lt;span class=&#34;math inline&#34;&gt;\(Score=150\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(Score=50\)&lt;/span&gt;的兩名學生，當他們同時都是喜歡或者同時都不喜歡打工時，&lt;span class=&#34;math inline&#34;&gt;\(Score = 150\)&lt;/span&gt;的學生的出勤率平均比 &lt;span class=&#34;math inline&#34;&gt;\(Score = 50\)&lt;/span&gt; 的學生的出勤率高 &lt;span class=&#34;math inline&#34;&gt;\(0.32 \times (150-50)/200 = 0.16\)&lt;/span&gt;。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;b2&lt;/code&gt;的事後均值是&lt;span class=&#34;math inline&#34;&gt;\(-0.14\)&lt;/span&gt;，所以，同樣地，&lt;span class=&#34;math inline&#34;&gt;\(Score\)&lt;/span&gt;相同的兩名學生，喜歡打工的學生比不喜歡打工的學生出勤率平均要低 &lt;span class=&#34;math inline&#34;&gt;\(0.14\)&lt;/span&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;step-3.-看圖確認模型擬合狀況&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 3. 看圖確認模型擬合狀況&lt;/h2&gt;
&lt;p&gt;下圖繪製了上面貝葉斯多重線性回歸模型計算獲得的事後貝葉斯預測區間，和觀測值&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;出勤率之間的直觀關係：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;source(&amp;quot;commonRstan.R&amp;quot;)

ms &amp;lt;- rstan::extract(fit)

Score_new &amp;lt;- 50:200
N_X &amp;lt;- length(Score_new)
N_mcmc &amp;lt;- length(ms$lp__)

set.seed(1234)
y_base_mcmc &amp;lt;- as.data.frame(matrix(nrow=N_mcmc, ncol=N_X))
y_base_a0_mcmc &amp;lt;- as.data.frame(matrix(nrow = N_mcmc, ncol = N_X))
y_mcmc &amp;lt;- as.data.frame(matrix(nrow=N_mcmc, ncol=N_X))
y_a0_mcmc &amp;lt;- as.data.frame(matrix(nrow=N_mcmc, ncol=N_X))

for (i in 1:N_X) {
  y_base_mcmc[,i] &amp;lt;- ms$b1 + ms$b2 + ms$b3 * Score_new[i]/200
  y_base_a0_mcmc[] &amp;lt;- ms$b1 + ms$b2*0 + ms$b3 * Score_new[i]/200
  y_mcmc[,i] &amp;lt;- rnorm(n=N_mcmc, mean=y_base_mcmc[,i], sd=ms$sigma)
  y_a0_mcmc[,i] &amp;lt;- rnorm(n=N_mcmc, mean=y_base_a0_mcmc[,i], sd=ms$sigma)
}

customize.ggplot.axis &amp;lt;- function(p) {
  p &amp;lt;- p + labs(x=&amp;#39;Score&amp;#39;, y=&amp;#39;Y&amp;#39;)
  p &amp;lt;- p + scale_y_continuous(breaks=seq(from=-0.2, to=0.8, by=0.2))
  p &amp;lt;- p + coord_cartesian(xlim=c(50, 200), ylim=c(-0.2, 0.6))
  return(p)
}

d_est &amp;lt;- data.frame.quantile.mcmc(x=Score_new, y_mcmc=y_mcmc)
d_esta0 &amp;lt;- data.frame.quantile.mcmc(x=Score_new, y_mcmc=y_a0_mcmc)
# p &amp;lt;- ggplot.5quantile(data=d_est)
# p2 &amp;lt;- ggplot.5quantile(data = d_esta0)
# p &amp;lt;- p + geom_point(data=d[d$A==1, ], aes(x=Score, y=Y), shape=24, size=5)
# p2 &amp;lt;- p2 + geom_point(data=d[d$A==0, ], aes(x=Score, y=Y), shape=1, size=5)
# p &amp;lt;- customize.ggplot.axis(p)
# p2 &amp;lt;- customize.ggplot.axis(p2)

visuals = rbind(d_est,d_esta0)
visuals$A=c(rep(1,151),rep(0,151)) # 151 points of each flavour

qn &amp;lt;- colnames(visuals)[-1]
p &amp;lt;- ggplot(data=visuals, aes(x=X, y=p50, group = A))
p &amp;lt;- p + my_theme()
p &amp;lt;- p + geom_ribbon(aes_string(ymin=qn[1], ymax=qn[5]), fill=&amp;#39;black&amp;#39;, alpha=1/6)
p &amp;lt;- p + geom_ribbon(aes_string(ymin=qn[2], ymax=qn[4]), fill=&amp;#39;black&amp;#39;, alpha=2/6)
p &amp;lt;- p + geom_line(size=1)
p &amp;lt;- p + geom_point(data=d[d$A==1, ], aes(x=Score, y=Y), shape=24, size=5)
p &amp;lt;- p + geom_point(data=d[d$A==0, ], aes(x=Score, y=Y), shape=20, size=5)
p &amp;lt;- customize.ggplot.axis(p)
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:fig52&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-22-rstan-wonderful-r-multi-lm_files/figure-html/fig52-1.png&#34; alt=&#34;黑色原點(不喜歡打工)，和無色三角形(喜歡打工)的學生的出勤率，和模型計算獲得的貝葉斯事後預測區間。黑色線是中位數，灰色帶是50%預測區間和95%預測區間。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: 黑色原點(不喜歡打工)，和無色三角形(喜歡打工)的學生的出勤率，和模型計算獲得的貝葉斯事後預測區間。黑色線是中位數，灰色帶是50%預測區間和95%預測區間。
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;上述觀察預測值區間和實際觀測之間的關係的視覺化圖形，在多重線性回歸模型只有兩個預測變量的事後還較爲容易獲得，當模型中有三個或以上的預測變量時，可視化變得困難重重。&lt;/p&gt;
&lt;p&gt;此時我們推薦繪製“實際觀測值和預測值”，以及模型給出的每個預測值的隨機誤差&lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt;分佈範圍，相結合的圖形來判斷模型擬合程度。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d_qua &amp;lt;- t(apply(ms$y_pred, 2, quantile, prob=c(0.1, 0.5, 0.9)))
colnames(d_qua) &amp;lt;- c(&amp;#39;p10&amp;#39;, &amp;#39;p50&amp;#39;, &amp;#39;p90&amp;#39;)
d_qua &amp;lt;- data.frame(d, d_qua)
d_qua$A &amp;lt;- as.factor(d_qua$A)

p &amp;lt;- ggplot(data=d_qua, aes(x=Y, y=p50, ymin=p10, ymax=p90, shape=A, fill=A))
p &amp;lt;- p + theme_bw(base_size=18) + theme(legend.key.height=grid::unit(2.5,&amp;#39;line&amp;#39;))
p &amp;lt;- p + coord_fixed(ratio=1, xlim=c(0, 0.5), ylim=c(0, 0.5))
p &amp;lt;- p + geom_pointrange(size=0.8, color=&amp;#39;grey5&amp;#39;)
p &amp;lt;- p + geom_abline(aes(slope=1, intercept=0), color=&amp;#39;black&amp;#39;, alpha=3/5, linetype=&amp;#39;31&amp;#39;)
p &amp;lt;- p + scale_shape_manual(values=c(21, 24))
p &amp;lt;- p + scale_fill_manual(values=c(&amp;#39;white&amp;#39;, &amp;#39;grey70&amp;#39;))
p &amp;lt;- p + labs(x=&amp;#39;Observed&amp;#39;, y=&amp;#39;Predicted&amp;#39;)
p &amp;lt;- p + scale_x_continuous(breaks=seq(from=0, to=0.5, by=0.1))
p &amp;lt;- p + scale_y_continuous(breaks=seq(from=0, to=0.5, by=0.1))
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:fig53&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-22-rstan-wonderful-r-multi-lm_files/figure-html/fig53-1.png&#34; alt=&#34;觀測值(x)，和預測值(y)的散點圖，以及預測值的80%預測區間。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4: 觀測值(x)，和預測值(y)的散點圖，以及預測值的80%預測區間。
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;從上圖中可以看出，大多數的觀測點和預測點以及預測的80%區間基本都在 &lt;span class=&#34;math inline&#34;&gt;\(y = x\)&lt;/span&gt; 這條對角線上。大致可以認爲本次貝葉斯多重線性回歸擬合效果尚且能夠接受。&lt;/p&gt;
&lt;p&gt;隨機誤差 &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon[n]\)&lt;/span&gt; 被認爲服從 &lt;span class=&#34;math inline&#34;&gt;\(\text{Normal}(0, \sigma)\)&lt;/span&gt; 的正態分佈。從模型中可以計算獲得每個學生出勤率的預測值和實際觀測值之間的差，這就是隨機誤差。貝葉斯框架之下，我們實際獲得的會是每名學生隨機誤差的分佈：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;N_mcmc &amp;lt;- length(ms$lp__)

d_noise &amp;lt;- data.frame(t(-t(ms$mu) + d$Y))
colnames(d_noise) &amp;lt;- paste0(&amp;#39;noise&amp;#39;, 1:nrow(d))
d_est &amp;lt;- data.frame(mcmc=1:N_mcmc, d_noise)
d_melt &amp;lt;- reshape2::melt(d_est, id=c(&amp;#39;mcmc&amp;#39;), variable.name=&amp;#39;X&amp;#39;)

d_mode &amp;lt;- data.frame(t(apply(d_noise, 2, function(x) {
  dens &amp;lt;- density(x)
  mode_i &amp;lt;- which.max(dens$y)
  mode_x &amp;lt;- dens$x[mode_i]
  mode_y &amp;lt;- dens$y[mode_i]
  c(mode_x, mode_y)
})))
colnames(d_mode) &amp;lt;- c(&amp;#39;X&amp;#39;, &amp;#39;Y&amp;#39;)

p &amp;lt;- ggplot()
p &amp;lt;- p + theme_bw(base_size=18)
p &amp;lt;- p + geom_line(data=d_melt, aes(x=value, group=X), stat=&amp;#39;density&amp;#39;, color=&amp;#39;black&amp;#39;, alpha=0.4)
p &amp;lt;- p + geom_segment(data=d_mode, aes(x=X, xend=X, y=Y, yend=0), color=&amp;#39;black&amp;#39;, linetype=&amp;#39;dashed&amp;#39;, alpha=0.4)
p &amp;lt;- p + geom_rug(data=d_mode, aes(x=X), sides=&amp;#39;b&amp;#39;)
p &amp;lt;- p + labs(x=&amp;#39;value&amp;#39;, y=&amp;#39;density&amp;#39;)
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:fig54left&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-22-rstan-wonderful-r-multi-lm_files/figure-html/fig54left-1.png&#34; alt=&#34;每名學生的出勤率隨機誤差的分佈&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 5: 每名學生的出勤率隨機誤差的分佈
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;實際上我們只需要選取每名學生模型計算獲得的事後隨機誤差的代表值，比如可以是平均值，中央值，或者是MAP值（事後確率最大推定値，maximum a posteriori estimate），來觀察就可以了：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s_dens &amp;lt;- density(ms$s)
s_MAP &amp;lt;- s_dens$x[which.max(s_dens$y)]
bw &amp;lt;- 0.01
p &amp;lt;- ggplot(data=d_mode, aes(x=X))
p &amp;lt;- p + theme_bw(base_size=18)
p &amp;lt;- p + geom_histogram(binwidth=bw, color=&amp;#39;black&amp;#39;, fill=&amp;#39;white&amp;#39;)
p &amp;lt;- p + geom_density(eval(bquote(aes(y=..count..*.(bw)))), alpha=0.5, color=&amp;#39;black&amp;#39;, fill=&amp;#39;gray20&amp;#39;)
p &amp;lt;- p + stat_function(fun=function(x) nrow(d)*bw*dnorm(x, mean=0, sd=s_MAP), linetype=&amp;#39;dashed&amp;#39;)
p &amp;lt;- p + labs(x=&amp;#39;value&amp;#39;, y=&amp;#39;count&amp;#39;)
p &amp;lt;- p + xlim(range(density(d_mode$X)$x))
p&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 2 rows containing missing values (geom_bar).&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:fig54right&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-22-rstan-wonderful-r-multi-lm_files/figure-html/fig54right-1.png&#34; alt=&#34;每名學生事後出勤率隨機誤差的MAP值的柱狀圖，和相應的概率密度函數（灰色鐘罩），點狀虛線是均值爲0，標準差是模型計算的事後隨機誤差標準差的 MAP 值的正態分佈的形狀。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 6: 每名學生事後出勤率隨機誤差的MAP值的柱狀圖，和相應的概率密度函數（灰色鐘罩），點狀虛線是均值爲0，標準差是模型計算的事後隨機誤差標準差的 MAP 值的正態分佈的形狀。
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;step-4.-mcmc-樣本的散點圖矩陣&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 4. MCMC 樣本的散點圖矩陣&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(GGally)
library(hexbin)


d &amp;lt;- data.frame(b1=ms$b1, b2=ms$b2, b3=ms$b3, sigma=ms$sigma, mu1=ms$mu[,1], mu50=ms$mu[,50], lp__=ms$lp__)
N_col &amp;lt;- ncol(d)
ggp &amp;lt;- ggpairs(d, upper=&amp;#39;blank&amp;#39;, diag=&amp;#39;blank&amp;#39;, lower=&amp;#39;blank&amp;#39;)

label_list &amp;lt;- list(b1=&amp;#39;b1&amp;#39;, b2=&amp;#39;b2&amp;#39;, b3=&amp;#39;b3&amp;#39;, sigma=&amp;#39;sigma&amp;#39;, mu1=&amp;#39;mu[1]&amp;#39;, mu50=&amp;#39;mu[50]&amp;#39;, lp__=&amp;#39;lp__&amp;#39;)
for(i in 1:N_col) {
  x &amp;lt;- d[,i]
  bw &amp;lt;- (max(x)-min(x))/10
  p &amp;lt;- ggplot(data.frame(x), aes(x))
  p &amp;lt;- p + theme_bw(base_size=14)
  p &amp;lt;- p + theme(axis.text.x=element_text(angle=60, vjust=1, hjust=1))
  p &amp;lt;- p + geom_histogram(binwidth=bw, fill=&amp;#39;white&amp;#39;, color=&amp;#39;grey5&amp;#39;)
  p &amp;lt;- p + geom_line(eval(bquote(aes(y=..count..*.(bw)))), stat=&amp;#39;density&amp;#39;)
  p &amp;lt;- p + geom_label(data=data.frame(x=-Inf, y=Inf, label=label_list[[colnames(d)[i]]]), aes(x=x, y=y, label=label), hjust=0, vjust=1)
  ggp &amp;lt;- putPlot(ggp, p, i, i)
}

zcolat &amp;lt;- seq(-1, 1, length=81)
zcolre &amp;lt;- c(zcolat[1:40]+1, rev(zcolat[41:81]))

for(i in 1:(N_col-1)) {
  for(j in (i+1):N_col) {
    x &amp;lt;- as.numeric(d[,i])
    y &amp;lt;- as.numeric(d[,j])
    r &amp;lt;- cor(x, y, method=&amp;#39;spearman&amp;#39;, use=&amp;#39;pairwise.complete.obs&amp;#39;)
    zcol &amp;lt;- lattice::level.colors(r, at=zcolat, col.regions=grey(zcolre))
    textcol &amp;lt;- ifelse(abs(r) &amp;lt; 0.4, &amp;#39;grey20&amp;#39;, &amp;#39;white&amp;#39;)
    ell &amp;lt;- ellipse::ellipse(r, level=0.95, type=&amp;#39;l&amp;#39;, npoints=50, scale=c(.2, .2), centre=c(.5, .5))
    p &amp;lt;- ggplot(data.frame(ell), aes(x=x, y=y))
    p &amp;lt;- p + theme_bw() + theme(
      plot.background=element_blank(),
      panel.grid.major=element_blank(), panel.grid.minor=element_blank(),
      panel.border=element_blank(), axis.ticks=element_blank()
    )
    p &amp;lt;- p + geom_polygon(fill=zcol, color=zcol)
    p &amp;lt;- p + geom_text(data=NULL, x=.5, y=.5, label=100*round(r, 2), size=6, col=textcol)
    ggp &amp;lt;- putPlot(ggp, p, i, j)
  }
}

for(j in 1:(N_col-1)) {
  for(i in (j+1):N_col) {
    x &amp;lt;- d[,j]
    y &amp;lt;- d[,i]
    p &amp;lt;- ggplot(data.frame(x, y), aes(x=x, y=y))
    p &amp;lt;- p + theme_bw(base_size=14)
    p &amp;lt;- p + theme(axis.text.x=element_text(angle=60, vjust=1, hjust=1))
    p &amp;lt;- p + geom_hex()
    p &amp;lt;- p + scale_fill_gradientn(colours=gray.colors(7, start=0.1, end=0.9))
    ggp &amp;lt;- putPlot(ggp, p, i, j)
  }
}
ggp&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:fig55&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-22-rstan-wonderful-r-multi-lm_files/figure-html/fig55-1.png&#34; alt=&#34;MCMC樣本的事後矩陣。對角線上是各個參數事後樣本的柱狀圖和相應的概率密度曲線。右上角是各個參數事後樣本之間的 Spearman 秩相關係數。左下角是兩兩的散點圖。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 7: MCMC樣本的事後矩陣。對角線上是各個參數事後樣本的柱狀圖和相應的概率密度曲線。右上角是各個參數事後樣本之間的 Spearman 秩相關係數。左下角是兩兩的散點圖。
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Simple linear regression using Rstan--Rstan Wonderful R-(2)</title>
      <link>https://wangcc.me/post/simple-linear-regression-using-rstan/</link>
      <pubDate>Tue, 15 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/simple-linear-regression-using-rstan/</guid>
      <description>
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#step-1-確認數據分佈&#34;&gt;Step 1, 確認數據分佈&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-2-描述線性模型&#34;&gt;Step 2, 描述線性模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-3-寫下stan模型&#34;&gt;Step 3, 寫下Stan模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-4-診斷stan貝葉斯模型的收斂程度&#34;&gt;Step 4, 診斷Stan貝葉斯模型的收斂程度&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-5修改mcmc條件設定&#34;&gt;Step 5，修改MCMC條件設定&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-6-並行平行計算的設定&#34;&gt;Step 6, 並行（平行）計算的設定&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-7-計算貝葉斯可信區間和貝葉斯預測區間&#34;&gt;Step 7, 計算貝葉斯可信區間和貝葉斯預測區間&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#練習題&#34;&gt;練習題&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap04/input/data-salary.txt&#34;&gt;數據 data-salary.txt&lt;/a&gt;是架空的。&lt;/p&gt;
&lt;p&gt;某公司社員的年齡 &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;（歲），和年收入 &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;（萬日元）的數據如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;X,Y
24,472
24,403
26,454
32,575
33,546
35,781
38,750
40,601
40,814
43,792
43,745
44,837
48,868
52,988
56,1092
56,1007
57,1233
58,1202
59,1123
59,1314
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;年收入 &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; 被認爲是由基本年收 &lt;span class=&#34;math inline&#34;&gt;\(y_{base}\)&lt;/span&gt; 和其他影響因素 &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt; 構成。由於該公司是典型的年功序列式的日本傳統企業，所以基本年收本身和社員年齡成正比例。 &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt; 則被認爲是由該員工當年的業績等隨機誤差造成的，但是所有員工的 &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt; 的均值被認爲是零。&lt;/p&gt;
&lt;p&gt;g分析目的：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;借用這個數據來分析並回答如下的問題：在該公司如果採用了一名50歲的員工，他/她的年收入的預期值會是多少。&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;step-1-確認數據分佈&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 1, 確認數據分佈&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Salary &amp;lt;- read.table(&amp;quot;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap04/input/data-salary.txt&amp;quot;, 
                     sep = &amp;quot;,&amp;quot;, header = T)
library(ggplot2)

ggplot(Salary, aes(x = X, y = Y)) + 
  geom_point(shape = 1, size = 4)  + theme(plot.subtitle = element_text(vjust = 1), 
    plot.caption = element_text(vjust = 1), 
    axis.line = element_line(colour = &amp;quot;bisque4&amp;quot;, 
        size = 0.2, linetype = &amp;quot;solid&amp;quot;), 
    axis.ticks = element_line(size = 0.7), 
    axis.title = element_text(size = 16), 
    axis.text = element_text(size = 16, colour = &amp;quot;gray0&amp;quot;), 
    panel.background = element_rect(fill = &amp;quot;gray98&amp;quot;)) +
  scale_y_continuous(limits = c(200, 1400), breaks = c(200, 600, 1000, 1400))&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step1&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-15-simple-linear-regression-using-rstan_files/figure-html/step1-1.png&#34; alt=&#34;橫軸爲 $X$，縱軸爲 $Y$ 的散點圖&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: 橫軸爲 &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;，縱軸爲 &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; 的散點圖
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;從這個散點圖的特徵可以看出年收入確實似乎和年齡呈線性正相關。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-2-描述線性模型&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 2, 描述線性模型&lt;/h2&gt;
&lt;p&gt;這個簡單線性回歸模型的數學表達式可以描述如下：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{array}{l}
Y[n]        = y_{base}[n] + \varepsilon [n]&amp;amp;  n = 1,2,\dots,N \\
y_{base}[n] = a + bX[n]                    &amp;amp;  n = 1,2,\dots,N \\
\varepsilon[n] \sim \text{Normal}(0, \sigma) &amp;amp; n = 1,2,\dots,N \\ 
\end{array}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;同樣的模型你可以簡化描述成爲：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y[n] \sim \text{Normal}(a + bX[n], \sigma)\;\; n = 1,2,\dots,N
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;那麼如果一個統計師只有經過傳統概率論觀點的訓練，他/她會在R裏面這樣來分析這個數據：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res_lm &amp;lt;- lm(Y ~ X, data = Salary)
summary(res_lm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Y ~ X, data = Salary)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -155.471  -51.523   -6.663   52.822  141.349 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) -119.697     68.148  -1.756    0.096 .  
## X             21.904      1.518  14.428 2.47e-11 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 79.1 on 18 degrees of freedom
## Multiple R-squared:  0.9204, Adjusted R-squared:  0.916 
## F-statistic: 208.2 on 1 and 18 DF,  p-value: 2.466e-11&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 用這個線性回歸模型來對上面模型中的參數作出預測：

X_new &amp;lt;- data.frame(X=23:60)
conf_95 &amp;lt;- predict(res_lm, X_new, interval = &amp;quot;confidence&amp;quot;, level = 0.95)
pred_95 &amp;lt;- predict(res_lm, X_new, interval = &amp;quot;prediction&amp;quot;, level = 0.95)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;temp_var &amp;lt;- predict(res_lm, interval=&amp;quot;prediction&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in predict.lm(res_lm, interval = &amp;quot;prediction&amp;quot;): predictions on current data refer to _future_ responses&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_df &amp;lt;- cbind(Salary, temp_var)

ggplot(new_df, aes(x = X, y = Y)) + 
  geom_point(shape = 1, size = 4)  + theme(plot.subtitle = element_text(vjust = 1), 
    plot.caption = element_text(vjust = 1), 
    axis.line = element_line(colour = &amp;quot;bisque4&amp;quot;, 
        size = 0.2, linetype = &amp;quot;solid&amp;quot;), 
    axis.ticks = element_line(size = 0.7), 
    axis.title = element_text(size = 16), 
    axis.text = element_text(size = 16, colour = &amp;quot;gray0&amp;quot;), 
    panel.background = element_rect(fill = &amp;quot;gray98&amp;quot;)) + 
  geom_smooth(method = lm, se=TRUE, size = 0.3)+
  scale_y_continuous(limits = c(200, 1400), breaks = c(200, 600, 1000, 1400)) +
   geom_line(aes(y=lwr), color = &amp;quot;red&amp;quot;, linetype = &amp;quot;dashed&amp;quot;)+
    geom_line(aes(y=upr), color = &amp;quot;red&amp;quot;, linetype = &amp;quot;dashed&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step2&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-15-simple-linear-regression-using-rstan_files/figure-html/step2-1.png&#34; alt=&#34;用簡單線性回歸模型計算的基本年收的信賴區間(灰色陰影)和預測區間(紅色點線)。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: 用簡單線性回歸模型計算的基本年收的信賴區間(灰色陰影)和預測區間(紅色點線)。
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;step-3-寫下stan模型&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 3, 寫下Stan模型&lt;/h2&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;data {
    int N; 
    real X[N]; 
    real Y[N];
}

parameters {
    real a;
    real b;
    real&amp;lt;lower=0&amp;gt; sigma;
}

model {
    for(n in 1:N) {
        Y[n] ~ normal(a + b*X[n], sigma);
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;參數部分 &lt;code&gt;real&amp;lt;lower=0&amp;gt; sigma&lt;/code&gt; 的代碼表示標準差不可採集負數作爲樣本。&lt;/p&gt;
&lt;p&gt;實際運行上面的Stan代碼：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rstan)
data &amp;lt;- list(N=nrow(Salary), X=Salary$X, Y = Salary$Y)
fit &amp;lt;- sampling(model4_5, data, seed = 1234) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &amp;#39;5b73686886069c0bad70513d4ea4141a&amp;#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 1.5e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.093322 seconds (Warm-up)
## Chain 1:                0.054914 seconds (Sampling)
## Chain 1:                0.148236 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &amp;#39;5b73686886069c0bad70513d4ea4141a&amp;#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 4e-06 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.098838 seconds (Warm-up)
## Chain 2:                0.061329 seconds (Sampling)
## Chain 2:                0.160167 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &amp;#39;5b73686886069c0bad70513d4ea4141a&amp;#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 4e-06 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.091017 seconds (Warm-up)
## Chain 3:                0.053765 seconds (Sampling)
## Chain 3:                0.144782 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &amp;#39;5b73686886069c0bad70513d4ea4141a&amp;#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 4e-06 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.100685 seconds (Warm-up)
## Chain 4:                0.062005 seconds (Sampling)
## Chain 4:                0.16269 seconds (Total)
## Chain 4:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Stan model: 5b73686886069c0bad70513d4ea4141a.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##          mean se_mean    sd    2.5%     25%     50%    75%  97.5% n_eff Rhat
## a     -117.45    1.93 71.31 -257.66 -164.65 -119.17 -71.98  23.17  1358 1.00
## b       21.86    0.04  1.60   18.68   20.83   21.89  22.91  24.97  1331 1.00
## sigma   84.51    0.41 15.21   61.09   73.72   82.41  93.18 120.03  1381 1.01
## lp__   -93.61    0.04  1.26  -96.86  -94.19  -93.27 -92.69 -92.14  1164 1.01
## 
## Samples were drawn using NUTS(diag_e) at Mon May 18 14:21:36 2020.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;輸出結果的前三行，是該次MCMC的設定條件，其中模型名稱是Rmarkdown文件中隨機產生的。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;第二行則說明的是該次MCMC進行了4條鏈的採樣，每條鏈2000次，其中前1000次被當作是 burn-in (或者叫 warmup)。可以看到一共獲得了4000個事後樣本。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;接下來的五行是參數的事後樣本的事後分析總結，一共有11列。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第1列是參數名稱，最後一個 &lt;code&gt;lp__&lt;/code&gt;是Stan特有的算法得到的產物，具體解釋爲對數事後概率 (log posterior)，當然它也需要得到收斂才行。&lt;/li&gt;
&lt;li&gt;第2列是獲得的4000個參數的事後樣本的事後平均值(posterior mean)。例如&lt;code&gt;b&lt;/code&gt;（回歸直線的斜率）的事後平均值是21.96，也就是說年齡每增加一歲，基本年收入平均增加21.96萬日元。你可以和之前的概率論算法相比較(&lt;code&gt;b = 21.904&lt;/code&gt;)。&lt;/li&gt;
&lt;li&gt;第3列&lt;code&gt;se_mean&lt;/code&gt;是事後平均值的標準誤(standard error of posterior mean)。說白了是MCMC事後樣本的方差除以第10列的有效樣本量&lt;code&gt;n_eff&lt;/code&gt;之後取根號獲得的值。&lt;/li&gt;
&lt;li&gt;第4列&lt;code&gt;sd&lt;/code&gt;是MCMC事後樣本的標準差(standard deviation of posterior MCMC sample)。&lt;/li&gt;
&lt;li&gt;第5-9列是MCMC事後樣本的四分位點。也就是貝葉斯統計算法獲得的事後可信區間。&lt;/li&gt;
&lt;li&gt;第10列&lt;code&gt;n_eff&lt;/code&gt;是Stan在基於事後樣本自相關程度來判斷的有效事後樣本量大小。爲了有效地計算和繪製事後分佈的統計量，這個有效樣本量需要至少有100個以上吧（作者觀點）。如果報告給出的事後有效樣本量過小的話也是模型收斂不佳的表現之一。&lt;/li&gt;
&lt;li&gt;第11列&lt;code&gt;Rhat&lt;/code&gt;&lt;span class=&#34;math inline&#34;&gt;\((\hat R)\)&lt;/span&gt;是主要用於判斷模型是否達到收斂的重要指標，每個參數都會被計算一個&lt;code&gt;Rhat&lt;/code&gt;值。當MCMC鏈條數在3以上，且同時所有的模型參數的 &lt;code&gt;Rhat &amp;lt; 1.1&lt;/code&gt;的話，可以認爲模型達到了良好的收斂。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;step-4-診斷stan貝葉斯模型的收斂程度&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 4, 診斷Stan貝葉斯模型的收斂程度&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggmcmc)

ggmcmc(ggs(fit, inc_warmup = TRUE, stan_include_auxiliar = TRUE), plot = &amp;quot;traceplot&amp;quot;, dev_type_html = &amp;quot;png&amp;quot;, 
       file = &amp;quot;trace.html&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;上面的代碼，會自動生成四個模型參數的軌跡MCMC鏈式圖報告。&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step4&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../../static/img/traceplot-model4-5.png&#34; alt=&#34;用ggmcmc函數製作而成的MCMC鏈式圖報告。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: 用ggmcmc函數製作而成的MCMC鏈式圖報告。
&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(bayesplot)

color_scheme_set(&amp;quot;mix-brightblue-gray&amp;quot;)

posterior2 &amp;lt;- rstan::extract(fit, inc_warmup = TRUE, permuted = FALSE)

p &amp;lt;- mcmc_trace(posterior2, n_warmup = 0,
                facet_args = list(nrow = 2, labeller = label_parsed))
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step41&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-15-simple-linear-regression-using-rstan_files/figure-html/step41-1.png&#34; alt=&#34;用 bayesplot包數繪製的MCMC鏈式圖。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4: 用 bayesplot包數繪製的MCMC鏈式圖。
&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- mcmc_acf_bar(posterior2)
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step42&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-15-simple-linear-regression-using-rstan_files/figure-html/step42-1.png&#34; alt=&#34;用 bayesplot包數繪製的事後樣本自相關圖(autocorrelation)。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 5: 用 bayesplot包數繪製的事後樣本自相關圖(autocorrelation)。
&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- mcmc_dens_overlay(posterior2, color_chains = T)
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step43&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-15-simple-linear-regression-using-rstan_files/figure-html/step43-1.png&#34; alt=&#34;用 bayesplot包數繪製的事後樣本密度分佈圖。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 6: 用 bayesplot包數繪製的事後樣本密度分佈圖。
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;step-5修改mcmc條件設定&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 5，修改MCMC條件設定&lt;/h2&gt;
&lt;p&gt;進行貝葉斯模型擬合的過程中，常常需要不停地修改模型的條件，例如縮短warm-up等。下面的Rstan代碼可以實現簡便地頻繁修改MCMC條件設定：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# library(rstan) uncomment if run for the first time
data &amp;lt;- list(N=nrow(Salary), X=Salary$X, Y = Salary$Y)
fit2 &amp;lt;- sampling(
    model4_5, 
    data = data, 
    pars = c(&amp;quot;b&amp;quot;, &amp;quot;sigma&amp;quot;), 
    init = function(){
      list(a = runif(1, -10, 10), b = runif(1, 0, 10), sigma = 10)
    },
    seed = 123,
    chains = 3, iter = 1000, warmup = 200, thin = 2
) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &amp;#39;5b73686886069c0bad70513d4ea4141a&amp;#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 1.2e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 1: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 1: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 1: Iteration: 201 / 1000 [ 20%]  (Sampling)
## Chain 1: Iteration: 300 / 1000 [ 30%]  (Sampling)
## Chain 1: Iteration: 400 / 1000 [ 40%]  (Sampling)
## Chain 1: Iteration: 500 / 1000 [ 50%]  (Sampling)
## Chain 1: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 1: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 1: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 1: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 1: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.05316 seconds (Warm-up)
## Chain 1:                0.036218 seconds (Sampling)
## Chain 1:                0.089378 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &amp;#39;5b73686886069c0bad70513d4ea4141a&amp;#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 2e-06 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 2: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 2: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 2: Iteration: 201 / 1000 [ 20%]  (Sampling)
## Chain 2: Iteration: 300 / 1000 [ 30%]  (Sampling)
## Chain 2: Iteration: 400 / 1000 [ 40%]  (Sampling)
## Chain 2: Iteration: 500 / 1000 [ 50%]  (Sampling)
## Chain 2: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 2: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 2: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 2: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 2: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.052191 seconds (Warm-up)
## Chain 2:                0.036956 seconds (Sampling)
## Chain 2:                0.089147 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &amp;#39;5b73686886069c0bad70513d4ea4141a&amp;#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 2e-06 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 3: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 3: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 3: Iteration: 201 / 1000 [ 20%]  (Sampling)
## Chain 3: Iteration: 300 / 1000 [ 30%]  (Sampling)
## Chain 3: Iteration: 400 / 1000 [ 40%]  (Sampling)
## Chain 3: Iteration: 500 / 1000 [ 50%]  (Sampling)
## Chain 3: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 3: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 3: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 3: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 3: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.052083 seconds (Warm-up)
## Chain 3:                0.042977 seconds (Sampling)
## Chain 3:                0.09506 seconds (Total)
## Chain 3:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Stan model: 5b73686886069c0bad70513d4ea4141a.
## 3 chains, each with iter=1000; warmup=200; thin=2; 
## post-warmup draws per chain=400, total post-warmup draws=1200.
## 
##         mean se_mean    sd   2.5%    25%    50%    75%  97.5% n_eff Rhat
## b      21.90    0.06  1.56  18.56  20.96  21.92  22.91  24.83   587 1.00
## sigma  85.49    0.58 15.60  61.11  74.29  83.15  94.85 122.67   727 1.01
## lp__  -93.62    0.05  1.29 -96.82 -94.22 -93.30 -92.70 -92.16   609 1.00
## 
## Samples were drawn using NUTS(diag_e) at Mon May 18 14:27:30 2020.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中&lt;code&gt;fit&lt;/code&gt;的最後一行是修改各種條件的示例：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;chains&lt;/code&gt;至少要三條；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;iter&lt;/code&gt;一開始可以設定在500~1000左右，確定模型可以收斂以後，再加大這個數值以獲得穩定的事後統計量，多多益善；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;warmup&lt;/code&gt;，也就MCMC採樣開始後多少樣本可以丟棄。這個數值需要參考trace plot；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;thin&lt;/code&gt;，通常只需要保持默認值 1。和WinBUGS, JAGS相比Stan算法採集的事後樣本自相關比較低。&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;step-6-並行平行計算的設定&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 6, 並行（平行）計算的設定&lt;/h2&gt;
&lt;p&gt;如果你寫出來的貝葉斯模型需要很長時間的計算和收斂，可以充分利用你的計算機的多核計算，把每條MCMC鏈單獨進行計算加速這個過程：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;parallel::detectCores() #我的桌上型電腦有8個核可以用於平行計算&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;但是平行計算時如果計算中出錯則由於每條鏈都是相互獨立地進行，報錯就減少了。所以如果要使用多核同時計算的話，建議先減少採樣數，確認不會報錯以後再用多核平行計算增加採樣量。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;step-7-計算貝葉斯可信區間和貝葉斯預測區間&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 7, 計算貝葉斯可信區間和貝葉斯預測區間&lt;/h2&gt;
&lt;p&gt;這一步就又回到一開始提出的研究問題上來，我們來計算基本年收的貝葉斯可信區間和貝葉斯預測區間。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ms &amp;lt;- rstan::extract(fit)

quantile(ms$b, probs = c(0.025, 0.975))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     2.5%    97.5% 
## 18.67987 24.97108&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d_mcmc &amp;lt;- data.frame(a = ms$a, b = ms$b, sigma = ms$sigma)

head(d_mcmc)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            a        b    sigma
## 1 -103.92295 22.04743 70.39829
## 2  -30.41656 20.16582 74.35210
## 3  -95.35165 21.32534 75.19714
## 4  -10.88849 19.01547 68.02757
## 5 -177.04183 23.49984 81.40161
## 6 -146.45260 22.73838 95.97706&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1 &amp;lt;- ggplot(d_mcmc, aes(x = a, y = b)) + 
 geom_point(shape = 1, size = 4)

ggExtra::ggMarginal(
  p = p1,
  type = &amp;#39;density&amp;#39;,
  margins = &amp;#39;both&amp;#39;,
  size = 4,
  colour = &amp;#39;black&amp;#39;,
  fill = &amp;#39;#2D077A&amp;#39;
)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step71&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-15-simple-linear-regression-using-rstan_files/figure-html/step71-1.png&#34; alt=&#34;MCMC樣本的兩個模型參數的事後散點圖，及它們之間的邊緣分佈密度圖。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 7: MCMC樣本的兩個模型參數的事後散點圖，及它們之間的邊緣分佈密度圖。
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;從圖&lt;a href=&#34;#fig:step71&#34;&gt;7&lt;/a&gt;中可觀察到該貝葉斯線性模型獲得的事後模型參數樣本中，截距&lt;code&gt;a&lt;/code&gt;，和斜率&lt;code&gt;b&lt;/code&gt;之間呈極強的負相關關係。也就是說，截距是工資的起點（年齡爲0歲時），這個起點的理論值越低，斜率越大（歲年齡增加工資上升的速度越大）。&lt;/p&gt;
&lt;p&gt;根據上面分析的結果，下面的R代碼可以計算一名50歲的人被這家公司採用的時候，她/他的預期基本年收入的分佈（中獲得的MCMC樣本），和她/他的預期總年收的預測分佈（中獲得的MCMC樣本）。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;N_mcmc &amp;lt;- length(ms$lp__)
y50_base &amp;lt;- ms$a + ms$b*50
y50 &amp;lt;- rnorm(n = N_mcmc, mean = y50_base, sd = ms$sigma)
d_mcmc &amp;lt;- data.frame(a = ms$a, b = ms$b, sigma = ms$sigma, y50_base, y50)
head(d_mcmc)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            a        b    sigma y50_base       y50
## 1 -103.92295 22.04743 70.39829 998.4488  953.4024
## 2  -30.41656 20.16582 74.35210 977.8746  861.2176
## 3  -95.35165 21.32534 75.19714 970.9152 1076.7587
## 4  -10.88849 19.01547 68.02757 939.8852  877.2139
## 5 -177.04183 23.49984 81.40161 997.9499 1109.8183
## 6 -146.45260 22.73838 95.97706 990.4664 1063.0656&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# the following codes are also available from the author&amp;#39;s page:
# https://github.com/MatsuuraKentaro/RStanBook/blob/master/chap04/fig4-8.R
# library(ggplot2)
source(&amp;#39;commonRstan.R&amp;#39;)

# load(&amp;#39;output/result-model4-5.RData&amp;#39;)
ms &amp;lt;- rstan::extract(fit)

X_new &amp;lt;- 23:60
N_X &amp;lt;- length(X_new)
N_mcmc &amp;lt;- length(ms$lp__)

set.seed(1234)
y_base_mcmc &amp;lt;- as.data.frame(matrix(nrow=N_mcmc, ncol=N_X))
y_mcmc &amp;lt;- as.data.frame(matrix(nrow=N_mcmc, ncol=N_X))
for (i in 1:N_X) {
  y_base_mcmc[,i] &amp;lt;- ms$a + ms$b * X_new[i]
  y_mcmc[,i] &amp;lt;- rnorm(n=N_mcmc, mean=y_base_mcmc[,i], sd=ms$sigma)
}

customize.ggplot.axis &amp;lt;- function(p) {
  p &amp;lt;- p + labs(x=&amp;#39;X&amp;#39;, y=&amp;#39;Y&amp;#39;)
  p &amp;lt;- p + scale_y_continuous(breaks=seq(from=200, to=1400, by=400))
  p &amp;lt;- p + coord_cartesian(xlim=c(22, 61), ylim=c(200, 1400))
  return(p)
}

d_est &amp;lt;- data.frame.quantile.mcmc(x=X_new, y_mcmc=y_base_mcmc)
p &amp;lt;- ggplot.5quantile(data=d_est)
p &amp;lt;- p + geom_point(data=Salary, aes(x=X, y=Y), shape=1, size=3)
p &amp;lt;- customize.ggplot.axis(p)
# ggsave(file=&amp;#39;output/fig4-8-left.png&amp;#39;, plot=p, dpi=300, w=4, h=3)
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step72&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-15-simple-linear-regression-using-rstan_files/figure-html/step72-1.png&#34; alt=&#34;MCMC樣本計算獲得的基本年收的貝葉斯可信區間。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 8: MCMC樣本計算獲得的基本年收的貝葉斯可信區間。
&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d_est &amp;lt;- data.frame.quantile.mcmc(x=X_new, y_mcmc=y_mcmc)
p &amp;lt;- ggplot.5quantile(data=d_est)
p &amp;lt;- p + geom_point(data=Salary, aes(x=X, y=Y), shape=1, size=3)
p &amp;lt;- customize.ggplot.axis(p)
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step73&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-15-simple-linear-regression-using-rstan_files/figure-html/step73-1.png&#34; alt=&#34;MCMC樣本計算獲得的預期總年收的貝葉斯預測區間。（顏色較深的是50%預測區間帶，黑線是事後樣本的中央值）&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 9: MCMC樣本計算獲得的預期總年收的貝葉斯預測區間。（顏色較深的是50%預測區間帶，黑線是事後樣本的中央值）
&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# ggsave(file=&amp;#39;output/fig4-8-right.png&amp;#39;, plot=p, dpi=300, w=4, h=3)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;練習題&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;練習題&lt;/h2&gt;
&lt;p&gt;用模擬數據來嘗試進行貝葉斯t檢驗&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)
N1 &amp;lt;- 30
N2 &amp;lt;- 20
Y1 &amp;lt;- rnorm(n=N1, mean=0, sd=5)
Y2 &amp;lt;- rnorm(n=N2, mean=1, sd=4)&lt;/code&gt;&lt;/pre&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;請繪製上面代碼生成的兩組數據的示意圖&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d1 &amp;lt;- data.frame(group=1, Y=Y1)
d2 &amp;lt;- data.frame(group=2, Y=Y2)
d &amp;lt;- rbind(d1, d2)
d$group &amp;lt;- as.factor(d$group)

p &amp;lt;- ggplot(data=d, aes(x=group, y=Y, group=group, col=group))
p &amp;lt;- p + geom_boxplot(outlier.size=0)
p &amp;lt;- p + geom_point(position=position_jitter(w=0.4, h=0), size=2)
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:exe11&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://wangcc.me/post/2019-01-15-simple-linear-regression-using-rstan_files/figure-html/exe11-1.png&#34; alt=&#34;隨機生成的兩組數據的散點圖和箱式圖。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 10: 隨機生成的兩組數據的散點圖和箱式圖。
&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#ggsave(file=&amp;#39;fig-ex1.png&amp;#39;, plot=p, dpi=300, w=4, h=3)&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;寫下相當於t檢驗的數學式，表示各組之間方差或者標準差如果相等時，均值比較的檢驗模型。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;hypotheses:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;observations in each group follow a normal distribution&lt;/li&gt;
&lt;li&gt;all observations are independent&lt;/li&gt;
&lt;li&gt;The two population variance/standard deviations are known (and can be considered equal)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{H}_0: \mu_2 - \mu_1 = 0 \\
\text{H}_1: \mu_2 - \mu_1 \neq 0 \\ 
\text{If H}_0 \text{ is true, then:} \\
Z=\frac{\bar{Y_2} - \bar{Y_1}}{\sqrt{(\sigma_2^2/n_2) + (\sigma_1^2/n_1)}} \\
\text{follows a standard normal distribution with zero mean} \\
\Rightarrow \text{ if two variances are considered the same}\\ 
Y_1[n] \sim N(\mu_1, \sigma) \;\; n = 1,2,\dots,N \\
Y_2[n] \sim N(\mu_2, \sigma) \;\; n = 1,2,\dots,N
\]&lt;/span&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;寫下上一步模型的Stan代碼，並嘗試在R裏運行&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Stan代碼如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data {
  int N1;
  int N2;
  real Y1[N1];
  real Y2[N2];
}

parameters {
  real mu1;
  real mu2;
  real&amp;lt;lower=0&amp;gt; sigma;
}

model {
  for (n in 1:N1)
    Y1[n] ~ normal(mu1, sigma);
  for (n in 1:N2)
    Y2[n] ~ normal(mu2, sigma);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;R代碼如下：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rstan)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: StanHeaders&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggplot2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## rstan (Version 2.19.3, GitRev: 2e1f913d3ca3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## For execution on a local, multicore CPU with excess RAM we recommend calling
## options(mc.cores = parallel::detectCores()).
## To avoid recompilation of unchanged Stan programs, we recommend calling
## rstan_options(auto_write = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data &amp;lt;- list(N1=N1, N2=N2, Y1=Y1, Y2=Y2)
exe13 &amp;lt;- stan_model(file = &amp;quot;stanfiles/ex3.stan&amp;quot;)
fit &amp;lt;- sampling(exe13, data=data, seed=1234)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &amp;#39;ex3&amp;#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 1.5e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.027413 seconds (Warm-up)
## Chain 1:                0.022648 seconds (Sampling)
## Chain 1:                0.050061 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &amp;#39;ex3&amp;#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 4e-06 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.028538 seconds (Warm-up)
## Chain 2:                0.021555 seconds (Sampling)
## Chain 2:                0.050093 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &amp;#39;ex3&amp;#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 5e-06 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.026656 seconds (Warm-up)
## Chain 3:                0.021916 seconds (Sampling)
## Chain 3:                0.048572 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &amp;#39;ex3&amp;#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 3e-06 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.027425 seconds (Warm-up)
## Chain 4:                0.020884 seconds (Sampling)
## Chain 4:                0.048309 seconds (Total)
## Chain 4:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Stan model: ex3.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##         mean se_mean   sd    2.5%    25%    50%    75%  97.5% n_eff Rhat
## mu1    -0.24    0.01 0.84   -1.93  -0.80  -0.23   0.30   1.40  3805    1
## mu2     1.59    0.02 0.99   -0.32   0.92   1.61   2.25   3.50  3739    1
## sigma   4.46    0.01 0.46    3.66   4.14   4.42   4.76   5.45  3402    1
## lp__  -97.74    0.03 1.23 -100.89 -98.32 -97.42 -96.84 -96.33  1879    1
## 
## Samples were drawn using NUTS(diag_e) at Mon May 18 16:51:40 2020.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;從獲取到的事後參數的MCMC樣本計算 &lt;span class=&#34;math inline&#34;&gt;\(\text{Prob}[\mu_1 &amp;lt; \mu_2]\)&lt;/span&gt;：&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ms &amp;lt;- extract(fit)
prob &amp;lt;- mean(ms$mu1 &amp;lt; ms$mu2)  #=&amp;gt; 0.932
prob&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9235&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;所以可以認爲地一組均值，小於第二組均值的事後概率是93.2%&lt;/p&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;如果不能認爲兩組的方差相等的話，模型又該改成什麼樣子？&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y_1[n] \sim N(\mu_1, \sigma_1) \;\; n = 1,2,\dots,N \\
Y_2[n] \sim N(\mu_2, \sigma_2) \;\; n = 1,2,\dots,N
\]&lt;/span&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data {
  int N1;
  int N2;
  real Y1[N1];
  real Y2[N2];
}

parameters {
  real mu1;
  real mu2;
  real&amp;lt;lower=0&amp;gt; sigma1;
  real&amp;lt;lower=0&amp;gt; sigma2;
}

model {
  for (n in 1:N1)
    Y1[n] ~ normal(mu1, sigma1);
  for (n in 1:N2)
    Y2[n] ~ normal(mu2, sigma2);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;下面的代碼相當於實施Welch的t檢驗：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rstan)
data &amp;lt;- list(N1=N1, N2=N2, Y1=Y1, Y2=Y2)
exe15 &amp;lt;- stan_model(file = &amp;quot;stanfiles/ex5.stan&amp;quot;)

fit &amp;lt;- sampling(exe15, data=data, seed=1234)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &amp;#39;ex5&amp;#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 1.2e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.03039 seconds (Warm-up)
## Chain 1:                0.023831 seconds (Sampling)
## Chain 1:                0.054221 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &amp;#39;ex5&amp;#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 4e-06 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.028121 seconds (Warm-up)
## Chain 2:                0.026473 seconds (Sampling)
## Chain 2:                0.054594 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &amp;#39;ex5&amp;#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 1.6e-05 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.030293 seconds (Warm-up)
## Chain 3:                0.027682 seconds (Sampling)
## Chain 3:                0.057975 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &amp;#39;ex5&amp;#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 3e-06 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.030591 seconds (Warm-up)
## Chain 4:                0.0252 seconds (Sampling)
## Chain 4:                0.055791 seconds (Total)
## Chain 4:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Stan model: ex5.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##          mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat
## mu1     -0.22    0.01 0.94  -2.07  -0.85  -0.23   0.40   1.68  4084    1
## mu2      1.63    0.01 0.82   0.04   1.09   1.62   2.15   3.25  4068    1
## sigma1   5.12    0.01 0.71   3.94   4.62   5.05   5.53   6.74  3863    1
## sigma2   3.63    0.01 0.64   2.66   3.18   3.55   3.98   5.08  3002    1
## lp__   -95.33    0.03 1.44 -98.86 -96.05 -95.04 -94.28 -93.52  1916    1
## 
## Samples were drawn using NUTS(diag_e) at Mon May 18 16:52:15 2020.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ms &amp;lt;- rstan::extract(fit)
prob &amp;lt;- mean(ms$mu1 &amp;lt; ms$mu2)  #=&amp;gt; 0.93725
prob&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9345&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Rstan Wonderful R-(1)</title>
      <link>https://wangcc.me/post/rstan-wonderful-r/</link>
      <pubDate>Thu, 22 Nov 2018 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/rstan-wonderful-r/</guid>
      <description>
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;P16&lt;/p&gt;
&lt;p&gt;事後分布 &lt;span class=&#34;math inline&#34;&gt;\(p(\theta | Y)\)&lt;/span&gt;の値が最大になる点&lt;span class=&#34;math inline&#34;&gt;\(\theta^*\)&lt;/span&gt;を事後確率最大推定値 (maximum a posteriori estimate)と呼ぶ．略してMAP推定値 (MAP estimate)．&lt;/p&gt;
&lt;p&gt;我們把能夠將事後概率分布取極大值的參數點 &lt;span class=&#34;math inline&#34;&gt;\(\theta^*\)&lt;/span&gt; 稱爲事後概率的最大似然估計值 (maximum a posteriori estimate)，簡稱 MAP估計值 (MAP estimate)。&lt;/p&gt;
&lt;p&gt;P19&lt;/p&gt;
&lt;p&gt;統計建模的一般順序&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;確定分析目的&lt;/li&gt;
&lt;li&gt;確定數據分布&lt;/li&gt;
&lt;li&gt;想象數據產生本身的機制：思考數據與數據之間可能的關系&lt;/li&gt;
&lt;li&gt;寫下你所認爲的數據模型的數學表達式&lt;/li&gt;
&lt;li&gt;用 R 模擬(simulation)並確認前一步寫下的數學模型的性質，特點&lt;/li&gt;
&lt;li&gt;用 Stan 實際進行模型參數的推斷&lt;/li&gt;
&lt;li&gt;獲得推斷結果，解釋其事後概率分布的意義，繪制易於理解的模型示意圖&lt;/li&gt;
&lt;li&gt;繪制成功之後的模型示意圖和最先使用的模型之間進行比對，重新查缺補漏&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;P23&lt;/p&gt;
&lt;p&gt;ただいたずらにモデルを複雑化させるのは解釈のしにくさを招く．&lt;/p&gt;
&lt;p&gt;P30&lt;/p&gt;
&lt;p&gt;最初にmodel ブロックの尤度の部分（と事前分布の部分）を書く．その尤度の部分に登場した変数のうち，データの変数をdataブロックに，残りの変数をparametersブロックに書いていく．&lt;/p&gt;
&lt;p&gt;Stan的基本文法構成&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data {
 數據描述
}

parameters {
 想要進行MCMC事後樣本採集的參數描述
}

model {
 p(Y|theta) 似然的描述
 先驗概率分布 p(theta) 的描述
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;把下面的模型&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
Y[n] &amp;amp; \sim \text{Normal}(\mu, 1) \;\; n = 1, \dots, N \\
\mu  &amp;amp; \sim \text{Normal}(0, 100)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;翻譯成爲 Stan 模型語言是：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data {
  int N;
  real Y[N];
}

parameters {
  real mu;
}

model {
  for (n in 1:N) {
    Y[n] ~ normal(mu, 1);
  }
  mu ~ normal(0, 100);
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中我們按照實際模型書寫的順序 model -&amp;gt; data -&amp;gt; parameter 來逐個解釋：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;model&lt;/code&gt; 模塊中 &lt;code&gt;for (n in 1:N)&lt;/code&gt; 開始的循環部分（三行）對應數學模型的 $Y[n] (, 1) n = 1, , N $　部分。&lt;/li&gt;
&lt;li&gt;Stan 語言中，每一行描述的結尾需要用分號 &lt;code&gt;;&lt;/code&gt; 來結束。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mu ~ normal(0,100)&lt;/code&gt; 則對應數學模型中寫的先驗概率 &lt;span class=&#34;math inline&#34;&gt;\(\mu \sim \text{Normal}(0, 100)\)&lt;/span&gt; 部分。這裏給均值的先驗概率分佈是一個方差很大的無信息先驗概率分佈 (noninformative prior)。事實上在 Stan 軟件語言中，如果不特別指出先驗概率分佈，系統會默認給參數以無信息的先驗概率分佈，這樣即使沒有這一行，模型也是可以跑的。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;data&lt;/code&gt; 模塊中寫明的是 &lt;code&gt;model&lt;/code&gt; 模塊中描述的模型將要使用的數據。它包括宣示數據的個數（樣本量 &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;），以及數據本身。其中 &lt;code&gt;int N&lt;/code&gt; 意爲樣本量的數量是整數個 (integer)，&lt;code&gt;real Y[N]&lt;/code&gt; 則宣示實數有 N 個作爲數據。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;parameter&lt;/code&gt; 模塊是告訴軟件需要採樣且關注的未知參數 (parameter) 是 &lt;code&gt;mu&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;在 Stan 語言中，還可以和其他語言一樣爲模型加註解釋的文字，只需要在想要做註釋的文字最開始的部分增加 &lt;code&gt;//&lt;/code&gt;，如果註釋的文字超過一行，那麼在註釋的模塊前後加上 &lt;code&gt;/*&lt;/code&gt; 和 &lt;code&gt;*/&lt;/code&gt; 即可。&lt;/li&gt;
&lt;li&gt;另外，目前爲止主流的貝葉斯模型軟件中使用精確度 (precision) ，也就是方差的倒數來描述正態分佈 &lt;code&gt;normal(mean, 1/variance)&lt;/code&gt; ，但是在Stan的語法中使用的是 &lt;code&gt;normal(mean, sd)&lt;/code&gt;，也就是用標準差來描述正態分佈。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;寫Stan（或者說寫大多數的代碼）時，請遵守以下的原則：&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;適當縮進，以便於閱讀；&lt;/li&gt;
&lt;li&gt;表示數據的部分用大寫字母，表示參數的部分，用小寫字母；&lt;/li&gt;
&lt;li&gt;每個部分之間至少使用一個空行加以區分；&lt;/li&gt;
&lt;li&gt;請不要用&lt;code&gt;camelCase&lt;/code&gt;這樣的方式（單詞之間用大寫隔開），請在單詞之間用下劃線 &lt;code&gt;camel_case&lt;/code&gt; 的標記方法；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;~&lt;/code&gt;或者&lt;code&gt;=&lt;/code&gt;前後用一個字符大小的空格來隔開。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;最低限度的話，也請依照1,2兩個標準來書寫你的Stan代碼。不爲他人，也爲自己將來再讀代碼時能快速理解其涵義。往Stan的官方論壇投稿時，也必須遵守它們在手冊裏提供的 “Stan Program Style Guide” 代碼書寫規則，也是對其他寫，讀代碼的人的尊重。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Summer Project Schedule</title>
      <link>https://wangcc.me/post/summer-project-schedule/</link>
      <pubDate>Sun, 24 Jun 2018 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/summer-project-schedule/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Data analysis finish by 2018-07-&lt;del&gt;24&lt;/del&gt;31&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Paper structure confirm by 2018-08-01&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Paper draft complete by 2018-08-16&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;2018-06-24
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Read and try to repeat Rll&amp;rsquo;s method in R and familarize the dataset ASAP&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Two papers applying Repeated Measures LCA&lt;/del&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;2018-06-25
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Meeting with supervisor and Susanna&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Confirm the cutoff of carborhydrate consumption&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Talk with Rll ask about the methodology and dataset&lt;/del&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;2018-06-26
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Send the summarised memo of meeting to Supervisor and etc.&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Read the first part fundamentals of LCA.&lt;/del&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;2018-06-27
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;a href=&#34;https://www.londonr.org/&#34;&gt;London R in UCL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Germany lost their game against South Korea, UNBELIEVEABLE&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;2018-06-28
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Read the book collins2010latent - Latent Class and Latent Transition Analysis: With Applications in the Social, Behavioral, and Health Sciences (Done until 4.2)&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Learn how to do LCA in R&lt;/del&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;2018-06-29
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Read the book collins2010latent - Latent Class and Latent Transition Analysis: With Applications in the Social, Behavioral, and Health Sciences (Done until 4.3)&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Data management for NDNS 8 years data (70%)&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Learn how to do LCA in R&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Start to analysis the data according to the discussion on 25th(30%)&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Day1 data analysis results summary&lt;/del&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;2018-07-01
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Relax and do nothing&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Buy some drink to enjoy the night with classmates(HB)&lt;/del&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;2018-07-02/03
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Send some preliminary results to co-authors&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;&lt;a href=&#34;http://www.the-afc.com/competitions/fifa-world-cup/latest/news/japan-fa-president-proud-of-blue-samurai&#34;&gt;Japan lost the game to Belgium, but they are the glory of Asia&amp;ndash;heartbreaking&lt;/a&gt;&lt;/del&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;2018-07-04
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;&amp;ldquo;consider separating weekdays from weekends if we are not averaging the four days?&amp;quot;&lt;/del&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;2018-07-05
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Test and confirm the availability of LCA in SAS&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Learn how to do LCA in SAS with NDNS data&lt;/del&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;2018-07-06
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Learn how to do LCA with random effects in SAS&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Find whether there is any possibility of conducting the same method in R or STATA (no there is no way)&lt;/del&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;2018-07-07~09
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;&amp;ldquo;Maybe we should try with the threshold at 25% only as per the existing guidelines (although those are per meal)?&amp;quot;&lt;/del&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;2018-07-10
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; ~~Meet with tutor;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Start writing about the methodology;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Try to start writing about the introduction;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;2018-07-11
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Try to summarise the meeting memo yesterday;&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Re-analyse the data with new cut-off values (25, 50, 75);&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Re-analyse the data with new cut-off values (50);&lt;/del&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;2018-07-12~22
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Use latent class growth analysis;&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Use multilevel latent class analysis;&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Think about the mathmatical theory behind the mixed LCA, write to PROC LCA group if necessary;&lt;/del&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;2018-07-23~25
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Learn about the survey package in R&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Finish writing about the methodology;&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Write some introduction;&lt;/del&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;2018-07-26
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Let&amp;rsquo;s finish analysis of the classes and health outcomes.&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Read about the carbo-fibre ratio references.&lt;/del&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;2018-08-15
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;PM review&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Finish most of the discussion outlines and 2 pages of them.&lt;/del&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;2018-08-31
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Finish revising the report according to comments from LP and SAM;&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Read RT&amp;rsquo;s report and send the comments;&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Confirm the deadline for funding applications;&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Prepare the abstract for conferences (UK and JP);&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Start preparing the paper for submit (MLCA part alone);&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Think about the schedules and plans after leaving London;&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;del&gt;Finish the post of Scotland trip.&lt;/del&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>對數似然比 Log-likelihood ratio</title>
      <link>https://wangcc.me/post/log-likelihood-ratio/</link>
      <pubDate>Sun, 05 Nov 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/log-likelihood-ratio/</guid>
      <description>
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;對數似然比-log-likelihood-ratio&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;對數似然比 Log-likelihood ratio&lt;/h3&gt;
&lt;p&gt;對數似然比的想法來自於將對數似然方程圖形的 &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; 軸重新調節 (rescale) 使之最大值爲零。這可以通過計算該分佈方程的&lt;strong&gt;對數似然比 (log-likelihood ratio)&lt;/strong&gt; 來獲得：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[llr(\theta)=\ell(\theta|data)-\ell(\hat{\theta}|data)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;由於 &lt;span class=&#34;math inline&#34;&gt;\(\ell(\theta)\)&lt;/span&gt; 的最大值在 &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt; 時， 所以，&lt;span class=&#34;math inline&#34;&gt;\(llr(\theta)\)&lt;/span&gt; 就是個當 &lt;span class=&#34;math inline&#34;&gt;\(\theta=\hat{\theta}\)&lt;/span&gt; 時取最大值，且最大值爲零的方程。很容易理解我們叫這個方程爲對數似然比，因爲這個方程就是將似然比 &lt;span class=&#34;math inline&#34;&gt;\(LR(\theta)=\frac{L(\theta)}{L(\hat{\theta})}\)&lt;/span&gt; 取對數而已。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://winterwang.github.io/post/likelihood/&#34;&gt;之前&lt;/a&gt;我們也確證了，不包含我們感興趣的參數的方程部分可以忽略掉。還是用上一節 10人中4人患病的例子：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[L(\pi|X=4)=\binom{10}{4}\pi^4(1-\pi)^{10-4}\\
\Rightarrow \ell(\pi)=log[\pi^4(1-\pi)^{10-4}]\\
\Rightarrow llr(\pi)=\ell(\pi)-\ell(\hat{\pi})=log\frac{\pi^4(1-\pi)^{10-4}}{0.4^4(1-0.4)^{10-4}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其實由上也可以看出 &lt;span class=&#34;math inline&#34;&gt;\(llr(\theta)\)&lt;/span&gt; 只是將對應的似然方程的 &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; 軸重新調節了一下而已。形狀是沒有改變的：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow=c(1,2))
x &amp;lt;- seq(0,1,by=0.001)
y &amp;lt;- (x^4)*((1-x)^6)/(0.4^4*0.6^6)
z &amp;lt;- log((x^4)*((1-x)^6))-log(0.4^4*0.6^6)
plot(x, y, type = &amp;quot;l&amp;quot;, ylim = c(0,1.1),yaxt=&amp;quot;n&amp;quot;,
     frame.plot = FALSE, ylab = &amp;quot;LR(\U03C0)&amp;quot;, xlab = &amp;quot;\U03C0&amp;quot;)
axis(2, at=seq(0,1, 0.2), las=2)
title(main = &amp;quot;Binomial likelihood ratio&amp;quot;)
abline(h=1.0, lty=2)
segments(x0=0.4, y0=0, x1=0.4, y1=1, lty = 2)
plot(x, z, type = &amp;quot;l&amp;quot;, ylim = c(-10, 1), yaxt=&amp;quot;n&amp;quot;, frame.plot = FALSE,
     ylab = &amp;quot;llr(\U03C0)&amp;quot;, xlab = &amp;quot;\U03C0&amp;quot; )
axis(2, at=seq(-10, 0, 2), las=2)
title(main = &amp;quot;Binomial log-likelihood ratio&amp;quot;)
abline(h=0, lty=2)
segments(x0=0.4, y0=-10, x1=0.4, y1=0, lty = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/post/2017-11-05-log-likelihood-ratio_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;正態分佈數據的最大似然和對數似然比&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;正態分佈數據的最大似然和對數似然比&lt;/h4&gt;
&lt;p&gt;假設單個樣本 &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; 是來自一組服從正態分佈數據的觀察值：&lt;span class=&#34;math inline&#34;&gt;\(Y\sim N(\mu, \tau^2)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;那麼有：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(y|\mu) &amp;amp;= \frac{1}{\sqrt{2\pi\tau^2}}e^{(-\frac{1}{2}(\frac{y-\mu}{\tau})^2)} \\
\Rightarrow L(\mu|y) &amp;amp;=\frac{1}{\sqrt{2\pi\tau^2}}e^{(-\frac{1}{2}(\frac{y-\mu}{\tau})^2)} \\
\Rightarrow \ell(\mu)&amp;amp;=log(\frac{1}{\sqrt{2\pi\tau^2}})-\frac{1}{2}(\frac{y-\mu}{\tau})^2\\
omitting&amp;amp;\;terms\;not\;in\;\mu \\
&amp;amp;= -\frac{1}{2}(\frac{y-\mu}{\tau})^2 \\
\Rightarrow \ell^\prime(\mu) &amp;amp;= 2\cdot[-\frac{1}{2}(\frac{y-\mu}{\tau})\cdot\frac{-1}{\tau}] \\
&amp;amp;=\frac{y-\mu}{\tau^2} \\
let \; \ell^\prime(\mu) &amp;amp;= 0 \\
\Rightarrow \frac{y-\mu}{\tau^2} &amp;amp;= 0 \Rightarrow \hat{\mu} = y\\
\because \ell^{\prime\prime}(\mu) &amp;amp;=  \frac{-1}{\tau^2} &amp;lt; 0 \\
\therefore \hat{\mu} &amp;amp;= y \Rightarrow \ell(\hat{\mu}=y)_{max}=0 \\
llr(\mu)&amp;amp;=\ell(\mu)-\ell(\hat{\mu})=\ell(\mu)\\
&amp;amp;=-\frac{1}{2}(\frac{y-\mu}{\tau})^2
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;n-個獨立正態分佈樣本的對數似然比&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 個獨立正態分佈樣本的對數似然比&lt;/h3&gt;
&lt;p&gt;假設一組觀察值來自正態分佈 &lt;span class=&#34;math inline&#34;&gt;\(X_1,\cdots,X_n\stackrel{i.i.d}{\sim}N(\mu,\sigma^2)\)&lt;/span&gt;，先假設 &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; 已知。將觀察數據 &lt;span class=&#34;math inline&#34;&gt;\(x_1,\cdots, x_n\)&lt;/span&gt; 標記爲 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}\)&lt;/span&gt;。 那麼：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
L(\mu|\underline{x}) &amp;amp;=\prod_{i=1}^nf(x_i|\mu)\\
\Rightarrow \ell(\mu|\underline{x}) &amp;amp;=\sum_{i=1}^nlogf(x_i|\mu)\\
&amp;amp;=\sum_{i=1}^n[-\frac{1}{2}(\frac{x_i-\mu}{\sigma})^2]\\
&amp;amp;=-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2\\
&amp;amp;=-\frac{1}{2\sigma^2}[\sum_{i=1}^n(x_i-\bar{x})^2+\sum_{i=1}^n(\bar{x}-\mu)^2]\\
omitting&amp;amp;\;terms\;not\;in\;\mu \\
&amp;amp;=-\frac{1}{2\sigma^2}\sum_{i=1}^n(\bar{x}-\mu)^2\\
&amp;amp;=-\frac{n}{2\sigma^2}(\bar{x}-\mu)^2 \\
&amp;amp;=-\frac{1}{2}(\frac{\bar{x}-\mu}{\sigma/\sqrt{n}})^2\\
\because \ell(\hat{\mu}) &amp;amp;= 0 \\
\therefore llr(\mu) &amp;amp;= \ell(\mu)-\ell(\hat{\mu}) = \ell(\mu)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;n-個獨立正態分佈樣本的對數似然比的分佈&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 個獨立正態分佈樣本的對數似然比的分佈&lt;/h3&gt;
&lt;p&gt;假設我們用 &lt;span class=&#34;math inline&#34;&gt;\(\mu_0\)&lt;/span&gt; 表示總體均數這一參數的值。要注意的是，每當樣本被重新取樣，似然，對數似然方程，對數似然比都隨着觀察值而變 (即有自己的分佈)。&lt;/p&gt;
&lt;p&gt;考慮一個服從正態分佈的單樣本 &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;: &lt;span class=&#34;math inline&#34;&gt;\(Y\sim N(\mu_0,\tau^2)\)&lt;/span&gt;。那麼它的對數似然比：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[llr(\mu_0|Y)=\ell(\mu_0)-\ell(\hat{\mu})=-\frac{1}{2}(\frac{Y-\mu_0}{\tau})^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;根據&lt;a href=&#34;https://winterwang.github.io/post/chi-square-distribution/&#34;&gt;卡方分佈&lt;/a&gt;的定義：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\because \frac{Y-\mu_0}{\tau}\sim N(0,1)\\
\Rightarrow (\frac{Y-\mu_0}{\tau})^2 \sim \mathcal{X}_1^2\\
\therefore -2llr(\mu_0|Y) \sim \mathcal{X}_1^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;所以，如果有一組服從正態分佈的觀察值：&lt;span class=&#34;math inline&#34;&gt;\(X_1,\cdots,X_n\stackrel{i.i.d}{\sim}N(\mu_0,\sigma^2)\)&lt;/span&gt;，且 &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; 已知的話：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[-2llr(\mu_0|\bar{X})\sim \mathcal{X}_1^2\]&lt;/span&gt;&lt;/p&gt;
根據&lt;a href=&#34;https://winterwang.github.io/post/central-limit-theory/&#34;&gt;中心極限定理&lt;/a&gt;，可以將上面的結論一般化：

&lt;div class=&#34;theorem&#34;&gt;
&lt;span id=&#34;thm:unnamed-chunk-2&#34; class=&#34;theorem&#34;&gt;&lt;strong&gt;Theorem 1  &lt;/strong&gt;&lt;/span&gt;如果 &lt;span class=&#34;math inline&#34;&gt;\(X_1,\cdots,X_n\stackrel{i.i.d}{\sim}f(x|\theta)\)&lt;/span&gt;。 那麼當重複多次從參數爲 &lt;span class=&#34;math inline&#34;&gt;\(\theta_0\)&lt;/span&gt; 的總體中取樣時，那麼統計量 &lt;span class=&#34;math inline&#34;&gt;\(-2llr(\theta_0)\)&lt;/span&gt; 會漸進於自由度爲 &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; 的卡方分佈： &lt;span class=&#34;math display&#34;&gt;\[-2llr(\theta_0)=-2\{\ell(\theta_0)-\ell(\hat{\theta})\}\xrightarrow[n\rightarrow\infty]{}\;\sim \mathcal{X}_1^2\]&lt;/span&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;似然比信賴區間&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;似然比信賴區間&lt;/h3&gt;
&lt;p&gt;如果樣本量 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 足夠大 (通常應該大於 &lt;span class=&#34;math inline&#34;&gt;\(30\)&lt;/span&gt;)，根據上面的定理：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[-2llr(\theta_0)=-2\{\ell(\theta_0)-\ell(\hat{\theta})\}\sim \mathcal{X}_1^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;所以：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Prob(-2llr(\theta_0)\leqslant \mathcal{X}_{1,0.95}^2=3.84) = 0.95\\
\Rightarrow Prob(llr(\theta_0)\geqslant-3.84/2=-1.92) = 0.95\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;故似然比的 &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; 信賴區間就是能夠滿足 &lt;span class=&#34;math inline&#34;&gt;\(llr(\theta)=-1.92\)&lt;/span&gt; 的兩個 &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; 值。&lt;/p&gt;
&lt;div id=&#34;以二項分佈數據爲例&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;以二項分佈數據爲例&lt;/h4&gt;
&lt;p&gt;繼續用本文開頭的例子：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[llr(\pi)=\ell(\pi)-\ell(\hat{\pi})=log\frac{\pi^4(1-\pi)^{10-4}}{0.4^4(1-0.4)^{10-4}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;如果令 &lt;span class=&#34;math inline&#34;&gt;\(llr(\pi)=-1.92\)&lt;/span&gt; 在代數上可能較難獲得答案。然而從圖形上，如果我們在 &lt;span class=&#34;math inline&#34;&gt;\(y=-1.92\)&lt;/span&gt; 畫一條橫線，和該似然比方程曲線相交的兩個點就是我們想要求的信賴區間的上限和下限：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- seq(0,1,by=0.001)
z &amp;lt;- log((x^4)*((1-x)^6))-log(0.4^4*0.6^6)
plot(x, z, type = &amp;quot;l&amp;quot;, ylim = c(-10, 1), yaxt=&amp;quot;n&amp;quot;, frame.plot = FALSE,
     ylab = &amp;quot;llr(\U03C0)&amp;quot;, xlab = &amp;quot;\U03C0&amp;quot; )
axis(2, at=seq(-10, 0, 2), las=2)
abline(h=0, lty=2)
abline(h=-1.92, lty=2)
segments(x0=0.15, y0=-12, x1=0.15, y1=-1.92, lty = 2)
segments(x0=0.7, y0=-12, x1=0.7, y1=-1.92, lty = 2)
axis(1, at=c(0.15,0.7))
text(0.9, -1, &amp;quot;-1.92&amp;quot;)
arrows(0.8, -1.92, 0.8, 0, lty = 1, length = 0.08)
arrows( 0.8, 0, 0.8, -1.92, lty = 1, length = 0.08)
title(main = &amp;quot;Log-likelihood ratio for binomial example, \n with 95% likelihood confidence interval shown&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/post/2017-11-05-log-likelihood-ratio_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;從上圖中可以讀出，&lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; 對數似然比信賴區間就是 &lt;span class=&#34;math inline&#34;&gt;\((0.15, 0.7)\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;以正態分佈數據爲例&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;以正態分佈數據爲例&lt;/h4&gt;
&lt;p&gt;本文前半部分證明過，
&lt;span class=&#34;math inline&#34;&gt;\(X_1,\cdots,X_n\stackrel{i.i.d}{\sim}N(\mu,\sigma^2)\)&lt;/span&gt;，先假設 &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; 已知。將觀察數據 &lt;span class=&#34;math inline&#34;&gt;\(x_1,\cdots, x_n\)&lt;/span&gt; 標記爲 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}\)&lt;/span&gt;。 那麼：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[llr(\mu|\underline{x}) = \ell(\mu|\underline{x})-\ell(\hat{\mu}) = \ell(\mu|\underline{x}) \\
=-\frac{1}{2}(\frac{\bar{x}-\mu}{\sigma/\sqrt{n}})^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;很顯然，這是一個關於 &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; 的二次方程，且最大值在 MLE &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mu}=\bar{x}\)&lt;/span&gt; 時取值 &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;。所以可以通過對數似然比法求出均值的 &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; 信賴區間公式：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[-2\times[-\frac{1}{2}(\frac{\bar{x}-\mu}{\sigma/\sqrt{n}})^2]=3.84\\
\Rightarrow L=\bar{x}-\sqrt{3.84}\frac{\sigma}{\sqrt{n}} \\
U=\bar{x}+\sqrt{3.84}\frac{\sigma}{\sqrt{n}} \\
note: \;\sqrt{3.84}=1.96\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;注意到這和我們&lt;a href=&#34;https://winterwang.github.io/post/frequentist-statistical-inference02/&#34;&gt;之前&lt;/a&gt;求的正態分佈均值的信賴區間公式完全一致。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Exercise&lt;/h3&gt;
&lt;div id=&#34;q1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Q1&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;假設十個對象中有三人死亡，用二項分佈模型來模擬這個例子，求這個例子中參數 &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; 的似然方程和圖形 (likelihood) ?&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;解&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;解&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\begin{aligned}  L(\pi|3) &amp;amp;= \binom{10}{3}\pi^3(1-\pi)^{10-3} \\  omitting\;&amp;amp;terms\;not\;in\;\mu \\  \Rightarrow \ell(\pi|3) &amp;amp;= log[\pi^3(1-\pi)^7] \\  &amp;amp;= 3log\pi+7log(1-\pi)\\  \Rightarrow \ell^\prime(\pi|3)&amp;amp;= \frac{3}{\pi}-\frac{7}{1-\pi} \\  let \; \ell^\prime&amp;amp; =0\\  &amp;amp;\frac{3}{\pi}-\frac{7}{1-\pi} = 0 \\  &amp;amp;\frac{3-10\pi}{\pi(1-\pi)} = 0 \\  \Rightarrow MLE &amp;amp;= \hat\pi = 0.3 \end{aligned}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/post/2017-11-05-log-likelihood-ratio_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;計算似然比，並作圖，注意方程圖形未變，&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; 軸的變化；取對數似然比，並作圖&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;LR &amp;lt;- L/max(L) ; head(LR)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0000000000 0.0004191759 0.0031233631 0.0098110584 0.0216286076
## [6] 0.0392577320&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(pi, LR, type = &amp;quot;l&amp;quot;, ylim = c(0, 1),yaxt=&amp;quot;n&amp;quot;, col=&amp;quot;darkblue&amp;quot;,
     frame.plot = FALSE, ylab = &amp;quot;&amp;quot;, xlab = &amp;quot;\U03C0&amp;quot;)
grid(NA, 5, lwd = 1)
axis(2, at=seq(0,1,0.2), las=2)
title(main = &amp;quot;Binomial likelihood ratio function\n 3 out of 10 subjects&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/post/2017-11-05-log-likelihood-ratio_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logLR &amp;lt;- log(L/max(L))
plot(pi, logLR, type = &amp;quot;l&amp;quot;, ylim = c(-4, 0),yaxt=&amp;quot;n&amp;quot;, col=&amp;quot;darkblue&amp;quot;,
     frame.plot = FALSE, ylab = &amp;quot;&amp;quot;, xlab = &amp;quot;\U03C0&amp;quot;)
grid(NA, 5, lwd = 1)
axis(2, at=seq(-4,0,1), las=2)
title(main = &amp;quot;Binomial log-likelihood ratio function\n 3 out of 10 subjects&amp;quot;)
abline(h=-1.92, lty=1, col=&amp;quot;red&amp;quot;)
axis(4, at=-1.92, las=0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/post/2017-11-05-log-likelihood-ratio_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;q2&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Q2&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;與上面用同樣的模型，但是觀察人數變爲 &lt;span class=&#34;math inline&#34;&gt;\(100\)&lt;/span&gt; 人 患病人數爲 &lt;span class=&#34;math inline&#34;&gt;\(30\)&lt;/span&gt; 人，試作對數似然比方程之圖形，與上圖對比：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/post/2017-11-05-log-likelihood-ratio_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;p&gt;可以看出，兩組數據的 MLE 都是一致的， &lt;span class=&#34;math inline&#34;&gt;\(\hat\pi=0.3\)&lt;/span&gt;，但是對數似然比方程圖形在 樣本量爲 &lt;span class=&#34;math inline&#34;&gt;\(n=100\)&lt;/span&gt; 時比 &lt;span class=&#34;math inline&#34;&gt;\(n=10\)&lt;/span&gt; 時窄很多，由此產生的似然比信賴區間也就窄很多（精確很多）。所以對數似然比方程的曲率（二階導數），反映了觀察獲得數據提供的對總體參數 &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; 推斷過程中的信息量。而且當樣本量較大時，對數似然比方程也更加接近左右對稱的二次方程曲線。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;q3&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Q3&lt;/h4&gt;
&lt;p&gt;在一個實施了160人年的追蹤調查中，觀察到8個死亡案例。使用泊松分佈模型，繪製對數似然比方程圖形，從圖形上目視推測極大似然比的 &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; 信賴區間。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;解-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;解&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\begin{aligned}  d = 8, \;p &amp;amp;= 160\; person\cdot year \\  \Rightarrow D\sim Poi(\mu &amp;amp;=\lambda p) \\  L(\lambda|data) &amp;amp;= Prob(D=d=8) \\  &amp;amp;= e^{-\mu}\frac{\mu^d}{d!} \\  &amp;amp;= e^{-\lambda p}\frac{\lambda^d p^d}{d!} \\  omitting&amp;amp;\;terms\;not\;in\;\lambda \\  &amp;amp;= e^{-\lambda p}\lambda^d \\ \Rightarrow \ell(\lambda|data)&amp;amp;= log(e^{-\lambda p}\lambda^d) \\  &amp;amp;= d\cdot log(\lambda)-\lambda p \\  &amp;amp; = 8\times log(\lambda) - 160\times\lambda \end{aligned}\)&lt;/span&gt;&lt;/p&gt;
&lt;img src=&#34;https://wangcc.me/post/2017-11-05-log-likelihood-ratio_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;480&#34; /&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
lambda
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
LogLR
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.010
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-6.4755033
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.011
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-5.8730219
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.012
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-5.3369308
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.013
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-4.8565892
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.014
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-4.4237254
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.015
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-4.0317824
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.016
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-3.6754743
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.017
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-3.3504773
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.018
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-3.0532100
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.019
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.7806722
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.020
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.5303259
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.021
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.3000045
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;&#34;&gt;
0.022
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;&#34;&gt;
-2.0878444
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;&#34;&gt;
0.023
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;&#34;&gt;
-1.8922303
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.024
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.7117534
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.025
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.5451774
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.026
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.3914117
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.027
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.2494891
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.028
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.1185480
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.029
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.9978174
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.030
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.8866050
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.031
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.7842864
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.032
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.6902968
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.033
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.6041236
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.034
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.5252998
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.035
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.4533996
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.036
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.3880325
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.037
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.3288407
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.038
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.2754948
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.039
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.2276909
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.040
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.1851484
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.041
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.1476075
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.042
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.1148271
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.043
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0865831
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.044
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0626670
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.045
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0428841
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.046
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0270529
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.047
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0150032
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.048
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0065760
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.049
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0016217
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.050
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000000
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.051
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0015790
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.052
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0062343
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.053
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0138487
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.054
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0243117
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.055
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0375186
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.056
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0533705
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.057
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0717739
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.058
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0926400
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.059
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.1158845
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.060
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.1414275
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.061
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.1691931
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.062
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.1991090
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.063
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.2311062
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.064
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.2651194
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.065
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.3010859
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.066
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.3389461
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.067
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.3786431
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.068
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.4201224
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.069
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.4633320
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.070
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.5082221
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.071
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.5547450
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.072
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.6028551
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.073
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.6525085
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.074
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.7036633
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.075
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.7562791
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.076
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.8103173
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.077
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.8657407
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.078
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.9225134
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.079
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.9806012
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.080
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.0399710
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.081
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.1005908
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.082
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.1624301
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.083
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.2254592
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.084
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.2896497
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.085
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.3549740
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.086
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.4214057
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.087
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.4889191
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.088
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.5574895
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.089
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.6270931
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.090
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.6977067
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.091
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.7693080
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.092
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.8418754
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;&#34;&gt;
0.093
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;&#34;&gt;
-1.9153881
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;&#34;&gt;
0.094
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;&#34;&gt;
-1.9898258
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.095
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.0651689
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.096
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.1413985
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.097
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.2184962
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.098
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.2964442
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.099
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.3752252
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.100
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.4548226
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;所以從列表數據結合圖形， 可以找到信賴區間的下限在 0.022~0.023 之間， 上限在 0.093～0.094 之間。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>偉大的中心極限定理</title>
      <link>https://wangcc.me/post/central-limit-theory/</link>
      <pubDate>Thu, 19 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/central-limit-theory/</guid>
      <description>
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;最近明顯可以感覺到課程的步驟開始加速。看我的課表：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/img/IMG_0522.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;手機畫面太小了。早上都是9點半開始，下午基本都是到5點。週一更慘，到7點。週二-週五中午都被統計中心的講座佔據。簡直是非人的生活。&lt;/p&gt;
&lt;p&gt;這周概率論基礎結束。中心極限定理講完以後我們正式進入了 Inference 統計推斷的課程。我們花了一天時間講什麼是樣本估計 (Estimation)，什麼是參數精確度 (Precision)，什麼是自由度 (degree of freedom)，怎樣進行不偏的估計 (unbiased inference)。然後還有似然方程 (likelihood function)。&lt;/p&gt;
&lt;p&gt;今天的更新還是簡單的把概率論掃尾一下。感受一下中心極限定理的偉大。&lt;/p&gt;
&lt;div id=&#34;協方差-covariance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;協方差 Covariance&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://winterwang.github.io/post/probability2-4/&#34;&gt;之前我們定義過&lt;/a&gt;，兩個獨立連續隨機變量 &lt;span class=&#34;math inline&#34;&gt;\(X,Y\)&lt;/span&gt; 之和的方差 Variance ：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Var(X+Y)=Var(X)+Var(Y)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;然而如果他們並不相互獨立的話：&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
Var(X+Y) &amp;amp;= E[((X+Y)-E(X+Y))^2] \\
         &amp;amp;= E[(X+Y)-(E(X)+E(Y))^2] \\
         &amp;amp;= E[(X-E(X)) - (Y-E(Y))^2] \\
         &amp;amp;= E[(X-E(X))^2+(Y-E(Y))^2 \\
         &amp;amp; \;\;\; +2(X-E(X))(Y-E(Y))] \\
         &amp;amp;= Var(X)+Var(Y)+2E[(X-E(X))(Y-E(Y))]
\end{aligned}\]&lt;/span&gt;
&lt;p&gt;可以發現在兩者和的方差公式展開之後多了一部分 &lt;span class=&#34;math inline&#34;&gt;\(E[(X-E(X))(Y-E(Y))]\)&lt;/span&gt;。 這個多出來的一部分就說明了二者 &lt;span class=&#34;math inline&#34;&gt;\((X, Y)\)&lt;/span&gt; 之間的關係。它被定義爲協方差 (Covariance):
&lt;span class=&#34;math display&#34;&gt;\[Cov(X,Y) = E[(X-E(X))(Y-E(Y))]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;所以：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Var(X+Y)=Var(X)+Var(Y)+2Cov(X,Y)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    要記住，協方差只能用於評價&lt;span class=&#34;math inline&#34;&gt;(X,Y)&lt;/span&gt;之間的線性關係 (Linear Association)。
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;以下是協方差 (Covariance) 的一些特殊性質：&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Cov(X,X)=Var(X)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Cov(X,Y)=Cov(Y,X)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Cov(aX,bY)=ab\:Cov(X,Y)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Cov(aR+bS,cX+dY)=ac\:Cov(R,X)+ad\:Cov(R,Y)\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+bc\:Cov(S,X)+bd\:Cov(S,Y)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Cov(aX+bY,cX+dY)=ac\:Var(X)+ad\:Var(Y)\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+(ad+bc)Cov(X,Y)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Cov(X+Y,X-Y)=Var(X)-Var(Y)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;If &lt;span class=&#34;math inline&#34;&gt;\(X, Y\)&lt;/span&gt; are independent. &lt;span class=&#34;math inline&#34;&gt;\(Cov(X,Y)=0\)&lt;/span&gt; &lt;span class=&#34;diff_alert&#34;&gt;But not vise-versa !&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;相關-correlation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;相關 Correlation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;協方差雖然&lt;span class=&#34;math inline&#34;&gt;\(Cov(X,Y)\)&lt;/span&gt; 的大小很大程度上會被他們各自的單位和波動大小左右。&lt;/li&gt;
&lt;li&gt;我們將協方差標準化(除以各自的標準差 s.d.) (standardization) 之後，就可以得到相關係數 Corr (&lt;span class=&#34;math inline&#34;&gt;\(-1\sim1\)&lt;/span&gt;):
&lt;span class=&#34;math display&#34;&gt;\[Corr(X,Y)=\frac{Cov(X,Y)}{SD(X)SD(Y)}=\frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;中心極限定理-the-central-limit-theory&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;中心極限定理 the Central Limit Theory&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;diff_add&#34;&gt;&lt;strong&gt;如果從人羣中多次選出樣本量爲 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 的樣本，並計算樣本均值, &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}_n\)&lt;/span&gt;。那麼這個樣本均值 &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}_n\)&lt;/span&gt; 的分佈，會隨着樣本量增加 &lt;span class=&#34;math inline&#34;&gt;\(n\rightarrow\infty\)&lt;/span&gt;，而接近正態分佈。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;偉大的中心極限定理告訴我們：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;diff_alert&#34;&gt;&lt;strong&gt;當樣本量足夠大時，樣本均值 &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}_n\)&lt;/span&gt; 的分佈爲正態分佈，這個特性與樣本來自的人羣的分佈 &lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt; 無關。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;再說一遍：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;如果對象是獨立同分佈 i.i.d (identically and independently distributed)。那麼它的總體期望和方差分別是: &lt;span class=&#34;math inline&#34;&gt;\(E(X)=\mu;\;Var(X)=\sigma^2\)&lt;/span&gt;。
根據中心極限定理，可以得到：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;當樣本量增加，樣本均值的分佈服從正態分佈：
&lt;span class=&#34;math display&#34;&gt;\[\bar{X}_n\sim N(\mu, \frac{\sigma^2}{n})\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;也可以寫作，當樣本量增加：
&lt;span class=&#34;math display&#34;&gt;\[\sum_{i=1}^nX_i \sim N(n\mu,n\sigma^2)\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;有了這個定理，我們可以拋開樣本空間(&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;)的分佈，也不用假定它服從正態分佈。&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;diff_alert&#34;&gt;但是樣本的均值，卻總是服從正態分佈的。&lt;/span&gt;簡直是太完美了！！！！！！&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>你買的彩票中獎概率到底有多少？</title>
      <link>https://wangcc.me/post/probability3/</link>
      <pubDate>Wed, 11 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/probability3/</guid>
      <description>
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;二項分佈的概念-binomial-distribution&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;二項分佈的概念 Binomial distribution&lt;/h3&gt;
&lt;p&gt;二項分佈在醫學研究中至關重要，一組二項分佈的數據，指的通常是 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 次相互獨立的&lt;a href=&#34;https://winterwang.github.io/post/probability2-4/&#34;&gt;成功率爲 &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; 的伯努利實驗&lt;/a&gt; (&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; independent Bernoulli trials) 中成功的次數。&lt;/p&gt;
&lt;p&gt;當 &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 服從二項分佈，記爲 &lt;span class=&#34;math inline&#34;&gt;\(X \sim binomial(n, \pi)\)&lt;/span&gt; 或&lt;span class=&#34;math inline&#34;&gt;\(X \sim bin(n, \pi)\)&lt;/span&gt;。它的(第 &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; 次實驗的)概率被定義爲：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
P(X=x) &amp;amp;= ^nC_x\pi^x(1-\pi)^{n-x} \\
       &amp;amp;= \binom{n}{x}\pi^x(1-\pi)^{n-x} \\
       &amp;amp; for\;\; x = 0,1,2,\dots,n
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;二項分佈的期望和方差&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;二項分佈的期望和方差&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;期望 &lt;span class=&#34;math inline&#34;&gt;\(E(X)\)&lt;/span&gt;
&lt;ul&gt;
&lt;li&gt;若 &lt;span class=&#34;math inline&#34;&gt;\(X \sim bin(n,\pi)\)&lt;/span&gt;，那麼 &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 就是這一系列獨立伯努利實驗中成功的次數。&lt;/li&gt;
&lt;li&gt;用 &lt;span class=&#34;math inline&#34;&gt;\(X_i, i =1,\dots, n\)&lt;/span&gt; 標記每個相互獨立的伯努利實驗。&lt;/li&gt;
&lt;li&gt;那麼我們可以知道 &lt;span class=&#34;math inline&#34;&gt;\(X=\sum_{i=1}^nX_i\)&lt;/span&gt;。
&lt;span class=&#34;math display&#34;&gt;\[\begin{align} E(X) &amp;amp;= E(\sum_{i=1}^nX_i)\\
                   &amp;amp;= E(X_1+X_2+\cdots+X_n) \\
                   &amp;amp;= E(X_1)+E(X_2)+\cdots+E(X_n)\\
                   &amp;amp;= \sum_{i=1}^nE(X_i)\\
                   &amp;amp;= \sum_{i=1}^n\pi \\
                   &amp;amp;= n\pi
\end{align}\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;方差 &lt;span class=&#34;math inline&#34;&gt;\(Var(X)\)&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
Var(X) &amp;amp;= Var(\sum_{i=1}^nX_i) \\
      &amp;amp;= Var(X_i+X_2+\cdots+X_n) \\
      &amp;amp;= Var(X_i)+Var(X_2)+\cdots+Var(X_n) \\
      &amp;amp;= \sum_{i=1}^nVar(X_i) \\
      &amp;amp;= n\pi(1-\pi) \\
\end{align}\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;超幾何分佈-hypergeometric-distribution&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;超幾何分佈 hypergeometric distribution&lt;/h3&gt;
&lt;p&gt;假設我們從總人數爲 &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; 的人羣中，採集一個樣本 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;。假如已知在總體人羣中(&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;)有 &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; 人患有某種疾病。請問採集的樣本 &lt;span class=&#34;math inline&#34;&gt;\(X=n\)&lt;/span&gt; 中患有這種疾病的人，服從怎樣的分佈？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;從人羣(&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;)中取出樣本(&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;)，有 &lt;span class=&#34;math inline&#34;&gt;\(^NC_n\)&lt;/span&gt; 種方法。&lt;/li&gt;
&lt;li&gt;從患病人羣(&lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt;)中取出患有該病的人(&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;)有 &lt;span class=&#34;math inline&#34;&gt;\(^MC_x\)&lt;/span&gt; 種方法。&lt;/li&gt;
&lt;li&gt;樣本中不患病的人(&lt;span class=&#34;math inline&#34;&gt;\(n-x\)&lt;/span&gt;)被採樣的方法有 &lt;span class=&#34;math inline&#34;&gt;\(^{N-M}C_{n-x}\)&lt;/span&gt; 種。&lt;/li&gt;
&lt;li&gt;採集一次 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 人作爲樣本的概率都一樣。因此：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(X=x)=\frac{\binom{M}{x}\binom{N-M}{n-x}}{\binom{N}{n}}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;樂透中獎概率問題&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;樂透中獎概率問題：&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;從數字 &lt;span class=&#34;math inline&#34;&gt;\(1\sim59\)&lt;/span&gt; 中選取 &lt;span class=&#34;math inline&#34;&gt;\(6\)&lt;/span&gt; 個任意號碼&lt;/li&gt;
&lt;li&gt;開獎時從 &lt;span class=&#34;math inline&#34;&gt;\(59\)&lt;/span&gt; 個號碼球中隨機抽取 &lt;span class=&#34;math inline&#34;&gt;\(6\)&lt;/span&gt; 個&lt;/li&gt;
&lt;li&gt;如果六個號碼全部猜中(不分順序)，你可以成爲百萬富翁。請問一次猜中全部 &lt;span class=&#34;math inline&#34;&gt;\(6\)&lt;/span&gt; 個號碼的概率是多少？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;從 &lt;span class=&#34;math inline&#34;&gt;\(59\)&lt;/span&gt; 個號碼中隨機取出任意 &lt;span class=&#34;math inline&#34;&gt;\(6\)&lt;/span&gt; 個號碼的方法有 &lt;span class=&#34;math inline&#34;&gt;\(^{59}C_6\)&lt;/span&gt; 種。
&lt;span class=&#34;math display&#34;&gt;\[^{59}C_6=\frac{59!}{6!(59-6)!}=45,057,474\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;每次選取六個號碼做爲一組的可能性相同，所以，你買了一組樂透號碼，能中獎的概率就是 &lt;span class=&#34;math inline&#34;&gt;\(1/45,057,474 = 0.00000002219\)&lt;/span&gt;。你還會再去買彩票麼？&lt;/p&gt;
&lt;div id=&#34;如果我只想中其中的-3-個號碼概率有多大&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;如果我只想中其中的 &lt;span class=&#34;math inline&#34;&gt;\(3\)&lt;/span&gt; 個號碼，概率有多大？&lt;/h4&gt;
&lt;p&gt;用超幾何分佈的概率公式：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
P(X=3) &amp;amp;= \frac{^6C_3\times ^{53}C_3}{^{59}C_6} \\
       &amp;amp;= 0.010
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;你有 &lt;span class=&#34;math inline&#34;&gt;\(1\%\)&lt;/span&gt; 的可能中獎。換句話說，如果中三個以上的數字算中獎的話，你買的彩票中獎的概率低於 &lt;span class=&#34;math inline&#34;&gt;\(1\%\)&lt;/span&gt;。是不是覺得下次送錢給博彩公司的時候還不如跟我一起喝一杯咖啡划算？&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;泊松分佈-poisson-distribution&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;泊松分佈 Poisson Distribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;當一個事件，在一段時間 (&lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;) 中可能發生的次數是 &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; 。那麼我們可以認爲，經過時間 &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;，該時間發生的期望次數是 &lt;span class=&#34;math inline&#34;&gt;\(E(X)=\lambda T\)&lt;/span&gt;。&lt;/li&gt;
&lt;li&gt;利用微分思想，將這段時間 &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; 等分成 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 個時間段，當 &lt;span class=&#34;math inline&#34;&gt;\(n\rightarrow\infty\)&lt;/span&gt; 直到每個微小的時間段內最多發生一次該事件。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;那麼&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;每個微小的時間段，可以視爲是一個伯努利實驗（有事件發生或者沒有）&lt;/li&gt;
&lt;li&gt;那麼這整段時間 &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; 內發生的事件可以視爲是一個二項分佈實驗。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;令 &lt;span class=&#34;math inline&#34;&gt;\(X=\)&lt;/span&gt; 一次事件發生時所經過的所有時間段。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(X \sim Bin(n, \pi)\)&lt;/span&gt;，其中 &lt;span class=&#34;math inline&#34;&gt;\(n\rightarrow\infty\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 爲時間段。&lt;/li&gt;
&lt;li&gt;在每個分割好的時間段內，事件發生的概率都是：&lt;span class=&#34;math inline&#34;&gt;\(\pi=\frac{\lambda T}{n}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;期望 &lt;span class=&#34;math inline&#34;&gt;\(\mu=\lambda T \Rightarrow \pi=\mu/n\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;所以 &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 的概率方程就是：
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
P(X=x) &amp;amp;= \binom{n}{x}\pi^x(1-\pi)^{n-x} \\
     &amp;amp;= \binom{n}{x}(\frac{\mu}{n})^x(1-\frac{\mu}{n})^{n-x} \\
     &amp;amp;= \frac{n!}{x!(n-x)!}(\frac{\mu}{n})^x(1-\frac{\mu}{n})^{n-x} \\
     &amp;amp;=\frac{n!}{n^x(n-x)!}\frac{\mu^x}{x!}(1-\frac{\mu}{n})^{n-x}\\
當 n\rightarrow\infty   &amp;amp;\; x \ll n (x遠小於n) 時\\
\frac{n!}{n^x(n-x)!} &amp;amp;=\frac{n(n-1)\dots(n-x+1)}{n^x} \rightarrow 1\\
(1-\frac{\mu}{n})^{n-x} &amp;amp;\approx  (1-\frac{\mu}{n})^n \rightarrow e^{-\mu}\\
所以 我們可&amp;amp;以得到泊松分佈的概率公式：   \\
P(X=x) &amp;amp;\rightarrow \frac{\mu^x}{x!}e^{-\mu}
\end{align}\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;當數據服從泊松分佈時，記爲 &lt;span class=&#34;math inline&#34;&gt;\(X\sim Poisson(\mu=\lambda T)\;\; or\;\; X\sim Poi(\mu)\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;證明泊松分佈的參數特徵&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;證明泊松分佈的參數特徵：&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(E(X)=\mu\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
E(X)  &amp;amp;=  \sum_{x=0}^\infty xP(X=x) \\
      &amp;amp;=  \sum_{x=0}^\infty x\frac{\mu^x}{x!}e^{-\mu} \\
      &amp;amp;= 0+ \sum_{x=1}^\infty x\frac{\mu^x}{x!}e^{-\mu} \\
      &amp;amp;=  \sum_{x=1}^\infty \frac{\mu^x}{(x-1)!}e^{-\mu} \\
      &amp;amp;=  \mu\sum_{x=1}^\infty \frac{\mu^{x-1}}{(x-1)!}e^{-\mu} \\
這個時候我們用i&amp;amp;=x-1 替換掉所有的 x \\
      &amp;amp;=  \mu\sum_{i=0}^\infty \frac{\mu^{i}}{i!}e^{-\mu} \\
注意到右半部分 &amp;amp;\sum_{i=0}^\infty \frac{\mu^{i}}{i!}e^{-\mu}=1 是一個\\泊松分佈的所有&amp;amp;概率和 \\
      &amp;amp;= \mu
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Var(x)=\mu\)&lt;/span&gt;
爲了找到 &lt;span class=&#34;math inline&#34;&gt;\(Var(X)\)&lt;/span&gt;，我們用公式 &lt;span class=&#34;math inline&#34;&gt;\(Var(X)=E(X^2)-E(X)^2\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;我們需要找到 &lt;span class=&#34;math inline&#34;&gt;\(E(X^2)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
E(X^2) &amp;amp;= \sum_{x=0}^\infty x^2\frac{\mu^x}{x!}e^{-\mu} \\
       &amp;amp;= \mu \sum_{x=1}^\infty x\frac{\mu^{x-1}}{(x-1)!}e^{-\mu} \\
這個時候我們用i&amp;amp;=x-1 替換掉所有的 x \\
       &amp;amp;= \mu \sum_{i=0}^\infty (i+1)\frac{\mu^{i}}{i!}e^{-\mu} \\
       &amp;amp;= \mu(\sum_{i=0}^\infty i\frac{\mu^i}{i!}e^{-\mu} + \sum_{i=0}^\infty \frac{\mu^i}{i!}e^{-\mu}) \\
       &amp;amp;= \mu(E(X)+1) \\
       &amp;amp;= \mu^2+\mu \\
因此，代入上面&amp;amp;提到的方差公式： \\
Var(X) &amp;amp;= E(X^2) - E(X)^2 \\
       &amp;amp;= \mu^2 + \mu -\mu^2 \\
       &amp;amp;= \mu
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>正態分佈</title>
      <link>https://wangcc.me/post/normal-distribution/</link>
      <pubDate>Wed, 11 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/normal-distribution/</guid>
      <description>
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;概率密度曲線-probability-density-function-pdf&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;概率密度曲線 probability density function， PDF&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;一個隨機連續型變量 &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 它的性質由一個對應的&lt;strong&gt;概率密度方程 (probability density function, PDF)&lt;/strong&gt; 決定。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;在給定的範圍區間內，如 &lt;span class=&#34;math inline&#34;&gt;\(a\sim b, (a &amp;lt; b)\)&lt;/span&gt;，它的概率滿足:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(a\leqslant X \leqslant b) = \int_a^bf(x)dx\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;這個相關的方程，在 &lt;span class=&#34;math inline&#34;&gt;\(a\sim b\)&lt;/span&gt; 區間內的積分，就是這個連續變量在這個區間內取值的概率。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R codes for drawing a standard normal distribution by using ggplot2
library(ggplot2)
p &amp;lt;- ggplot(data.frame(x=c(-3,3)), aes(x=x)) +
  stat_function(fun = dnorm)
p + annotate(&amp;quot;text&amp;quot;, x=2, y=0.3, parse=TRUE, label=&amp;quot;frac(1, sqrt(2*pi)) * e ^(-z^2/2)&amp;quot;) +
  theme(plot.subtitle = element_text(vjust = 1),
        plot.caption = element_text(vjust = 1),
        axis.text.x = element_text(size = 12),
        axis.text.y = element_text(size = 12),
        plot.title = element_text(size = 10, face = &amp;quot;bold&amp;quot;, hjust = 0.5),
        panel.background = element_rect(fill = &amp;quot;ivory&amp;quot;)) +
  labs(title = &amp;quot;Probability density functions \n for standard normal distribution&amp;quot;,
       x = NULL, y = NULL) +
  stat_function(fun = dnorm,
                xlim = c(-1.3,0.4),
                geom = &amp;quot;area&amp;quot;,fill=&amp;quot;#00688B&amp;quot;, alpha= 0.2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/post/2017-10-11-normal-distribution_files/figure-html/normal%20distribution%20graph-1.png&#34; width=&#34;528&#34; /&gt;&lt;/p&gt;
&lt;p&gt;注意：整個方程的曲線下面積等於 &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;：
&lt;span class=&#34;math display&#34;&gt;\[\int_{-\infty}^\infty f(x)dx=1\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;期望 &lt;span class=&#34;math inline&#34;&gt;\(E(X)=\int_{-\infty}^\infty xf(x)dx\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;方差 &lt;span class=&#34;math inline&#34;&gt;\(Var(X)=\int_{-\infty}^\infty (x-\mu)^2f(x)dx\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;正態分佈&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;正態分佈&lt;/h3&gt;
&lt;p&gt;如果一組數據服從正態分佈，我們通常用它的期望（或者叫平均值）&lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;，和它的方差 &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;，來描述這組數據。記爲：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X \sim N(\mu, \sigma^2)\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;它的概率密度方程可以表述爲：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[f(x)=\frac{1}{\sqrt{2\pi\sigma^2}}exp(-\frac{(x-\mu)^2}{2\sigma^2})\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(E(x) =\mu\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Var(x)=\sigma^2\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;標準正態分佈&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;標準正態分佈&lt;/h3&gt;
&lt;p&gt;標準正態分佈的期望（或者均值）爲 &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;，方差爲 &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;記爲：&lt;span class=&#34;math inline&#34;&gt;\(Z \sim N(0,1)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;它的概率密度方程表述爲：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{1}{\sqrt{2\pi}}exp(-\frac{z^2}{2})\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;它的累積分佈方程 (cumulative distribution function， CDF)，是將概率密度方程 (PDF) 積分以後獲得的方程。通常我們記爲 &lt;span class=&#34;math inline&#34;&gt;\(\Phi(z)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;再看一下標準正態分佈的概率密度方程曲線：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/post/2017-10-11-normal-distribution_files/figure-html/normal%20distribution%20graph2-1.png&#34; width=&#34;528&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;95% 的曲線下面積在標準差 standard deviation &lt;span class=&#34;math inline&#34;&gt;\(-1.96\sim1.96\)&lt;/span&gt; 之間的區域。&lt;/li&gt;
&lt;li&gt;而且，&lt;span class=&#34;math inline&#34;&gt;\(\phi(-x)=1-\phi(x)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;任何一個正態分佈都可以通過下面的公式，標準化成爲標準正態分佈：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Z=\frac{X-\mu}{\sigma}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>概率論2</title>
      <link>https://wangcc.me/post/probability2-4/</link>
      <pubDate>Tue, 10 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/probability2-4/</guid>
      <description>
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;bayes-理論的概念&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bayes 理論的概念&lt;/h3&gt;
&lt;p&gt;許多時候，我們需要將概率中的條件相互對調。
例如：
在已知該人羣中有20%的人有吸菸習慣(&lt;span class=&#34;math inline&#34;&gt;\(P(S)\)&lt;/span&gt;)，吸菸的人有9%的概率有哮喘(&lt;span class=&#34;math inline&#34;&gt;\(P(A|S)\)&lt;/span&gt;)，不吸菸的人有7%的概率有哮喘(&lt;span class=&#34;math inline&#34;&gt;\(P(A|\bar{S})\)&lt;/span&gt;)的前提下，有個人前來門診，發現是哮喘患者，那麼這個人有多大的概率是一個菸民？也就是要求 &lt;span class=&#34;math inline&#34;&gt;\(P(S|A)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;這裏先引入貝葉斯的概念：&lt;/p&gt;
&lt;p&gt;我們可以將 &lt;span class=&#34;math inline&#34;&gt;\(P(A\cap S)\)&lt;/span&gt; 寫成：
&lt;span class=&#34;math display&#34;&gt;\[P(A\cap S)=P(A|S)P(S)\\or\\
P(A\cap S)=P(S|A)P(A)\]&lt;/span&gt;
這兩個等式是完全等價的。我們將他們連起來：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(S|A)P(A)=P(A|S)P(S)\\
\Rightarrow P(S|A)=\frac{P(A|S)P(S)}{P(A)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;是不是看起來又像是寫了一堆&lt;strong&gt;廢話&lt;/strong&gt;？
沒錯，你看出來是一堆廢話的時候，證明你也同意這背後的簡單邏輯。&lt;/p&gt;
&lt;p&gt;再繼續，我們可以利用另外一個&lt;strong&gt;廢話&lt;/strong&gt;：&lt;span class=&#34;math inline&#34;&gt;\(\because S+\bar{S}=1\\ \therefore P(A)=P(A\cap S)+P(A\cap\bar{S})\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;用上面的公式替換掉 &lt;span class=&#34;math inline&#34;&gt;\(P(A\cap S)+P(A\cap\bar{S}） \\ \therefore P(A)=P(A|S)P(S)+P(A|\bar{S})P(\bar{S})\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;可以得到&lt;strong&gt;貝葉斯理論公式&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(S|A)=\frac{P(A|S)P(S)}{P(A|S)P(S)+P(A|\bar{S})P(\bar{S})}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;回到上面說到的哮喘人中有多少比例吸菸的問題。可以繼續使用概率樹來方便的計算：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/img/Selection_073.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
P(S|A) &amp;amp;= \frac{P(A|S)P(S)}{P(A|S)P(S)+P(A|\bar{S})P(\bar{S})} \\
        &amp;amp;= \frac{0.09\times0.2}{0.09\times0.2+0.07\times0.8} \\
        &amp;amp;= 0.24
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;所以我們的結論就是，在已知該人羣中有20%的人有吸菸習慣(&lt;span class=&#34;math inline&#34;&gt;\(P(S)\)&lt;/span&gt;)，吸菸的人有9%的概率有哮喘(&lt;span class=&#34;math inline&#34;&gt;\(P(A|S)\)&lt;/span&gt;)，不吸菸的人有7%的概率有哮喘(&lt;span class=&#34;math inline&#34;&gt;\(P(A|\bar{S})\)&lt;/span&gt;)的前提下，有個人前來門診，發現是哮喘患者，那麼這個人有24% 的概率是一個菸民(&lt;span class=&#34;math inline&#34;&gt;\(P(S|A)\)&lt;/span&gt;)。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;期望-expectation-或均值-or-mean-和-方差-variance&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;期望 Expectation (或均值 or mean) 和 方差 Variance&lt;/h3&gt;
&lt;p&gt;期望（或均值）是用來描述一組數據中心位置的指標（另一個是中位數 Median）。
對於離散型隨機變量 &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; (discrete random variables)，它的期望被定義爲：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[E(X)=\sum_x xP(X=x)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;所以就是將所有 &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 可能取到的值乘以相應的概率後求和。這個期望（或均值）常常用希臘字母 &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; 來標記。&lt;/p&gt;
&lt;p&gt;方差 Variance 是衡量一組數據變化幅度(dispersion/variability)的指標之一。 方差的定義是：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Var(X)=E((X-\mu)^2)\\其中，\mu=E(x)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;實際上我們更加常用的是它的另外一個公式：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Var(X)=E(X^2)-E(X)^2\]&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;證明-上面兩個方差公式相等&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;證明 上面兩個方差公式相等&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
Var(x)  &amp;amp;= E((X-\mu)^2) \\
        &amp;amp;= E(X^2-2X\mu+\mu^2)\\
        &amp;amp;= E(X^2) - 2\mu E(X) + \mu^2\\
        &amp;amp;= E(X^2) - 2\mu^2 + \mu^2 \\
        &amp;amp;= E(X^2) - \mu^2 \\
        &amp;amp;= E(X^2) - E(X)^2
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;方差的性質&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;方差的性質：&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Var(X+b)=Var(X)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Var(aX)=a^2Var(X)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Var(aX+b)=a^2Var(X)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;伯努利分佈-bernoulli-distribution&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;伯努利分佈 Bernoulli distribution&lt;/h3&gt;
&lt;p&gt;伯努利分佈，說的就是一個簡單的二分變量 (1, 0)，它取1時的概率如果是 &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt;。那麼我們可以計算這個分佈的期望值:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
E(X) &amp;amp;=\sum_x xP(X=x) \\
     &amp;amp;=1\times\pi + 0\times(1-\pi)\\
     &amp;amp;=\pi
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;由於 &lt;span class=&#34;math inline&#34;&gt;\(x=x^2\)&lt;/span&gt;，因爲 &lt;span class=&#34;math inline&#34;&gt;\(x=0,1\)&lt;/span&gt;, 所以 &lt;span class=&#34;math inline&#34;&gt;\(E[X^2]=E[X]\)&lt;/span&gt;，那麼方差爲：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
Var(X) &amp;amp;=E[X^2]-E[X]^2 \\
       &amp;amp;=E[X]-E[X]^2 \\
       &amp;amp;=\pi - \pi^2 \\
       &amp;amp;=\pi(1-\pi)
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;證明xy-爲互爲獨立的隨機離散變量時-a-exyexey-b-varxyvarxvary&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;證明，&lt;span class=&#34;math inline&#34;&gt;\(X,Y\)&lt;/span&gt; 爲互爲獨立的隨機離散變量時，&lt;br&gt;a) &lt;span class=&#34;math inline&#34;&gt;\(E(XY)=E(X)E(Y)\)&lt;/span&gt; ; &lt;br&gt;b) &lt;span class=&#34;math inline&#34;&gt;\(Var(X+Y)=Var(X)+Var(Y)\)&lt;/span&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;strong&gt;證明&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
E(XY) &amp;amp;= \sum_x\sum_y xyP(X=x, Y=y) \\
\because &amp;amp;\; X,Y are\;independent\;to\;each\;other \\
\therefore &amp;amp;= \sum_x\sum_y xyP(X=x)P(Y=y)\\
      &amp;amp;=\sum_x xP(X=x)\sum_y yP(Y=y)\\
      &amp;amp;=E(X)E(Y)
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;ol start=&#34;2&#34; style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;strong&gt;證明&lt;/strong&gt;
根據方差的定義：
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
Var(X+Y) &amp;amp;= E((X+Y)^2)-E(X+Y)^2 \\
    &amp;amp; \; Expand \\
    &amp;amp;=E(X^2+2XY+Y^2)-(E(X)+E(Y))^2\\
    &amp;amp;=E(X^2)+E(Y^2)+2E(XY)\\
    &amp;amp;\;\;\; - E(X)^2-E(Y)^2-2E(X)E(Y)\\
    &amp;amp;\; We\;just\;showed\; E(XY)=E(X)E(Y)\\
    &amp;amp;=E(X^2)-E(X)^2+E(Y^2)-E(Y)^2 \\
    &amp;amp;=Var(X)+Var(Y)
\end{align}\]&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>你會用概率論來賭博嗎？</title>
      <link>https://wangcc.me/post/probability-gambling/</link>
      <pubDate>Sun, 08 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/probability-gambling/</guid>
      <description>
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;轉眼我已經進入課程的第二週了，總體來說，我們一半的時間都在電腦房練習 Stata 的數據清理和簡單的描述統計 (descriptive statistics)。從我個人的經驗來說，數據分析的過程，其實一大半的時間是消耗在 data cleaning 上的，即使手頭拿到了所謂的乾淨的數據，到真正要分析的時候就會發現一大堆的問題在裏面，需要重新整理，重新添加標記以使之變得更加讓人類可以讀懂。電腦是機器，他是不管你的數據是否乾淨的。只要你放了數據進去，邏輯還可以，沒有編程上的語法錯誤，它總歸會出來一些報告和結果的。如果就這麼直接用的話，大部分的人就會掉進陷阱。畢竟數據不光會說出事實真相，&lt;strong&gt;更多的情況下還會把真相給掩蓋住了。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;我的其餘大部分時間都用在了複習高等數學的微積分上了。感覺好似回到了高中時代。其實大學的時候線性代數得分還是接近滿分的。後來多年不用，生疏了。剛打開複習的書的時候，許多微分積分的規則都已經忘記。通過這一週的辛苦練習，終於是找回了一點狀態。如果你也想有空的時候複習以下高中數學知識，這本書可以推薦給你：&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.amazon.co.uk/gp/product/0471827223/ref=oh_aui_detailpage_o04_s00?ie=UTF8&amp;amp;psc=1&#34;&gt;Quick Calculus: Short Manual of Self-instruction&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/img/Selection_070.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;上面這本書的內容可以一邊閱讀，一邊練習。實在是複習的一本好書。我花了一週的課餘時間，從頭到尾把裏面的習題和解答全部完成。收穫很大。感覺年輕時的數學思維又開始在大腦裏復甦了。一身輕鬆。&lt;/p&gt;
&lt;p&gt;下面想介紹一下上週學習的概率的基礎問題。&lt;/p&gt;
&lt;div id=&#34;首先是最基礎的三個概率的公理&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;首先是最基礎的&lt;strong&gt;三個概率的公理&lt;/strong&gt;：&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;對於任意事件 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;，它發生的概率 &lt;span class=&#34;math inline&#34;&gt;\(P(A)\)&lt;/span&gt; 滿足這樣的不等式： &lt;span class=&#34;math inline&#34;&gt;\(0 \leqslant P(A) \leqslant 1\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(\Omega)=1\)&lt;/span&gt; , &lt;span class=&#34;math inline&#34;&gt;\(\Omega\)&lt;/span&gt; 是全樣本空間 (total sample space)&lt;/li&gt;
&lt;li&gt;對於互斥（相互獨立）的事件 &lt;span class=&#34;math inline&#34;&gt;\(A_1, A_2, \dots, A_n\)&lt;/span&gt; 有如下的等式關係： &lt;span class=&#34;math inline&#34;&gt;\(P(A_1\cup A_2 \cup \cdots \cup A_n)=P(A_1)+P(A_2)+\cdots+P(A_n)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;你是不是覺得上面三條公理都是&lt;strong&gt;廢話&lt;/strong&gt;。
不用擔心，我也是這麼覺得的。因爲所有人都認同的道理，才能成爲公理 (axiom)，因爲它們是不需要證明的自然而然形成的人人都接受的觀念。&lt;code&gt;(axiom: a saying that is widely accepted on its own merits; its truth is assumed to be self-evident)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;然而，正是這樣顯而易見的道理，確是拿來建築理論的基石，千萬不能小看了他們。例如，我們看下面這個看似也應該成爲公理的公式，你能證明嗎：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(A_1\cup A_2) = P(A_1) + P(A_2) - P(A_1 \cap A_2)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/img/venngram.png&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;證明&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;證明：&lt;/h4&gt;
&lt;p&gt;先考慮 &lt;span class=&#34;math inline&#34;&gt;\(A_1 \cup A_2\)&lt;/span&gt; 是什麼（拆分成三個互斥事件）&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(A_1 \cup A_2 = (A_1\cap \bar{A_2})\cup(\bar{A_1}\cap A_2)\cup(A_1\cap A_2)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;運用上面的公理&lt;del&gt;2&lt;/del&gt; 3&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\therefore P(A_1 \cup A_2) = P(A_1\cap \bar{A_2}) + P(\bar{A_1}\cap A_2) + P(A_1\cap A_2) \;\;\;\;\;\;(1)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;再考慮 &lt;span class=&#34;math inline&#34;&gt;\(A_1=(A_1\cap A_2)\cup(A_1\cap\bar{A_2})\)&lt;/span&gt; 繼續拆分成兩個互斥事件&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\therefore P(A_1)=P(A_1\cap A_2)+P(A_1\cap\bar{A_2})\)&lt;/span&gt; 整理一下：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(A_1\cap\bar{A_2})=P(A_1)-P(A_1\cap A_2)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;同理可得: &lt;span class=&#34;math inline&#34;&gt;\(P(\bar{A_1}\cap A_2)=P(A_2)-P(A_1\cap A_2)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;代入上面第(1)式可得：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(A_1 \cup A_2) =P(A_1)-P(A_1\cap A_2)\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+P(A_2)-P(A_1\cap A_2)\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+P(A_1\cap A_2)\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;=P(A_1) + P(A_2) - P(A_1 \cap A_2)\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;條件概率-conditional-probability&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;條件概率 Conditional probability&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(A|S)=\frac{P(A\cap S)}{P(S)}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(A\cap S) = P(A|S)P(S)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;獨立-independence-的定義&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;獨立 (independence) 的定義&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;兩個事件定義爲互爲獨立時 (&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; are said to be independent &lt;strong&gt;if and only if&lt;/strong&gt;)
&lt;span class=&#34;math display&#34;&gt;\[P(A\cap B)=P(A)P(B)\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;因爲從條件概率的概念我們已知&lt;br&gt; &lt;span class=&#34;math inline&#34;&gt;\(P(A\cap B) = P(A|B)P(B)\)&lt;/span&gt; &lt;br&gt;所以&lt;span class=&#34;math inline&#34;&gt;\(P(A|B)=P(A)\)&lt;/span&gt; 即：事件 &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; 無法提供事件 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 的任何有效訊息 (&lt;strong&gt;&lt;span class=&#34;math inline&#34;&gt;\(A, B\)&lt;/span&gt; 互相獨立&lt;/strong&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;賭博問題&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;賭博問題&lt;/h2&gt;
&lt;p&gt;終於來到本次話題的重點了。我要扣題了哦。語文老師快在此加分。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://wangcc.me/img/Selection_071.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;假設你在一個電視遊戲節目。有上圖一樣的三扇門。其中一扇門後面有一輛保時捷，另兩扇門後面則是&lt;a href=&#34;https://winterwang.github.io/post/black-meal/&#34;&gt;(味道奇特的)山羊&lt;/a&gt;。遊戲規則是主持人會讓你先選擇其中一扇門（先不打開你選的這扇門）。主持人隨後打開另外兩扇門中的一扇沒有保時捷的門。主持人問你，你要堅持選擇之前選中的那扇門，還是要改變主意換一扇門去猜是否可以猜中保時捷。
請問，堅持選擇之前選中的門猜中保時捷的概率高，還是主持人打開一扇門以後改變主意猜中保時捷的概率更高呢？&lt;/p&gt;
&lt;p&gt;答案明天揭曉。&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Matrix Revisions</title>
      <link>https://wangcc.me/post/matrix-revision/</link>
      <pubDate>Sat, 30 Sep 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/matrix-revision/</guid>
      <description>
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;basic-definition-and-notations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Basic Definition and notations:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;An &lt;span class=&#34;math inline&#34;&gt;\(m\times n\)&lt;/span&gt; &lt;strong&gt;matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;&lt;/strong&gt; is a rectangular array of numbers with &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; rows and &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; columns.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;strong&gt;elements&lt;/strong&gt; of a matrix &lt;span class=&#34;math inline&#34;&gt;\(A_{m\times n}\)&lt;/span&gt; are &lt;span class=&#34;math inline&#34;&gt;\(a_{ij}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;strong&gt;order&lt;/strong&gt; of a matrix is the number of rows by the number of columns, i.e. &lt;span class=&#34;math inline&#34;&gt;\(m\times n\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A &lt;strong&gt;column vector&lt;/strong&gt; with &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; elements, &lt;span class=&#34;math inline&#34;&gt;\(y = \left( \begin{array}{c} y_1\\ y_2\\ \vdots\\ y_n \end{array} \right)\)&lt;/span&gt;, is a matrix with only one column i.e. an &lt;span class=&#34;math inline&#34;&gt;\(m\times 1\)&lt;/span&gt; matrix.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A &lt;strong&gt;row vector&lt;/strong&gt; with &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; elements, &lt;span class=&#34;math inline&#34;&gt;\(x=(x_1,x_2,x_3, \cdots, x_n)\)&lt;/span&gt;, is a matrix with only one row, i.e. an &lt;span class=&#34;math inline&#34;&gt;\(1\times n\)&lt;/span&gt; matrix.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Transposed matrix&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(A^T\)&lt;/span&gt; (or &lt;span class=&#34;math inline&#34;&gt;\(A&amp;#39;\)&lt;/span&gt;) arises from the matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; by interchanging the column vectors and the row vectors i.e. &lt;span class=&#34;math inline&#34;&gt;\(a_{ij}^T = a_{ji}\)&lt;/span&gt; (so a column vector is converted into a row vector and vise versa)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;A partitioned matrix&lt;/strong&gt; is a matrix written in terms of sub-matrices. &lt;span class=&#34;math inline&#34;&gt;\(A=\left( \begin{array}{c} A_{11} &amp;amp; A_{12}\\ A_{21} &amp;amp; A_{22}\\ \end{array} \right)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(A_{11},A_{12},A_{21},A_{22}\)&lt;/span&gt; are sub-matrices&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(A_{11}, A_{21}\)&lt;/span&gt; have the same number of columns, so do &lt;span class=&#34;math inline&#34;&gt;\(A_{12}, A_{22}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(A_{11}, A_{12}\)&lt;/span&gt; have the same number of rows, so do &lt;span class=&#34;math inline&#34;&gt;\(A_{21}, A_{22}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;partitioning is not restricted to dividing a matrix into just four sub-matrices&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;A square matrix&lt;/strong&gt; has exactly as many rows as it has columns i.e. the order of the matrix is &lt;span class=&#34;math inline&#34;&gt;\(n\times n\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;The main diagonal&lt;/strong&gt; (or leading diagnonal) of a square matrix &lt;span class=&#34;math inline&#34;&gt;\(A (n\times n)\)&lt;/span&gt; are the elements lying on the diagnoal &lt;strong&gt;from top left to bottom right.&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(a_{11},a_{22},a_{33},\cdots,a_{nn}\)&lt;/span&gt; i.e. all &lt;span class=&#34;math inline&#34;&gt;\(a_{ii}, i= 1,\cdots, n\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;The trace &lt;/strong&gt; of a square matrix is the sum of the diagonal elements &lt;span class=&#34;math inline&#34;&gt;\(tr(A)=a_{11}+a_{22}+\cdots+a_{nn}=\sum_{i=1}^na_{ii}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;special-matrices&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Special matrices&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;A symmetric matrix&lt;/strong&gt; is a square matrix for which the following is true for all the off diagonal elements. &lt;span class=&#34;math inline&#34;&gt;\(a_{ij}=a_{ji}\)&lt;/span&gt; i.e. &lt;span class=&#34;math inline&#34;&gt;\(A^T=A\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Diagonal matrix&lt;/strong&gt; is a square matrix having zero for all the non-diagonal elements i.e. &lt;span class=&#34;math inline&#34;&gt;\(A=\left( \begin{array}{c} a_{11} &amp;amp; \cdots &amp;amp; 0\\ \vdots &amp;amp; \ddots &amp;amp; \vdots\\ 0 &amp;amp; \cdots &amp;amp; a_{nn} \end{array} \right)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Zero matrix&lt;/strong&gt; (null matrix) is a matrix whose all elements are zero&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Identity matrix&lt;/strong&gt; (or unit matrix) is a diagonal matrix having all diagonal elements equal to 1 and off diagonal elements equal to zero. i.e. &lt;span class=&#34;math inline&#34;&gt;\(I=\left( \begin{array}{c} 1 &amp;amp; \cdots &amp;amp; 0\\ \vdots &amp;amp; \ddots &amp;amp; \vdots\\ 0 &amp;amp; \cdots &amp;amp; 1 \end{array} \right)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;“Summing vector”&lt;/strong&gt; is a vector whose every element is 1 i.e. &lt;span class=&#34;math inline&#34;&gt;\(1_{n}=(1\cdots1)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;“J matrix”&lt;/strong&gt; is a matrix (not necessarily square) whose every element is 1 i.e. &lt;span class=&#34;math inline&#34;&gt;\(J_{m\times n}=\left( \begin{array}{c} 1 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 1\\ 1 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 1 \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ 1 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 1 \end{array} \right)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;basic-operations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Basic Operations&lt;/h2&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;Addition (Substraction)&lt;/strong&gt; can take place only when the matrices involved are of the same order. i.e.
Two matrices can be added (subtracted) only if they have the same numbers of rows and the same numbers of columns.
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(A+B=B+A\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((A+B)+C=A+(B+C)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(A+0=0+A=A\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(A+(-A)=0\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((A+B)^T=A^T+B^T\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Multiplication by scalar:&lt;/strong&gt;
- &lt;span class=&#34;math inline&#34;&gt;\(cA=Ac\)&lt;/span&gt;
- &lt;span class=&#34;math inline&#34;&gt;\(c(dA)=(cd)A\)&lt;/span&gt;
- &lt;span class=&#34;math inline&#34;&gt;\((c\pm d)A=cA\pm dA\)&lt;/span&gt;
- &lt;span class=&#34;math inline&#34;&gt;\(c(A\pm B)=cA \pm cB\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Multiplication of an &lt;span class=&#34;math inline&#34;&gt;\(2\times2\)&lt;/span&gt; matrix by a column vector which has 2 rows yields a column vector with &lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt; rows.&lt;/strong&gt;
&lt;span class=&#34;math display&#34;&gt;\[Ax=\left(
\begin{array}{c}
a_{11} &amp;amp; a_{12}\\
a_{21} &amp;amp; a_{22}\\
\end{array}
\right)\left(
\begin{array}{c}
x_{1}\\
x_{2}\\
\end{array}
\right)=\left(
\begin{array}{c}
a_{11}x_1+a_{12}x_2\\
a_{21}x_1+a_{22}x_2\\
\end{array}
\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;generally&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Generally:&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Multiplication of an &lt;span class=&#34;math inline&#34;&gt;\(m\times n\)&lt;/span&gt; matrix&lt;/strong&gt; by a &lt;strong&gt;column vector&lt;/strong&gt; which has &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; rows &lt;strong&gt;yields a column vector&lt;/strong&gt; with &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; rows.
&lt;span class=&#34;math display&#34;&gt;\[Ax=\left(
\begin{array}{c}
a_{11} &amp;amp; \cdots &amp;amp; a_{1n} \\
\vdots &amp;amp; \ddots &amp;amp; \vdots \\
a_{m1} &amp;amp; \cdots &amp;amp; a_{mn}
\end{array}
\right)\left(
\begin{array}{c}
x_{1}\\
x_{2}\\
\vdots \\
x_{n}
\end{array}
\right)=\left(
\begin{array}{c}
a_{11}x_{1}+a_{12}x_2+\cdots+a_{1n}x_n\\
\vdots \\
a_{m1}x_{1}+a_{m2}x_2+\cdots+a_{mn}x_n
\end{array}
\right)=y \\
i.e. y_i=\sum_{j=1}^na_{ij}x_j, \; i=1,\cdots, m\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://winterwang.github.io/post/2017-02-22/&#34;&gt;Multiplication of matrices&lt;/a&gt;:&lt;/strong&gt; The product &lt;span class=&#34;math inline&#34;&gt;\(AB=C\)&lt;/span&gt; is &lt;strong&gt;defined only when &lt;span class=&#34;math inline&#34;&gt;\(A_{m\times r}\)&lt;/span&gt; has exactly as many columns as &lt;span class=&#34;math inline&#34;&gt;\(B_{r\times n}\)&lt;/span&gt; has rows&lt;/strong&gt;. And the elements of &lt;span class=&#34;math inline&#34;&gt;\(C_{m\times n}\)&lt;/span&gt; are given as
&lt;span class=&#34;math display&#34;&gt;\[c_{ij}=\sum_{l=1}^na_{il}b_{lj}, \;\; i=1,\cdots,m \; and \; j=1,\cdots, n\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(AB \neq BA\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((AB)C=A(BC)=ABC\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(A(B+C)=AB+AC\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((B+C)A=BA+CA\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(IA=AI=A\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;further-definitions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Further definitions&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://winterwang.github.io/post/2017-03-15/&#34;&gt;&lt;strong&gt;The determinant&lt;/strong&gt;&lt;/a&gt; of a second order square matrix is &lt;span class=&#34;math inline&#34;&gt;\(det(A)=|A|=\begin{vmatrix} a_{11} &amp;amp; a_{12} \\ a_{21} &amp;amp; a_{22} \end{vmatrix}=a_{11}a_{22}-a_{12}a_{21}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://winterwang.github.io/post/2017-07-06/&#34;&gt;The inverse of a matrix&lt;/a&gt;&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(A^{-1}\)&lt;/span&gt; if it exists, is a matrix whose product with &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is the identity matrix i.e. &lt;span class=&#34;math inline&#34;&gt;\(AA^{-1}=A^{-1}A=I\)&lt;/span&gt;. (&lt;strong&gt;Note: both &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(A^{-1}\)&lt;/span&gt; have to be square&lt;/strong&gt;) For second order matrices:&lt;span class=&#34;math inline&#34;&gt;\(A^{-1}=\frac{1}{det(A)}\left( \begin{array}{c} a_{22} &amp;amp; -a_{12}\\ -a_{21} &amp;amp; a_{11}\\ \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://winterwang.github.io/post/2017-07-06/&#34;&gt;Singular or non-invertible matrix&lt;/a&gt;&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(det(A)=0\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Idempotent matrices(冪等矩陣)&lt;/strong&gt; are square and the following is true: &lt;span class=&#34;math inline&#34;&gt;\(AA=A^2=A\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://winterwang.github.io/post/2017-03-08/&#34;&gt;Orthogonal matrices&lt;/a&gt;&lt;/strong&gt; have the following property: &lt;span class=&#34;math inline&#34;&gt;\(AA^T=A^TA=I\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記 25</title>
      <link>https://wangcc.me/post/cramers-formula/</link>
      <pubDate>Wed, 09 Aug 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/cramers-formula/</guid>
      <description>
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;克萊姆法則-cramers-formula&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;克萊姆法則 Cramer’s Formula&lt;/h3&gt;
&lt;p&gt;當 &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 爲&lt;a href=&#34;https://winterwang.github.io/post/2017-07-06/&#34;&gt;正則矩陣&lt;/a&gt;（&lt;span class=&#34;math inline&#34;&gt;\(|X|\neq0\)&lt;/span&gt;）時 連立一次方程式：&lt;span class=&#34;math inline&#34;&gt;\(X\underline{a}=\underline{y}\)&lt;/span&gt; 的解可以寫作：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[a_j=\frac{|X_j|}{|X|} (j=1,2,\cdots, n)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中： &lt;span class=&#34;math inline&#34;&gt;\(|X_j|\)&lt;/span&gt; 爲矩陣 &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 的第 &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; 列替換爲 &lt;span class=&#34;math inline&#34;&gt;\(\underline{y}\)&lt;/span&gt; 以後的矩陣的行列式。&lt;/p&gt;
&lt;hr /&gt;
&lt;div id=&#34;練習-解下列連立一次方程式&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;練習 解下列連立一次方程式&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
\left\{
\begin{array}{ll}
a_1+2a_2+a_3  = 2\\
2a_1+a_2+a_3  = 3\\
a_1+a_2+2a_3  = 3
\end{array}
\right.
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;解&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;解&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X=\left(
\begin{array}{c}
1 &amp;amp; 2 &amp;amp; 1 \\
2 &amp;amp; 1 &amp;amp; 1 \\
1 &amp;amp; 1 &amp;amp; 2
\end{array}
\right), \underline{a}=\left(
\begin{array}{c}
a_1 \\
a_2 \\
a_3 \\
\end{array}
\right), \underline{y}=\left(
\begin{array}{c}
2 \\
3 \\
3 \\
\end{array}
\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中 &lt;span class=&#34;math inline&#34;&gt;\(|X|=-4\)&lt;/span&gt; &lt;a href=&#34;https://winterwang.github.io/post/2017-03-15/&#34;&gt;(三次行列式的計算)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 的第一列置換成 &lt;span class=&#34;math inline&#34;&gt;\(\underline{y}\)&lt;/span&gt; 則:
&lt;span class=&#34;math display&#34;&gt;\[|X_1|=\begin{vmatrix}
2 &amp;amp; 2 &amp;amp;  1\\
3 &amp;amp; 1 &amp;amp;  1\\
3 &amp;amp; 1 &amp;amp;  2\\
\end{vmatrix}=-4\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 的第二列置換成 &lt;span class=&#34;math inline&#34;&gt;\(\underline{y}\)&lt;/span&gt; 則:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[|X_2|=\begin{vmatrix}
1 &amp;amp; 2 &amp;amp;  1\\
2 &amp;amp; 3 &amp;amp;  1\\
1 &amp;amp; 3 &amp;amp;  2\\
\end{vmatrix}=0\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 的第三列置換成 &lt;span class=&#34;math inline&#34;&gt;\(\underline{y}\)&lt;/span&gt; 則:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[|X_3|=\begin{vmatrix}
1 &amp;amp; 2 &amp;amp;  2\\
2 &amp;amp; 1 &amp;amp;  3\\
1 &amp;amp; 1 &amp;amp;  3\\
\end{vmatrix}=-4\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\therefore a_1=\frac{|X_1|}{|X|}=1, \\
a_2=\frac{|X_2|}{|X|}=0, \\
a_3=\frac{|X_3|}{|X|}=1\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記 24</title>
      <link>https://wangcc.me/post/inverse-matrix-method/</link>
      <pubDate>Sun, 06 Aug 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/inverse-matrix-method/</guid>
      <description>
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;逆矩陣法解連立一次方程式&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;逆矩陣法解連立一次方程式&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 為&lt;a href=&#34;https://winterwang.github.io/post/2017-07-06/&#34;&gt;正則矩陣&lt;/a&gt;時(&lt;span class=&#34;math inline&#34;&gt;\(|X|\neq0\)&lt;/span&gt;)，給 &lt;span class=&#34;math inline&#34;&gt;\(X\underline{a}=\underline{y}\)&lt;/span&gt; 等式兩邊同時乘以 &lt;span class=&#34;math inline&#34;&gt;\(X^{-1}\)&lt;/span&gt;，可以得到 &lt;span class=&#34;math inline&#34;&gt;\(X^{-1}X\underline{a}=X^{-1}\underline{y}\rightarrow E\underline{a}=X^{-1}\underline{y}\)&lt;/span&gt;。由此方法可以得到 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=X^{-1}\underline{y}\)&lt;/span&gt;。&lt;/p&gt;
&lt;hr /&gt;
&lt;div id=&#34;練習-解下列連立一次方程式&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;練習 解下列連立一次方程式&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
\left\{
\begin{array}{ll}
a_1+2a_2+a_3  = 2\\
2a_1+a_2+a_3  = 3\\
a_1+a_2+2a_3  = 3
\end{array}
\right.
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;解&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;解&lt;/h4&gt;
&lt;p&gt;元連立方程式可以寫作&lt;span class=&#34;math inline&#34;&gt;\(X\underline{a}=\underline{y}\)&lt;/span&gt;，其中
&lt;span class=&#34;math display&#34;&gt;\[X=\left(
\begin{array}{c}
1 &amp;amp; 2 &amp;amp; 1 \\
2 &amp;amp; 1 &amp;amp; 1 \\
1 &amp;amp; 1 &amp;amp; 2
\end{array}
\right), \underline{a}=\left(
\begin{array}{c}
a_1 \\
a_2 \\
a_3 \\
\end{array}
\right), \underline{y}=\left(
\begin{array}{c}
2 \\
3 \\
3 \\
\end{array}
\right)\]&lt;/span&gt;
之前我們已經用&lt;a href=&#34;https://winterwang.github.io/post/2017-07-07/&#34;&gt;行的基本變形法&lt;/a&gt;和&lt;a href=&#34;https://winterwang.github.io/post/inverse-matrix/&#34;&gt;逆矩陣法&lt;/a&gt;分別計算過了 &lt;span class=&#34;math inline&#34;&gt;\(X^{-1}\)&lt;/span&gt; ：
&lt;span class=&#34;math display&#34;&gt;\[X^{-1}=\left(\begin{array}{c}
-1/4 &amp;amp; 3/4 &amp;amp; -1/4\\
3/4 &amp;amp; -1/4 &amp;amp; -1/4\\
-1/4 &amp;amp; -1/4 &amp;amp; -3/4\\
\end{array}\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\therefore\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
\underline{a} &amp;amp; =X^{-1}\underline{y} \\
&amp;amp; =\left(\begin{array}{c}
-1/4 &amp;amp; 3/4 &amp;amp; -1/4\\
3/4 &amp;amp; -1/4 &amp;amp; -1/4\\
-1/4 &amp;amp; -1/4 &amp;amp; 3/4\\
\end{array}\right)\left(
\begin{array}{c}
2 \\
3 \\
3 \\
\end{array}
\right)\\
&amp;amp;=\left(
\begin{array}{c}
-1/4\times2+3/4\times3-1/4\times3 \\
3/4\times1+(-1/4)\times3-1/4\times3 \\
-1/4\times2-1/4\times3+3/4\times3 \\
\end{array}
\right) \\
&amp;amp; = \left(
\begin{array}{c}
1 \\
0 \\
1 \\
\end{array}
\right)
\end{align} \]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記22</title>
      <link>https://wangcc.me/post/inverse-matrix/</link>
      <pubDate>Thu, 03 Aug 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/inverse-matrix/</guid>
      <description>&lt;p&gt;正方形矩陣 $A$ 的行列式滿足 $|A| \neq 0$ 時，逆矩陣可以表達爲(當 $|A|=0$ 時，正方形矩陣 $A$ 沒有逆矩陣)：
$$A^{-1}=\frac{1}{|A|}adj(A)=\frac{1}{|A|}(A_{ij})^t$$&lt;/p&gt;
&lt;p&gt;$$=\frac{1}{|A|}\lbrace(-1)^{i+j}D_{ij}\rbrace^t$$&lt;/p&gt;
&lt;p&gt;其中:&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$adj(A)$ 爲&lt;a href=&#34;https://winterwang.github.io/post/2017-03-15/&#34;&gt;餘因子矩陣&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;$A_{ij}$ 爲&lt;a href=&#34;https://winterwang.github.io/post/2017-03-15/&#34;&gt;餘因子&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;$D_{ij}$ &lt;a href=&#34;https://winterwang.github.io/post/2017-03-15/&#34;&gt;爲小行列式&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(1) 之前舉過的例子再拿來試試看：&lt;/p&gt;
&lt;p&gt;$$X=\left(
\begin{array}{c}
1 &amp;amp; 2 &amp;amp; 1 \newline
2 &amp;amp; 1 &amp;amp; 1 \newline
1 &amp;amp; 1 &amp;amp; 2
\end{array}
\right)=\left(\begin{array}{c}
x_{11} &amp;amp; x_{12} &amp;amp; x_{13} \newline
x_{21} &amp;amp; x_{22} &amp;amp; x_{23} \newline
x_{31} &amp;amp; x_{32} &amp;amp; x_{33}
\end{array}\right)$$
&lt;br&gt;&lt;/p&gt;
&lt;p&gt;元素 $x_{ij}$ 的餘因子 $X_{ij}(i,j=1,2,3)$ 爲：&lt;/p&gt;
&lt;p&gt;$$X_{11}=(-1)^{1+1}\left|
\begin{array}{c}
1 &amp;amp; 1 \newline
1 &amp;amp; 2
\end{array}\right|=1$$&lt;/p&gt;
&lt;p&gt;$$X_{12}=(-1)^{1+2}\left|
\begin{array}{c}
2 &amp;amp; 1 \newline
1 &amp;amp; 2
\end{array}\right|=-3$$&lt;/p&gt;
&lt;p&gt;$$X_{13}=(-1)^{1+3}\left|
\begin{array}{c}
2 &amp;amp; 1 \newline
1 &amp;amp; 1
\end{array}\right|=1$$&lt;/p&gt;
&lt;p&gt;$$X_{21}=(-1)^{2+1}\left|
\begin{array}{c}
2 &amp;amp; 1 \newline
1 &amp;amp; 2
\end{array}\right|=-3$$&lt;/p&gt;
&lt;p&gt;$$X_{22}=(-1)^{2+2}\left|
\begin{array}{c}
1 &amp;amp; 1 \newline
1 &amp;amp; 2
\end{array}\right|=1$$&lt;/p&gt;
&lt;p&gt;$$X_{23}=(-1)^{2+3}\left|
\begin{array}{c}
1 &amp;amp; 2 \newline
1 &amp;amp; 1
\end{array}\right|=1$$&lt;/p&gt;
&lt;p&gt;$$X_{31}=(-1)^{3+1}\left|
\begin{array}{c}
2 &amp;amp; 1 \newline
1 &amp;amp; 1
\end{array}\right|=1$$&lt;/p&gt;
&lt;p&gt;$$X_{32}=(-1)^{3+2}\left|
\begin{array}{c}
1 &amp;amp; 1 \newline
2 &amp;amp; 1
\end{array}\right|=1$$&lt;/p&gt;
&lt;p&gt;$$X_{33}=(-1)^{3+3}\left|
\begin{array}{c}
1 &amp;amp; 2 \newline
2 &amp;amp; 1
\end{array}\right|=-3$$&lt;/p&gt;
&lt;p&gt;因此餘因子矩陣爲：$adj(X)=\left(
\begin{array}{c}
1 &amp;amp; -3 &amp;amp; 1 \newline
-3 &amp;amp; 1 &amp;amp; 1 \newline
1 &amp;amp; 1 &amp;amp; -3
\end{array}
\right)^t=\left(
\begin{array}{c}
1 &amp;amp; -3 &amp;amp; 1 \newline
-3 &amp;amp; 1 &amp;amp; 1 \newline
1 &amp;amp; 1 &amp;amp; -3
\end{array}
\right)$&lt;/p&gt;
&lt;p&gt;我們看見這個餘因子矩陣是一個對稱矩陣，這是由於原矩陣 $X$ 本身就是一個對稱矩陣。另外，行列式爲：&lt;/p&gt;
&lt;p&gt;$$\begin{align}|X|&amp;amp;=1\times X_{11}+2\times X_{12}+1\times X_{13}\newline&amp;amp;=1\times1+2\times(-3)+1\times1\newline&amp;amp;=-4\end{align}$$&lt;/p&gt;
&lt;p&gt;因此所求的逆矩陣爲：&lt;/p&gt;
&lt;p&gt;$$\begin{align}X^{-1}&amp;amp;=\frac{1}{|X|}adj(X)\newline
&amp;amp;=\frac{1}{-4}\left(
\begin{array}{c}
1 &amp;amp; -3 &amp;amp; 1 \newline
-3 &amp;amp; 1 &amp;amp; 1 \newline
1 &amp;amp; 1 &amp;amp; -3
\end{array}
\right)\newline
&amp;amp;=\left(
\begin{array}{c}
-\frac{1}{4} &amp;amp; \frac{3}{4} &amp;amp; -\frac{1}{4} \newline
\frac{3}{4} &amp;amp; -\frac{1}{4} &amp;amp; -\frac{1}{4} \newline
-\frac{1}{4} &amp;amp; -\frac{1}{4} &amp;amp; \frac{3}{4}
\end{array}
\right)\end{align}$$&lt;/p&gt;
&lt;p&gt;(2) 試求矩陣 $A=\left(
\begin{array}{c}
1 &amp;amp; 2 &amp;amp; 1 \newline
2 &amp;amp; 3 &amp;amp; 1 \newline
1 &amp;amp; 2 &amp;amp; 2
\end{array}
\right)=\left(
\begin{array}{c}
a_{11} &amp;amp; a_{12}  &amp;amp; a_{13} \newline
a_{21} &amp;amp; a_{22}  &amp;amp; a_{23} \newline
a_{31} &amp;amp; a_{32}  &amp;amp; a_{33}
\end{array}
\right)$ 的逆矩陣 $A^{-1}$:&lt;/p&gt;
&lt;p&gt;$$\begin{array}
=A_{11}=6-2=4, &amp;amp; A_{12}=-(4-1)=-3, &amp;amp; A_{13}=4-3=1 \newline
A_{21}=-(4-2)=-2, &amp;amp; A_{22}=2-1=1, &amp;amp; A_{23}=-(2-2)=0 \newline
A_{31}=2-3=-1, &amp;amp; A_{32}=-(1-2)=1, &amp;amp; A_{33}=3-4=-1
\end{array}$$&lt;/p&gt;
&lt;p&gt;$$adj(A)=\left(
\begin{array}{c}
4 &amp;amp; -3 &amp;amp; 1 \newline
-2 &amp;amp; 1 &amp;amp; 0 \newline
-1 &amp;amp; 1 &amp;amp; -1
\end{array}
\right)^t=\left(
\begin{array}{c}
4 &amp;amp; -2 &amp;amp; -1 \newline
-3 &amp;amp; 1 &amp;amp; 1 \newline
1 &amp;amp; 0 &amp;amp; -1
\end{array}
\right)$$&lt;/p&gt;
&lt;p&gt;$$\begin{align}
|A| &amp;amp;=1\times A_{11}+2\times A_{12}+1\times A_{13} \newline
&amp;amp;=1\times4+2\times(-3)+1\times1 \newline
&amp;amp;=4-6+1 \newline
&amp;amp;=-1
\end{align}$$&lt;/p&gt;
&lt;p&gt;$$
\therefore
\begin{align}
A^{-1} &amp;amp;= \frac{1}{(-1)}\left(
\begin{array}{c}
4 &amp;amp; -2 &amp;amp; -1 \newline
-3 &amp;amp; 1 &amp;amp; 1 \newline
1 &amp;amp; 0 &amp;amp; -1
\end{array}
\right) \newline
&amp;amp;=\left(
\begin{array}{c}
-4 &amp;amp; 2 &amp;amp; 1 \newline
3 &amp;amp; -1 &amp;amp; -1 \newline
-1 &amp;amp; 0 &amp;amp; 1
\end{array}
\right)
\end{align}$$&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記21</title>
      <link>https://wangcc.me/post/2017-07-07/</link>
      <pubDate>Fri, 07 Jul 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/2017-07-07/</guid>
      <description>
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;行的基本變形&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;行的基本變形&lt;/h2&gt;

&lt;div class=&#34;theorem&#34;&gt;
&lt;p&gt;&lt;span id=&#34;thm:line&#34; class=&#34;theorem&#34;&gt;&lt;strong&gt;Theorem 1  (行的基本變形)  &lt;/strong&gt;&lt;/span&gt;對矩陣進行下列操作的過程，被稱爲是行的基本變形（行的基本操作, elementary row operations）。&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;給任意一行乘以/除以一個非零的數。&lt;/li&gt;
&lt;li&gt;給任意一行加上/減去另外任意行的倍數。&lt;/li&gt;
&lt;li&gt;將任意兩行的對應元素互換。
&lt;/div&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;練習基本變形&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;練習基本變形：&lt;/h3&gt;
&lt;hr /&gt;
&lt;p&gt;用行的基本變形求矩陣 &lt;span class=&#34;math inline&#34;&gt;\(X=\left(\begin{array}{c} 1&amp;amp; 2&amp;amp; 1\\ 2&amp;amp; 1&amp;amp; 1\\ 1&amp;amp; 1&amp;amp; 2\\ \end{array}\right)\)&lt;/span&gt; 的逆矩陣 &lt;span class=&#34;math inline&#34;&gt;\(X^{-1}\)&lt;/span&gt; &lt;br&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;首先，將矩陣 &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 和同次單位矩陣 &lt;span class=&#34;math inline&#34;&gt;\(E_3\)&lt;/span&gt; 的元素寫成如下的左右並列的形式（用點隔開）&lt;span class=&#34;math inline&#34;&gt;\((X, E)\)&lt;/span&gt;。數字 (1) (2) (3) 表示行數：&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\left(\begin{array}{c}
1&amp;amp; 2&amp;amp; 1 &amp;amp; \vdots &amp;amp; 1 &amp;amp; 0 &amp;amp; 0\\
2&amp;amp; 1&amp;amp; 1 &amp;amp; \vdots &amp;amp; 0 &amp;amp; 1 &amp;amp; 0\\
1&amp;amp; 1&amp;amp; 2 &amp;amp; \vdots &amp;amp; 0 &amp;amp; 0 &amp;amp; 1\\
\end{array}\right) \begin{align}
\left\{
\begin{array}{rr}
(1)\\
(2)\\
(3)
\end{array}
\right.
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;可以變形成爲下面的形式：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\left(\begin{array}{c}
1&amp;amp; 2&amp;amp; 1 &amp;amp; \vdots &amp;amp; 1 &amp;amp; 0 &amp;amp; 0\\
0&amp;amp; -3&amp;amp; -1 &amp;amp; \vdots &amp;amp; -2 &amp;amp; 1 &amp;amp; 0\\
0&amp;amp; -1&amp;amp; 1 &amp;amp; \vdots &amp;amp; -1 &amp;amp; 0 &amp;amp; 1\\
\end{array}\right) \begin{align}
\left\{
\begin{array}{l}
(1)\\
(2)=(2)-2\times(1)\\
(3)=(3)-(1)
\end{array}
\right.
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;繼續變形成如下的形式：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\left(\begin{array}{c}
1&amp;amp; 0&amp;amp; 3 &amp;amp; \vdots &amp;amp; -1 &amp;amp; 0 &amp;amp; 2\\
0&amp;amp; -4&amp;amp; 0 &amp;amp; \vdots &amp;amp; -3 &amp;amp; 1 &amp;amp; 1\\
0&amp;amp; 1&amp;amp; -1 &amp;amp; \vdots &amp;amp; 1 &amp;amp; 0 &amp;amp; -1\\
\end{array}\right) \begin{align}
\left\{
\begin{array}{l}
(1)=(1)+2\times(3)\\
(2)=(2)+(3)\\
(3)=-1\times(3)
\end{array}
\right.
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Next:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\left(\begin{array}{c}
1&amp;amp; 0&amp;amp; 3 &amp;amp; \vdots &amp;amp; -1 &amp;amp; 0 &amp;amp; 2\\
0&amp;amp; 1&amp;amp; 0 &amp;amp; \vdots &amp;amp; 3/4 &amp;amp; -1/4 &amp;amp; -1/4\\
0&amp;amp; 1&amp;amp; -1 &amp;amp; \vdots &amp;amp; 1 &amp;amp; 0 &amp;amp; -1\\
\end{array}\right) \begin{align}
\left\{
\begin{array}{l}
(1)=(1)\\
(2)=(2)\div(-4)\\
(3)=(3)
\end{array}
\right.
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Next:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\left(\begin{array}{c}
1&amp;amp; 0&amp;amp; 3 &amp;amp; \vdots &amp;amp; -1 &amp;amp; 0 &amp;amp; 2\\
0&amp;amp; 1&amp;amp; 0 &amp;amp; \vdots &amp;amp; 3/4 &amp;amp; -1/4 &amp;amp; -1/4\\
0&amp;amp; 0&amp;amp; -1 &amp;amp; \vdots &amp;amp; 1/4 &amp;amp; 1/4 &amp;amp; -3/4\\
\end{array}\right) \begin{align}
\left\{
\begin{array}{l}
(1)=(1)\\
(2)=(2)\\
(3)=(3)-(2)
\end{array}
\right.
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Next:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\left(\begin{array}{c}
1&amp;amp; 0&amp;amp; 0 &amp;amp; \vdots &amp;amp; -1/4 &amp;amp; 3/4 &amp;amp; -1/4\\
0&amp;amp; 1&amp;amp; 0 &amp;amp; \vdots &amp;amp; 3/4 &amp;amp; -1/4 &amp;amp; -1/4\\
0&amp;amp; 0&amp;amp; 1 &amp;amp; \vdots &amp;amp; -1/4 &amp;amp; -1/4 &amp;amp; -3/4\\
\end{array}\right) \begin{align}
\left\{
\begin{array}{l}
(1)=(1)+3\times(3)\\
(2)=(2)\\
(3)=-1\times(3)
\end{array}
\right.
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;點 “&lt;span class=&#34;math inline&#34;&gt;\(\vdots\)&lt;/span&gt;” 的左側變形成爲單位矩陣時，行變形結束。右側便是所求的逆矩陣 &lt;span class=&#34;math inline&#34;&gt;\(X^{-1}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X^{-1}=\left(\begin{array}{c}
-1/4 &amp;amp; 3/4 &amp;amp; -1/4\\
3/4 &amp;amp; -1/4 &amp;amp; -1/4\\
-1/4 &amp;amp; -1/4 &amp;amp; 3/4\\
\end{array}\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;q-如果有行的基本變形請問有沒有列的基本變形-elementary-column-operations&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Q: 如果有行的基本變形，請問有沒有列的基本變形 (elementary column operations)？&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;a-有把行的基本變形中的定義-refthmline-的行改成列既是列的基本變形的定義&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A: 有。把行的基本變形中的定義 (&lt;a href=&#34;#thm:line&#34;&gt;1&lt;/a&gt;) 的行改成列，既是列的基本變形的定義。&lt;/h3&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記20</title>
      <link>https://wangcc.me/post/2017-07-06/</link>
      <pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/2017-07-06/</guid>
      <description>
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;逆矩陣&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;逆矩陣&lt;/h2&gt;
&lt;div id=&#34;逆矩陣定義&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;逆矩陣定義&lt;/h3&gt;

&lt;div class=&#34;theorem&#34;&gt;
&lt;span id=&#34;thm:unnamed-chunk-1&#34; class=&#34;theorem&#34;&gt;&lt;strong&gt;Theorem 1  &lt;/strong&gt;&lt;/span&gt;如果對於正方形矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;，存在一個&lt;strong&gt;正方形矩陣&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 滿足 &lt;span class=&#34;math inline&#34;&gt;\(AX=XA=E\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(E\)&lt;/span&gt; 爲單位矩陣) 時，這個正方形矩陣 &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 被叫做 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 的&lt;strong&gt;逆矩陣&lt;/strong&gt;，寫作 &lt;span class=&#34;math inline&#34;&gt;\(A^{-1}\)&lt;/span&gt;。&lt;br&gt;
存在逆矩陣 &lt;span class=&#34;math inline&#34;&gt;\((A^{-1})\)&lt;/span&gt; 的 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; ，被叫做&lt;strong&gt;正則矩陣&lt;/strong&gt; (regular matrix, nonsingular matrix)。&lt;br&gt;
不存在逆矩陣的 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;，被叫做&lt;strong&gt;奇異矩陣&lt;/strong&gt; (singular matrix)。&lt;br&gt;
滿足 &lt;span class=&#34;math inline&#34;&gt;\(|A|\neq 0\)&lt;/span&gt; 的矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 被叫做正則矩陣。滿足 &lt;span class=&#34;math inline&#34;&gt;\(|A|=0\)&lt;/span&gt; 的矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 被叫做奇異矩陣。&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 爲正則矩陣時，滿足：&lt;span class=&#34;math inline&#34;&gt;\(A^{-1}A=AA^{-1}=E\)&lt;/span&gt; 。&lt;br&gt;
顯然，單位矩陣的逆矩陣也是一個單位矩陣: &lt;br&gt;
&lt;span class=&#34;math display&#34;&gt;\[E^{-1}E=EE^{-1}=E, E^{-1}=E\]&lt;/span&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;逆矩陣的性質&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;逆矩陣的性質&lt;/h3&gt;
&lt;p&gt;對於正則矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A, B\)&lt;/span&gt; 有以下性質：&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((AB)^{-1}=B^{-1}A^{-1}\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;diff_alert&#34;&gt;注意此處矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A，B\)&lt;/span&gt; 的順序對調了。&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((A^{-1})^{-1}=A\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((A^{t})^{-1}=(A^{-1})^t\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((\lambda A)^{-1}=\frac{1}{\lambda}A^{-1} (\lambda \ne 0)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;對角矩陣 &lt;span class=&#34;math inline&#34;&gt;\(D_n=diag(a_{11},a_{22},\dotsm,a_{nn})\)&lt;/span&gt; 的逆矩陣寫作： &lt;span class=&#34;math inline&#34;&gt;\(D_n^{-1}=diag(1/a_{11}, 1/a_{22},\dotsm,1/a_{nn})\)&lt;/span&gt;；&lt;br&gt;
&lt;span class=&#34;diff_alert&#34;&gt;注意此處的條件爲所有對角成分均非零: &lt;span class=&#34;math inline&#34;&gt;\(a_{11}a_{22}\dotsm a_{nn}\neq 0\)&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;證明&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;證明&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\((AB)(AB)^{-1}=E\)&lt;/span&gt; &lt;br&gt;
&lt;span class=&#34;diff_alert&#34;&gt;等式兩邊從左往右乘以 &lt;span class=&#34;math inline&#34;&gt;\(A^{-1}\)&lt;/span&gt;&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\((A^{-1}A)B(AB)^{-1}=A^{-1}E\\ B(AB)^{-1}=A^{-1}\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;diff_alert&#34;&gt;等式兩邊從左往右乘以 &lt;span class=&#34;math inline&#34;&gt;\(B^{-1}\)&lt;/span&gt;&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\((B^{-1}B)(AB)^{-1}=B^{-1}A^{-1}\\ E(AB)^{-1}=B^{-1}A^{-1}\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;diff_alert&#34;&gt;&lt;a href=&#34;https://winterwang.github.io/post/2017-03-08/&#34;&gt;根據單位矩陣的性質：&lt;/a&gt;&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\therefore (AB)^{-1}=B^{-1}A^{-1}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(E=E^{-1}=(A^{-1}A)^{-1}=A^{-1}(A^{-1})^{-1}\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;diff_alert&#34;&gt;等式兩邊從左往右乘以 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(AE=AA^{-1}(A^{-1})^{-1}\\ \therefore A=(A^{-1})^{-1}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(E=E^t=(A^{-1}A)^t=A^t(A^{-1})^t\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;diff_alert&#34;&gt;等式兩邊從左往右乘以 &lt;span class=&#34;math inline&#34;&gt;\((A^t)^{-1}\)&lt;/span&gt;&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\((A^t)^{-1}E=(A^t)^{-1}A^t(A^{-1})^t\\ \therefore (A^t)^{-1}=(A^{-1})^t\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記19</title>
      <link>https://wangcc.me/post/2017-04-02/</link>
      <pubDate>Sun, 02 Apr 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/2017-04-02/</guid>
      <description>
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;行列式的性質&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;行列式的性質&lt;/h2&gt;
&lt;p&gt;具體的行列式的值，可以通過以下介紹的行列式性質，儘量簡潔地求解。本節也是爲了簡易示範，僅僅使用3次行列式作例子。4次以上的行列式性質依然相同，依此類推即可。&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;轉置矩陣的行列式，與轉置前的行列式一致。即：&lt;span class=&#34;math inline&#34;&gt;\(|A^t|=|A|\)&lt;/span&gt;。 &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(|A|=\begin{vmatrix}  1 &amp;amp; 2 &amp;amp; 3 \\  4 &amp;amp; 5 &amp;amp; 6 \\  7 &amp;amp; 8 &amp;amp; 9 \\ \end{vmatrix}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;任意一列（或者任意一行）若乘以 &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; 倍，那麼這個矩陣的行列式結果也將是乘以 &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; 倍。&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(|A|=\begin{vmatrix} a_{11} &amp;amp; a_{12} &amp;amp; a_{13}\\ \lambda a_{21} &amp;amp;\lambda a_{22} &amp;amp; \lambda a_{23}\\ a_{31} &amp;amp; a_{32} &amp;amp; a_{33}\\ \end{vmatrix}\\ \;\;\;\;=|A|=\lambda \begin{vmatrix} a_{11} &amp;amp; a_{12} &amp;amp; a_{13}\\ a_{21} &amp;amp; a_{22} &amp;amp; a_{23}\\ a_{31} &amp;amp; a_{32} &amp;amp; a_{33}\\ \end{vmatrix}\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(|A|=\begin{vmatrix} a_{11} &amp;amp; a_{12} &amp;amp; \lambda a_{13}\\ a_{21} &amp;amp; a_{22} &amp;amp; \lambda a_{23}\\ a_{31} &amp;amp; a_{32} &amp;amp; \lambda a_{33}\\ \end{vmatrix}\\ \;\;\;\;=|A|=\lambda \begin{vmatrix} a_{11} &amp;amp; a_{12} &amp;amp; a_{13}\\ a_{21} &amp;amp; a_{22} &amp;amp; a_{23}\\ a_{31} &amp;amp; a_{32} &amp;amp; a_{33}\\ \end{vmatrix}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;任意一列（或者任意一行）的各成分乘以 &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; 倍，與其他任意一列（或者任意一行）的各成分進行加運算（或者減運算）獲得的矩陣的行列式與原矩陣的行列式相同。&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\begin{vmatrix} a_{11} &amp;amp; a_{12} &amp;amp; a_{13}\\ a_{21}\pm \lambda a_{11} &amp;amp; a_{22}\pm \lambda a_{12} &amp;amp; a_{23}\pm \lambda a_{13}\\ a_{31} &amp;amp; a_{32} &amp;amp; a_{33}\\ \end{vmatrix}，\\ \begin{vmatrix} a_{11} &amp;amp; a_{12}\pm \lambda a_{11} &amp;amp; a_{13}\\ a_{21} &amp;amp; a_{22}\pm \lambda a_{21} &amp;amp; a_{23}\\ a_{31} &amp;amp; a_{32}\pm \lambda a_{31} &amp;amp; a_{33}\\ \end{vmatrix},\)&lt;/span&gt; &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\begin{vmatrix} a_{11} &amp;amp; a_{12} &amp;amp; a_{13}\\ a_{21} \pm \frac{a_{11}}{\lambda} &amp;amp; a_{22}\pm \frac{a_{12}}{\lambda} &amp;amp; a_{23}\pm \frac{a_{13}}{\lambda}\\ a_{31} &amp;amp; a_{32} &amp;amp; a_{33}\\ \end{vmatrix}, \\ \begin{vmatrix} a_{11} &amp;amp; a_{12}\pm \frac{a_{11}}{\lambda} &amp;amp; a_{13}\\ a_{21} &amp;amp; a_{22}\pm \frac{a_{21}}{\lambda} &amp;amp; a_{23}\\ a_{31} &amp;amp; a_{32}\pm \frac{a_{31}}{\lambda} &amp;amp; a_{33}\\ \end{vmatrix}\)&lt;/span&gt;&lt;br&gt;
上述行列式與行列式 &lt;span class=&#34;math inline&#34;&gt;\(\begin{vmatrix} a_{11} &amp;amp; a_{12} &amp;amp; a_{13}\\ a_{21} &amp;amp; a_{22} &amp;amp; a_{23}\\ a_{31} &amp;amp; a_{32} &amp;amp; a_{33}\\ \end{vmatrix}\)&lt;/span&gt; 結果相同。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;符合下列條件時，行列式的值爲 &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;任意一行（或者列）的全部成分均爲 &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; 時。&lt;/li&gt;
&lt;li&gt;矩陣中若有兩行（或者兩列）的對應成分全部相同時。&lt;/li&gt;
&lt;li&gt;矩陣中若有兩行（或者兩列）的對應成分均成一定比例時。&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\begin{vmatrix} a_{11} &amp;amp; a_{12} &amp;amp; a_{13}\\ a_{21} &amp;amp; a_{22} &amp;amp; a_{23}\\ 0 &amp;amp; 0 &amp;amp; 0\\ \end{vmatrix}=0\\ \begin{vmatrix} a &amp;amp; b &amp;amp; c\\ a &amp;amp; b &amp;amp; c\\ d &amp;amp; e &amp;amp; f\\ \end{vmatrix}=0\\ \begin{vmatrix} a &amp;amp; b &amp;amp; c\\ ka &amp;amp; kb &amp;amp; kc\\ d &amp;amp; e &amp;amp; f\\ \end{vmatrix}=0\)&lt;/span&gt;&lt;br&gt;
由於上面的後兩條成立，所以當矩陣中任意兩列（或者兩行）的對應成分幾乎相等，或者比值無限接近時，行列式的值也可以說就接近爲 &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;。此性質與多重線性迴歸的多重共線性有直接關係。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;一個矩陣中其中兩列（或者兩行）的成分交換以後獲得的矩陣，其行列式值爲原矩陣的行列式的值的相反數。（即符號相反）&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\begin{vmatrix} a_{11} &amp;amp; a_{12} &amp;amp; a_{13}\\ a_{21} &amp;amp; a_{22} &amp;amp; a_{23}\\ a_{31} &amp;amp; a_{32} &amp;amp; a_{33}\\ \end{vmatrix}=-\begin{vmatrix} a_{21} &amp;amp; a_{22} &amp;amp; a_{23}\\ a_{11} &amp;amp; a_{12} &amp;amp; a_{13}\\ a_{31} &amp;amp; a_{32} &amp;amp; a_{33}\\ \end{vmatrix}\)&lt;/span&gt; （第一行和第二行對調成分）&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;對角矩陣，上三角矩陣，下三角矩陣的行列式的值，等於對角成分的積&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\begin{vmatrix} a_{11} &amp;amp; 0 &amp;amp; 0\\ 0 &amp;amp; a_{22} &amp;amp; 0\\ 0 &amp;amp; 0 &amp;amp; a_{33}\\ \end{vmatrix}=a_{11}a_{22}a_{33},\\ \begin{vmatrix} a_{11} &amp;amp; a_{12} &amp;amp; a_{13}\\ 0 &amp;amp; a_{22} &amp;amp; a_{23}\\ 0 &amp;amp; 0 &amp;amp; a_{33}\\ \end{vmatrix}=a_{11}a_{22}a_{33},\\ \begin{vmatrix} a_{11} &amp;amp; 0 &amp;amp; 0\\ a_{21} &amp;amp; a_{22} &amp;amp; 0\\ a_{31} &amp;amp; a_{32} &amp;amp; a_{33}\\ \end{vmatrix}=a_{11}a_{22}a_{33}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;矩陣中如果有任意一行（或列），衹有一個成分為非零成分，可以將該矩陣的行列式降次：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\begin{vmatrix} a_{11} &amp;amp; 0 &amp;amp; 0\\ a_{21} &amp;amp; a_{22} &amp;amp; a_{23}\\ a_{31} &amp;amp; a_{32} &amp;amp; a_{33}\\ \end{vmatrix}=a_{11}(-1)^{1+1}\begin{vmatrix} a_{22} &amp;amp; a_{23} \\ a_{32} &amp;amp; a_{33} \end{vmatrix}\)&lt;/span&gt; &lt;br&gt;
&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\begin{vmatrix} a_{11} &amp;amp; a_{12} &amp;amp; 0\\ a_{21} &amp;amp; a_{22} &amp;amp; a_{23}\\ a_{31} &amp;amp; a_{32} &amp;amp; 0\\ \end{vmatrix}=a_{23}(-1)^{1+1}\begin{vmatrix} a_{11} &amp;amp; a_{12} \\ a_{31} &amp;amp; a_{32} \end{vmatrix}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(A, B\)&lt;/span&gt; 同時都是正方形矩陣時，&lt;span class=&#34;math inline&#34;&gt;\(|AB|=|A|·|B|\)&lt;/span&gt;。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;
&lt;div id=&#34;證明&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;strong&gt;證明&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(A=\left(\begin{array}{c} 0 &amp;amp; 4 &amp;amp; 2\\ -1 &amp;amp; 3 &amp;amp; 7\\ 6 &amp;amp; 5 &amp;amp; 9\\  \end{array}\right)\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(B=\left(\begin{array}{c} 2 &amp;amp; 3 &amp;amp; 4\\ -2 &amp;amp; 7 &amp;amp; 1\\ 4 &amp;amp; 6 &amp;amp; 0\\  \end{array}\right)\)&lt;/span&gt; 時， &lt;span class=&#34;math inline&#34;&gt;\(|AB|=|A|·|B|\)&lt;/span&gt; 成立&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;解&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;解&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\because AB=\left(\begin{array}{c}  0 &amp;amp; 40 &amp;amp; 4\\  20 &amp;amp; 60 &amp;amp; -1\\  38 &amp;amp; 107 &amp;amp; 29\\  \end{array}\right)\)&lt;/span&gt; &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\therefore |AB|=\begin{vmatrix}  0 &amp;amp; 40 &amp;amp; 4\\  20 &amp;amp; 60 &amp;amp; -1\\  38 &amp;amp; 107 &amp;amp; 29\\  \end{vmatrix}\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;diff_alert&#34;&gt;利用性質3： 第2列 - 第3列 &lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt; 10 作新的第2列&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(=\begin{vmatrix} 0 &amp;amp; 0 &amp;amp; 4\\ 20 &amp;amp; 70 &amp;amp; -1\\ 38 &amp;amp; -183 &amp;amp; 29\\ \end{vmatrix}\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;diff_alert&#34;&gt;利用性質7： 第一行衹有第三個元素非零，可以降次。&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(=4(-1)^{1+3}\begin{vmatrix}  20 &amp;amp; 70 \\  38 &amp;amp; -183\end{vmatrix}\)&lt;/span&gt; &lt;br&gt;
&lt;span class=&#34;diff_alert&#34;&gt;利用性質2: 第一行所有元素除以10, 將 10 提前。&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(=4\times10\begin{vmatrix}  2 &amp;amp; 7 \\  38 &amp;amp; -183\end{vmatrix}\\  =40(-366-266)\\=-25280\)&lt;/span&gt;&lt;br&gt;
&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(|A|=\begin{vmatrix}  0 &amp;amp; 4 &amp;amp; 2\\  -1 &amp;amp; 3 &amp;amp; 7\\  6 &amp;amp; 5 &amp;amp; 9\\  \end{vmatrix}\)&lt;/span&gt; &lt;br&gt;
&lt;span class=&#34;diff_alert&#34;&gt;利用性質3: 第2列 - 第3列 &lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt; 2 作爲新的第二列元素&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(=\begin{vmatrix} 0 &amp;amp; 0 &amp;amp; 2\\ -1 &amp;amp; -11 &amp;amp; 7\\ 6 &amp;amp; -13 &amp;amp; 9\\ \end{vmatrix}\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;diff_alert&#34;&gt;利用性質7: 第一行衹有第三個元素非零，降次。&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(=2(-1)^{1+3}\begin{vmatrix} -1 &amp;amp; -11 \\ 6 &amp;amp; -13\end{vmatrix}\\=2(13+66)=158\)&lt;/span&gt; &lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(|B|=\begin{vmatrix} 2 &amp;amp; 3 &amp;amp; 4\\ -2 &amp;amp; 7 &amp;amp; 1\\ 4 &amp;amp; 6 &amp;amp; 0\\ \end{vmatrix}\)&lt;/span&gt; &lt;br&gt;
&lt;span class=&#34;diff_alert&#34;&gt;利用性質3: 第1行 &lt;span class=&#34;math inline&#34;&gt;\(+\)&lt;/span&gt; 第2行作新的第1行； 第3行 - 第1行 &lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt; 2 作新的第三行&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(=\begin{vmatrix} 0 &amp;amp; 10 &amp;amp; 5\\ -2 &amp;amp; 7 &amp;amp; 1\\ 0 &amp;amp; 0 &amp;amp; -8\\ \end{vmatrix}\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;diff_alert&#34;&gt;利用性質7: 第三行衹有第三個元素非零，降次。&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(=-8(-1)^{3+3}\begin{vmatrix} 0 &amp;amp; 10 \\ -2 &amp;amp; 7\end{vmatrix}\\=-8\times20=-160\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;綜上可得&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;綜上可得&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(|A|·|B|=158\times(-160)=-25280=|AB|\)&lt;/span&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;試用這一節介紹的行列式性質求解前一節例3的行列式值&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;試用這一節介紹的行列式性質，求解前一節&lt;a href=&#34;https://winterwang.github.io/post/2017-03-15/&#34;&gt;例(3)&lt;/a&gt;的行列式值。&lt;/h3&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;解-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;解&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\begin{vmatrix}  -2 &amp;amp; 3 &amp;amp;4 &amp;amp; 1\\  4 &amp;amp; 2&amp;amp; 0&amp;amp; 5\\  2 &amp;amp;-3&amp;amp; -4&amp;amp; 2\\  2 &amp;amp; 1&amp;amp; 2&amp;amp; -3 \end{vmatrix}\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;diff_alert&#34;&gt;利用性質3&lt;br&gt;
1. 第1行 &lt;span class=&#34;math inline&#34;&gt;\(+\)&lt;/span&gt; 第3行，作新的第一行；
2. 第2行 &lt;span class=&#34;math inline&#34;&gt;\(-\)&lt;/span&gt; 第3行 &lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt; 2，作新的第2行；
3. 第4行 &lt;span class=&#34;math inline&#34;&gt;\(-\)&lt;/span&gt; 第3行 作新的第4行&lt;/span&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(=\begin{vmatrix}  0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 3\\  0 &amp;amp; 8 &amp;amp; 8 &amp;amp; 1\\  2 &amp;amp; -3 &amp;amp; -4 &amp;amp; 2\\  0 &amp;amp; 4 &amp;amp; 6 &amp;amp; -5 \end{vmatrix}\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;diff_alert&#34;&gt;利用性質7: 第1行衹有第4個元素非零，降次。&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(=3(-1)^{1+4}\begin{vmatrix} 0 &amp;amp; 8 &amp;amp; 8 \\ 2 &amp;amp; -3 &amp;amp; -4 \\ 0 &amp;amp; 4 &amp;amp; 6 \end{vmatrix}\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;diff_alert&#34;&gt;利用性質7: 第1列衹有第2個元素非零，降次。&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(=-3 \times 2(-1)^{1+2}\begin{vmatrix} 8 &amp;amp; 8 \\ 4 &amp;amp; 6 \end{vmatrix}\\ =6 \times (8 \times 6 - 4\times 8)\\ =96\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記18</title>
      <link>https://wangcc.me/post/2017-03-15/</link>
      <pubDate>Wed, 15 Mar 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/2017-03-15/</guid>
      <description>
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;行列式的定義與計算&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;行列式的定義與計算&lt;/h2&gt;

&lt;div class=&#34;theorem&#34;&gt;
&lt;span id=&#34;thm:unnamed-chunk-1&#34; class=&#34;theorem&#34;&gt;&lt;strong&gt;Theorem 1  (determinant)  &lt;/strong&gt;&lt;/span&gt;&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 次正方形矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A= (a_{ij})=\left( \begin{array}{c} a_{11}&amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n}\\ a_{21}&amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n}\\ \vdots&amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ a_{n1}&amp;amp; a_{n2} &amp;amp; \cdots &amp;amp; a_{nn} \end{array} \right)\)&lt;/span&gt; 的&lt;strong&gt;行列式(determinant)&lt;/strong&gt;被定義爲是，&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 的全部成分 &lt;span class=&#34;math inline&#34;&gt;\(a_{11},a_{12},\cdots,a_{nn}\)&lt;/span&gt; 的函數，這個函數是一個&lt;strong&gt;標量(scalar)&lt;/strong&gt;。
&lt;/div&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;次正方形矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 的行列式(&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;次行列式)，被記作：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(|A|, |a_{ij}|, \det(A), \det(a_{ij})， \begin{vmatrix} a_{11}&amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n}\\ a_{21}&amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n}\\ \vdots&amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ a_{n1}&amp;amp; a_{n2} &amp;amp; \cdots &amp;amp; a_{nn} \notag \end{vmatrix}\)&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;1次行列式：
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
A=(a_{11}), |A|=a_{11}
(\#eq:determinant1)
\end{equation}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;2次行列式：
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
A=\left(
\begin{array}{}
a_{11} &amp;amp; a_{12}\\
a_{21} &amp;amp; a_{22}\\
\end{array}
\right), |A|=a_{11}a_{12}-a_{12}a_{21}
(\#eq:determinant2)
\end{equation}\]&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(n-1\)&lt;/span&gt; 次行列式
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
A_{(n-1)\times(n-1)}, 假設行列式 |A| 有被定義
(\#eq:determinant3)
\end{equation}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 次行列式&lt;br&gt;
假如&lt;a href=&#34;#eq:determinant3&#34;&gt;(&lt;strong&gt;??&lt;/strong&gt;)&lt;/a&gt;成立：&lt;br&gt;
對於：&lt;span class=&#34;math inline&#34;&gt;\(A_{n\times n}=\left( \begin{array}{c} a_{11}&amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n}\\ a_{21}&amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n}\\ \vdots&amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ a_{n1}&amp;amp; a_{n2} &amp;amp; \cdots &amp;amp; a_{nn} \end{array} \right)\\ |A|=\begin{vmatrix} a_{11}&amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n}\\ a_{21}&amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n}\\ \vdots&amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ a_{n1}&amp;amp; a_{n2} &amp;amp; \cdots &amp;amp; a_{nn} \notag \end{vmatrix}\\ \left\{ \begin{array}{} (4)\;=a_{i1}A_{i1}+a_{i2}A_{i2}+\cdots+a_{ij}A_{ij}+\cdots+a_{in}A_{in}\\ (5)\;=a_{1j}A_{i1}+a_{2j}A_{2j}+\cdots+a_{ij}A_{ij}+\cdots+a_{nj}A_{nj}\\  \end{array} \right.\)&lt;/span&gt;&lt;br&gt;
式子 &lt;span class=&#34;math inline&#34;&gt;\((4)\)&lt;/span&gt; 被稱爲行列式 &lt;span class=&#34;math inline&#34;&gt;\(|A|\)&lt;/span&gt; 的第 &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; 行&lt;strong&gt;展開式(expansion of &lt;span class=&#34;math inline&#34;&gt;\(|A|\)&lt;/span&gt; according to elements of row &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;)&lt;/strong&gt;。同樣的，式子 &lt;span class=&#34;math inline&#34;&gt;\((5)\)&lt;/span&gt; 被稱爲行列式 &lt;span class=&#34;math inline&#34;&gt;\(|A|\)&lt;/span&gt; 的第 &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; 列展開式。&lt;span class=&#34;math inline&#34;&gt;\(|A_{ij}|(i=1,2,\cdots,n;j=1,2,\cdots,n)\)&lt;/span&gt; 被稱爲 成分 &lt;span class=&#34;math inline&#34;&gt;\(a_{ij}\)&lt;/span&gt; 的&lt;strong&gt;餘因子(cofactor)&lt;/strong&gt;，定義如下：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(A_{ij}=(-1)^{i+j}D_{ij}\\ \;\;\;\;=(-1)^{i+j}\begin{vmatrix} a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1,j-1} &amp;amp; a_{1,j+1} &amp;amp; \cdots &amp;amp; a_{1n}\\ a_{21} &amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2,j-1} &amp;amp; a_{2,j+1} &amp;amp; \cdots &amp;amp; a_{2n}\\ \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots\\ a_{i-1,1} &amp;amp; a_{i-1,2} &amp;amp; \cdots &amp;amp; a_{i-1,j-1} &amp;amp; a_{i-1,j+1} &amp;amp; \cdots &amp;amp; a_{i-1,n}\\ a_{i+1,1} &amp;amp; a_{i+1,2} &amp;amp; \cdots &amp;amp; a_{i+1,j-1} &amp;amp; a_{i+1,,j+1} &amp;amp; \cdots &amp;amp; a_{i+1,n}\\ \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots\\ a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1,j-1} &amp;amp; a_{1,j+1} &amp;amp; \cdots &amp;amp; a_{1n}\\ \end{vmatrix}\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(D_{ij}\)&lt;/span&gt; 正如上面等式最右端所寫，其實是行列式 &lt;span class=&#34;math inline&#34;&gt;\(A_{n\times n}\)&lt;/span&gt; 剔除了第 &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; 行和第 &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; 列的 &lt;span class=&#34;math inline&#34;&gt;\((n-1)\)&lt;/span&gt; 次行列式，又被叫做行列式 &lt;span class=&#34;math inline&#34;&gt;\(A_{n\times n}\)&lt;/span&gt; 的&lt;strong&gt;小行列式(minor)&lt;/strong&gt;。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;餘因子矩陣&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;餘因子矩陣&lt;/h2&gt;
&lt;p&gt;以 &lt;span class=&#34;math inline&#34;&gt;\(A_{n\times n}\)&lt;/span&gt; 的成分 &lt;span class=&#34;math inline&#34;&gt;\(a_{ij}\)&lt;/span&gt; 的餘因子 &lt;span class=&#34;math inline&#34;&gt;\(A_{ij}\)&lt;/span&gt; 作成分&lt;strong&gt;的矩陣&lt;/strong&gt;&lt;span class=&#34;diff_alert&#34;&gt;的轉置矩陣&lt;/span&gt;作被稱爲 &lt;span class=&#34;math inline&#34;&gt;\(A_{n\times n}\)&lt;/span&gt; 的&lt;strong&gt;餘因子矩陣(adjoint matrix, adjugate matrix)&lt;/strong&gt;。標記爲 &lt;span class=&#34;math inline&#34;&gt;\(adj(A)\)&lt;/span&gt;。也就是說：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(adj(A)=\left( \begin{array}{c} A_{11}&amp;amp; A_{12} &amp;amp; \cdots &amp;amp; A_{1n}\\ A_{21}&amp;amp; A_{22} &amp;amp; \cdots &amp;amp; A_{2n}\\ \vdots&amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ A_{n1}&amp;amp; A_{n2} &amp;amp; \cdots &amp;amp; A_{nn} \end{array} \right)^t=\left( \begin{array}{c} A_{11}&amp;amp; A_{21} &amp;amp; \cdots &amp;amp; A_{n1}\\ A_{12}&amp;amp; A_{22} &amp;amp; \cdots &amp;amp; A_{n2}\\ \vdots&amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ A_{1n}&amp;amp; A_{2n} &amp;amp; \cdots &amp;amp; A_{nn} \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;我們來試着計算行列式：&lt;br&gt;
1. &lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt; 次行列式&lt;br&gt;
以方程&lt;a href=&#34;#eq:determinant2&#34;&gt;(&lt;strong&gt;??&lt;/strong&gt;)&lt;/a&gt;的定義計算：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\begin{vmatrix}  a_{11} &amp;amp; a_{12}\\  a_{21} &amp;amp; a_{22}\\  \end{vmatrix}=a_{11}a_{22}-a_{12}a_{21}\)&lt;/span&gt;&lt;br&gt;
此公式可以用下列 &lt;strong&gt;示意圖(薩呂法則, Sarrus’ rule)&lt;/strong&gt; 來記憶:
　&lt;img src=&#34;https://wangcc.me/img/sarrus.png&#34; /&gt;&lt;br&gt;
也就是，沿着右下方向將所有成分相乘以後用加號 &lt;span class=&#34;math inline&#34;&gt;\(+\)&lt;/span&gt; 號連接起來，沿着左下的方向的所有成分則相乘以後用減號 &lt;span class=&#34;math inline&#34;&gt;\(-\)&lt;/span&gt; 號連接起來。最後將這兩者相加獲得行列式的值。&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;練習： 求 &lt;span class=&#34;math inline&#34;&gt;\(A=\left( \begin{array}{} 4 &amp;amp; 2\\ 1 &amp;amp; 3\\ \end{array} \right)\)&lt;/span&gt; 的行列式和餘因子矩陣。&lt;/p&gt;
&lt;p&gt;解： &lt;span class=&#34;math inline&#34;&gt;\(|A|=\begin{vmatrix} 4 &amp;amp; 2\\ 1 &amp;amp; 3\\ \end{vmatrix}=4\times3-2\times1=10\)&lt;/span&gt; &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(adj(A)=\left( \begin{array}{} A_{11} &amp;amp; A_{21}\\ A_{12} &amp;amp; A_{22}\\ \end{array} \right)\)&lt;/span&gt; &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\because A_{11}=(-1)^{(1+1)}\times3\\ A_{21}=(-1)^{(2+1)}\times2\\ A_{12}=(-1)^{(1+2)}\times1\\ A_{22}=(-1)^{2+2}\times4\\ \therefore adj(A)=\left( \begin{array}{r} 3 &amp;amp; -2\\ -1 &amp;amp; 4\\ \end{array} \right)\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;diff_alert&#34;&gt; 注意：餘因子&lt;strong&gt;矩陣&lt;/strong&gt;，終究是一個矩陣而非行列式。&lt;/span&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;三次矩陣&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(A=\left( \begin{array}{c}  a_{11} &amp;amp; a_{12} &amp;amp; a_{13}\\  a_{21} &amp;amp; a_{22} &amp;amp; a_{23}\\  a_{31} &amp;amp; a_{32} &amp;amp; a_{33}\\ \end{array} \right)\)&lt;/span&gt; 的行列式 &lt;span class=&#34;math inline&#34;&gt;\(|A|\)&lt;/span&gt; 要如何用 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 的成分來表示呢？ &lt;br&gt;
我們發現，代入上面第 &lt;span class=&#34;math inline&#34;&gt;\((4)\)&lt;/span&gt; 個式子 &lt;span class=&#34;math inline&#34;&gt;\(n=3\)&lt;/span&gt; 的情況來計算。&lt;br&gt;
在這裏，我們就按照 &lt;span class=&#34;math inline&#34;&gt;\(i=1\)&lt;/span&gt; 的情況來展開。&lt;span class=&#34;diff_alert&#34;&gt; (注意：&lt;span class=&#34;math inline&#34;&gt;\(i=2, i=3\)&lt;/span&gt; 的情況展開，結果也是一樣的。)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
 |A| &amp;amp;= a_{11}A_{11}+a_{12}A_{12}+a_{13}A_{13}\\
     &amp;amp;= a_{11}(-1)^{1+1}D_{11}+a_{12}(-1)^{1+2}D_{12}\\
     &amp;amp;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+a_{13}(-1)^{1+3}D_{13}\\
     &amp;amp;= a_{11}\begin{vmatrix}a_{22} &amp;amp; a_{23}\\ a_{32} &amp;amp; a_{33}\\\end{vmatrix}-a_{12}\begin{vmatrix}a_{21} &amp;amp; a_{23}\\ a_{31} &amp;amp; a_{33}\\\end{vmatrix}\\
     &amp;amp;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+a_{13}\begin{vmatrix}a_{21} &amp;amp; a_{22}\\ a_{31} &amp;amp; a_{32}\\\end{vmatrix}\\
     &amp;amp;= a_{11}(a_{22}a_{23}-a_{23}a_{32})-a_{12}(a_{21}a_{33}-a_{31}a_{23})\\
     &amp;amp;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+a_{13}(a_{21}a_{32}-a_{22}a_{31})\\
     &amp;amp;=a_{11}a_{22}a_{33}+a_{12}a_{23}a_{31}+a_{13}a_{21}a_{32}-a_{11}a_{23}a_{32}\\
     &amp;amp;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;-a_{12}a_{21}a_{33}-a_{13}a_{22}a_{31}
\end{align}\]&lt;/span&gt;&lt;br&gt;
我們也可以利用薩呂法則（下圖）來記住計算過程：&lt;br&gt;
&lt;img src=&#34;https://wangcc.me/img/sarrus33.png&#34; /&gt;&lt;br&gt;
另外，可以得到如下的餘因子：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(A_{11}=\begin{vmatrix}a_{22} &amp;amp; a_{23}\\ a_{32} &amp;amp; a_{33}\\\end{vmatrix}, A_{12}=-\begin{vmatrix}a_{21} &amp;amp; a_{23}\\ a_{31} &amp;amp; a_{33}\\\end{vmatrix}, A_{13}=\begin{vmatrix}a_{21} &amp;amp; a_{22}\\ a_{31} &amp;amp; a_{32}\\\end{vmatrix}\\ A_{21}=-\begin{vmatrix}a_{12} &amp;amp; a_{13}\\ a_{32} &amp;amp; a_{33}\\\end{vmatrix}, A_{22}=\begin{vmatrix}a_{11} &amp;amp; a_{13}\\ a_{31} &amp;amp; a_{33}\\\end{vmatrix}, A_{23}=-\begin{vmatrix}a_{11} &amp;amp; a_{12}\\ a_{31} &amp;amp; a_{32}\\\end{vmatrix}\\ A_{31}=\begin{vmatrix}a_{12} &amp;amp; a_{13}\\ a_{22} &amp;amp; a_{23}\\\end{vmatrix}, A_{32}=-\begin{vmatrix}a_{11} &amp;amp; a_{13}\\ a_{21} &amp;amp; a_{23}\\\end{vmatrix}, A_{33}=\begin{vmatrix}a_{11} &amp;amp; a_{12}\\ a_{21} &amp;amp; a_{22}\\\end{vmatrix}\)&lt;/span&gt;&lt;br&gt;
因此餘因子矩陣爲：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(adj(A)=\left( \begin{array}{c}  A_{11} &amp;amp; A_{12} &amp;amp; A_{13}\\  A_{21} &amp;amp; A_{22} &amp;amp; A_{23}\\  A_{31} &amp;amp; A_{32} &amp;amp; A_{33}\\ \end{array} \right)^t=\left( \begin{array}{c}  A_{11} &amp;amp; A_{21} &amp;amp; A_{23}\\  A_{12} &amp;amp; A_{22} &amp;amp; A_{32}\\  A_{13} &amp;amp; A_{23} &amp;amp; A_{33}\\ \end{array} \right)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;table style=&#34;width:7%;&#34;&gt;
&lt;colgroup&gt;
&lt;col width=&#34;6%&#34; /&gt;
&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;練習：試求矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A=\left( \begin{array}{r} 6 &amp;amp; 1 &amp;amp; -3\\ 3 &amp;amp; 5 &amp;amp; 7\\ 2 &amp;amp; -1 &amp;amp; 3\\ \end{array} \right)\)&lt;/span&gt; 的行列式和餘因子矩陣&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;解： &lt;span class=&#34;math inline&#34;&gt;\(|A|= \begin{vmatrix} 6 &amp;amp; 1 &amp;amp; -3\\ 3 &amp;amp; 5 &amp;amp; 7\\ 2 &amp;amp; -1 &amp;amp; 3\\ \end{vmatrix}\\ \;\;\;\;\:=6\times5\times3+1\times7\times2+(-3)\times3\times(-1)\\ \;\;\;\;\;\:\;\;\;\;\:-\{6\times7\times(-1)+1\times3\times3+(-3)\times5\times2\}\\ \;\;\;\;\:=113-(-63)=176\\ \\ A_{11}=\begin{vmatrix}5 &amp;amp; 7\\ -1 &amp;amp; 3\\\end{vmatrix}=15-(-7)=22\\ A_{12}=\begin{vmatrix}3 &amp;amp; 7\\ 2&amp;amp; 3\\\end{vmatrix}=9-14=-5\\ A_{13}=\begin{vmatrix}3 &amp;amp; 5\\ 2 &amp;amp; -1\\\end{vmatrix}=-3-10=-13\\ A_{21}=\begin{vmatrix}1 &amp;amp; -3\\ -1 &amp;amp; 3\\\end{vmatrix}=3-3=0\\ A_{22}=\begin{vmatrix}6 &amp;amp; -3\\ 2 &amp;amp; 3\\\end{vmatrix}=18-(-6)=24\\ A_{23}=\begin{vmatrix}6 &amp;amp; 1\\ 2 &amp;amp; -1\\\end{vmatrix}=-6-2=-8\\ A_{31}=\begin{vmatrix}1 &amp;amp; -3\\5 &amp;amp; 7\\\end{vmatrix}=7-(-15)=22\\ A_{32}=\begin{vmatrix}6 &amp;amp; -3\\3 &amp;amp; 7\\\end{vmatrix}=42-(-9)=51\\ A_{33}=\begin{vmatrix}6 &amp;amp; 1\\ 3 &amp;amp; 5\\\end{vmatrix}=30-3=27\\ \Longrightarrow adj(A)=\left( \begin{array}{r} 22 &amp;amp; 5 &amp;amp; -13 \\ 0 &amp;amp; 24&amp;amp; -8 \\ 22 &amp;amp; 51&amp;amp; 27 \\ \end{array} \right)^t=\left( \begin{array}{r} 22 &amp;amp; 0 &amp;amp; 22\\ 5 &amp;amp; 24 &amp;amp; 51\\ -13 &amp;amp; -8 &amp;amp; 27\\ \end{array} \right)\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table style=&#34;width:7%;&#34;&gt;
&lt;colgroup&gt;
&lt;col width=&#34;6%&#34; /&gt;
&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;練習： 求3次矩陣的固有值時(將來敘述)需要的行列式&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\begin{vmatrix} a-\lambda &amp;amp; b &amp;amp; c\\ d &amp;amp; e-\lambda &amp;amp; f\\ g &amp;amp; h &amp;amp; i-\lambda \end{vmatrix}\)&lt;/span&gt;&lt;br&gt;
展開以後，整理爲關於 &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; 的式子：&lt;br&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;解： &lt;span class=&#34;math inline&#34;&gt;\(\begin{vmatrix} a-\lambda &amp;amp; b &amp;amp; c\\ d &amp;amp; e-\lambda &amp;amp; f\\ g &amp;amp; h &amp;amp; i-\lambda \end{vmatrix}\\ =(a-\lambda)(e-\lambda)(i-\lambda)+bfg+dhc\\ \;\;\;\;\;-\{g(e-\lambda)c+bd(i-\lambda)+(a-\lambda)fh\}\\ =-\lambda^3+(a+e+i)\lambda^2+(bd+cg+fh-ae-ei-ai)\lambda\\ \;\;\;\;\;+(aei+bfg+cdh-afh-bdi-ecg)\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;4次行列式：&lt;br&gt;
試求&lt;span class=&#34;math inline&#34;&gt;\(A=\left( \begin{array}{}  a_{11}&amp;amp; a_{12}&amp;amp; a_{13}&amp;amp; a_{14}\\  a_{21}&amp;amp; a_{22}&amp;amp; a_{23}&amp;amp; a_{24}\\  a_{31}&amp;amp; a_{32}&amp;amp; a_{33}&amp;amp; a_{34}\\  a_{41}&amp;amp; a_{42}&amp;amp; a_{43}&amp;amp; a_{44} \end{array} \right)\\ \;\;=\left( \begin{array}{r}  -2 &amp;amp; 3 &amp;amp;4 &amp;amp; 1\\  4 &amp;amp; 2&amp;amp; 0&amp;amp; 5\\  2 &amp;amp;-3&amp;amp; -4&amp;amp; 2\\  2 &amp;amp; 1&amp;amp; 2&amp;amp; -3 \end{array} \right)\)&lt;/span&gt; &lt;br&gt;的行列式 &lt;span class=&#34;math inline&#34;&gt;\(|A|\)&lt;/span&gt;：&lt;br&gt;
由於第2行有成分 &lt;span class=&#34;math inline&#34;&gt;\(a_{23}=0\)&lt;/span&gt; 我們以第二行展開行列式，因爲 &lt;span class=&#34;math inline&#34;&gt;\(a_{23}=0\)&lt;/span&gt;，所以 &lt;span class=&#34;math inline&#34;&gt;\(a_{23}A_{23}=0\)&lt;/span&gt; 可以省略：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(|A|=a_{21}A_{21}+a_{22}A_{22}+a_{24}A_{24}\\ \;\;\;\;\:=a_{21}(-1)^{2+1}D_{21}+a_{22}(-1)^{2+2}D_{22}+a_{24}(-1)^{2+4}D_{24}\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\because A_{21}=(-1)^{2+1}\begin{vmatrix}  a_{12} &amp;amp; a_{13} &amp;amp; a_{14}\\  a_{32} &amp;amp; a_{33} &amp;amp; a_{34}\\  a_{42} &amp;amp; a_{43} &amp;amp; a_{44}\\ \end{vmatrix}\\ \;\;\;\;\;\;\;\;\;=-\begin{vmatrix}  3 &amp;amp; 4 &amp;amp; 1\\  -3 &amp;amp; -4 &amp;amp; 2\\  1 &amp;amp; 2&amp;amp; -3\\ \end{vmatrix}=6\\ A_{22}=(-1)^{2+2}\begin{vmatrix}  a_{11} &amp;amp; a_{13} &amp;amp; a_{14}\\  a_{31} &amp;amp; a_{33} &amp;amp; a_{34}\\  a_{41} &amp;amp; a_{43} &amp;amp; a_{44}\\ \end{vmatrix}\\ \;\;\;\;\;\;\;\;\;=\begin{vmatrix}  -2 &amp;amp; 4 &amp;amp;1\\  2 &amp;amp; -4 &amp;amp;2\\  2 &amp;amp; 2 &amp;amp;-3\\ \end{vmatrix}=36\\ A_{24}=(-1)^{2+4}\begin{vmatrix}  a_{11} &amp;amp; a_{12} &amp;amp; a_{13}\\  a_{31} &amp;amp; a_{32} &amp;amp; a_{33}\\  a_{41} &amp;amp; a_{42} &amp;amp; a_{43}\\ \end{vmatrix}\\ \;\;\;\;\;\;\;\;\;=-\begin{vmatrix}  -2 &amp;amp; 3 &amp;amp;4\\  2 &amp;amp; -3 &amp;amp;-4\\  2 &amp;amp; 1 &amp;amp;2\\ \end{vmatrix}=0\\ \therefore |A|=4\times6+2\times36+5\times0=96\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;diff_alert&#34;&gt;然而，&lt;strong&gt;4次以上的矩陣的行列式計算，沒有類似薩呂法則的計算方法。&lt;/strong&gt;&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記17</title>
      <link>https://wangcc.me/post/2017-03-13/</link>
      <pubDate>Mon, 13 Mar 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/2017-03-13/</guid>
      <description>
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;正定半正定-正值半正值&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;正定，半正定 (正值，半正值)&lt;/h2&gt;
&lt;p&gt;對於任意的非零向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}(\neq\underline{0})\)&lt;/span&gt; ，如果2次型 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}^tA\underline{x}\)&lt;/span&gt; 始終滿足 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}^tA\underline{x} &amp;gt; 0\)&lt;/span&gt; &lt;strong&gt;注意此處無等號&lt;/strong&gt;。我們稱這個2次型爲&lt;strong&gt;正定(positive definite)&lt;/strong&gt;，&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;爲&lt;strong&gt;正定矩陣(positive definite matrix)&lt;/strong&gt;。另外，如果任意非零向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}(\neq\underline{0})\)&lt;/span&gt; 始終滿足2次型 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}^tA\underline{x} \geqslant 0\)&lt;/span&gt;， 這個2次型被叫做&lt;strong&gt;半正定(positive semi-definite)&lt;/strong&gt;，&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;爲&lt;strong&gt;半正定矩陣(positive semi-definite matrix)&lt;/strong&gt;。&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(x=\left( \begin{array}{} x_1\\ x_2\\ x_3 \end{array} \right), \ A=\left( \begin{array}{} 5 &amp;amp; 2 &amp;amp; 4\\ 2 &amp;amp; 2 &amp;amp; 3\\ 4 &amp;amp; 3 &amp;amp; 25 \end{array} \right)\)&lt;/span&gt;，2次型 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}^tA\underline{x}\)&lt;/span&gt; 是正定。因爲：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}^tA\underline{x}=5x_1^2+2x_2^2+25x_3^2\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+4x_1x_2+8x_1x_3+6x_2x_3\\\;\;\;\;\;\;\;\;\;\:=(2x_1+x_2)^2+(x_2+3x_3)^2+(x_1+4x_3)^2\\ \because \underline{x}\neq\underline{0}=\left( \begin{array}{} 0\\ 0\\ 0 \end{array} \right)\\ \therefore \underline{x}^tA\underline{x}&amp;gt;0\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(x=\left( \begin{array}{} x_1\\ x_2\\ x_3 \end{array} \right), \ A=\left( \begin{array}{} 5 &amp;amp; -6 &amp;amp; 3\\ -6 &amp;amp; 25 &amp;amp; 32\\ 3 &amp;amp; 32 &amp;amp; 73 \end{array} \right)\)&lt;/span&gt;，2次型 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}^tA\underline{x}\)&lt;/span&gt; 是半正定。因爲：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}^tA\underline{x}=5x_1^2+25x_2^2+73x_3^2\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;-12x_1x_2+6x_2x_3+64x_1x_3\\ \;\;\;\;\;\;\;\;\;\:=(2x_1-3x_3)^2+(x_1+3x_3)^2+(4x_2+8x_3)^2\\\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\because \underline{x}=\left( \begin{array}{c} 3\\ 2\\ -1 \end{array} \right)\)&lt;/span&gt; 時 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}^tA\underline{x}=0\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\therefore \underline{x}^tA\underline{x} \geqslant0\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}=\left( \begin{array}{c} x_1\\ x_2\\ \vdots\\ x_n \end{array} \right)(\neq\underline{0})\)&lt;/span&gt; 與單位矩陣 &lt;span class=&#34;math inline&#34;&gt;\(E_n\)&lt;/span&gt; 構成的2次型 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}^tE_n\underline{x}=\underline{x}^t\underline{x}=\sum\limits_{i=1}^nx_i^2&amp;gt;0\)&lt;/span&gt; 是爲正定。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}=\left( \begin{array}{c} x_1\\ x_2\\ \vdots\\ x_n \end{array} \right), \underline{\frac{1}{n}}=\left( \begin{array}{c} \frac{1}{n}\\ \frac{1}{n}\\ \vdots \\ \frac{1}{n} \end{array} \right)\)&lt;/span&gt; 已知，&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}\underline{1/n}=\sum\limits_{i=1}^nx_i\cdot \frac{1}{n}=\bar{x}\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}\)&lt;/span&gt; 的平均值)，包含了 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 個 &lt;span class=&#34;math inline&#34;&gt;\(\bar{x}\)&lt;/span&gt; 的橫向量： &lt;span class=&#34;math inline&#34;&gt;\((\bar{x}，\bar{x},\cdots,\bar{x})\)&lt;/span&gt; 展開以後成爲：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\((\bar{x}，\bar{x},\cdots,\bar{x})\\ \;\;\;\;\;\;\;\;\;\:=(x_1, x_2, \cdots, x_n)\left( \begin{array}{c} \frac{1}{n} &amp;amp; \frac{1}{n} &amp;amp; \cdots &amp;amp; \frac{1}{n}\\ \frac{1}{n} &amp;amp; \frac{1}{n} &amp;amp; \cdots &amp;amp; \frac{1}{n}\\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ \frac{1}{n} &amp;amp; \frac{1}{n} &amp;amp; \cdots &amp;amp; \frac{1}{n} \end{array} \right)\\ \;\;\;\;\;\;\;\;\;\:=\underline{x}U\)&lt;/span&gt;&lt;br&gt;
令 &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; 代表上面第二個等式中有半部分的矩陣。那麼將之從右往左乘以 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}\)&lt;/span&gt; 我們可以得到：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}^tU\underline{x}=(\bar{x},\bar{x},\cdots,\bar{x})\underline{x}=(\bar{x},\bar{x},\cdots,\bar{x})\left( \begin{array}{c} x_1\\ x_2\\ \vdots\\ x_n \end{array} \right)\\ =\sum\limits_{i=1}^n\bar{x}x_i=\bar{x}\sum\limits_{i=1}^nx_i=n\bar{x}^2\)&lt;/span&gt;&lt;br&gt;
利用上面的式子，我們可以得到，&lt;strong&gt;偏差平方和(sum of squared deviation, SS)&lt;/strong&gt;：&lt;span class=&#34;math inline&#34;&gt;\(E_n\)&lt;/span&gt; 爲 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 次單位矩陣。&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(SS=\sum\limits_{i=1}^n(x_i-\bar{x})^2\\ \;\;\;\;\:=\sum\limits_{i=1}^n(x_i^2-2x_i\bar{x}+\bar{x}^2)\\ \;\;\;\;\:=\sum\limits_{i=1}^nx_i^2-2\bar{x}\sum\limits_{i=1}^nx_i+n\bar{x}^2\\ \;\;\;\;\:=\sum\limits_{i=1}^nx_i^2-2n\bar{x}^2+n\bar{x}^2\\ \;\;\;\;\:=\sum\limits_{i=1}^nx_i^2-n\bar{x}^2\\ \;\;\;\;\:=\sum\limits_{i=1}^nx_i^2-\underline{x}^tU\underline{x}\\ \;\;\;\;\:=\underline{x}^t\underline{x}-\underline{x}^tU\underline{x}\\ \;\;\;\;\:=\underline{x}^tE_n\underline{x}-\underline{x}^tU\underline{x}\\ \;\;\;\;\:=\underline{x}^t(E_n-U)\underline{x}\\ \because when \underline{x}=\left( \begin{array}{c} x_1\\ x_2\\ \vdots\\ x_n \end{array} \right)=\left( \begin{array}{c} \bar{x}\\ \bar{x}\\ \vdots\\ \bar{x} \end{array} \right), \&amp;amp; (\bar{x}\neq0), SS=0\\ \therefore \underline{x}^t(E_n-U)\underline{x}\;是半正定2次型。\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;雙一次型&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;雙一次型&lt;/h2&gt;
&lt;p&gt;對於 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}=\left( \begin{array}{c} x_1\\ x_2\\ \vdots\\ x_m \end{array} \right), A=\left( \begin{array}{c} a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n}\\ a_{21} &amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n}\\ \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots\\ a_{m1} &amp;amp; a_{m2} &amp;amp; \cdots &amp;amp; a_{mn} \end{array} \right), \underline{y}=\left( \begin{array}{c} y_1\\ y_2\\ \vdots\\ y_n \end{array} \right)\)&lt;/span&gt; 來說，&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}^tA\underline{y}=\sum\limits_{i=1}^m\sum\limits_{j=1}^na_{ij}x_iy_j\)&lt;/span&gt; 既是 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}\)&lt;/span&gt; 的1次型，也是 &lt;span class=&#34;math inline&#34;&gt;\(\underline{y}\)&lt;/span&gt; 的1次型，所以又叫做 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}\underline{y}\)&lt;/span&gt; 的&lt;strong&gt;雙1次型(bilinear form)&lt;/strong&gt;。雙1次型是一個標量(scalar)。&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;對於 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}=\left( \begin{array}{c} x_1\\ x_2\\ x_3 \end{array} \right), B=(b_{ij})=\left( \begin{array}{c} 1 &amp;amp; 0 &amp;amp; 0\\ 0 &amp;amp; 1 &amp;amp; 1\\ 1 &amp;amp; 0 &amp;amp; 1 \end{array} \right), \underline{y}=\left( \begin{array}{c} y_1\\ y_2\\ y_3 \end{array} \right)\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}^tB\underline{y}=\sum\limits_{i=1}^3\sum\limits_{j=1}^3b_{ij}x_iy_j=(x_1+x_3, x_2, x_2+x_3)\left( \begin{array}{c} y_1\\ y_2\\ y_3 \end{array} \right)\\ \;\;\;\;\:=x_1y_1+x_2y_2+x_2y_3+x_3y_1+x_3y_3\\ \;\;\;\;\:=x_1y_1+x_2(y_2+y_3)+x_3(y_1+y_3) \; \bf (\underline{x} 的1次型)\)&lt;/span&gt; &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\;\;\;\;\;\:=y_1(x_1+x_3)+x_2y_2+(x_2+x_3)y_3 \;\bf (\underline{y} 的1次型)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;對於 &lt;span class=&#34;math inline&#34;&gt;\(\underline{l}=\left( \begin{array}{c} l_1\\ l_2\\ \end{array} \right), T=\left( \begin{array}{c} t_{11} &amp;amp; t_{12} &amp;amp; t_{13}\\ t_{21} &amp;amp; t_{22} &amp;amp; t_{23}\\ \end{array} \right), \underline{m}=\left( \begin{array}{c} m_1\\ m_2\\ m_3 \end{array} \right)\)&lt;/span&gt; &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{l}^tT\underline{m}=\sum\limits_{i=1}^2\sum\limits_{j=1}^3t_{ij}l_im_j\\ \;\;\;\;\;\;\;\;\;\;=l_1t_{11}m_1+l_1t_{12}m_2+l_1t_{13}m_3\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+l_2t_{21}m_1+l_2t_{22}m_2+l_3t_{23}m_3\\ \;\;\;\;\;\;\;\;\;\;=l_1(t_{11}m_1+t_{12}m_2+t_{13}m_3)\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+l_2(t_{21}m_1+t_{22}m_2+t_{23}m_3) \;\;(\underline{l} \textbf{的1次型})\\ \;\;\;\;\;\;\;\;\;\;=(t_{11}l_1+t_{21}l_2)m_1+(t_{12}l_1+t_{22}l_2)m_2\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+(t_{13}l_1+t_{23}l_2)m_3\;\;(\underline{m} \textbf{的1次型})\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記16</title>
      <link>https://wangcc.me/post/2017-03-11/</link>
      <pubDate>Sat, 11 Mar 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/2017-03-11/</guid>
      <description>
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;二次型形式&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;二次型(形式)&lt;/h2&gt;
&lt;p&gt;對於 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}=\left( \begin{array}{c} x_{1}\\ x_{2}\\ \vdots\\ x_{n} \end{array} \right), A=\left( \begin{array}{c} a_{11}&amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n}\\ a_{21}&amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n}\\ \vdots&amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots\\ a_{n1}&amp;amp; a_{n2} &amp;amp; \cdots &amp;amp; a_{nn} \end{array} \right)\)&lt;/span&gt; 那麼：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}^tA\underline{x}=\sum\limits_{i=1}^n\sum\limits_{j=1}^na_{ij}x_ix_j\\ \;\;\;\;\;\;\;\;\:\:=\sum\limits_{i=1}^na_{ii}x_i^2+\mathop{\sum\limits^n\sum\limits^n}_{i \neq j}a_{ij}x_ix_j\\ \;\;\;\;\;\;\;\;\:\:=\sum\limits_{i=1}^na_{ii}x_i^2+\mathop{\sum\limits^n\sum\limits^n}_{i\ &amp;lt;\ j}(a_{ij}+a_{ji})x_ix_j\)&lt;/span&gt;&lt;br&gt;
被稱爲 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}\)&lt;/span&gt; 的同次2次式。又被叫做關於 &lt;span class=&#34;math inline&#34;&gt;\(x_1,x_2,\cdots,x_n\)&lt;/span&gt; 的&lt;strong&gt;2次型(quadratic form)&lt;/strong&gt;。特別的，當 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 爲對稱矩陣時的2次型：&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}^tA\underline{x}=\sum\limits_{i=1}^na_{ii}x_i^2+2\mathop{\sum\limits^n\sum\limits^n}_{i\ &amp;lt;\ j}a_{ij}x_ix_j\)&lt;/span&gt; 在多元變量分析中十分重要。&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(x=\left( \begin{array}{} x_1\\ x_2 \end{array} \right),\ A=\left( \begin{array}{} a_{11} &amp;amp; a_{12}\\ a_{12} &amp;amp; a_{22} \end{array} \right)\)&lt;/span&gt;, 那麼： &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}^tA\underline{x}=a_{11}x_1^2+a_{22}x_2^2+2a_{12}x_1x_2\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(x=\left( \begin{array}{} x_1\\ x_2\\ x_3 \end{array} \right),\ A=\left( \begin{array}{} a_{11} &amp;amp; a_{12} &amp;amp; a_{13}\\ a_{12} &amp;amp; a_{22} &amp;amp; a_{23}\\ a_{13} &amp;amp; a_{23} &amp;amp; a_{33} \end{array} \right)\)&lt;/span&gt;，那麼： &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}^tA\underline{x}=a_{11}x_1^2+a_{22}x_2^2+a_{33}x_3^2+2a_{12}x_1x_2\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+2a_{13}x_1x_3+2a_{23}x_2x_3\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(x=\left( \begin{array}{} x_1\\ x_2\\ x_3 \end{array} \right), \ A=\left( \begin{array}{} 3 &amp;amp; 2 &amp;amp; 4\\ 6 &amp;amp; 5 &amp;amp; 1\\ -2 &amp;amp; 5 &amp;amp; 8 \end{array} \right)非對稱矩陣\)&lt;/span&gt;，那麼：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}^tA\underline{x}=3x_1^2+5x_2^2+8x_3^2\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+2x_1x_2+4x_1x_3+6x_2x_1\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+x_2x_3-2x_3x_1+5x_3x_2\\ \;\;\;\;\;\;\;\;\;\:=3x_1^2+5x_2^2+8x_3^2\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+(2+6)x_1x_2+(4-2)x_1x_3\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+(5+1)x_2x_3\\ \;\;\;\;\;\;\;\;\;\:=3x_1^2+5x_2^2+8x_3^2\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+8x_1x_2+2x_1x_3+6z_2x_3\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;有式子 &lt;span class=&#34;math inline&#34;&gt;\(3x_1^2-5x_2^2+7x_3^2+8x_1x_2+4x_1x_3-12x_2x_3\)&lt;/span&gt; 如果要將它改寫成 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}^tA\underline{x}\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;是對稱矩陣) 的話，試求 &lt;span class=&#34;math inline&#34;&gt;\(A=\left( \begin{array}{} a_{11} &amp;amp; a_{12} &amp;amp; a_{13}\\ a_{22} &amp;amp; a_{22} &amp;amp; a_{23}\\ a_{33} &amp;amp; a_{32} &amp;amp; a_{33} \end{array} \right)=\left( \begin{array}{} a_{11} &amp;amp; a_{12} &amp;amp; a_{13}\\ a_{12} &amp;amp; a_{22} &amp;amp; a_{23}\\ a_{13} &amp;amp; a_{23} &amp;amp; a_{33} \end{array} \right)\)&lt;/span&gt; 的各個成分。&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 的對角成分：&lt;span class=&#34;math inline&#34;&gt;\(a_{11},a_{22},a_{33}\)&lt;/span&gt; 分別是 &lt;span class=&#34;math inline&#34;&gt;\(x_1^2,x_2^2,x_3^2\)&lt;/span&gt; 的系數： &lt;span class=&#34;math inline&#34;&gt;\(3,-5,7\)&lt;/span&gt;。非對角成分 &lt;span class=&#34;math inline&#34;&gt;\(a_{12}(=a_{21})\)&lt;/span&gt; 等於 &lt;span class=&#34;math inline&#34;&gt;\(x_1x_2\)&lt;/span&gt; 系數的一半：&lt;span class=&#34;math inline&#34;&gt;\(4\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(a_{13}(=a_{31})\)&lt;/span&gt; 等於 &lt;span class=&#34;math inline&#34;&gt;\(x_1x_3\)&lt;/span&gt; 系數的一半:&lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt;,
&lt;span class=&#34;math inline&#34;&gt;\(a_23(=a_{32})\)&lt;/span&gt; 等於 &lt;span class=&#34;math inline&#34;&gt;\(x_2x_3\)&lt;/span&gt; 系數的一半：&lt;span class=&#34;math inline&#34;&gt;\(-6\)&lt;/span&gt;。&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\therefore A=\left( \begin{array}{} 3 &amp;amp; 4 &amp;amp; 2\\ 4 &amp;amp; -5 &amp;amp; -6\\ 2 &amp;amp; -6 &amp;amp; 7 \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記15</title>
      <link>https://wangcc.me/post/2017-03-08/</link>
      <pubDate>Wed, 08 Mar 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/2017-03-08/</guid>
      <description>
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;單位矩陣&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;單位矩陣&lt;/h2&gt;
&lt;p&gt;對角成分全部都是 &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; (此時我們假定有 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 個)，的對角矩陣被叫做&lt;strong&gt;單位矩陣(identity matrix, unit matrix)&lt;/strong&gt;。寫作：
&lt;span class=&#34;math inline&#34;&gt;\(\left( \begin{array}{c} 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\ 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\ 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; \vdots \\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \ddots &amp;amp; 0 \\ 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 1 \end{array} \right)=E_n=I_n\)&lt;/span&gt; 下標 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 常被省略。一般的，將 &lt;span class=&#34;math inline&#34;&gt;\(E_n\)&lt;/span&gt; 從左往右乘以 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 次正方形矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;，的結果和從右往左相乘的結果是相等的： &lt;span class=&#34;math inline&#34;&gt;\(E_nA=AE_n=A\)&lt;/span&gt;。&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;單位矩陣 &lt;span class=&#34;math inline&#34;&gt;\(E=\left( \begin{array}{c} 1 &amp;amp; 0 &amp;amp; 0 \\ 0 &amp;amp; 1 &amp;amp; 0 \\ 0 &amp;amp; 0 &amp;amp; 1 \\ \end{array} \right)\)&lt;/span&gt; 和矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A=\left( \begin{array}{c} a_1 &amp;amp; a_2 &amp;amp; a_3 \\ b_1 &amp;amp; b_2 &amp;amp; b_3 \\ c_1 &amp;amp; c_2 &amp;amp; c_3 \\ \end{array} \right)\)&lt;/span&gt; 的積爲：&lt;span class=&#34;math inline&#34;&gt;\(EA=\left( \begin{array}{c} a_1 &amp;amp; a_2 &amp;amp; a_3 \\ b_1 &amp;amp; b_2 &amp;amp; b_3 \\ c_1 &amp;amp; c_2 &amp;amp; c_3 \\ \end{array} \right)=AE=A\)&lt;/span&gt;，矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 的&lt;strong&gt;所有成分均不變。&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(E_nE_n=E_n\)&lt;/span&gt;。像這樣，自己與自己相乘，結果等於自己的矩陣，被叫做&lt;strong&gt;冪等矩陣(idempotent matrix, 冪等行列「べきとうぎょうれつ」)&lt;/strong&gt;。即，&lt;span class=&#34;math inline&#34;&gt;\(HH(=H^2)=H\)&lt;/span&gt; 成立時，&lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; 是冪等矩陣。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}=E\underline{x}, \lambda\underline{x}=\lambda E\underline{x}\)&lt;/span&gt; 此等式會在後面&lt;strong&gt;特徵值(eigenvalue, 固有値問題)&lt;/strong&gt;時使用。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://winterwang.github.io/post/2017-03-01/&#34;&gt;前一個小節&lt;/a&gt;中的對角矩陣(diagonal matrix) &lt;span class=&#34;math inline&#34;&gt;\(D^{\frac{1}{2}}\)&lt;/span&gt; 則具有這樣的性質： &lt;span class=&#34;math inline&#34;&gt;\(D^{\frac{1}{2}}D^{-\frac{1}{2}}=D^{-\frac{1}{2}}D^{\frac{1}{2}}=E_n\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;逆矩陣-inverse-matrix&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;逆矩陣 inverse matrix&lt;/h2&gt;
&lt;p&gt;如果正方形矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 存在另一個正放心矩陣 &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 使得他們滿足 &lt;span class=&#34;math inline&#34;&gt;\(AX=XA=E\)&lt;/span&gt;，即乘積爲一個單位矩陣，那麼我們說 &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 是 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 的&lt;strong&gt;逆矩陣(inverse matrix)&lt;/strong&gt;，寫作：&lt;span class=&#34;math inline&#34;&gt;\(A^{-1}\)&lt;/span&gt;。可以將上面的連等式改成：&lt;span class=&#34;math inline&#34;&gt;\(AA^{-1}=A^{-1}A=E\)&lt;/span&gt;。&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;如果矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A=\left( \begin{array}{c} a &amp;amp; b \\ c &amp;amp; d \\ \end{array} \right)\)&lt;/span&gt; 的成分滿足： &lt;span class=&#34;math inline&#34;&gt;\(ad -bc \neq 0\)&lt;/span&gt;，那麼有 &lt;span class=&#34;math inline&#34;&gt;\(A^{-1}=\frac{1}{ad-bc}\left( \begin{array}{c} d &amp;amp; -b \\ -c &amp;amp; a \\ \end{array} \right)\)&lt;/span&gt;。&lt;strong&gt;如果， &lt;span class=&#34;math inline&#34;&gt;\(ad-bc=0\)&lt;/span&gt; 那麼我們認爲 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 的逆矩陣不存在。&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;矩陣 &lt;span class=&#34;math inline&#34;&gt;\(P=\left( \begin{array}{c} \cos \theta &amp;amp; -\sin \theta \\ \sin \theta &amp;amp; \cos \theta \\ \end{array} \right)\)&lt;/span&gt; 的逆矩陣 &lt;span class=&#34;math inline&#34;&gt;\(P^{-1}=\left( \begin{array}{c} \cos \theta &amp;amp; \sin \theta \\ -\sin \theta &amp;amp; \cos \theta \\ \end{array} \right)\)&lt;/span&gt; &lt;span class=&#34;diff_alert&#34;&gt;注意此處出現了逆矩陣的逆矩陣爲元矩陣的例子。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;對稱矩陣(symmetric matrix)&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(A=\left( \begin{array}{c} 1 &amp;amp; 2 &amp;amp; 3 \\ 2 &amp;amp; 4 &amp;amp; 5 \\ 3 &amp;amp; 5 &amp;amp; 6 \\ \end{array} \right)\)&lt;/span&gt; 的逆矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A^{-1}=\left( \begin{array}{c} 1 &amp;amp; -3 &amp;amp; 2 \\ -3 &amp;amp; 3 &amp;amp; -1 \\ 2 &amp;amp; -1 &amp;amp; 0 \\ \end{array} \right)\)&lt;/span&gt; &lt;span class=&#34;diff_alert&#34;&gt;注意此處出現了對稱矩陣的逆矩陣還是對稱矩陣的例子。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A=\left( \begin{array}{c} -11 &amp;amp; 2 &amp;amp; 2 \\ -4 &amp;amp; 0 &amp;amp; 1 \\ 6 &amp;amp; -1 &amp;amp; -1 \\ \end{array} \right)\)&lt;/span&gt; 的逆矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A^{-1}=\left( \begin{array}{c} 1 &amp;amp; 0 &amp;amp; 2 \\ 2 &amp;amp; -1 &amp;amp; 3 \\ 4 &amp;amp; 1 &amp;amp; 8 \\ \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;正交矩陣-orthogonal-matrix&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;正交矩陣 orthogonal matrix&lt;/h2&gt;
&lt;p&gt;如果正方形矩陣 &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; 滿足： &lt;span class=&#34;math inline&#34;&gt;\(PP^t=P^tP=E\)&lt;/span&gt; (單位矩陣)；或者滿足 &lt;span class=&#34;math inline&#34;&gt;\(P^t=P^{-1}\)&lt;/span&gt; 時，我們說這個正方形矩陣 &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; 爲&lt;strong&gt;正交矩陣(orthogonal matrix，直交行列「ちょっこうぎょうれつ」)&lt;/strong&gt;。正交矩陣如果用列向量來表示，那麼這些組成正交矩陣的列向量被稱爲&lt;strong&gt;規範正交系(orthonomal system，正規直交系)&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;矩陣 &lt;span class=&#34;math inline&#34;&gt;\(P=\left( \begin{array}{c} \cos \theta &amp;amp; -\sin \theta \\ \sin \theta &amp;amp; \cos \theta \\ \end{array} \right)\)&lt;/span&gt; 是2次的正交矩陣。如果 &lt;span class=&#34;math inline&#34;&gt;\(\underline{p}_1=\left( \begin{array}{c} \cos \theta \\ \sin \theta \\ \end{array} \right), \; \underline{p}_2=\left( \begin{array}{c} -\sin \theta \\ \cos \theta \\ \end{array} \right)\)&lt;/span&gt;，那麼列向量的長度有：&lt;span class=&#34;math inline&#34;&gt;\(\| \underline{p}_1 \|=\| \underline{p}_2 \|=1\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(\underline{p}_1\cdot\underline{p}_2=0\)&lt;/span&gt;。因此組成矩陣 &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; 的兩個列向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{p}_1,\underline{p}_2\)&lt;/span&gt; 構成了一個規範正交系。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;矩陣 &lt;span class=&#34;math inline&#34;&gt;\(P=\left( \begin{array}{c} \frac{1}{\sqrt{3}} &amp;amp; \frac{1}{\sqrt{2}} &amp;amp; \frac{1}{\sqrt{6}} \\ \frac{1}{\sqrt{3}} &amp;amp; -\frac{1}{\sqrt{2}} &amp;amp; \frac{1}{\sqrt{6}} \\ \frac{1}{\sqrt{3}} &amp;amp; 0 &amp;amp; -\frac{2}{\sqrt{6}} \\ \end{array} \right)\)&lt;/span&gt; 是個3次正交矩陣。如果 &lt;span class=&#34;math inline&#34;&gt;\(\underline{p}_1=\left( \begin{array}{c} \frac{1}{\sqrt{3}}\\ \frac{1}{\sqrt{3}}\\ \frac{1}{\sqrt{3}}\\ \end{array} \right), \underline{p}_2=\left( \begin{array}{c} \frac{1}{\sqrt{2}} \\ -\frac{1}{\sqrt{2}} \\ 0 \\ \end{array} \right), \underline{p}_3=\left( \begin{array}{c} \frac{1}{\sqrt{6}}\\ \frac{1}{\sqrt{6}}\\ -\frac{2}{\sqrt{6}}\\ \end{array} \right)\)&lt;/span&gt; 這三個列向量構成了一個規範正交系。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;三角矩陣-triangular-matrix&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;三角矩陣 triangular matrix&lt;/h2&gt;
&lt;p&gt;主對角線的左下部分全部爲 &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; 的正方形矩陣被叫做：&lt;strong&gt;上三角矩陣(upper triangular matrix)&lt;/strong&gt;，右上部分的成分全部爲 &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; 的正方形矩陣被叫做： &lt;strong&gt;下三角矩陣(lower triangular matrix)&lt;/strong&gt;。上三角矩陣，下三角矩陣，統稱爲&lt;strong&gt;三角矩陣&lt;/strong&gt;。有時候左下部分或者右上部分就簡略的只寫一個大的 &lt;span class=&#34;math inline&#34;&gt;\(O\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;類型相同的兩個上三角矩陣的積依然是一個上三角矩陣。兩個類型相同的下三角矩陣的積也依然是一個下三角矩陣。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
上三角矩陣： \left(
\begin{array}{c}
a_{11}&amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n}\\
0     &amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n}\\
\vdots&amp;amp; \ddots &amp;amp; \ddots &amp;amp; \vdots\\
0     &amp;amp; \cdots &amp;amp; 0      &amp;amp; a_{nn}
\end{array}
\right)
=\left(
\begin{array}{c}
a_{11}&amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n}\\
      &amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n}\\
      &amp;amp;        &amp;amp; \ddots &amp;amp; \vdots\\
\Huge{0}  &amp;amp;        &amp;amp;        &amp;amp; a_{nn}
\end{array}
\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
下三角矩陣：\left(
\begin{array}{c}
a_{11}&amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\
a_{21}&amp;amp; a_{22} &amp;amp; \ddots &amp;amp; \vdots \\
\vdots&amp;amp; \cdots &amp;amp; \ddots &amp;amp; 0\\
a_{n1}&amp;amp; a_{n2} &amp;amp; \cdots   &amp;amp; a_{nn}
\end{array}
\right)=\left(
\begin{array}{c}
a_{11}&amp;amp; &amp;amp;&amp;amp;\Huge{0}  \\
a_{21}&amp;amp; a_{22} &amp;amp;  \\
\vdots&amp;amp; \cdots &amp;amp; \ddots &amp;amp;\\
a_{n1}&amp;amp; a_{n2} &amp;amp; \cdots   &amp;amp; a_{nn}
\end{array}
\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\left( \begin{array}{c} 2 &amp;amp; 1 \\ 0 &amp;amp; 8 \\ \end{array} \right), \left( \begin{array}{c} -3 &amp;amp; 0 &amp;amp; 6 \\ 0 &amp;amp; 5 &amp;amp; 2 \\ 0 &amp;amp; 0 &amp;amp; 4 \\ \end{array} \right), \left( \begin{array}{c} 5 &amp;amp; -6 &amp;amp; 3 &amp;amp; 2 \\ 0 &amp;amp; 9 &amp;amp;-2 &amp;amp; 4 \\ 0 &amp;amp; 0 &amp;amp; 3 &amp;amp; 7 \\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 \end{array} \right)\)&lt;/span&gt; 這些都是上三角矩陣。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\left( \begin{array}{c} 2 &amp;amp; 0 \\ 5 &amp;amp; 8 \\ \end{array} \right), \left( \begin{array}{c} -3 &amp;amp; 0 &amp;amp; 0 \\ 8 &amp;amp; 5 &amp;amp; 0 \\ 7 &amp;amp; 2 &amp;amp; 4 \\ \end{array} \right), \left( \begin{array}{c} 5 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\ 2 &amp;amp; 9 &amp;amp; 0 &amp;amp; 0 \\ 3 &amp;amp; 10 &amp;amp; 3 &amp;amp; 0 \\ 5 &amp;amp; 1 &amp;amp; 34 &amp;amp; 0 \end{array} \right)\)&lt;/span&gt; 這些都是下三角矩陣。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;階梯形矩陣-echelon-matrix&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;階梯形矩陣 echelon matrix&lt;/h2&gt;
&lt;p&gt;如下所示，第1行，第2行，第3行，行數增加的同時，左側的成分中 &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; 的個數跟着增加的矩陣被叫做&lt;strong&gt;階梯形矩陣(echelon matrix)&lt;/strong&gt;。 &lt;span class=&#34;math inline&#34;&gt;\(\#\)&lt;/span&gt; 表示非 &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; 的數， &lt;span class=&#34;math inline&#34;&gt;\(*\)&lt;/span&gt; 表示任意數。&lt;span class=&#34;math inline&#34;&gt;\(\#\)&lt;/span&gt; 的個數，或者說此矩陣的非零向量的個數被定義爲這個矩陣的&lt;strong&gt;階數 (rank)&lt;/strong&gt;。階梯形矩陣的階數記爲： &lt;span class=&#34;math inline&#34;&gt;\(rank(A)\)&lt;/span&gt;。零矩陣 &lt;span class=&#34;math inline&#34;&gt;\(O\)&lt;/span&gt; 的階數： &lt;span class=&#34;math inline&#34;&gt;\(rank(O)=0\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\left(
\begin{array}{c}
\# &amp;amp; * &amp;amp; * &amp;amp; * &amp;amp; * &amp;amp; * &amp;amp; * &amp;amp; *  &amp;amp; *\\
0 &amp;amp; \# &amp;amp; * &amp;amp; * &amp;amp; * &amp;amp; * &amp;amp; * &amp;amp; *  &amp;amp; * \\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \# &amp;amp; * &amp;amp; * &amp;amp; * &amp;amp; *  &amp;amp; * \\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0  &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \# &amp;amp; *  \\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0  &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0  &amp;amp; 0
\end{array}
\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(A=\left( \begin{array}{c} 2 &amp;amp; 5 &amp;amp; 6 &amp;amp; 9\\ 0 &amp;amp; 5 &amp;amp; -1 &amp;amp; 4\\ 0 &amp;amp; 0 &amp;amp; 5 &amp;amp; 0\\ \end{array} \right)， rank(A)=3\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(B=\left( \begin{array}{c} 4 &amp;amp; 0 &amp;amp; 6 &amp;amp; 0\\ 0 &amp;amp; 5 &amp;amp; 0 &amp;amp; 4\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 5\\ \end{array} \right)， rank(B)=3\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(C=\left( \begin{array}{c} 2 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\ 0 &amp;amp; 0 &amp;amp; -7 &amp;amp; 4\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; -1\\ \end{array} \right)， rank(C)=3\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(D=\left( \begin{array}{c} 4 &amp;amp; 0 &amp;amp; 6 &amp;amp; 0\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 4\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\ \end{array} \right), rank(D)=2\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(F=\left( \begin{array}{c} 0 &amp;amp; 2 &amp;amp; 0 &amp;amp; 0\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\ \end{array} \right), rank(F)=1\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(O=\left( \begin{array}{c} 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\ \end{array} \right), rank(O)=0\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記14</title>
      <link>https://wangcc.me/post/2017-03-01/</link>
      <pubDate>Wed, 01 Mar 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/2017-03-01/</guid>
      <description>
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;updated: 2017-03-07&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;對稱矩陣&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;對稱矩陣&lt;/h2&gt;

&lt;div class=&#34;theorem&#34;&gt;
&lt;span id=&#34;thm:unnamed-chunk-1&#34; class=&#34;theorem&#34;&gt;&lt;strong&gt;Theorem 1  (symmetric matrix)  &lt;/strong&gt;&lt;/span&gt;矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 如果完全和它的轉置矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A^t\)&lt;/span&gt; 相同，即：&lt;span class=&#34;math inline&#34;&gt;\(A=A^t\)&lt;/span&gt; 成立時，這樣的正方形矩陣被稱爲&lt;strong&gt;對稱矩陣(symmetric matrix)&lt;/strong&gt;。對稱矩陣的成分是以主對角線(main diagonal)對稱的。
&lt;/div&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(A=\left( \begin{array}{c} 4 &amp;amp; 3 &amp;amp; 2 &amp;amp; 1 \\ 3 &amp;amp; 5 &amp;amp; 6 &amp;amp; 7 \\ 2 &amp;amp; 6 &amp;amp; 8 &amp;amp; 9 \\ 1 &amp;amp; 7 &amp;amp; 9 &amp;amp; 0 \end{array} \right)\)&lt;/span&gt; 是典型的4次對稱矩陣。&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;數學&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;物理&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;化學&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;數學&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(0.72\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(0.62\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;物理&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(0.72\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(-0.55\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;化學&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(0.62\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(-0.55\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr /&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;上表是幾名學生的數學，物理，化學成績得分的相關系數。&lt;br&gt;
如果提取出數字的部分，左右用圓括號括起來，會得到這樣一個矩陣：&lt;span class=&#34;math inline&#34;&gt;\(R=\left( \begin{array}{c} 1 &amp;amp; 0.72 &amp;amp; 0.62 \\ 0.72 &amp;amp; 1 &amp;amp; -0.55 \\ 0.62 &amp;amp; -0.55 &amp;amp; 1 \\ \end{array} \right)\)&lt;/span&gt; 這樣類型的矩陣被特別的稱爲&lt;strong&gt;相關矩陣(correlation matrix)&lt;/strong&gt;。類似相關矩陣這樣的明確爲對稱矩陣的情況下，常常像下面這樣簡略的記左下或者右上部分：
&lt;span class=&#34;math display&#34;&gt;\[\left(
\begin{array}{c}
1 &amp;amp;  &amp;amp;  \\
0.72 &amp;amp; 1 &amp;amp;  \\
0.62 &amp;amp; -0.55 &amp;amp; 1 \\
\end{array}
\right)， \left(
\begin{array}{c}
1 &amp;amp; 0.72 &amp;amp; 0.62 \\
 &amp;amp; 1 &amp;amp; -0.55 \\
 &amp;amp;  &amp;amp; 1 \\
\end{array}
\right)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;下面的對稱矩陣，對角成分是&lt;strong&gt;方差(variance, 分散)&lt;/strong&gt;，非對角成分是&lt;strong&gt;協方差(covariance, 共分散)&lt;/strong&gt;，被稱爲&lt;strong&gt;方差協方差矩陣(variance-covariance matrix, 分散共分散行列)&lt;/strong&gt;。&lt;br&gt;
&lt;span class=&#34;math display&#34;&gt;\[\sum=\left(
\begin{array}{c}
\sigma_{1}^2 &amp;amp; \sigma_{2}   &amp;amp; \cdots &amp;amp; \sigma_{1n} \\
\sigma_{12}  &amp;amp; \sigma_{2}^2 &amp;amp; \cdots &amp;amp; \sigma_{2n} \\
\vdots       &amp;amp; \vdots       &amp;amp; \ddots &amp;amp; \vdots      \\
\sigma_{1n}  &amp;amp; \sigma_{2}   &amp;amp; \cdots &amp;amp; \sigma_{n}^2
\end{array}
\right), S=\left(
\begin{array}{c}
s_{1}^2 &amp;amp; s_{2}   &amp;amp; \cdots &amp;amp; s_{1n} \\
s_{12}  &amp;amp; s_{2}^2 &amp;amp; \cdots &amp;amp; s_{2n} \\
\vdots  &amp;amp; \vdots  &amp;amp; \ddots &amp;amp; \vdots  \\
s_{1n}  &amp;amp; s_{2}   &amp;amp; \cdots &amp;amp; s_{n}^2
\end{array}
\right)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;矩陣&lt;span class=&#34;math inline&#34;&gt;\(X=\left( \begin{array}{c} x_{11} &amp;amp; x_{12} &amp;amp; x_{13} \\ x_{21} &amp;amp; x_{22} &amp;amp; x_{23} \\ \end{array} \right)\)&lt;/span&gt; ，那麼，&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(XX^t=\left( \begin{array}{c} x_{11} &amp;amp; x_{12} &amp;amp; x_{13} \\ x_{21} &amp;amp; x_{22} &amp;amp; x_{23} \\ \end{array} \right)\left( \begin{array}{c} x_{11} &amp;amp; x_{12} \\ x_{12} &amp;amp; x_{22} \\ x_{13} &amp;amp; x_{13} \end{array} \right)\\ \;\;\;\;\;\;\;=\left( \begin{array}{c} x_{11}^2+x_{12}^2+x_{13}^2 &amp;amp; x_{11}x_{21}+x_{12}x_{22}+x_{13}x_{23} \\ x_{21}x_{11}+x_{22}x_{12}+x_{23}x_{13} &amp;amp; x_{21}^2+x_{22}^2+x_{23}^2 \\ \end{array} \right)\)&lt;/span&gt; 是一個對稱矩陣。&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(X^tX=\left( \begin{array}{c} x_{11}^2+x_{21}^2 &amp;amp; x_{11}x_{12}+x_{21}x_{22} &amp;amp; x_{11}x_{13}+x_{21}x_{23} \\ x_{12}x_{11}+x_{22}x_{21} &amp;amp; x_{12}^2+x_{22}^2 &amp;amp; x_{12}x_{13}+x_{22}x_{23} \\ x_{13}x_{11}+x_{23}x_{21} &amp;amp; x_{13}x_{12}+x_{23}x_{22} &amp;amp; x_{13}^2+x_{23}^2 \end{array} \right)\)&lt;/span&gt; 也是一個對稱矩陣。 &lt;br&gt;
且，他們的&lt;strong&gt;跡(trace)&lt;/strong&gt;也是一樣的，均爲 &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; 各個成分的平方和：
&lt;span class=&#34;math display&#34;&gt;\[tr(XX^t)=tr(X^tX)=x_{11}^2+x_{12}^2+x_{13}^2+x_{21}^2+x_{22}^2+x_{23}^2\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;對角矩陣&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;對角矩陣&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;非對角成分(off-diagonal element)&lt;/strong&gt;均爲零 &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; 的正方形矩陣被稱爲&lt;strong&gt;對角矩陣(diagonal matrix)&lt;/strong&gt;。寫成如下形式：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\left(
\begin{array}{c}
a_{11} &amp;amp; 0  &amp;amp; 0 &amp;amp; \cdots  &amp;amp; 0\\
0 &amp;amp; a_{22}  &amp;amp; 0 &amp;amp; \cdots  &amp;amp; 0\\
0 &amp;amp; 0  &amp;amp; a_{33} &amp;amp; \cdots  &amp;amp; 0\\
0 &amp;amp; \cdots  &amp;amp; 0 &amp;amp; \ddots  &amp;amp; 0\\
0 &amp;amp; 0  &amp;amp; \cdots &amp;amp; 0       &amp;amp;  a_{nn}
\end{array}
\right)=D_n=\Delta_n\\=diag(a_{11},a_{22},a_{33},\cdots,a_{nn})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;這樣的矩陣也常把左下部分右上部分的非對角成分用一個大的 &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; 來表示：
&lt;span class=&#34;math display&#34;&gt;\[
\left(
    \begin{array}{ccccc}
    a_{11}                         \\
      &amp;amp; a_{22}            &amp;amp;   &amp;amp; \Huge0 \\
      &amp;amp;               &amp;amp; a_{33}          \\
      &amp;amp; \Huge 0       &amp;amp;   &amp;amp; \ddots           \\
      &amp;amp;               &amp;amp;   &amp;amp;      &amp;amp; a_{nn}
    \end{array}
    \right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;下面也是一個對角矩陣的例子：
&lt;span class=&#34;math display&#34;&gt;\[\left(
\begin{array}{c}
\sqrt{a_{11}} &amp;amp; 0  &amp;amp; 0 &amp;amp; \cdots  &amp;amp; 0\\
0 &amp;amp; \sqrt{a_{22}}  &amp;amp; 0 &amp;amp; \cdots  &amp;amp; 0\\
0 &amp;amp; 0  &amp;amp; \sqrt{a_{33}} &amp;amp; \cdots  &amp;amp; 0\\
0 &amp;amp; \cdots  &amp;amp; 0 &amp;amp; \ddots  &amp;amp; 0\\
0 &amp;amp; 0  &amp;amp; \cdots &amp;amp; 0       &amp;amp;  \sqrt{a_{nn}}
\end{array}
\right)=D_n^{\frac{1}{2}}=\Delta_n^{\frac{1}{2}}\\=diag(\sqrt{a_{11}},\sqrt{a_{22}},\sqrt{a_{33}},\cdots,\sqrt{a_{nn}})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;對角成分也可以是分母非零的分數：
&lt;span class=&#34;math display&#34;&gt;\[\left(
\begin{array}{c}
1/a_{11} &amp;amp; 0  &amp;amp; 0 &amp;amp; \cdots  &amp;amp; 0\\
0 &amp;amp; 1/a_{22}  &amp;amp; 0 &amp;amp; \cdots  &amp;amp; 0\\
0 &amp;amp; 0  &amp;amp; 1/a_{33} &amp;amp; \cdots  &amp;amp; 0\\
0 &amp;amp; \cdots  &amp;amp; 0 &amp;amp; \ddots  &amp;amp; 0\\
0 &amp;amp; 0  &amp;amp; \cdots &amp;amp; 0       &amp;amp;  1/a_{nn}
\end{array}
\right)=D_n^{-1}=\Delta_n^{-1}\\=diag(a_{11}^{-1},a_{22}^{-1},a_{33}^{-1},\cdots,a_{nn}^{-1})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;當然如下的例子也是對角矩陣，默認根號內爲正：
&lt;span class=&#34;math display&#34;&gt;\[\left(
\begin{array}{c}
\frac{1}{\sqrt{a_{11}}} &amp;amp; 0  &amp;amp; 0 &amp;amp; \cdots  &amp;amp; 0\\
0 &amp;amp; \frac{1}{\sqrt{a_{22}}}  &amp;amp; 0 &amp;amp; \cdots  &amp;amp; 0\\
0 &amp;amp; 0  &amp;amp; \frac{1}{\sqrt{a_{33}}} &amp;amp; \cdots  &amp;amp; 0\\
0 &amp;amp; \cdots  &amp;amp; 0 &amp;amp; \ddots  &amp;amp; 0\\
0 &amp;amp; 0  &amp;amp; \cdots &amp;amp; 0       &amp;amp;  \frac{1}{\sqrt{a_{nn}}}
\end{array}
\right)=D_n^{-\frac{1}{2}}=\Delta_n^{-\frac{1}{2}}\\=diag(\frac{1}{\sqrt{a_{11}}},\frac{1}{\sqrt{a_{22}}},\frac{1}{\sqrt{a_{33}}},\cdots,\frac{1}{\sqrt{a_{nn}}})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;當然，上述對角矩陣之間具有這樣的關系：&lt;span class=&#34;math inline&#34;&gt;\(D_n^{\frac{1}{2}}D_n^{\frac{1}{2}}=D_n\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(D_n^{-\frac{1}{2}}D_n^{-\frac{1}{2}}=D_n^{-1}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 或者向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}\)&lt;/span&gt; 與對角矩陣 &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; 從左向右乘時，&lt;span class=&#34;math inline&#34;&gt;\(DA, D\underline{x}\)&lt;/span&gt; 的第 &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; 行成分是：&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 或 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}\)&lt;/span&gt; 的第 &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; 行乘以 &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; 的第 &lt;span class=&#34;math inline&#34;&gt;\((i,i)\)&lt;/span&gt; 成分。例如：
&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(D=\left( \begin{array}{c} \lambda_1 &amp;amp; 0 &amp;amp; 0\\ 0 &amp;amp; \lambda_2 &amp;amp; 0\\ 0 &amp;amp; 0 &amp;amp; \lambda_3\\ \end{array} \right), A=\left( \begin{array}{c} a_1 &amp;amp; a_2 &amp;amp; a_3 \\ b_1 &amp;amp; b_2 &amp;amp; b_3 \\ c_1 &amp;amp; c_2 &amp;amp; c_3 \\ \end{array} \right),\\ \underline{x}=\left( \begin{array}{c} x_1\\ x_2\\ x_3\\ \end{array} \right)\)&lt;/span&gt; 時，&lt;span class=&#34;math inline&#34;&gt;\(DA=\left( \begin{array}{c} \lambda_1a_1 &amp;amp; \lambda_1a_2 &amp;amp; \lambda_1a_3 \\ \lambda_2b_1 &amp;amp; \lambda_2b_2 &amp;amp; \lambda_2b_3 \\ \lambda_3c_1 &amp;amp; \lambda_3c_2 &amp;amp; \lambda_3c_3 \\ \end{array} \right) \\ D\underline{x}=\left( \begin{array}{c} \lambda_1x_1\\ \lambda_2x_2\\ \lambda_3x_3\\ \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記13</title>
      <link>https://wangcc.me/post/2017-02-28/</link>
      <pubDate>Tue, 28 Feb 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/2017-02-28/</guid>
      <description>
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;連立一次方程式與矩陣向量的積&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;連立一次方程式與矩陣向量的積&lt;/h2&gt;
&lt;p&gt;連立一次方程式可以改寫爲&lt;strong&gt;矩陣與向量的積形成的向量&lt;/strong&gt;的形式。特別的，以連立方程式的系數作成分的矩陣被叫做&lt;strong&gt;系數矩陣(coefficient matrix)&lt;/strong&gt;。當我們看到連立方程式，應該能立刻條件反射地聯想到其對應的&lt;strong&gt;矩陣和向量的積&lt;/strong&gt;。&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\begin{align} \left\{ \begin{array}{rr}  a_1+2a_2+3a_3 = 3\\ 2a_1+4a_2+5a_3 = 5\\ 3a_1+5a_2+6a_3 = 7 \end{array} \right. \end{align}\)&lt;/span&gt; 可以改寫成 &lt;span class=&#34;math inline&#34;&gt;\(\left( \begin{array}{c} 1 &amp;amp; 2 &amp;amp; 3 \\ 2 &amp;amp; 4 &amp;amp; 5 \\ 3 &amp;amp; 5 &amp;amp; 6 \end{array} \right)\left( \begin{array}{c} a_1\\ a_2\\ a_3 \end{array} \right)=\left( \begin{array}{c} 3\\ 5\\ 7 \end{array} \right)\)&lt;/span&gt; 的形式。&lt;br&gt;
如果把等號右邊的列向量寫到&lt;strong&gt;系數矩陣&lt;/strong&gt;的右側，形成的矩陣被叫做&lt;strong&gt;擴大系數矩陣(augmented coefficient)&lt;/strong&gt;：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\left( \begin{array}{c} 1 &amp;amp; 2 &amp;amp; 3 &amp;amp; 3 \\ 2 &amp;amp; 4 &amp;amp; 5 &amp;amp; 5 \\ 3 &amp;amp; 5 &amp;amp; 6 &amp;amp; 7 \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\begin{align} \left\{ \begin{array}{rr} a_{11}x_1+a_{12}x_2+a_{13}x_3=0\\ a_{21}x_1+a_{22}x_2+a_{23}x_3=0\\ \end{array} \right. \end{align}\)&lt;/span&gt; 可以改寫成 &lt;span class=&#34;math inline&#34;&gt;\(\left( \begin{array}{c} a_{11} &amp;amp; a_{12} &amp;amp; a_{13} \\ a_{21} &amp;amp; a_{22} &amp;amp; a_{23} \\ \end{array} \right)\left( \begin{array}{c} x_1\\ x_2\\ x_3 \end{array} \right)=\left( \begin{array}{c} 0\\ 0\\ \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\begin{align} \left\{ \begin{array}{rr} (5-\lambda)x_1+ x_2+ x_3 = 0\\  x_1+(3-\lambda)x_2+ x_3 = 0\\  x_1+ x_2+(3-\lambda)x_3 = 0 \end{array} \right. \end{align}\)&lt;/span&gt; 可以改寫爲 &lt;span class=&#34;math inline&#34;&gt;\(\left( \begin{array}{c} 5-\lambda &amp;amp; 1 &amp;amp; 1 \\ 1 &amp;amp; 3-\lambda &amp;amp; 1 \\ 1 &amp;amp; 1 &amp;amp; 3-\lambda \end{array} \right)\left( \begin{array}{c} x_1\\ x_2\\ x_3 \end{array} \right)=\left( \begin{array}{c} 0\\ 0\\ 0 \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;矩形矩陣&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;矩形矩陣&lt;/h2&gt;
&lt;p&gt;列數行數不相等的矩陣，被稱爲&lt;strong&gt;矩形矩陣(rectangular matrix)&lt;/strong&gt;。特別的行數 &lt;span class=&#34;math inline&#34;&gt;\(&amp;gt;\)&lt;/span&gt; 列數的矩陣被叫做&lt;strong&gt;垂直型矩形矩陣&lt;/strong&gt;。行數 &lt;span class=&#34;math inline&#34;&gt;\(&amp;lt;\)&lt;/span&gt; 列數的矩陣被叫做&lt;strong&gt;水平型矩形矩陣&lt;/strong&gt;。多元變量分析時，數據常常被加工稱爲垂直型矩形矩陣的形式。&lt;/p&gt;
&lt;hr /&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;個体&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;体重 &lt;span class=&#34;math inline&#34;&gt;\((kg)\)&lt;/span&gt;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;身長 &lt;span class=&#34;math inline&#34;&gt;\((cm)\)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;安倍さん&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(53\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(157\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;伊藤さん&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(67\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(172\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;植村さん&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(49\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(163\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;江川さん&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(80\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(178\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;小野さん&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(74\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(181\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr /&gt;
&lt;ul&gt;
&lt;li&gt;5人的體重和身高數據被表示爲上面的表格： &lt;br&gt;
如果只提取出表格中的數字寫成垂直型矩形矩陣： &lt;span class=&#34;math inline&#34;&gt;\(\left(  \begin{array}{c} 53 &amp;amp; 157 \\ 67 &amp;amp; 172 \\ 49 &amp;amp; 163 \\ 80 &amp;amp; 178 \\ 74 &amp;amp; 181 \\  \end{array}  \right)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;正方形矩陣&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;正方形矩陣&lt;/h2&gt;
&lt;p&gt;行數和列數相等的矩陣被稱爲&lt;strong&gt;正方形矩陣(sqare matrix)&lt;/strong&gt;。一個正方形的矩陣如果類型爲 &lt;span class=&#34;math inline&#34;&gt;\((n,n)\)&lt;/span&gt;，又被叫做是 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 次正方矩陣或者 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 次矩陣。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[A_{n\times n}= \left(
\begin{array}{c}
a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n} \\
a_{21} &amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n} \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
a_{n1} &amp;amp; a_{n2} &amp;amp; \cdots &amp;amp; a_{nn}
\end{array}
\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;從左上角往右下角方向劃一條對角線，這條對角線的名稱爲&lt;strong&gt;主對角線(main diagonal)&lt;/strong&gt;。主對角線上有的成分 &lt;span class=&#34;math inline&#34;&gt;\(a_{11},a_{22},\cdots, a_{nn}\)&lt;/span&gt;，被叫做&lt;strong&gt;對角成分(diagonal element)&lt;/strong&gt;。其餘的成分被叫做&lt;strong&gt;非對角成分(off-diagonal element)&lt;/strong&gt;。對角成分的和被叫做是該矩陣的跡(trace/spur)，寫作 &lt;span class=&#34;math inline&#34;&gt;\(tr(A)=\sum\limits_{i=1}^na_{ii}\)&lt;/span&gt;
。
&lt;span class=&#34;math inline&#34;&gt;\(A=\left( \begin{array}{c} 1 &amp;amp; 2 &amp;amp; 3 \\ 4 &amp;amp; 5 &amp;amp; 6 \\ 7 &amp;amp; 8 &amp;amp; 9 \end{array} \right)\)&lt;/span&gt; 是 &lt;span class=&#34;math inline&#34;&gt;\(3\)&lt;/span&gt; 次正方形矩陣， &lt;span class=&#34;math inline&#34;&gt;\(tr(A)=1+5+9=15\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;矩陣轉置&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;矩陣轉置&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\((m,n)\)&lt;/span&gt; 型矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A_{m\times n}=(a_{ij})\)&lt;/span&gt; 的行與列互相對調，被叫做&lt;strong&gt;轉置(transpose)&lt;/strong&gt;，形成的新 &lt;span class=&#34;math inline&#34;&gt;\((n,m)\)&lt;/span&gt; 型矩陣，被叫做 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 的&lt;strong&gt;轉置矩陣 (transposed matrix)&lt;/strong&gt; ： &lt;span class=&#34;math inline&#34;&gt;\((a_{ji})\)&lt;/span&gt; 有多種標記方式：&lt;span class=&#34;math inline&#34;&gt;\(A^t, A^\prime, A^T, ^TA\)&lt;/span&gt; 等，我們今後統一使用 &lt;span class=&#34;math inline&#34;&gt;\(A^t\)&lt;/span&gt;。轉置矩陣具有如下的性質：&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((A^t)^t=A\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((AB)^t=B^tA^t\)&lt;/span&gt; &lt;span class=&#34;diff_alert&#34;&gt;注意： 不是&lt;span class=&#34;math inline&#34;&gt;\(A^tB^t\)&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((A+B)^t=A^t+B^t\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((kA)^t=kA^t\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;爲標量 scalar)&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;練習&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;練習&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(A=\left( \begin{array}{c} 1 &amp;amp; 2 &amp;amp; 3 \\ 4 &amp;amp; 5 &amp;amp; 6 \\ 7 &amp;amp; 8 &amp;amp; 9 \end{array} \right)\)&lt;/span&gt; 的轉置矩陣爲：&lt;span class=&#34;math inline&#34;&gt;\(A^t=\left( \begin{array}{c} 1 &amp;amp; 4 &amp;amp; 7 \\ 2 &amp;amp; 5 &amp;amp; 8 \\ 3 &amp;amp; 6 &amp;amp; 9 \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(B=\left( \begin{array}{c} 1 &amp;amp; 2 &amp;amp; 3 &amp;amp; 4\\ 5 &amp;amp; 6 &amp;amp; 7 &amp;amp; 8\\ \end{array} \right)\)&lt;/span&gt; 的轉置矩陣爲：&lt;span class=&#34;math inline&#34;&gt;\(B^t=\left( \begin{array}{c} 1 &amp;amp; 5 \\ 2 &amp;amp; 6 \\ 3 &amp;amp; 7 \\ 4 &amp;amp; 8 \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記12</title>
      <link>https://wangcc.me/post/2017-02-22/</link>
      <pubDate>Wed, 22 Feb 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/2017-02-22/</guid>
      <description>
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;矩陣乘法運算&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;矩陣乘法運算&lt;/h2&gt;
&lt;div id=&#34;矩陣乘法定義&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;矩陣乘法定義&lt;/h4&gt;

&lt;div class=&#34;theorem&#34;&gt;
&lt;span id=&#34;thm:unnamed-chunk-1&#34; class=&#34;theorem&#34;&gt;&lt;strong&gt;Theorem 1  (matrix multiplication)  &lt;/strong&gt;&lt;/span&gt;兩個矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A, B\)&lt;/span&gt; ，只有 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 的列數和 &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; 的行數相等(這種特徵又被稱爲：矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A,B\)&lt;/span&gt; &lt;strong&gt;可整合的&lt;/strong&gt;，conformable)時，才有定義：&lt;span class=&#34;math inline&#34;&gt;\(AB\)&lt;/span&gt;。&lt;span class=&#34;math inline&#34;&gt;\(AB\)&lt;/span&gt; 則爲新的矩陣，類型爲 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 的行數， &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;的列數。即：&lt;span class=&#34;math inline&#34;&gt;\(A_{k\times l}, \; B_{m\times n}\)&lt;/span&gt; 且 &lt;span class=&#34;math inline&#34;&gt;\(l=m\)&lt;/span&gt; 時才能計算乘積: &lt;span class=&#34;math inline&#34;&gt;\(AB_{k\times n}\)&lt;/span&gt;。
&lt;/div&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(A_{2\times3}=\left( \begin{array}{c} 4 &amp;amp; 6 &amp;amp; 8\\ 2 &amp;amp; 1 &amp;amp; 3\\ \end{array} \right),\; B_{3\times2}=\left( \begin{array}{c} 0 &amp;amp; 8\\ 2 &amp;amp; -1\\ 9 &amp;amp; 4 \\ \end{array} \right)\)&lt;/span&gt; 時，&lt;br&gt;
“&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;的列數” &lt;span class=&#34;math inline&#34;&gt;\(=\)&lt;/span&gt; “&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; 的行數” &lt;span class=&#34;math inline&#34;&gt;\(= 3\)&lt;/span&gt;，因此積 &lt;span class=&#34;math inline&#34;&gt;\(AB\)&lt;/span&gt; 被定義，類型是 &lt;span class=&#34;math inline&#34;&gt;\((2,2)\)&lt;/span&gt; &lt;br&gt;
“&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;的列數” &lt;span class=&#34;math inline&#34;&gt;\(=\)&lt;/span&gt; “&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 的行數” &lt;span class=&#34;math inline&#34;&gt;\(= 2\)&lt;/span&gt;，因此積 &lt;span class=&#34;math inline&#34;&gt;\(BA\)&lt;/span&gt; 被定義，類型是 &lt;span class=&#34;math inline&#34;&gt;\((3,3)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(C_{3\times2}=\left( \begin{array}{c} 4 &amp;amp; 2 \\ 5 &amp;amp; 6 \\ 7 &amp;amp; 3 \end{array} \right),\; D_{2\times4}=\left( \begin{array}{c} 2 &amp;amp; 0 &amp;amp; 9 &amp;amp; -1 \\ 4 &amp;amp; 7 &amp;amp; 6 &amp;amp; 5 \\ \end{array} \right)\)&lt;/span&gt; 時， &lt;br&gt;
“&lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;的列數” &lt;span class=&#34;math inline&#34;&gt;\(=\)&lt;/span&gt; “&lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; 的行數” &lt;span class=&#34;math inline&#34;&gt;\(= 2\)&lt;/span&gt;，因此積 &lt;span class=&#34;math inline&#34;&gt;\(CD\)&lt;/span&gt; 被定義，類型是 &lt;span class=&#34;math inline&#34;&gt;\((3,4)\)&lt;/span&gt; &lt;br&gt;
“&lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt;的列數”&lt;span class=&#34;math inline&#34;&gt;\(= 4\)&lt;/span&gt;，“&lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; 的行數” &lt;span class=&#34;math inline&#34;&gt;\(= 3\)&lt;/span&gt;，因此積 &lt;span class=&#34;math inline&#34;&gt;\(DC\)&lt;/span&gt; 不被定義，&lt;span class=&#34;math inline&#34;&gt;\(DC\)&lt;/span&gt; 不可整合。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;矩陣的積的向量表達形式&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;矩陣的積的向量表達形式&lt;/h4&gt;
&lt;p&gt;矩陣也可以被看做是一個個相同類型（大小，方向）的向量組成。那麼當，下面&lt;span class=&#34;math inline&#34;&gt;\(A, B\)&lt;/span&gt;兩個矩陣滿足：&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 的行向量的維度，和&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;的列向量的維度相等時，&lt;span class=&#34;math inline&#34;&gt;\(AB\)&lt;/span&gt;被定義。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(A=\left( \begin{array}{c} \underline{a}_1^t\\ \underline{a}_2^t\\ \vdots\\ \underline{a}_k^t \end{array} \right), \; B=(\underline{b}_1,\underline{b}_2,\cdots,\underline{b}_n)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(AB=A(\underline{b}_1,\underline{b}_2,\cdots,\underline{b}_n)=(A\underline{b}_1,A\underline{b}_2,\cdots,A\underline{b}_n)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;或者：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(AB=\left( \begin{array}{c} \underline{a}_1^t\\ \underline{a}_2^t\\ \vdots\\ \underline{a}_k^t \end{array} \right)B=\left( \begin{array}{c} \underline{a}_1^tB\\ \underline{a}_2^tB\\ \vdots\\ \underline{a}_k^tB \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;特殊情況當&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;僅有一個行向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}^t\)&lt;/span&gt; 時，&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(AB=\underline{a}^tB=\underline{a}^t(\underline{b}_1,\underline{b}_2,\cdots,\underline{b}_n)=(\underline{a}^t\underline{b}_1,\underline{a}^t\underline{b}_2,\cdots,\underline{a}^t\underline{b}_n)\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;矩陣的積的成分&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;矩陣的積的成分&lt;/h4&gt;

&lt;div class=&#34;theorem&#34;&gt;
&lt;span id=&#34;thm:unnamed-chunk-2&#34; class=&#34;theorem&#34;&gt;&lt;strong&gt;Theorem 2  (matrix multiplication component)  &lt;/strong&gt;&lt;/span&gt;兩個矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A_{k\times l}, \; B_{m\times n}\)&lt;/span&gt; 的積有被定義時，矩陣 &lt;span class=&#34;math inline&#34;&gt;\(AB_{k\times n}\)&lt;/span&gt; 的任意成分&lt;span class=&#34;math inline&#34;&gt;\((i,j)\)&lt;/span&gt;，被定義爲：&lt;strong&gt;“&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 的第 &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; 行向量與 &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;的第 &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; 列向量的內積”&lt;/strong&gt;。
&lt;/div&gt;
&lt;p&gt;故：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(AB\)&lt;/span&gt; 的 &lt;span class=&#34;math inline&#34;&gt;\((1,1)\)&lt;/span&gt; 成分是，“&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 的第 &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; 行向量與 &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;的第 &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; 列向量的內積”&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(AB\)&lt;/span&gt; 的 &lt;span class=&#34;math inline&#34;&gt;\((1,2)\)&lt;/span&gt; 成分是，“&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 的第 &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; 行向量與 &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;的第 &lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt; 列向量的內積”&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\vdots\)&lt;/span&gt; &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(AB\)&lt;/span&gt; 的 &lt;span class=&#34;math inline&#34;&gt;\((k,n)\)&lt;/span&gt; 成分是，“&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 的第 &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; 行向量與 &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;的第 &lt;span class=&#34;math inline&#34;&gt;\(ns\)&lt;/span&gt; 列向量的內積”&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;練習&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;練習&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(A=\left( \begin{array}{c} 1 &amp;amp; 2 \\ 3 &amp;amp; 4 \\ \end{array} \right), B=\left( \begin{array}{c} 4 &amp;amp; 1 \\ 5 &amp;amp; 2 \\ \end{array} \right)\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(AB=\left( \begin{array}{c} 1\times4+2\times5 &amp;amp; 1\times1+2\times2 \\ 3\times4+4\times5 &amp;amp; 3\times1+4\times2 \\ \end{array} \right)\\ \;\;\;\;\;=\left( \begin{array}{c} 14 &amp;amp; 5 \\ 32 &amp;amp; 11 \\ \end{array} \right)\)&lt;/span&gt; &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(BA=\left( \begin{array}{c} 4\times1+1\times3 &amp;amp; 4\times2+1\times4 \\ 5\times1+2\times3 &amp;amp; 5\times2+2\times4 \\ \end{array} \right)\\ \;\;\;\;\;=\left( \begin{array}{c} 7 &amp;amp; 12 \\ 11 &amp;amp; 18 \\ \end{array} \right)\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;diff_alert&#34;&gt;注意： &lt;span class=&#34;math inline&#34;&gt;\(AB\neq BA\)&lt;/span&gt;&lt;/span&gt;
&lt;br&gt;
另外：&lt;span class=&#34;math inline&#34;&gt;\(AA=\left( \begin{array}{c} 1\times1+2\times3 &amp;amp; 1\times2+2\times4 \\ 3\times1+4\times3 &amp;amp; 3\times2+4\times4 \\ \end{array} \right)\\ \;\;\;\;=\left( \begin{array}{c} 7 &amp;amp; 10 \\ 15 &amp;amp; 22 \\ \end{array} \right)=AA^2\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(A=\left( \begin{array}{c} 1 &amp;amp; 2 \\ -2 &amp;amp; -4 \\ \end{array} \right), B=\left( \begin{array}{c} 6 &amp;amp; -2 \\ -3 &amp;amp; 1 \\ \end{array} \right)\)&lt;/span&gt; &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(AB=\left( \begin{array}{c} 1\times6+2\times(-3) &amp;amp; 1\times(-2)+2\times1 \\ (-2)\times6+(-4)\times(-3) &amp;amp; (-2)\times(-2)+(-4)\times1 \\ \end{array} \right)\\ \;\;\;\;=\left( \begin{array}{c} 0 &amp;amp; 0 \\ 0 &amp;amp; 0 \\ \end{array} \right)=\Large 0\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;diff_alert&#34;&gt;這裏出現了非零矩陣相乘爲&lt;strong&gt;零矩陣 &lt;span class=&#34;math inline&#34;&gt;\(\large 0\)&lt;/span&gt;&lt;/strong&gt;的例子。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(A_{2\times3}=\left( \begin{array}{c} a_{11} &amp;amp; a_{12} &amp;amp; a_{13}\\ a_{21} &amp;amp; a_{22} &amp;amp; a_{23}\\ \end{array} \right)=\left( \begin{array}{c} \underline{a}_1^t\\ \underline{a}_2^t\\ \end{array} \right)=(\underline{c}_1,\underline{c}_2,\underline{c}_3); \\ B_{3\times2}=\left( \begin{array}{c} b_{11} &amp;amp; b_{12}\\ b_{21} &amp;amp; b_{22}\\ b_{31} &amp;amp; b_{32}\\ \end{array} \right)=(\underline{b}_1,\underline{b}_2)=\left( \begin{array}{c} \underline{d}^t_1\\ \underline{d}^t_2\\ \underline{d}^t_3\\ \end{array} \right)\)&lt;/span&gt; &lt;br&gt;
&lt;span class=&#34;diff_alert&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(AB_{2\times2}\)&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;math inline&#34;&gt;\(=\left( \begin{array}{c} \sum\limits_{k=1}^3a_{1k}b_{k1} &amp;amp;\sum\limits_{k=1}^3a_{1k}b_{k2} \\ \sum\limits_{k=1}^3a_{2k}b_{k1} &amp;amp;\sum\limits_{k=1}^3a_{2k}b_{k2} \\ \end{array} \right)\\=\left( \begin{array}{c} \underline{a}^t_1\underline{b}_1 &amp;amp; \underline{a}^t_1\underline{b}_2 \\ \underline{a}^t_2\underline{b}_1 &amp;amp; \underline{a}^t_2\underline{b}_2 \\ \end{array} \right)\)&lt;/span&gt; &lt;br&gt;
&lt;span class=&#34;diff_alert&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(BA_{3\times3}\)&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;math inline&#34;&gt;\(=\left( \begin{array}{c} \sum\limits_{k=1}^2b_{1k}a_{k1} &amp;amp; \sum\limits_{k=1}^2b_{1k}a_{k2} &amp;amp; \sum\limits_{k=1}^2b_{1k}a_{k3} \\ \sum\limits_{k=1}^2b_{2k}a_{k1} &amp;amp; \sum\limits_{k=1}^2b_{2k}a_{k2} &amp;amp; \sum\limits_{k=1}^2b_{2k}a_{k3} \\ \sum\limits_{k=1}^2b_{3k}a_{k1} &amp;amp; \sum\limits_{k=1}^2b_{3k}a_{k2} &amp;amp; \sum\limits_{k=1}^2b_{3k}a_{k3} \end{array} \right)\\ =\left( \begin{array}{c} \underline{d}^t_1\underline{c}_1 &amp;amp; \underline{d}^t_1\underline{c}_2 &amp;amp; \underline{d}^t_1\underline{c}_3 \\ \underline{d}^t_2\underline{c}_1 &amp;amp; \underline{d}^t_2\underline{c}_2 &amp;amp; \underline{d}^t_2\underline{c}_3 \\ \underline{d}^t_3\underline{c}_1 &amp;amp; \underline{d}^t_3\underline{c}_2 &amp;amp; \underline{d}^t_3\underline{c}_3 \\ \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}=\left( \begin{array}{c} x_1\\ x_2\\ \vdots\\ x_n\\ \end{array} \right)\)&lt;/span&gt; 時， &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}\underline{x}^t=\left( \begin{array}{c} x_1^2 &amp;amp; x_1x_2 &amp;amp; \cdots &amp;amp; x_1x_n \\ x_2x_1 &amp;amp; x_2^2 &amp;amp; \cdots &amp;amp; x_2x_n \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ x_nx_1 &amp;amp; x_nx_2 &amp;amp; \cdots &amp;amp; x_n^2 \\ \end{array} \right)\)&lt;/span&gt;，&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}^t\underline{x}=\sum\limits_{i=1}^nx_i^2\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=\left( \begin{array}{c} a_1\\ a_2\\ a_3\\ \end{array} \right), \underline{b}=\left( \begin{array}{c} b_1\\ b_2\\ b_3\\ \end{array} \right), \underline{c}=\left( \begin{array}{c} c_1\\ c_2\\ c_3\\ \end{array} \right)\\ A=\left( \begin{array}{c} a_1 &amp;amp; b_1 &amp;amp; c_1 \\ a_2 &amp;amp; b_2 &amp;amp; c_2 \\ a_3 &amp;amp; b_3 &amp;amp; c_3 \\ \end{array} \right)\)&lt;/span&gt; &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}\underline{a}^t+\underline{b}\underline{b}^t+\underline{c}\underline{c}^t\\ \;\;=\left( \begin{array}{c} a_1^2 &amp;amp; a_1a_2 &amp;amp; a_1a_3 \\ a_2a_1 &amp;amp; a_2^2 &amp;amp; a_2a_3 \\ a_3a_1 &amp;amp; a_3a_2 &amp;amp; a_3^2 \\ \end{array} \right)\\ \;\;\;\;\;\;+\left( \begin{array}{c} b_1^2 &amp;amp; b_1b_2 &amp;amp; b_1b_3 \\ b_2b_1 &amp;amp; b_2^2 &amp;amp; b_2b_3 \\ b_3b_1 &amp;amp; b_3b_2 &amp;amp; b_3^2 \\ \end{array} \right)\\ \;\;\;\;\;\;+\left( \begin{array}{c} c_1^2 &amp;amp; c_1c_2 &amp;amp; c_1c_3 \\ c_2c_1 &amp;amp; c_2^2 &amp;amp; c_2c_3 \\ c_3c_1 &amp;amp; c_3c_2 &amp;amp; c_3^2 \\ \end{array} \right)\\ =\left( \begin{array}{c} a_1 &amp;amp; b_1 &amp;amp; c_1 \\ a_2 &amp;amp; b_2 &amp;amp; c_2 \\ a_3 &amp;amp; b_3 &amp;amp; c_3 \\ \end{array} \right)\left( \begin{array}{c} a_1 &amp;amp; a_2 &amp;amp; a_3 \\ b_1 &amp;amp; b_2 &amp;amp; b_3 \\ c_1 &amp;amp; c_2 &amp;amp; c_3 \\ \end{array} \right)=AA^t\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}=\left( \begin{array}{c} x_1\\ x_2\\ \vdots\\ x_n\\ \end{array} \right), \; \underline{\frac{1}{n}}=\left( \begin{array}{c} \frac{1}{n}\\ \frac{1}{n}\\ \cdots\\ \frac{1}{n}\\ \end{array} \right)\)&lt;/span&gt; 時， &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}^t\underline{\frac{1}{n}}=\sum\limits_{i=1}^nx_i\cdot\frac{1}{n}=\bar{x}\)&lt;/span&gt; &lt;span class=&#34;diff_alert&#34;&gt;(&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}\)&lt;/span&gt; 的&lt;strong&gt;平均值&lt;/strong&gt;)&lt;/span&gt;
&lt;br&gt;
將這樣的 &lt;span class=&#34;math inline&#34;&gt;\(\bar{x}\)&lt;/span&gt; 寫成 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 個的橫向量：&lt;span class=&#34;math inline&#34;&gt;\((\bar{x},\bar{x},\cdots,\bar{x})\)&lt;/span&gt; &lt;br&gt;
這個向量如果寫成展開的形式就是：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\((\bar{x},\bar{x},\cdots,\bar{x})=(\underline{x}^t\underline{\frac{1}{n}}, \underline{x}^t\underline{\frac{1}{n}}, \cdots,\underline{x}^t\underline{\frac{1}{n}})\\ \;\;\;\;\;\;=\underline{x}^t(\underline{\frac{1}{n}},\underline{\frac{1}{n}},\cdots,\underline{\frac{1}{n}})\\ \;\;\;\;\;\;=(x_1,x_2,\cdots,x_n)\left( \begin{array}{c} \frac{1}{n} &amp;amp; \frac{1}{n} &amp;amp; \cdots &amp;amp; \frac{1}{n} \\ \frac{1}{n} &amp;amp; \frac{1}{n} &amp;amp; \cdots &amp;amp; \frac{1}{n} \\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ \frac{1}{n} &amp;amp; \frac{1}{n} &amp;amp; \cdots &amp;amp; \frac{1}{n} \\ \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;矩陣積的性質&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;矩陣積的性質&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;矩陣 &lt;span class=&#34;math inline&#34;&gt;\(AB\)&lt;/span&gt; 有定義時， &lt;span class=&#34;math inline&#34;&gt;\(BA\)&lt;/span&gt; 並不一定就有定義。無法整合時就沒有定義。&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(AB=BA\)&lt;/span&gt; 時，&lt;span class=&#34;math inline&#34;&gt;\(A, B\)&lt;/span&gt; 被稱爲&lt;strong&gt;可交換 commutative&lt;/strong&gt;，&lt;strong&gt;交換可能矩陣&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;當 &lt;span class=&#34;math inline&#34;&gt;\(AB, BA\)&lt;/span&gt; 都有定義時，也不一定就滿足 &lt;span class=&#34;math inline&#34;&gt;\(AB=BA\)&lt;/span&gt;。也就是說，多數情況下， &lt;span class=&#34;math inline&#34;&gt;\(AB\neq BA\)&lt;/span&gt;。爲了區分二者，&lt;span class=&#34;math inline&#34;&gt;\(AB\)&lt;/span&gt; 被稱爲 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 從右往左乘 &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; &lt;strong&gt;(postmultiplication of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; by &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;)&lt;/strong&gt;，&lt;span class=&#34;math inline&#34;&gt;\(BA\)&lt;/span&gt; 被稱爲 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 從左往右乘 &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; &lt;strong&gt;(postmultiplication of &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; by &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;)&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;相似的， &lt;span class=&#34;math inline&#34;&gt;\(AC=BC\)&lt;/span&gt; 時，應該理解爲： 等式&lt;span class=&#34;math inline&#34;&gt;\(A=B\)&lt;/span&gt;兩邊同時從右往左乘 &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(CA=CB\)&lt;/span&gt; 就是：等式&lt;span class=&#34;math inline&#34;&gt;\(A=B\)&lt;/span&gt;兩邊同時從左往右乘&lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;。&lt;/li&gt;
&lt;li&gt;即使 &lt;span class=&#34;math inline&#34;&gt;\(A\neq\Large 0\)&lt;/span&gt; 且 &lt;span class=&#34;math inline&#34;&gt;\(B\neq\Large 0\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(AB\)&lt;/span&gt; 也有可能等於 &lt;span class=&#34;math inline&#34;&gt;\(\Large 0\)&lt;/span&gt; (零矩陣)，此時我們說， &lt;span class=&#34;math inline&#34;&gt;\(A, B\)&lt;/span&gt; 是&lt;strong&gt;零因子 (zero divisor)&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記11</title>
      <link>https://wangcc.me/post/2017-02-21/</link>
      <pubDate>Tue, 21 Feb 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/2017-02-21/</guid>
      <description>
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;矩陣的定義&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;矩陣的定義&lt;/h2&gt;

&lt;div class=&#34;theorem&#34;&gt;
&lt;p&gt;&lt;span id=&#34;thm:unnamed-chunk-1&#34; class=&#34;theorem&#34;&gt;&lt;strong&gt;Theorem 1  (matrix)  &lt;/strong&gt;&lt;/span&gt;將&lt;span class=&#34;math inline&#34;&gt;\(m\times n\)&lt;/span&gt; 個數 &lt;span class=&#34;math inline&#34;&gt;\(a_{ij} (i=1,2,\cdots,m; j=1,2,\cdots,n)\)&lt;/span&gt;, 寫成縱 &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; 行， 橫 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 列的長方形或者正方形，左右用圓括號或者方括號包含在內。我們稱之爲 &lt;span class=&#34;math inline&#34;&gt;\(m\times n\)&lt;/span&gt; &lt;strong&gt;矩陣(matrix)&lt;/strong&gt;，或者 &lt;span class=&#34;math inline&#34;&gt;\((m, n)\)&lt;/span&gt; 矩陣。 &lt;span class=&#34;math inline&#34;&gt;\(m\times n\)&lt;/span&gt; 或者 &lt;span class=&#34;math inline&#34;&gt;\((m,n)\)&lt;/span&gt; 被稱爲是這個矩陣的類型。我們常用大寫字母來標記一個矩陣，如下面的矩陣我們標記爲 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;。 如果要特別明示矩陣的類型，可以寫作 &lt;span class=&#34;math inline&#34;&gt;\(\mathop{A}_{m\times n}, \mathop{A}_{(m, n)}, \; A(m\times n)\)&lt;/span&gt;。兩個矩陣如果行數相等，列數也相等，我們稱他們爲類型相同的矩陣。構成矩陣的一個個數 &lt;span class=&#34;math inline&#34;&gt;\(a_{11},a_{12},\cdots,a_{mn}\)&lt;/span&gt; 被叫做矩陣的成分(component, element, entry)。&lt;/p&gt;
第&lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;行，第&lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;列交叉的地方的成分，&lt;span class=&#34;math inline&#34;&gt;\(a_{ij}\)&lt;/span&gt; 被叫做 &lt;span class=&#34;math inline&#34;&gt;\((i,j)\)&lt;/span&gt; 成分。矩陣有時候也會寫成 &lt;span class=&#34;math inline&#34;&gt;\(A=(a_{ij})\)&lt;/span&gt;
&lt;/div&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\left( \begin{array}{c} a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1j} &amp;amp; \cdots &amp;amp; a_{1n}\\ a_{21} &amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2j} &amp;amp; \cdots &amp;amp; a_{2n}\\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ a_{i1} &amp;amp; a_{i2} &amp;amp; \cdots &amp;amp; a_{ij} &amp;amp; \cdots &amp;amp; a_{in}\\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ a_{m1} &amp;amp; a_{m2} &amp;amp; \cdots &amp;amp; a_{mj} &amp;amp; \cdots &amp;amp; a_{mn}\\ \end{array} \right), \\ \left[ \begin{array}{c} a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1j} &amp;amp; \cdots &amp;amp; a_{1n}\\ a_{21} &amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2j} &amp;amp; \cdots &amp;amp; a_{2n}\\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ a_{i1} &amp;amp; a_{i2} &amp;amp; \cdots &amp;amp; a_{ij} &amp;amp; \cdots &amp;amp; a_{in}\\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ a_{m1} &amp;amp; a_{m2} &amp;amp; \cdots &amp;amp; a_{mj} &amp;amp; \cdots &amp;amp; a_{mn}\\ \end{array} \right]\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;矩陣 &lt;span class=&#34;math inline&#34;&gt;\(\mathop{A}_{m\times n}\)&lt;/span&gt; 可以被看做是：&lt;br&gt;
以第&lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;行爲成分的行向量 &lt;span class=&#34;math inline&#34;&gt;\((a_{11},a_{12},\cdots,a_{1n})=\underline{b}_1^t\)&lt;/span&gt;；&lt;br&gt;
以第&lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt;行爲成分的行向量 &lt;span class=&#34;math inline&#34;&gt;\((a_{21},a_{22},\cdots,a_{2n})=\underline{b}_2^t\)&lt;/span&gt;；&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\vdots\)&lt;/span&gt;&lt;br&gt;
以第 &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; 行爲成分的行向量 &lt;span class=&#34;math inline&#34;&gt;\((a_{m1},a_{m2},\cdots,a_{mn})=\underline{b}_m^t\)&lt;/span&gt;；&lt;br&gt;
爲成分組成的列向量：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\left( \begin{array}{c} \underline{b}_1^t\\ \underline{b}_2^t\\ \vdots\\ \underline{b}_m^t\\ \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;類似的，矩陣 &lt;span class=&#34;math inline&#34;&gt;\(\mathop{A}_{m\times n}\)&lt;/span&gt; 可以被看做是：&lt;br&gt;
以第&lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;列爲成分的列向量： &lt;span class=&#34;math inline&#34;&gt;\(\left( \begin{array}{c} a_{11}\\ a_{21}\\ \vdots\\ a_{m1}\\ \end{array} \right)=\underline{c}_1\)&lt;/span&gt; &lt;br&gt;
以第&lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt;列爲成分的列向量：&lt;span class=&#34;math inline&#34;&gt;\(\left( \begin{array}{c} a_{12}\\ a_{22}\\ \vdots\\ a_{m2}\\ \end{array} \right)=\underline{c}_2\)&lt;/span&gt; &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\vdots\)&lt;/span&gt;&lt;br&gt;
以第&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;列爲成分的列向量：&lt;span class=&#34;math inline&#34;&gt;\(\left( \begin{array}{c} a_{1n}\\ a_{2n}\\ \vdots\\ a_{mn}\\ \end{array} \right)=\underline{c}_n\)&lt;/span&gt; &lt;br&gt;
爲成分組成的行向量：&lt;span class=&#34;math inline&#34;&gt;\((\underline{c}_1,\underline{c}_2,\cdots,\underline{c}_n)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;矩陣的運算和零矩陣&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;矩陣的運算，和零矩陣&lt;/h2&gt;
&lt;div id=&#34;矩陣的和與差&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;矩陣的和與差&lt;/h3&gt;

&lt;div class=&#34;theorem&#34;&gt;
&lt;span id=&#34;thm:unnamed-chunk-2&#34; class=&#34;theorem&#34;&gt;&lt;strong&gt;Theorem 2  (matrix plus or minus)  &lt;/strong&gt;&lt;/span&gt;類型(type)相同的矩陣之間的加減法運算，被定義爲各個對應成分的加減法結果作成分的矩陣。
&lt;/div&gt;
&lt;p&gt;對於&lt;span class=&#34;math inline&#34;&gt;\(A=\left( \begin{array}{c} a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n}\\ a_{21} &amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n}\\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ a_{m1} &amp;amp; a_{m2} &amp;amp; \cdots &amp;amp; a_{mn}\\ \end{array} \right),\\ B=\left( \begin{array}{c} b_{11} &amp;amp; b_{12} &amp;amp; \cdots &amp;amp; b_{1n}\\ b_{21} &amp;amp; b_{22} &amp;amp; \cdots &amp;amp; b_{2n}\\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ b_{m1} &amp;amp; b_{m2} &amp;amp; \cdots &amp;amp; b_{mn}\\ \end{array} \right)\)&lt;/span&gt; &lt;br&gt;
有：&lt;span class=&#34;math inline&#34;&gt;\(A\pm B=\left( \begin{array}{c} a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n}\\ a_{21} &amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n}\\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ a_{m1} &amp;amp; a_{m2} &amp;amp; \cdots &amp;amp; a_{mn}\\ \end{array} \right)\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\pm \left( \begin{array}{c} b_{11} &amp;amp; b_{12} &amp;amp; \cdots &amp;amp; b_{1n}\\ b_{21} &amp;amp; b_{22} &amp;amp; \cdots &amp;amp; b_{2n}\\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ b_{m1} &amp;amp; b_{m2} &amp;amp; \cdots &amp;amp; b_{mn}\\ \end{array} \right)\\ \;\;\;\;\;\;\;\;\;\;=\left( \begin{array}{c} a_{11}\pm b_{11} &amp;amp; a_{12}\pm b_{12} &amp;amp; \cdots &amp;amp; a_{1n}\pm b_{1n}\\ a_{21}\pm b_{21} &amp;amp; a_{22}\pm b_{22} &amp;amp; \cdots &amp;amp; a_{2n}\pm b_{2n}\\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ a_{m1}\pm b_{m1} &amp;amp; a_{m2}\pm b_{m2} &amp;amp; \cdots &amp;amp; a_{mn}\pm b_{mn}\\ \end{array} \right)(復号同順)\)&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(A=\left(  \begin{array}{c}  9 &amp;amp; 3 &amp;amp; 1\\  -2 &amp;amp; 5 &amp;amp; 8\\  \end{array}  \right)， B=\left(  \begin{array}{c}  4 &amp;amp; 2 &amp;amp; 1\\  3 &amp;amp; -3 &amp;amp; 5\\  \end{array}  \right)\)&lt;/span&gt; 那麼&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(A+B = \left(  \begin{array}{c}  9+4 &amp;amp; 3+2 &amp;amp; 1+1\\  -2+3 &amp;amp; 5+(-3) &amp;amp; 8+5\\  \end{array}  \right)=\left(  \begin{array}{c}  13 &amp;amp; 5 &amp;amp; 2\\  1 &amp;amp; 2 &amp;amp; 13\\  \end{array}  \right)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;用1.中的矩陣運算：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(A-B=\left(  \begin{array}{c}  9-4 &amp;amp; 3-2 &amp;amp; 1-1\\  -2-3 &amp;amp; 5-(-3) &amp;amp; 8-5\\  \end{array}  \right)=\left(  \begin{array}{c}  5 &amp;amp; 1 &amp;amp; 0\\  -5 &amp;amp; 8 &amp;amp; 3\\  \end{array}  \right)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;矩陣的相等&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;矩陣的相等&lt;/h3&gt;

&lt;div class=&#34;theorem&#34;&gt;
&lt;span id=&#34;thm:unnamed-chunk-3&#34; class=&#34;theorem&#34;&gt;&lt;strong&gt;Theorem 3  (matrix equal)  &lt;/strong&gt;&lt;/span&gt;類型相同的兩個矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A,B\)&lt;/span&gt;，如果他們對應的所有成分，一一相等，我們說這兩個矩陣是相等的。即：&lt;span class=&#34;math inline&#34;&gt;\(A=B\)&lt;/span&gt;。
&lt;/div&gt;
&lt;p&gt;對於&lt;span class=&#34;math inline&#34;&gt;\(A=\left( \begin{array}{c} a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n}\\ a_{21} &amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n}\\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ a_{m1} &amp;amp; a_{m2} &amp;amp; \cdots &amp;amp; a_{mn}\\ \end{array} \right),\\ B=\left( \begin{array}{c} b_{11} &amp;amp; b_{12} &amp;amp; \cdots &amp;amp; b_{1n}\\ b_{21} &amp;amp; b_{22} &amp;amp; \cdots &amp;amp; b_{2n}\\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ b_{m1} &amp;amp; b_{m2} &amp;amp; \cdots &amp;amp; b_{mn}\\ \end{array} \right)\)&lt;/span&gt;&lt;br&gt;
如果有：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(a_{11}=b_{11},a_{12}=b_{12},\cdots,a_{mn}=b_{mn}\)&lt;/span&gt;&lt;br&gt;
那麼 &lt;span class=&#34;math inline&#34;&gt;\(A=B\)&lt;/span&gt;。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;零矩陣&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;零矩陣&lt;/h3&gt;

&lt;div class=&#34;theorem&#34;&gt;
&lt;span id=&#34;thm:unnamed-chunk-4&#34; class=&#34;theorem&#34;&gt;&lt;strong&gt;Theorem 4  (zero matrix)  &lt;/strong&gt;&lt;/span&gt;所有的成分均爲數字 &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; 的 &lt;span class=&#34;math inline&#34;&gt;\(m\times n\)&lt;/span&gt; 矩陣，&lt;br&gt;
(共有　&lt;span class=&#34;math inline&#34;&gt;\(m\times n\)&lt;/span&gt; 個零。)&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\left( \begin{array}{c} 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0\\ 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0\\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0\\ \end{array} \right)\)&lt;/span&gt;&lt;br&gt;
被稱爲&lt;strong&gt;零矩陣(zero matrix, null matrix)&lt;/strong&gt;。寫作：&lt;span class=&#34;math inline&#34;&gt;\(\large 0, \mathop{\large 0}_{m\times n}, \mathop{\large 0}_{(m,n)}\)&lt;/span&gt;。要注意與標量的 &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; 區分。
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;矩陣的標量倍數運算&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;矩陣的標量倍數運算&lt;/h3&gt;

&lt;div class=&#34;theorem&#34;&gt;
&lt;span id=&#34;thm:unnamed-chunk-5&#34; class=&#34;theorem&#34;&gt;&lt;strong&gt;Theorem 5  (scalar times)  &lt;/strong&gt;&lt;/span&gt;矩陣 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 的所有的成分，均乘以一個標量 &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;，獲得新的矩陣的過程被稱爲矩陣的標量倍數運算。 寫作 &lt;span class=&#34;math inline&#34;&gt;\(kA\)&lt;/span&gt;。
&lt;/div&gt;
&lt;p&gt;對於 &lt;span class=&#34;math inline&#34;&gt;\(A=\left( \begin{array}{c} a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n}\\ a_{21} &amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n}\\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ a_{m1} &amp;amp; a_{m2} &amp;amp; \cdots &amp;amp; a_{mn}\\ \end{array} \right)\)&lt;/span&gt;，&lt;br&gt;
有：&lt;span class=&#34;math inline&#34;&gt;\(kA = k\left( \begin{array}{c} a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n}\\ a_{21} &amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n}\\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ a_{m1} &amp;amp; a_{m2} &amp;amp; \cdots &amp;amp; a_{mn}\\ \end{array} \right)\\ =\left( \begin{array}{c} ka_{11} &amp;amp; ka_{12} &amp;amp; \cdots &amp;amp; ka_{1n}\\ ka_{21} &amp;amp; ka_{22} &amp;amp; \cdots &amp;amp; ka_{2n}\\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ ka_{m1} &amp;amp; ka_{m2} &amp;amp; \cdots &amp;amp; ka_{mn}\\ \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;特別的，當 &lt;span class=&#34;math inline&#34;&gt;\(k=-1\)&lt;/span&gt; 時， &lt;span class=&#34;math inline&#34;&gt;\((-1)A=-A\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(k=0\)&lt;/span&gt; 時，&lt;span class=&#34;math inline&#34;&gt;\(0A=\Large 0\)&lt;/span&gt;。注意 &lt;span class=&#34;math inline&#34;&gt;\(\Large 0\)&lt;/span&gt; 是與 &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; 類型相同的零矩陣，而非標量 &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;。&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;對 &lt;span class=&#34;math inline&#34;&gt;\(A=\left( \begin{array}{c} a_{11} &amp;amp; a_{12} &amp;amp; a_{13}\\ a_{21} &amp;amp; a_{22} &amp;amp; a_{23}\\ a_{31} &amp;amp; a_{32} &amp;amp; a_{33}\\ \end{array} \right)\)&lt;/span&gt;， &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(kA=\left( \begin{array}{c} ka_{11} &amp;amp; ka_{12} &amp;amp; ka_{13}\\ ka_{21} &amp;amp; ka_{22} &amp;amp; ka_{23}\\ ka_{31} &amp;amp; ka_{32} &amp;amp; ka_{33}\\ \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;對 &lt;span class=&#34;math inline&#34;&gt;\(B=\left( \begin{array}{c} 1 &amp;amp; -2 &amp;amp; 3\\ -4 &amp;amp; 5 &amp;amp; -6\\ \end{array} \right)\)&lt;/span&gt;，&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(7B=\left( \begin{array}{c} 7\times1 &amp;amp; 7\times(-2) &amp;amp; 7\times3\\ 7\times(-4) &amp;amp; 7\times5 &amp;amp; 7\times(-6)\\ \end{array} \right)\\ \;\;\;\;=\left( \begin{array}{c} 7 &amp;amp; -14 &amp;amp; 21\\ -28 &amp;amp; 35 &amp;amp; -42\\ \end{array} \right)\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(-B=\left( \begin{array}{c} -1 &amp;amp; 2 &amp;amp; -3\\ 4 &amp;amp; -5 &amp;amp; 6\\ \end{array} \right)\)&lt;/span&gt;；&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(0B=\left( \begin{array}{c} 0 &amp;amp; 0 &amp;amp; 0\\ 0 &amp;amp; 0 &amp;amp; 0\\ \end{array} \right)=\mathop{\large 0}_{2\times3}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記10</title>
      <link>https://wangcc.me/post/2017-02-19/</link>
      <pubDate>Mon, 20 Feb 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/2017-02-19/</guid>
      <description>
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;向量的內積-inner-product&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;向量的內積 (inner product)&lt;/h2&gt;

&lt;div class=&#34;theorem&#34;&gt;
&lt;span id=&#34;thm:unnamed-chunk-1&#34; class=&#34;theorem&#34;&gt;&lt;strong&gt;Theorem 1  (vectors inner product)  &lt;/strong&gt;&lt;/span&gt;向量的&lt;strong&gt;內積&lt;/strong&gt;運算，僅限定於維度相同的兩個向量之間。一個向量爲橫向量寫在左側，一個向量爲列向量寫在右側，兩個向量的相對應成分一一相乘，然後將各成分乘積相加的過程，我們稱之爲內積(inner product, scalar product)運算。內積運算結果通常不會是向量，而是標量(scalar)，或正或負，或爲零。向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}\)&lt;/span&gt; 與向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{b}\)&lt;/span&gt; 的內積寫作：&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}^t\underline{b}, \underline{b}^t\underline{a}\)&lt;/span&gt; 或者寫作： &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}\cdot\underline{b}, (\underline{a},\underline{b}), &amp;lt;\underline{a},\underline{b}&amp;gt;\)&lt;/span&gt;。內積爲 &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; 的向量我們稱他們爲正交向量(orthogonal)，寫作：&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}\perp\underline{b}\)&lt;/span&gt;。
內積，與和記號: &lt;span class=&#34;math inline&#34;&gt;\(\sum\)&lt;/span&gt; 有緊密聯系。我們常常會把 &lt;span class=&#34;math inline&#34;&gt;\(\sum\)&lt;/span&gt; 式子/量寫成向量的內積形式。
&lt;/div&gt;
&lt;div id=&#34;練習&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;練習&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;列向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=\left( \begin{array}{c} a_1\\ a_2\\ a_3 \end{array} \right), \underline{b}=\left( \begin{array}{c} b_1\\ b_2\\ b_3 \end{array} \right)\)&lt;/span&gt; 的內積：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}^t\underline{b}=(a_1,a_2,a_3)\left( \begin{array}{c} b_1\\ b_2\\ b_3 \end{array} \right)=a_1b_1+a_2b_2+a_3b_3\\=\sum\limits_{i=1}^3a_ib_i=\sum\limits_{i=1}^3b_ia_i=\underline{b}^t\underline{a}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;橫向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=(a_1,a_2,a_3), \underline{b}=(b_1,b_2,b_3)\)&lt;/span&gt; 的內積：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}\underline{b}^t=(a_1,a_2,a_3)\left( \begin{array}{c} b_1\\ b_2\\ b_3 \end{array} \right)=a_1b_1+a_2b_2+a_3b_3\\=\sum\limits_{i=1}^3a_ib_i=\sum\limits_{i=1}^3b_ia_i=\underline{b}\underline{a}^t\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;完全相同的兩個列向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{x}=\left( \begin{array}{c} x_1\\ x_2\\ x_3 \end{array} \right),\;\underline{x}=\left( \begin{array}{c} x_1\\ x_2\\ x_3 \end{array} \right)\)&lt;/span&gt; 的內積：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}^t\underline{x}=(x_1,x_2,x_3)\left( \begin{array}{c} x_1\\ x_2\\ x_3 \end{array} \right)\\=x_1^2+x_2^2+x_3^2=\sum\limits_{i=1}^3x_i\cdot x_i=\sum\limits_{i=1}^3x_i^2\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;完全相同的兩個橫向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{y}=(y_1,y_2,y_3), \underline{y}=(y_1,y_2,y_3)\)&lt;/span&gt; 的內積：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{y}\underline{y}^t=(y_1,y_2,y_3)\left( \begin{array}{c} y_1\\ y_2\\ y_3 \end{array} \right)\\=y_1^2+y_2^2+y_3^2=\sum\limits_{i=1}^3y_i\cdot y_i=\sum\limits_{i=1}^3y_i^2\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=(2,0,-1), \underline{b}=(4,-2,8)\)&lt;/span&gt; 的內積：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}\underline{b}^t=(2,0,-1)\left( \begin{array}{c} 4\\ -2\\ 8 \end{array} \right)=2\times4+0\times(-2)+(-1)\times8=0\)&lt;/span&gt; &lt;br&gt;
因此我們稱這兩個向量正交。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{1}=\left( \begin{array}{c} 1\\ 1\\ 1 \end{array} \right), \underline{x}=\left( \begin{array}{c} x_1\\ x_2\\ x_3 \end{array} \right)\)&lt;/span&gt; 時：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{1}^t\underline{x}=1\cdot x_1+1\cdot x_2+1\cdot x_3 =\sum\limits_{i=1}^3x_i=\underline{x}^t\underline{1}\)&lt;/span&gt; &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{1}^t\underline{1}=\sum\limits_{i=1}^31\cdot 1=3\)&lt;/span&gt; &lt;br&gt;
前者的內積與後者內積的商： &lt;span class=&#34;math inline&#34;&gt;\(\frac{\underline{1}^t\underline{x}}{\underline{1}^t\underline{1}}=\frac{x_1+x_2+x_3}{3}\)&lt;/span&gt; 我們在統計學中用 &lt;span class=&#34;math inline&#34;&gt;\(\bar{x}\)&lt;/span&gt; (平均值) 來標記。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;問題-如果向量-underlinea-underlineb-有內積-請問有沒有所謂的外積-outer-product&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;問題： 如果，向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}, \underline{b}\)&lt;/span&gt; 有內積， 請問有沒有所謂的外積 (outer product) ？&lt;/h5&gt;
&lt;/div&gt;
&lt;div id=&#34;回答-有不過僅限於3維度的向量&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;回答： 有。不過，僅限於3維度的向量：&lt;/h5&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=\left( \begin{array}{c} a_1\\ a_2\\ a_3 \end{array} \right), \underline{b}=\left( \begin{array}{c} b_1\\ b_2\\ b_3 \end{array} \right)\)&lt;/span&gt; 的外積，我們用 &lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt; 來表示，寫作： &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}\times\underline{b}\)&lt;/span&gt;。 其運算被定義爲：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}\times\underline{b}=\left( \begin{array}{c} a_2b_3-a_3b_2\\ a_3b_1-a_1b_3\\ a_1b_2-a_2b_1 \end{array}\right)\)&lt;/span&gt;。與內積不同的是，外積運算的結果仍然是&lt;span class=&#34;math inline&#34;&gt;\(3\)&lt;/span&gt;維度的向量。外積有如下的性質：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}\times\underline{b}=-\underline{b}\times\underline{a}\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;向量的長度-length&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;向量的長度 (length)&lt;/h2&gt;

&lt;div class=&#34;theorem&#34;&gt;
&lt;p&gt;&lt;span id=&#34;thm:unnamed-chunk-2&#34; class=&#34;theorem&#34;&gt;&lt;strong&gt;Theorem 2  (vector length)  &lt;/strong&gt;&lt;/span&gt;向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}\)&lt;/span&gt; 的內積 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}^t\underline{a}\)&lt;/span&gt; 的平方根中，非負的量，我們稱之爲向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}\)&lt;/span&gt; 的&lt;strong&gt;長度&lt;/strong&gt;或者&lt;strong&gt;大小&lt;/strong&gt;。也就是：&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\underline{a}^t\underline{a}}\)&lt;/span&gt;。記作：&lt;span class=&#34;math inline&#34;&gt;\(\| \underline{a} \|\)&lt;/span&gt;。&lt;/p&gt;
兩個向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}, \underline{b}\)&lt;/span&gt; 類型(type：大小，維度)相同時，他們的差 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}-\underline{b}\)&lt;/span&gt; 依然是向量，這個新向量的長度爲：&lt;span class=&#34;math inline&#34;&gt;\(\| \underline{a}-\underline{b} \| = \sqrt{(\underline{a}-\underline{b})^t(\underline{a}-\underline{b})}\)&lt;/span&gt;
&lt;/div&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}=\left( \begin{array}{c} x_1\\ x_2\\ x_3 \end{array} \right)\)&lt;/span&gt; 的長度爲： &lt;span class=&#34;math inline&#34;&gt;\(\| \underline{x} \| =\sqrt{\underline{x}^t\underline{x}}=\sqrt{x_1^2+x_2^2+x_3^2}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=(a_1,a_2,a_3)\)&lt;/span&gt; 的長度爲： &lt;span class=&#34;math inline&#34;&gt;\(\| \underline{a} \| =\sqrt{\underline{a}\underline{a}^t}=\sqrt{a_1^2+a_2^2+a_3^2}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;兩個向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}, \underline{b}\)&lt;/span&gt; 的長度和內積有這樣的關系：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(-\| \underline{a} \|\| \underline{b} \|\leqslant \underline{a}^t\underline{b}\leqslant\| \underline{a} \|\| \underline{b} \|\)&lt;/span&gt;&lt;br&gt;
&lt;strong&gt;證明&lt;/strong&gt;: 以維度爲 &lt;span class=&#34;math inline&#34;&gt;\(3\)&lt;/span&gt; 的向量爲例進行證明，其他維度的向量，證明思路類似：&lt;br&gt;
令 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=\left( \begin{array}{c} a_1\\ a_2\\ a_3 \end{array} \right), \underline{b}=\left( \begin{array}{c} b_1\\ b_2\\ b_3 \end{array} \right)\)&lt;/span&gt;， &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; 爲任意實數。平方和：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\sum\limits_{i=1}^3(a_it+b_i)^2 =(a_1t+b_1)^2+(a_2t+b_2)^2+(a_3t+b_3)^2\\ \;\;\;\;\;\;\;=(a_1^2+a_2^2+a_3^2)t^2+2(a_1b_1+a_2b_2+a_3b_3)t\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;+(b_1^2+b_2^2+b_3^2)\\ \;\;\;\;\;\;\;=\| \underline{a} \|^2t^2+2\underline{a}^t\underline{b}t+\| \underline{b} \|^2\geqslant0\)&lt;/span&gt; &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\therefore \| \underline{a} \|^2(t+\frac{\| \underline{b} \|^2}{2\| \underline{a} \|^2})^2+\| \underline{b} \|^2-\frac{(2\underline{a}^t\underline{b})^2}{4\| \underline{a} \|^2}\geqslant0\)&lt;/span&gt;&lt;br&gt;
可見這是一個&lt;strong&gt;關於 &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; 的絕對不等式&lt;/strong&gt;。因此，&lt;strong&gt;判別式&lt;/strong&gt;：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\((2\underline{a}^t\underline{b})^2-4\times\| \underline{a} \|^2\| \underline{b} \|^2\leqslant0\\ \therefore (\underline{a}^t\underline{b})^2\leqslant\| \underline{a} \|^2\| \underline{b} \|^2\\ \therefore -\| \underline{a} \|\| \underline{b} \|\leqslant \underline{a}^t\underline{b}\leqslant \| \underline{a} \|\| \underline{b} \|\)&lt;/span&gt;&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\divideontimes\)&lt;/span&gt; 兩向量內積，除以兩向量各自的長度(正)，在統計學中被成爲是&lt;strong&gt;相關系數&lt;/strong&gt;，寫作 &lt;span class=&#34;math inline&#34;&gt;\(r=\frac{\underline{a}^t\underline{b}}{\| \underline{a} \|\| \underline{b} \|}\)&lt;/span&gt;，我們從上面的不等式也可以得出， &lt;span class=&#34;math inline&#34;&gt;\(-1 \leqslant r \leqslant 1\)&lt;/span&gt; 另外，兩個向量又可以表示爲兩條射線，這兩條射線構成的角度如果爲 &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(\cos\theta=r =\frac{\underline{a}^t\underline{b}}{\| \underline{a} \|\| \underline{b} \|}\)&lt;/span&gt;。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;兩個向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}, \underline{b}\)&lt;/span&gt; 的和 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}+\underline{b}\)&lt;/span&gt; 也是一個新的向量。這三個向量之間有：&lt;span class=&#34;math inline&#34;&gt;\(\| \underline{a}+\underline{b} \|\leqslant\| \underline{a} \|+\| \underline{b} \|\)&lt;/span&gt;。這個關系被稱爲&lt;strong&gt;三角不等式&lt;/strong&gt;，或者&lt;strong&gt;三角關系&lt;/strong&gt;(triangular inequality)。&lt;br&gt;
&lt;strong&gt;證明&lt;/strong&gt;：此處亦爲了簡便起見使用維度爲 &lt;span class=&#34;math inline&#34;&gt;\(3\)&lt;/span&gt; 的向量，即，前述3.的 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}, \underline{b}\)&lt;/span&gt;：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\| \underline{a}+\underline{b}\|^2=(a_1+b_1)^2+(a_2+b_2)^2+(a_3+b_3)^2\\ \;\;\;\;\;\;\;=(a_1^2+a_2^2+a_3^3)+2(a_1b_1+a_2b_2+a_3b_3)+(b_1^2+b_2^2+b_3^2)\\ \;\;\;\;\;\;\;=\| \underline{a} \|^2+2\underline{a}^t\underline{b}+\| \underline{b} \|^2\)&lt;/span&gt;&lt;br&gt;
如果我們把前面問題3.中的不等式代入：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\| \underline{a} \|^2+2\underline{a}^t\underline{b}+\| \underline{b} \|^2\leqslant \| \underline{a} \|^2+2\| \underline{a} \|\| \underline{b} \|+\| \underline{b} \|^2\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;=(\| \underline{a} \|+\| \underline{b} \|)^2\\ \therefore \| \underline{a}+\underline{b}\|^2 \leqslant (\| \underline{a} \|+\| \underline{b} \|)^2\\ \therefore \| \underline{a}+\underline{b}\|\leqslant\| \underline{a} \|+\| \underline{b} \|\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;向量正規化-normalize&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;向量正規化 normalize&lt;/h2&gt;

&lt;div class=&#34;theorem&#34;&gt;
&lt;p&gt;&lt;span id=&#34;thm:unnamed-chunk-3&#34; class=&#34;theorem&#34;&gt;&lt;strong&gt;Theorem 3  (normalize)  &lt;/strong&gt;&lt;/span&gt;長度不爲 &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; 的任意向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}(\neq\underline{0})\)&lt;/span&gt;，如果將它轉變成長度爲 &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; 的向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{e}_{\underline{a}}\)&lt;/span&gt;。這個過程被叫做向量的正規化(normalize)。通常只要將向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}\)&lt;/span&gt; 除以他的長度 &lt;span class=&#34;math inline&#34;&gt;\(\| \underline{a} \|\)&lt;/span&gt; 即可。&lt;/p&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{e}_{\underline{a}}=\frac{\underline{a}}{\| \underline{a} \|}=\frac{1}{\| \underline{a} \|}\underline{a}\)&lt;/span&gt;
&lt;/div&gt;
&lt;p&gt;例如：&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=\left( \begin{array}{c} a_1\\ a_2\\ a_3 \end{array} \right)\)&lt;/span&gt;， 則有 &lt;span class=&#34;math inline&#34;&gt;\(\underline{e}_{\underline{a}}=\frac{1}{\sqrt{a_1^2+a_2^2+a_3^2}}\left( \begin{array}{c} a_1\\ a_2\\ a_3 \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{b}=\left( \begin{array}{c} -2\\ 1\\ 2 \end{array} \right)\)&lt;/span&gt;，則有 &lt;span class=&#34;math inline&#34;&gt;\(\underline{e}_{\underline{a}}=\frac{1}{\sqrt{(-2)^2+1^2+2^2}}\left( \begin{array}{c} -2\\ 1\\ 2 \end{array} \right)=\left( \begin{array}{c} -\frac{2}{3}\\ \frac{1}{3}\\ \frac{2}{3} \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記9</title>
      <link>https://wangcc.me/post/2017-02-18/</link>
      <pubDate>Sat, 18 Feb 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/2017-02-18/</guid>
      <description>
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;特殊向量&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;特殊向量&lt;/h2&gt;
&lt;div id=&#34;零向量-zero-vector-null-vector&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;零向量 (zero vector, null vector)&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;全部的成分均爲&lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;的向量，我們稱之爲&lt;strong&gt;零向量(zero vector, null vector)&lt;/strong&gt;, 寫作： &lt;span class=&#34;math inline&#34;&gt;\(\underline{0}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;注意與&lt;strong&gt;標量(scalar)&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; 相區分。&lt;/li&gt;
&lt;li&gt;如果想要加注零向量的維度，我們可以在右下角加上 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;：&lt;span class=&#34;math inline&#34;&gt;\(\underline{0}_n\)&lt;/span&gt; ，意爲 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 維度的零向量。&lt;/li&gt;
&lt;li&gt;不是零向量的向量又被叫做，&lt;strong&gt;非零向量(non-zero vector, non-null vector)&lt;/strong&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;例如： 列向量：&lt;span class=&#34;math inline&#34;&gt;\(\underline{0}_3=\left( \begin{array}{c} 0\\ 0\\ 0\\ \end{array} \right)\)&lt;/span&gt;， 行向量：&lt;span class=&#34;math inline&#34;&gt;\(\underline{0}_3^t=(0,0,0)\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;向量-vector-with-all-elements-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; 向量 (vector with all elements 1)&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;當一個向量的全部成分都是數字 &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;，我們稱這個向量爲 &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; 向量。 &lt;span class=&#34;math inline&#34;&gt;\(\underline{1}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;這裏也需要注意與&lt;strong&gt;標量&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; 相區分。&lt;/li&gt;
&lt;li&gt;如果想要加注&lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;向量的維度，我們可以在右下角加上 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;：&lt;span class=&#34;math inline&#34;&gt;\(\underline{1}_n\)&lt;/span&gt; ，意爲 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 維度的&lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;向量。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;例如：列向量：&lt;span class=&#34;math inline&#34;&gt;\(\underline{1}_4=\left( \begin{array}{c} 1\\ 1\\ 1\\ 1 \end{array} \right)\)&lt;/span&gt;， 行向量：&lt;span class=&#34;math inline&#34;&gt;\(\underline{1}_4^t=(1,1,1,1)\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;第-i-基本向量&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;第 &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; 基本向量&lt;/h4&gt;

&lt;div class=&#34;theorem&#34;&gt;
&lt;span id=&#34;thm:unnamed-chunk-1&#34; class=&#34;theorem&#34;&gt;&lt;strong&gt;Theorem 1  (fundamental vector)  &lt;/strong&gt;&lt;/span&gt;&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 維度的向量，假如它的第 &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; 個成分是自然數 &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;，其他的成分全部都是 &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;， 我們稱這樣的向量爲&lt;strong&gt;第&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(\textbf{i}\)&lt;/span&gt; &lt;strong&gt;基本向量 (fundamental vector)&lt;/strong&gt;。寫作 &lt;span class=&#34;math inline&#34;&gt;\(\underline{\smash{e}}_i\)&lt;/span&gt;。
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;平時我們較少用到一個單獨的基本向量。大多情況下我們用的是由 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 個單獨向量組成的一組向量。這個類型的向量與坐標軸的關系緊密。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;例如：維度爲4的第 &lt;span class=&#34;math inline&#34;&gt;\(1\sim4\)&lt;/span&gt; 基本向量：&lt;span class=&#34;math inline&#34;&gt;\(\underline{e}_1=\left( \begin{array}{c} 1\\ 0\\ 0\\ 0 \end{array} \right), \; \underline{e}_2=\left( \begin{array}{c} 0\\ 1\\ 0\\ 0 \end{array} \right), \; \underline{e}_3=\left( \begin{array}{c} 0\\ 0\\ 1\\ 0 \end{array} \right), \; \underline{e}_4=\left( \begin{array}{c} 0\\ 0\\ 0\\ 1 \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;單位向量-unit-vector&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;單位向量 (unit vector)&lt;/h4&gt;

&lt;div class=&#34;theorem&#34;&gt;
&lt;span id=&#34;thm:unnamed-chunk-2&#34; class=&#34;theorem&#34;&gt;&lt;strong&gt;Theorem 2  (unit vector)  &lt;/strong&gt;&lt;/span&gt;求向量的各個成分平方和的正平方根，當結果爲 &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; 時，這個向量被稱作&lt;strong&gt;單位向量(unit vector)&lt;/strong&gt;。寫作： &lt;span class=&#34;math inline&#34;&gt;\(\underline{e}\)&lt;/span&gt;。
&lt;/div&gt;
&lt;p&gt;例如： 因爲 &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{(\frac{2}{3})^2+(-\frac{1}{3})^2+(\frac{2}{3})^2}=1\)&lt;/span&gt;，所以我們稱向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{e}=\left( \begin{array}{c} \frac{2}{3}\\ -\frac{1}{3}\\ \frac{2}{3}\\ \end{array} \right)\)&lt;/span&gt; 爲&lt;strong&gt;單位向量&lt;/strong&gt;。另外，&lt;span class=&#34;math inline&#34;&gt;\((\frac{1}{\sqrt{2}},-\frac{1}{\sqrt{2}},0)^t, \; (\frac{2}{\sqrt{6}},\frac{1}{\sqrt{6}},-\frac{1}{\sqrt{6}})^t\)&lt;/span&gt;，以及前一項的&lt;strong&gt;第 &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; 基本向量&lt;/strong&gt;，都是單位向量。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;向量的計算與相等&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;向量的計算，與相等&lt;/h2&gt;
&lt;div id=&#34;向量的和與差&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;向量的和與差&lt;/h4&gt;

&lt;div class=&#34;theorem&#34;&gt;
&lt;span id=&#34;thm:unnamed-chunk-3&#34; class=&#34;theorem&#34;&gt;&lt;strong&gt;Theorem 3  (vectorplus)  &lt;/strong&gt;&lt;/span&gt;類型(type)/成分，維度相同的向量之間的加減運算定義爲：相對應的成分之間的和或差。
&lt;/div&gt;
&lt;p&gt;例如：&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=\left( \begin{array}{c} a_1\\ a_2\\ \vdots\\ a_n \end{array} \right), \; \underline{b}=\left( \begin{array}{c} b_1\\ b_2\\ \vdots\\ b_n \end{array} \right)\)&lt;/span&gt;，則有： &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}\pm\underline{b}=\left( \begin{array}{c} a_1\\ a_2\\ \vdots\\ a_n \end{array} \right)\pm\left( \begin{array}{c} b_1\\ b_2\\ \vdots\\ b_n \end{array} \right)=\left( \begin{array}{c} a_1 \pm b_1\\ a_2 \pm b_2\\ \vdots\\ a_n \pm b_n \end{array} \right)\)&lt;/span&gt; &lt;strong&gt;複号同順&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}= (a_1,a_2,\cdots,a_n), \; \underline{b} = (b_1,b_2,\cdots,b_n)\)&lt;/span&gt;，則有： &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}\pm\underline{b}=(a_1,a_2,\cdots,a_n)\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+(b_1,b_2,\cdots,b_n)\\ \;\;\;\;\;\;\;\;\;\;=(a_1 \pm b_1, a_2 \pm b_2, \cdots, a_n \pm b_n)\)&lt;/span&gt; &lt;br&gt;&lt;strong&gt;複号同順&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;練習&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;練習&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=\left( \begin{array}{c} 6\\ 7\\ 8\\ \end{array} \right),\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\underline{b}=\left( \begin{array}{c} 1\\ 3\\ 5\\ \end{array} \right)\)&lt;/span&gt; 時，&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}+\underline{b} =\left( \begin{array}{c} 6+1\\ 7+3\\ 8+5\\ \end{array} \right)=\left( \begin{array}{c} 7\\ 10\\ 13\\ \end{array} \right)\)&lt;/span&gt; &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}-\underline{b}=\left( \begin{array}{c} 6-1\\ 7-3\\ 8-5\\ \end{array} \right)=\left( \begin{array}{c} 5\\ 4\\ 3\\ \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{c}=(6,0,9), \underline{d}=(7,-3,2)\)&lt;/span&gt; 時，&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{c}+\underline{d}=(6+7,0-3,9+2)=(13,-3,11)\)&lt;/span&gt; &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{c}-\underline{d}=(6-7,0-(-3),9-2)=(-1,3,7)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;向量的標量乘法scalar-multiplication&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;向量的標量乘法(scalar multiplication)&lt;/h4&gt;

&lt;div class=&#34;theorem&#34;&gt;
&lt;span id=&#34;thm:unnamed-chunk-4&#34; class=&#34;theorem&#34;&gt;&lt;strong&gt;Theorem 4  (scalar multiplication)  &lt;/strong&gt;&lt;/span&gt;向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}\)&lt;/span&gt; 的所有成分同時乘以標量 &lt;span class=&#34;math inline&#34;&gt;\((k)\)&lt;/span&gt; 以後的向量，我們稱爲 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}\)&lt;/span&gt; 的標量 &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; 倍。寫作： &lt;span class=&#34;math inline&#34;&gt;\(k\underline{a}\)&lt;/span&gt;。特別地，當 &lt;span class=&#34;math inline&#34;&gt;\(k=1\)&lt;/span&gt; 時，&lt;span class=&#34;math inline&#34;&gt;\(1\underline{a}=\underline{a}\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(k=-1\)&lt;/span&gt; 時，&lt;span class=&#34;math inline&#34;&gt;\((-1)\underline{a}=-\underline{a}\)&lt;/span&gt;。另外 &lt;span class=&#34;math inline&#34;&gt;\(k=0\)&lt;/span&gt; 時，&lt;span class=&#34;math inline&#34;&gt;\(0\underline{a}=\underline{0}\)&lt;/span&gt;。注意此時&lt;span class=&#34;math inline&#34;&gt;\(\underline{0}\)&lt;/span&gt;是與&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}\)&lt;/span&gt;同維度的零向量。不可寫作標量的 &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;。
&lt;span class=&#34;math display&#34;&gt;\[k\underline{a}=k\left(
\begin{array}{c}
a_1\\
a_2\\
\vdots\\
a_n
\end{array}
\right)=\left(
\begin{array}{c}
ka_1\\
ka_2\\
\vdots\\
ka_n
\end{array}
\right), \\k\underline{a}=k(a_1,a_2,\cdots,a_n)=(ka_1,ka_2,\cdots,ka_n)\]&lt;/span&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;練習-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;練習&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(k=5, l=\frac{1}{9}, \underline{a}=\left( \begin{array}{c} 3\\ 2\\ -7\\ \end{array} \right)\)&lt;/span&gt; 時，&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(k\underline{a}=5\left( \begin{array}{c} 3\\ 2\\ -7\\ \end{array} \right)=\left( \begin{array}{c} 5\times3\\ 5\times2\\ 5\times(-7)\\ \end{array} \right)=\left( \begin{array}{c} 15\\ 10\\ -35\\ \end{array} \right)\)&lt;/span&gt;, &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(l\underline{a}=\frac{1}{9}\left( \begin{array}{c} 3\\ 2\\ -7\\ \end{array} \right)=\left( \begin{array}{c} \frac{1}{9}\times3\\ \frac{1}{9}\times2\\ \frac{1}{9}\times(-7)\\ \end{array} \right)=\left( \begin{array}{c} \frac{1}{3}\\ \frac{2}{9}\\ -\frac{7}{9}\\ \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=\left( \begin{array}{c} 3\\ -2\\ 4\\ \end{array} \right), \underline{b}=\left( \begin{array}{c} 1\\ 1\\ -3\\ \end{array} \right), \underline{c}=\left( \begin{array}{c} 0\\ 5\\ 2\\ \end{array} \right)\)&lt;/span&gt; 時，&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(2\underline{a}-\underline{b}+3\underline{c}=\left( \begin{array}{c} 2\times3\\ 2\times(-2)\\ 2\times4\\ \end{array} \right)-\left( \begin{array}{c} 1\\ 1\\ -3\\ \end{array} \right)\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+\left( \begin{array}{c} 3\times0\\ 3\times5\\ 3\times2\\ \end{array} \right)=\left( \begin{array}{c} 6-1+0\\ -4-1+15\\ 8-(-3)+6\\ \end{array} \right)=\left( \begin{array}{c} 5\\ 10\\ 17\\ \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;向量相等-equal&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;向量相等 equal&lt;/h4&gt;

&lt;div class=&#34;theorem&#34;&gt;
&lt;span id=&#34;thm:unnamed-chunk-5&#34; class=&#34;theorem&#34;&gt;&lt;strong&gt;Theorem 5  (vectors equal)  &lt;/strong&gt;&lt;/span&gt;類型(type)/成分，維度相同的向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}, \underline{b}\)&lt;/span&gt;，其對應成分完全一致，我們就稱 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=\underline{b}\)&lt;/span&gt;，此時有 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}-\underline{b}=\underline{0}\)&lt;/span&gt; &lt;strong&gt;零向量&lt;/strong&gt;。
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;練習-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;練習：&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=\left( \begin{array}{c} a_1\\ a_2\\ a_3\\ \end{array} \right), \underline{b}=\left( \begin{array}{c} b_1\\ b_2\\ b_3\\ \end{array} \right)\)&lt;/span&gt; 如果相等，那麼 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=\underline{b}\)&lt;/span&gt;，&lt;br&gt;即：&lt;span class=&#34;math inline&#34;&gt;\(\begin{align} \left\{ \begin{array}{ll} a_1 = b_1 \\ a_2 = b_2 \\ a_3 = b_3 \end{array} \right. \end{align}\)&lt;/span&gt; 等價於：&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}-\underline{b}=0\)&lt;/span&gt;，或者&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\begin{align} \left\{ \begin{array}{ll} a_1 - b_1 =0\\ a_2 - b_2 =0\\ a_3 - b_3 =0 \end{array} \right. \end{align}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;向量等式：&lt;span class=&#34;math inline&#34;&gt;\(\left( \begin{array}{c} a_{11}x_1+a_{12}x_2+a_{13}x_3\\ a_{21}x_1+a_{22}x_2+a_{23}x_3\\ a_{31}x_1+a_{32}x_2+a_{33}x_3\\ \end{array} \right)=\left( \begin{array}{c} b_1\\ b_2\\ b_3\\ \end{array} \right)\)&lt;/span&gt; 等價於三個等式的連立方程：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\begin{align} \left\{ \begin{array}{ll} a_{11}x_1+a_{12}x_2+a_{13}x_3= b_1\\ a_{21}x_1+a_{22}x_2+a_{23}x_3= b_2\\ a_{31}x_1+a_{32}x_2+a_{33}x_3= b_3 \end{array} \right. \end{align}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;求滿足 &lt;span class=&#34;math inline&#34;&gt;\(5\left( \begin{array}{c} a_1\\ a_2\\ a_3\\ \end{array} \right)+\left( \begin{array}{c} 2\\ 5\\ -1\\ \end{array} \right)=\left( \begin{array}{c} 12\\ 25\\ 29\\ \end{array} \right)\)&lt;/span&gt; 的向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=\left( \begin{array}{c} a_1\\ a_2\\ a_3\\ \end{array} \right)\)&lt;/span&gt;。&lt;br&gt;
解：&lt;span class=&#34;math inline&#34;&gt;\(5\left( \begin{array}{c} a_1\\ a_2\\ a_3\\ \end{array} \right)=\left( \begin{array}{c} 12\\ 25\\ 29\\ \end{array} \right)-\left( \begin{array}{c} 2\\ 5\\ -1\\ \end{array} \right)=\left( \begin{array}{c} 10\\ 20\\ 30\\ \end{array} \right)\)&lt;/span&gt;&lt;br&gt;
因此，&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=\left( \begin{array}{c} a_1\\ a_2\\ a_3\\ \end{array} \right)=\left( \begin{array}{c} 2\\ 4\\ 6\\ \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記8</title>
      <link>https://wangcc.me/post/2017-02-17/</link>
      <pubDate>Fri, 17 Feb 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/2017-02-17/</guid>
      <description>
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;向量-vector&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;向量 vector&lt;/h2&gt;
&lt;div id=&#34;列向量-column-vector&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;列向量 column vector&lt;/h3&gt;
&lt;p&gt;在等號的右側，將數字寫成一列，左右用圓括號或者方括號包含在內的形式，被叫做列向量(column vector)：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=\left( \begin{array}{c} a_1\\ a_2\\ \vdots\\ a_i\\ \vdots\\ a_n \end{array} \right), \;\; \textbf{a}=\left[ \begin{array}{c} a_1\\ a_2\\ \vdots\\ a_i\\ \vdots\\ a_n \end{array} \right]\)&lt;/span&gt;&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;我們接下來將會繼續定義，向量的加減法，標量乘法(scalar multiplication)。把上述的向量用一個文字表示的時候，通常會記爲下劃線 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}\)&lt;/span&gt;，或者是加粗的小寫字母： &lt;span class=&#34;math inline&#34;&gt;\(\bf{a}\)&lt;/span&gt;。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;構成向量的各個數字，被命名爲&lt;strong&gt;成分(component, element, entry)&lt;/strong&gt;，從上往下第 &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; 個成分稱爲第 &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; 成分。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;成分的個數爲 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;，就被稱爲這個向量具有 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 個&lt;strong&gt;維度(次元，dimension)&lt;/strong&gt;，或者說這個向量的維度爲 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;。成分可以是數字，也可以是函數，或者式子。如果兩個列向量的維度一致，我們稱這兩個列向量的&lt;strong&gt;型(size, order)&lt;/strong&gt;,或者 &lt;strong&gt;類型(type)&lt;/strong&gt; 一致。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;成分只有一個的向量，被特別稱爲&lt;strong&gt;標量(scalar)&lt;/strong&gt;，原則上不加括號。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;將向量成分全部羅列出來，寫成上面的形式的過程，被稱爲&lt;strong&gt;成分表示&lt;/strong&gt;。在多元變量分析中，我們說到向量，多默認指的就是列向量。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{b}=\left( \begin{array}{c} 16\\ 59\\ 80\\ \end{array} \right)=\left[ \begin{array}{c} 16\\ 59\\ 80\\ \end{array} \right]=\textbf{b}\)&lt;/span&gt;&lt;br&gt;今後我們都用字母帶下劃線，圓括號包含數字的方式表示向量。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{c}=\left( \begin{array}{c} \sin t+\cos t\\ \cos t+\tan t-2\\ \tan t + \sin t\\ \end{array} \right)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;當 &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; 爲 &lt;span class=&#34;math inline&#34;&gt;\(a_1,a_2,a_3\)&lt;/span&gt; 的函數時，寫作 &lt;span class=&#34;math inline&#34;&gt;\(F(a_1,a_2,a_3)\)&lt;/span&gt;。 以&lt;strong&gt;三個未知數的偏微分&lt;/strong&gt;爲成分的向量(梯度向量，gradient vector)，寫成下面等式左邊的形式。可以簡略寫作: &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=\left( \begin{array}{c} a_1\\ a_2\\ a_3\\ \end{array} \right)\)&lt;/span&gt;。&lt;span class=&#34;math inline&#34;&gt;\(\nabla\)&lt;/span&gt;讀作&lt;code&gt;nabla&lt;/code&gt;。&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\left( \begin{array}{c} \frac{\partial F}{\partial a_1}\\ \frac{\partial F}{\partial a_2}\\ \frac{\partial F}{\partial a_3}\\ \end{array} \right)=\frac{\partial F}{\partial \underline{a}}=\nabla_{\underline{a}}F\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;橫向量行向量-row-vector&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;橫向量(行向量) row vector&lt;/h2&gt;
&lt;p&gt;在等號的右側，將數字寫成一行，左右用圓括號或者方括號包含在內的形式，被叫做橫向量(row vector):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=(a_1,a_2,\cdots,a_j,\cdots,a_n), \; \textbf{a}=[a_1,a_2,\cdots,a_j,\cdots,a_n]\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;成分，維度，類型等的定義與列向量相同。另外注意，維度相同，但是一個是橫向量，一個是列向量的話，這兩個向量是不同類型的。&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}=(x_1,x_2,x_3)\)&lt;/span&gt;&lt;br&gt;
有時也可以不用逗號分隔成分。 寫作 &lt;span class=&#34;math inline&#34;&gt;\((x_1 \; x_2 \;x_3)\)&lt;/span&gt;。下同。&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((\frac{\partial F}{\partial x_1},\frac{\partial F}{\partial x_2},\frac{\partial F}{\partial x_3})=\frac{\partial F}{\partial \underline{x}}=\nabla_{\underline{x}F}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{u}=(\sin\theta\cos\phi, \sin\theta\cos\theta, \cos\theta)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;向量的轉置-vector-transpose&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;向量的轉置 (vector transpose)&lt;/h2&gt;
&lt;p&gt;將列向量的每個成分，按照從上到下的順序，一字橫着排開寫成橫向量。這個向量稱爲原來列向量的轉置向量(transposed vector)。反之亦然。&lt;/p&gt;
&lt;p&gt;向量 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}\)&lt;/span&gt; 的轉置向量，可以標記爲 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}^t,\;\underline{a}^\prime,\;^t\underline{a},\;\underline{a}^T, \;^T\underline{a}\)&lt;/span&gt; 各種形式。今後統一用 &lt;span class=&#34;math inline&#34;&gt;\(\underline{a}^t\)&lt;/span&gt;。&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}=\left( \begin{array}{c} a_1\\ a_2\\ a_3\\ \end{array} \right)\)&lt;/span&gt; 的轉置向量我們會記爲：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{a}^t=\left( \begin{array}{c} a_1\\ a_2\\ a_3\\ \end{array} \right)^t=(a_1,a_2,a_3)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}=(x_1.x_2,x_3)\)&lt;/span&gt; 的轉置向量我們會記爲：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\underline{x}^t=(x_1.x_2,x_3)^t=\left( \begin{array}{c} x_1\\ x_2\\ x_3\\ \end{array} \right)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記7</title>
      <link>https://wangcc.me/post/2017-02-16/</link>
      <pubDate>Thu, 16 Feb 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/2017-02-16/</guid>
      <description>
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;分解平方和-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;分解平方和 1&lt;/h2&gt;
&lt;p&gt;樣本量均爲 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 的兩變量 &lt;span class=&#34;math inline&#34;&gt;\(z, \hat{z}\)&lt;/span&gt; 如下表，已知這兩個變量滿足條件：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\bar{z}=\frac{1}{n}\sum\limits_{i=1}^nz_i=\frac{1}{n}\sum\limits_{i=1}^n\hat{z}_i=\bar{\hat{z}},\)&lt;/span&gt; &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\sum\limits_{i=1}^n(z_i-\hat{z_i})(\hat{z_i}-\bar{z})=0\)&lt;/span&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;個体の番号&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;変量 &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;変量 &lt;span class=&#34;math inline&#34;&gt;\(\hat{z}\)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(z_1\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{z}_1\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(z_2\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{z}_2\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\vdots\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\vdots\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\vdots\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(z_i\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{z}_i\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\vdots\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\vdots\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\vdots\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(z_n\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{z}_n\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;此時我們有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;全平方和&lt;/strong&gt;(全変動，總平方和，總變動， &lt;strong&gt;Total sum of Squares&lt;/strong&gt;)：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(S_T=(z_i-\bar{z})^2+(z_2-\bar{z})^2+\cdots+(z_n-\bar{z})^2\\ \;\;\;\;=\sum\limits_{i=1}^n(z_i-\bar{z})^2\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;回歸平方和&lt;/strong&gt;(回歸變動，&lt;strong&gt;Regression sum of Squares&lt;/strong&gt;)&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(S_R=(\hat{z_1}-\bar{\hat{z}})^2+(\hat{z_2}-\bar{\hat{z}})^2+\cdots+(\hat{z_n}-\bar{\hat{z}})^2\\ \;\;\;\;=\sum\limits_{i=1}^n(\hat{z_i}-\bar{\hat{z}})^2=\sum\limits_{i=1}^n(\hat{z_i}-\bar{z})^2\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;殘差平方和&lt;/strong&gt;(誤差平方和，殘差變動，誤差變動，&lt;strong&gt;residual sum of Squares&lt;/strong&gt;)&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(S_e=(z_1-\hat{z_1})^2+(z_2-\hat{z_2})^2+\cdots+(z_n-\hat{z_n})^2\\ \;\;\;\;=\sum\limits_{i=1}^n(z_i-\hat{z_i})^2\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;上面三個平方和之間，有如下的關系：
&lt;span class=&#34;math display&#34; id=&#34;eq:Sumofsquares&#34;&gt;\[\begin{equation}
  S_T=S_R+S_e
  \tag{1}
  \end{equation}\]&lt;/span&gt;
&lt;br&gt;
既：全平方和等於殘差平方和與回歸平方和之和。&lt;a href=&#34;#eq:Sumofsquares&#34;&gt;(1)&lt;/a&gt;式被稱爲&lt;strong&gt;平方和的分解(decomposition of sum of squares)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;證明refeqsumofsquares式&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;證明&lt;a href=&#34;#eq:Sumofsquares&#34;&gt;(1)&lt;/a&gt;式&lt;/h5&gt;
&lt;/div&gt;
&lt;div id=&#34;解&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;解：&lt;/h5&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{equation}
\begin{split}
S_T &amp;amp; = \sum\limits_{i=1}^n(z_i-\bar{z})^2 \\
&amp;amp; = \sum\limits_{i=1}^n\left\{(z_i-\hat{z_i})+(\hat{z_i}-\bar{z})\right\}^2\\
&amp;amp; = \sum\limits_{i=1}^n\left\{(z_i-\hat{z_i})^2+(\hat{z_i}-\bar{z})^2+2(z_i-\hat{z_i})(\hat{z_i}-\bar{z})\right\}\\
&amp;amp; = \sum\limits_{i=1}^n(z_i-\hat{z_i})^2+\sum\limits_{i=1}^n(\hat{z_i}-\bar{z})^2 + 0\\
&amp;amp; = S_e + S_R
\end{split}
\end{equation}
\]&lt;/span&gt;
最後一步等式，利用了一開始給出的條件 &lt;span class=&#34;math inline&#34;&gt;\(\sum\limits_{i=1}^n(z_i-\hat{z_i})(\hat{z_i}-\bar{z})=0\)&lt;/span&gt;&lt;br&gt;
這裏的平方和分解與&lt;strong&gt;回歸分析&lt;/strong&gt;有着緊密的聯系。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;分解平方和-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;分解平方和 2&lt;/h2&gt;
&lt;p&gt;有樣本量爲 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 的變量 &lt;span class=&#34;math inline&#34;&gt;\(z_1\)&lt;/span&gt; 與樣本量爲 &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; 的變量 &lt;span class=&#34;math inline&#34;&gt;\(z_2\)&lt;/span&gt; 的數據如下表：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;変量 &lt;span class=&#34;math inline&#34;&gt;\(z_1\)&lt;/span&gt;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;変量 &lt;span class=&#34;math inline&#34;&gt;\(z_2\)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(z_{11}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(z_{12}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(z_{21}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(z_{22}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\vdots\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\vdots\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(z_{i1}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(z_{i2}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\vdots\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\vdots\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(z_{n1}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\vdots\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(z_{m2}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;此時我們有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;樣本&lt;strong&gt;平均值&lt;/strong&gt;： &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\bar{z_1}=\frac{1}{n}\sum\limits_{i=1}^nz_{i1}, \;\bar{z_2}=\frac{1}{m}\sum\limits_{i=1}^mz_{i2}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;樣本&lt;strong&gt;總平均值&lt;/strong&gt;： &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\bar{z}=\frac{1}{n+m}(\sum\limits_{i=1}^nz_{i1}+\sum\limits_{i=1}^mz_{i2})\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;全&lt;strong&gt;平方和&lt;/strong&gt; (全変動，總平方和，總變動, &lt;strong&gt;Total sum of Squares&lt;/strong&gt;)：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(S_T=\left\{(z_{11}-\bar{z})^2+(z_{21}-\bar{z})^2+\cdots+(z_{n1}-\bar{z})^2\right\}\\ \;\;\;\;\;\;\;\;+\left\{(z_{12}-\bar{z})^2+(z_{22}-\bar{z})^2+\cdots+(z_{m2}-\bar{z})^2\right\}\\ \;\;\;\;=\sum\limits_{i=1}^n(z_{i1}-\bar{z})^2+\sum\limits_{i=1}^m(z_{i2}-\bar{z})^2\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;羣內平方和&lt;/strong&gt;(組內平方和，級內平方和，羣內變動，級內變動，變量內平方和，變量內變動，&lt;strong&gt;Within-groups sum of Squares&lt;/strong&gt;)：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(S_W=\left\{(z_{11}-\bar{z_1})^2+(z_{21}-\bar{z_1})^2+\cdots+(z_{n1}-\bar{z_1})^2\right\}\\ \;\;\;\;\;\;\;\;+\left\{(z_{12}-\bar{z_2})^2+(z_{22}-\bar{z_2})^2+\cdots+(z_{m2}-\bar{z_2})^2\right\}\\ \;\;\;\;=\sum\limits_{i=1}^n(z_{i1}-\bar{z_1})^2+\sum\limits_{i=1}^m(z_{i2}-\bar{z_2})^2\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;羣間平方和&lt;/strong&gt;(組間平方和，級間平方和，羣間變動，級間變動，變量間平方和，變量間變動，&lt;strong&gt;Between-groups sum of Squares&lt;/strong&gt;)：&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(S_B=\left\{(\bar{z_1}-\bar{z})^2+(\bar{z_1}-\bar{z})^2+\cdots+(\bar{z_1}-\bar{z})^2\right\}\\ \;\;\;\;\;\;\;\;+\left\{(\bar{z_2}-\bar{z})^2+(\bar{z_2}-\bar{z})^2+\cdots+(\bar{z_2}-\bar{z})^2\right\}\\ \;\;\;\;=\sum\limits_{i=1}^n(\bar{z_1}-\bar{z})^2+\sum\limits_{i=1}^m(\bar{z_2}-\bar{z})^2\\ \;\;\;\;=n(\bar{z_1}-\bar{z})^2+m(\bar{z_2}-\bar{z})^2\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;上面三個平方和之間有如下的關系：
&lt;span class=&#34;math display&#34; id=&#34;eq:Sumofsqua&#34;&gt;\[\begin{equation}
S_T=S_W+S_B
\tag{2}
\end{equation}\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;證明refeqsumofsqua式&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;證明&lt;a href=&#34;#eq:Sumofsqua&#34;&gt;(2)&lt;/a&gt;式&lt;/h5&gt;
&lt;/div&gt;
&lt;div id=&#34;解-1&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;解：&lt;/h5&gt;
&lt;p&gt;注意利用：&lt;span class=&#34;math inline&#34;&gt;\(\sum\limits_{i=1}^n(z_{i1}-\bar{z_1})(\bar{z_1}-\bar{z})=(\bar{z_1}-\bar{z})\sum\limits_{i=1}^n(z_{i1}-\bar{z_1})\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\:=(\bar{z_1}-\bar{z})(\sum\limits_{i=1}^nz_{i1}-n\bar{z_1})\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\:=(\bar{z_1}-\bar{z})(n\bar{z_1}-n\bar{z_1})=0\)&lt;/span&gt;&lt;br&gt;因此&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{equation}
\begin{split}
S_T &amp;amp; = \sum_{i=1}^n(z_{i1}-\bar{z})^2+\sum_{i=1}^m(z_{i2}-\bar{z})^2\\
&amp;amp; = \sum_{i=1}^n\left\{(z_{i1}-\bar{z_1})+(\bar{z_1}-\bar{z})\right\}^2\\
&amp;amp;\;\;\;\;\; + \sum_{i=1}^m\left\{(z_{i2}-\bar{z_2})+(\bar{z_2}-\bar{z})\right\}^2\\
&amp;amp; = \sum_{i=1}^n\left\{(z_{i1}-\bar{z_1})^2+2(z_{i1}-\bar{z_1})(\bar{z_1}-\bar{z})+(\bar{z_1}-\bar{z})^2\right\}\\
&amp;amp;\;\;\;\;\; +\sum_{i=1}^m\left\{(z_{i2}-\bar{z_2})^2+2(z_{i2}-\bar{z_2})(\bar{z_2}-\bar{z})+(\bar{z_2}-\bar{z})^2\right\}\\
&amp;amp; = \sum_{i=1}^n(z_{i1}-\bar{z_1})^2 + n(\bar{z_1}-\bar{z})^2\\
&amp;amp;\;\;\;\;\; + \sum_{i=1}^m(z_{i2}-\bar{z_2})^2 + m(\bar{z_2}-\bar{z})^2\\
&amp;amp; = S_W+S_B
\end{split}
\end{equation}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;變量的合成與加權&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;變量的合成與加權&lt;/h2&gt;
&lt;p&gt;我們說，將 &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; 個變量 &lt;span class=&#34;math inline&#34;&gt;\(x_1,x_2,\cdots,x_p\)&lt;/span&gt; 轉變成一次式：&lt;span class=&#34;math inline&#34;&gt;\(w_1x_1+w_2x_2+\cdots+w_px_p (=\hat{y})\)&lt;/span&gt; 的過程稱爲變量的合成 &lt;strong&gt;(linear combination of variables)&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(\hat{y}\)&lt;/span&gt; 被叫做&lt;strong&gt;合成變量&lt;/strong&gt;。系數 &lt;span class=&#34;math inline&#34;&gt;\(w_1,w_2,\cdots,w_p\)&lt;/span&gt; 被叫做&lt;strong&gt;權重 (weight)&lt;/strong&gt;。假如 &lt;span class=&#34;math inline&#34;&gt;\(x_1,x_2,\cdots,x_p\)&lt;/span&gt; 是 &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; 個科目的考試得分，那麼:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(w_1=w_2=\cdots=w_p=1\)&lt;/span&gt; 時，&lt;span class=&#34;math inline&#34;&gt;\(\hat{y}\)&lt;/span&gt; 意思就是 &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; 個科目的總分&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(w_1=w_2=\cdots=w_p=\frac{1}{p}\)&lt;/span&gt; 時，&lt;span class=&#34;math inline&#34;&gt;\(\hat{y}\)&lt;/span&gt; 意思就是 &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; 個科目的平均分&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;多元變量分析中，我們實質上做的許多事就是思考如何合理的決定這個&lt;strong&gt;權重&lt;/strong&gt;。&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記6</title>
      <link>https://wangcc.me/post/2017-02-15/</link>
      <pubDate>Wed, 15 Feb 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/2017-02-15/</guid>
      <description>
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;數據的變換&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;數據的變換&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;平均值附近的偏差:
&lt;ul&gt;
&lt;li&gt;各個數值 &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; 與樣本平均值 &lt;span class=&#34;math inline&#34;&gt;\(\bar{x}\)&lt;/span&gt; 的差 &lt;span class=&#34;math display&#34;&gt;\[x_i^\prime=x_i-\bar{x} (i = 1,2,\cdots,n)\]&lt;/span&gt; &lt;br&gt;
稱爲數據 &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; 在它的平均值 &lt;span class=&#34;math inline&#34;&gt;\(\bar{x}\)&lt;/span&gt; 附近的&lt;strong&gt;偏差(deviation)&lt;/strong&gt;。通常我們說&lt;strong&gt;求偏差&lt;/strong&gt;，指的是，對數據 &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; 進行&lt;strong&gt;偏差轉換&lt;/strong&gt;。這個過程又被稱作是&lt;strong&gt;中心變換(centering)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;關於偏差，我們列舉如下兩個有特徵的的&lt;strong&gt;概括統計&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;樣本平均值：
&lt;span class=&#34;math display&#34; id=&#34;eq:samplemean&#34;&gt;\[\begin{equation}
   \bar{x}^\prime=\frac{1}{n}\sum_{i=1}^nx_i^\prime=0
   \tag{1}
   \end{equation}\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;樣本偏差平方和：
&lt;span class=&#34;math display&#34; id=&#34;eq:SSprime&#34;&gt;\[\begin{equation}
  SS^\prime=\sum_{i=1}^n(x^\prime)^2=SS
  \tag{2}
  \end{equation}\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;練習證明refeqsamplemean&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;練習：證明&lt;a href=&#34;#eq:samplemean&#34;&gt;(1)&lt;/a&gt;&lt;/h4&gt;
&lt;/div&gt;
&lt;div id=&#34;解&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;解：&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;證明&lt;a href=&#34;#eq:samplemean&#34;&gt;(1)&lt;/a&gt;&lt;/strong&gt;:
&lt;span class=&#34;math display&#34;&gt;\[\bar{x}^\prime=\frac{\sum\limits_{i=1}^n(x_i-\bar{x})}{n}\\
\;\;\;=\frac{\sum\limits_{i=1}^nx_i-n\bar{x}}{n}\\
\;\;\;=\frac{\sum\limits_{i=1}^nx_i}{n}-\bar{x}\\
\;\;\;=\bar{x}-\bar{x}=0\]&lt;/span&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;數據的標準化：
&lt;ul&gt;
&lt;li&gt;將數據 &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; 的平均值 &lt;span class=&#34;math inline&#34;&gt;\(\bar{x}\)&lt;/span&gt; 附近的偏差除以樣本標準偏差 &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; 從而獲得下面式子所表示的數據 &lt;span class=&#34;math inline&#34;&gt;\(z_i\)&lt;/span&gt; 的過程，被叫做&lt;strong&gt;數據的標準化 (standardization)&lt;/strong&gt;：
&lt;span class=&#34;math display&#34; id=&#34;eq:standardization&#34;&gt;\[\begin{equation}
 z_i=\frac{x_i-\bar{x}}{s}
 \tag{3}
 \end{equation}\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;標準化後的數據 &lt;span class=&#34;math inline&#34;&gt;\(z_i\)&lt;/span&gt; 的&lt;strong&gt;概括統計&lt;/strong&gt;有下列特徵：
&lt;ul&gt;
&lt;li&gt;樣本平均值：
&lt;span class=&#34;math display&#34; id=&#34;eq:samplemeans&#34;&gt;\[\begin{equation}
  \bar{z}=\frac{1}{n}\sum_{i=1}^nz_i=0
  \tag{4}
  \end{equation}\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;樣本方差:
&lt;span class=&#34;math display&#34; id=&#34;eq:Ssquare&#34;&gt;\[\begin{equation}
  s_{z}^2=\frac{1}{n}\sum_{i=1}^nz_i^2=1
  \tag{5}
  \end{equation}\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;由於標準化數據具有上述兩個非常顯著的特徵，均值爲 &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;，方差爲 &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;，因此我們實際分析數據過程中常常對數據進行標準化。標準化以後的數據，單位消失，變成了一組&lt;strong&gt;無名數&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\divideontimes\)&lt;/span&gt; 數據的標準化，有時你會看到被定義爲:
&lt;span class=&#34;math display&#34; id=&#34;eq:newstandardization&#34;&gt;\[\begin{equation}
 z_i=\frac{x_i-\bar{x}}{u}
 \tag{6}
 \end{equation}\]&lt;/span&gt; &lt;br&gt;
此時的不偏樣本方差爲：
&lt;span class=&#34;math display&#34; id=&#34;eq:unbsamplevar&#34;&gt;\[\begin{equation}
 u_z^2=\frac{1}{n-1}\sum_{i=1}{n}z_i^2=1
 \tag{7}
 \end{equation}\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;變量數據的概括統計&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2變量數據的概括統計：&lt;/h2&gt;
&lt;div id=&#34;樣本量同爲-n-的-2-變量-x_1x_2-的數據表示爲如下表格&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;樣本量同爲 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 的 &lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt; 變量 &lt;span class=&#34;math inline&#34;&gt;\(x_1,x_2\)&lt;/span&gt; 的數據，表示爲如下表格：&lt;/h5&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;個体の番号&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;変量 &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;変量 &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_{11}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_{12}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_{21}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_{22}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\vdots\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\vdots\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\vdots\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_{i1}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_{i2}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\vdots\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\vdots\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\vdots\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_{n1}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_{n2}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;按照變量-x_1x_2-各自的定義&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;按照變量 &lt;span class=&#34;math inline&#34;&gt;\(x_1,x_2\)&lt;/span&gt; 各自的定義：&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;樣本&lt;strong&gt;平均值&lt;/strong&gt;：&lt;span class=&#34;math inline&#34;&gt;\(\bar{x_1}=\frac{1}{n}\sum\limits_{i=1}^nx_{i1}, \; \bar{x_2}=\frac{1}{n}\sum\limits_{i=1}^nx_{i2}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;樣本&lt;strong&gt;偏差平方和&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(SS_1=\sum\limits_{i=1}^n(x_{i1}-\bar{x_1})^2, \; SS_2=\sum\limits_{i=1}^n(x_{i2}-\bar{x_2})^2\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;樣本&lt;strong&gt;方差&lt;/strong&gt;： &lt;span class=&#34;math inline&#34;&gt;\(s_1^2=\frac{SS_1}{n}, \; s_2^2=\frac{SS_2}{n}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;樣本&lt;strong&gt;標準偏差&lt;/strong&gt;： &lt;span class=&#34;math inline&#34;&gt;\(s_1=\sqrt{s_1^2}, \; s_2=\sqrt{s_2^2}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;不偏樣本方差&lt;/strong&gt;： &lt;span class=&#34;math inline&#34;&gt;\(u_1^2=\frac{SS}{n-1}, \; u_2^2=\frac{SS_2}{n-1}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;不偏樣本方差平方根&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(u_1=\sqrt{u_1^2}, \; u_2=\sqrt{u_2^2}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;對於這樣一對變量-x_1x_2-來說我們又可以追加如下的概括統計&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;對於這樣一對變量 &lt;span class=&#34;math inline&#34;&gt;\(x_1,x_2\)&lt;/span&gt; 來說，我們又可以追加如下的概括統計：&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;樣本&lt;strong&gt;總體平均值&lt;/strong&gt;： &lt;span class=&#34;math inline&#34;&gt;\(\bar{x}=\frac{1}{n+n}(\sum\limits_{i-1}^nx_{i1}+\sum\limits_{i-1}^nx_{i2})\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;樣本&lt;strong&gt;方差積和(cross-product)&lt;/strong&gt;：
&lt;span class=&#34;math display&#34; id=&#34;eq:crossproduct&#34;&gt;\[\begin{equation}
\begin{split}
S_{12} &amp;amp; = \sum_{i=1}^n(x_{i1}-\bar{x_1})\cdot(x_{i2}-\bar{x_2})\\
&amp;amp; = \sum_{i=1}^n(x_{i1}x_{i2}-\bar{x_1}x_{i2}-x_{i1}\bar{x_2}+\bar{x_1}\bar{x_2})\\
&amp;amp; = \sum_{i=1}^nx_{i1}x_{i2}-\bar{x_1}\sum_{i=1}^nx_{i2}-{\sum_{i=1}^nx_{i1}}\bar{x_2}+n\bar{x_1}\bar{x_2}\\
&amp;amp; = \sum_{i=1}^nx_{i1}x_{i2}-\bar{x_1}\cdot n\bar{x_2}-n\bar{x_1}\cdot\bar{x_2}+n\bar{x_1}\bar{x_2}\\
&amp;amp; = \sum_{i=1}^nx_{i1}x_{i2}-n\cdot\bar{x_1}\cdot\bar{x_2}\\
&amp;amp; = \sum_{i=1}^nx_{i1}x_{i2}-n\cdot\frac{\sum\limits_{i=1}^nx_{i1}}{n}\cdot\frac{\sum\limits_{i=1}^nx_{i2}}{n}\\
&amp;amp; = \sum_{i=1}^nx_{i1}x_{i2}-\frac{1}{n}(\sum_{i=1}^nx_{i1})(\sum_{i=1}^nx_{i2}) = S_{21}
\end{split}
\tag{8}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;樣本&lt;strong&gt;協方差(covariance，共分散)&lt;/strong&gt;：&lt;span class=&#34;math inline&#34;&gt;\(s_{12}=\frac{S_{12}}{n}=\frac{S_{21}}{n}=s_{21}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;樣本&lt;strong&gt;相關系數 (correlation coefficient)&lt;/strong&gt;： &lt;span class=&#34;math inline&#34;&gt;\(r_{11}=r_{22}=1\)&lt;/span&gt;
&lt;span class=&#34;math display&#34; id=&#34;eq:correlation&#34;&gt;\[\begin{equation}
\begin{split}
r_{12} &amp;amp; = \frac{S_{12}}{\sqrt{SS_1\cdot SS_2}}\\
&amp;amp; = \frac{\frac{S_{12}}{n}}{\sqrt{\frac{SS_1}{n}}\cdot\sqrt{\frac{SS_2}{n}}}\\
&amp;amp; = \frac{s_{12}}{s_1s_2}=r_{21}\\
\end{split}
\tag{9}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;此處我們再來證明一下標準化以後的數據的樣本協方差covariance和標準化以前原來的數據的樣本相關系數correlation-coefficient是相等的&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;此處我們再來證明一下，標準化以後的數據的樣本協方差(covariance)，和標準化以前原來的數據的樣本相關系數(correlation coefficient)是相等的：&lt;/h5&gt;
&lt;p&gt;假設，&lt;span class=&#34;math inline&#34;&gt;\(x_{i1}\)&lt;/span&gt; 標準化以後爲 &lt;span class=&#34;math inline&#34;&gt;\(z_{i1}=\frac{x_{i1}-\bar{x_1}}{s_1}\)&lt;/span&gt;； &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(x_{i2}\)&lt;/span&gt; 標準化以後爲 &lt;span class=&#34;math inline&#34;&gt;\(z_{i2}=\frac{x_{i2}-\bar{x_2}}{s_1}\)&lt;/span&gt;。 &lt;br&gt;
此時，&lt;span class=&#34;math inline&#34;&gt;\(z_{i1}, z_{i2}\)&lt;/span&gt; 的樣本協方差可以計算如下:
&lt;span class=&#34;math display&#34; id=&#34;eq:stcorrelation&#34;&gt;\[\begin{equation}
\begin{split}
s_{z_{12}} &amp;amp; = \frac{S_{z_{12}}}{n} \\
&amp;amp; = \frac{1}{n}\cdot\sum_{i=1}^n(z_{i1}-\bar{z_1})(z_{i2}-\bar{z_2})\\
&amp;amp; = \frac{1}{n}\sum_{i=1}^nz_{i1}z_{i2}\\
&amp;amp; = \frac{1}{n}\sum_{i=1}^n(\frac{x_{i1}-\bar{x_1}}{s_1})(\frac{x_{i2}-\bar{x_2}}{s_2})\\
&amp;amp; = \frac{S_{12}}{n}\cdot\frac{1}{s_1s_2} = \frac{s_{12}}{s_1s_2}=r_{12}
\end{split}
\tag{10}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記5</title>
      <link>https://wangcc.me/post/2017-02-13/</link>
      <pubDate>Tue, 14 Feb 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/2017-02-13/</guid>
      <description>
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;ul&gt;
&lt;li&gt;2017-02-15 &lt;strong&gt;updated.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;數據的種類和尺度&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;數據的種類和尺度&lt;/h2&gt;
&lt;table class=&#34;gmisc_table&#34; style=&#34;border-collapse: collapse; margin-top: 1em; margin-bottom: 1em;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;td colspan=&#34;14&#34; style=&#34;text-align: left;&#34;&gt;
表1. 20歳の若者9名のデータ
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th colspan=&#34;1&#34; style=&#34;font-weight: 900; border-top: 2px solid grey; text-align: center;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;border-top: 2px solid grey;; border-bottom: hidden;&#34;&gt;
 
&lt;/th&gt;
&lt;th colspan=&#34;2&#34; style=&#34;font-weight: 900; border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: center;&#34;&gt;
性別
&lt;/th&gt;
&lt;th style=&#34;border-top: 2px solid grey;; border-bottom: hidden;&#34;&gt;
 
&lt;/th&gt;
&lt;th colspan=&#34;5&#34; style=&#34;font-weight: 900; border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: center;&#34;&gt;
健康状態
&lt;/th&gt;
&lt;th style=&#34;border-top: 2px solid grey;; border-bottom: hidden;&#34;&gt;
 
&lt;/th&gt;
&lt;th colspan=&#34;1&#34; style=&#34;font-weight: 900; border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: center;&#34;&gt;
体温
&lt;/th&gt;
&lt;th style=&#34;border-top: 2px solid grey;; border-bottom: hidden;&#34;&gt;
 
&lt;/th&gt;
&lt;th colspan=&#34;1&#34; style=&#34;font-weight: 900; border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: center;&#34;&gt;
身長
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; text-align: center;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey;&#34; colspan=&#34;1&#34;&gt;
 
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; text-align: center;&#34;&gt;
男
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; text-align: center;&#34;&gt;
女
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey;&#34; colspan=&#34;1&#34;&gt;
 
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; text-align: center;&#34;&gt;
極良
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; text-align: center;&#34;&gt;
良好
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; text-align: center;&#34;&gt;
普通
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; text-align: center;&#34;&gt;
不良
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; text-align: center;&#34;&gt;
極悪
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey;&#34; colspan=&#34;1&#34;&gt;
 
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; text-align: center;&#34;&gt;
°C
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey;&#34; colspan=&#34;1&#34;&gt;
 
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; text-align: center;&#34;&gt;
cm
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
36.9
&lt;/td&gt;
&lt;td style colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
155
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
2
&lt;/td&gt;
&lt;td style colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
36.5
&lt;/td&gt;
&lt;td style colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
190
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
3
&lt;/td&gt;
&lt;td style colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
36.7
&lt;/td&gt;
&lt;td style colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
165
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
4
&lt;/td&gt;
&lt;td style colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
39
&lt;/td&gt;
&lt;td style colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
155
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
5
&lt;/td&gt;
&lt;td style colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
38.1
&lt;/td&gt;
&lt;td style colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
167
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
6
&lt;/td&gt;
&lt;td style colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
36.2
&lt;/td&gt;
&lt;td style colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
180
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
7
&lt;/td&gt;
&lt;td style colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
36.6
&lt;/td&gt;
&lt;td style colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
178
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
8
&lt;/td&gt;
&lt;td style colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
36.7
&lt;/td&gt;
&lt;td style colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;text-align: center;&#34;&gt;
170
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;border-bottom: 2px solid grey; text-align: center;&#34;&gt;
9
&lt;/td&gt;
&lt;td style=&#34;border-bottom: 2px solid grey;&#34; colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;border-bottom: 2px solid grey; text-align: center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;border-bottom: 2px solid grey; text-align: center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;border-bottom: 2px solid grey;&#34; colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;border-bottom: 2px solid grey; text-align: center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;border-bottom: 2px solid grey; text-align: center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;border-bottom: 2px solid grey; text-align: center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;border-bottom: 2px solid grey; text-align: center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;border-bottom: 2px solid grey; text-align: center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;border-bottom: 2px solid grey;&#34; colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;border-bottom: 2px solid grey; text-align: center;&#34;&gt;
36.5
&lt;/td&gt;
&lt;td style=&#34;border-bottom: 2px solid grey;&#34; colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;border-bottom: 2px solid grey; text-align: center;&#34;&gt;
166
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;數據按照尺度類型分4種：
&lt;ul&gt;
&lt;li&gt;定性數據：(qualitative data/categorical data)
&lt;ul&gt;
&lt;li&gt;名義尺度 (nominal scale):
&lt;br&gt;如番號，性別等，僅用於識別或者區分對象。&lt;/li&gt;
&lt;li&gt;順序尺度 (ordinal scale):
&lt;br&gt;如表1中的健康狀態，既具有名義尺度的性質，也具有順序(順位，前後，程度等)意義。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;定量數據：(quantitative data)
&lt;ul&gt;
&lt;li&gt;間隔尺度 (interval scale):
&lt;br&gt;又稱區間尺度，距離尺度。如體溫等數值之間的差具有意義，可以設定原點（零）的尺度。&lt;/li&gt;
&lt;li&gt;比例尺度 (ratio scale)
&lt;br&gt;如身高(身長)，不同人的身高差有意義，同時原點（零）又無意義。（身高爲零的人是不存在的）&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;樣本與總體(標本/サンプルと母集団)：
&lt;ul&gt;
&lt;li&gt;表格1 中，
&lt;br&gt;(1) 全國20歲的年輕人全體的性別，健康狀態，體溫，身高的數據
&lt;br&gt;(2) 任意抽選數名(此處爲9名)年輕人的性別，健康狀態，體溫，身高的數據。
&lt;br&gt;(1) 稱爲&lt;strong&gt;總體(population)&lt;/strong&gt;, (2) 稱爲&lt;strong&gt;樣本(sample)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;一般的，我們稱問卷調查的對象個人爲&lt;strong&gt;個體(subject, individual)&lt;/strong&gt;，人數爲&lt;strong&gt;樣本量(size)&lt;/strong&gt;，我們也常常假設總體有無窮大。在多元變量分析中，如果我們稱&lt;strong&gt;數據&lt;/strong&gt;，通常只能是指&lt;strong&gt;樣本數據&lt;/strong&gt;。我們在統計分析中，一般是通過樣本去推測總體的狀況或者利用一些樣本的&lt;strong&gt;參數(parameter)&lt;/strong&gt;來描述總體，去進行檢驗等。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;變量(variable)：
&lt;ul&gt;
&lt;li&gt;表1 中變量有“性別”，“健康狀態”，“體溫”，“身長”四個。&lt;/li&gt;
&lt;li&gt;進一步的，性別又細分了“男”，“女”，健康狀態又分爲“極好”，“良好”，“普通”，“不良”，“極差”五個小項目，這些小項目整個可以視爲一個變量，也可以視爲單獨的變量，如果視爲單獨變量，表1 中就共有9個變量。&lt;/li&gt;
&lt;li&gt;性別，健康狀態的各個變量中，我們看到表1 中的數據只有1或者0，這樣的變量稱爲&lt;strong&gt;啞變量(dummy variable，ダミー変量)&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;特別地，我們可以稱性別和健康狀態的變量爲&lt;strong&gt;項目(item)&lt;/strong&gt;，稱底下的小項目爲&lt;strong&gt;分類(category)&lt;/strong&gt;。我們又稱以這樣有項目，下面有分類的變量爲&lt;strong&gt;項目分類型數據(item-categorical data)&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;多元變量分析，顧名思義，指的是對一個具有一個以上變量的數據進行統計學分析的過程。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;單變量數據
&lt;ul&gt;
&lt;li&gt;將數據進行總結，分析提取特徵的過程，稱爲&lt;strong&gt;概括統計(summary statistics)&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;下表中爲樣本量-n-的單變量數據我們看看該數據可以有哪些概括統計&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;下表中爲樣本量 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 的單變量數據，我們看看該數據可以有哪些概括統計&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;個体の番号&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;変量 &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\vdots\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\vdots\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\vdots\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\vdots\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;樣本&lt;strong&gt;平均值(mean)&lt;/strong&gt;： &lt;span class=&#34;math inline&#34;&gt;\(\bar{x}=\frac{x_1+x_2+\cdots+x_n}{n}=\frac{1}{n}\sum\limits_{i=1}^nx_i\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;樣本&lt;strong&gt;範圍(range)&lt;/strong&gt;： &lt;span class=&#34;math inline&#34;&gt;\(R=\max(x_i) - \min(x_i)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;樣本&lt;strong&gt;平方和(偏差平方和，Sum of Squares)&lt;/strong&gt;:&lt;br&gt; &lt;span class=&#34;math inline&#34;&gt;\(SS=(x_1-\bar{x})^2+(x_2-\bar{x})^2+\cdots+(x_n-\bar{x})^2\\ \;\;\;\;\:=\sum\limits_{i=1}^{n}(x_i-\bar{x})^2\\ \;\;\;\;\:=\sum\limits_{i=1}^n(x_i^2-2\bar{x}x_i+\bar{x}^2)\\ \;\;\;\;\:=\sum\limits_{i=1}^nx_i^2-2\bar{x}\sum\limits_{i=1}^nx_i+\sum\limits_{i=1}^{n}\bar{x}^2\\ \;\;\;\;\:=\sum\limits_{i=1}^nx_i^2-2\cdot\frac{\sum\limits_{i=1}^nx_i}{n}\cdot\sum\limits_{i=1}^nx_i+n\cdot(\frac{\sum\limits_{i=1}^nx_i}{n})^2\\ \;\;\;\;\:=\sum\limits_{i=1}^nx_i^2-\frac{1}{n}(\sum\limits_{i=1}^nx_i)^2\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;樣本&lt;strong&gt;方差(variance, 分散)&lt;/strong&gt;：&lt;span class=&#34;math inline&#34;&gt;\(s^2=\frac{SS}{n}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;樣本&lt;strong&gt;標準差(standard deviation, S.D., 標準偏差)&lt;/strong&gt;： &lt;span class=&#34;math inline&#34;&gt;\(s=\sqrt{s^2}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;無偏樣本方差(unbiased sample variance, 不偏標本分散)&lt;/strong&gt;：&lt;span class=&#34;math inline&#34;&gt;\(u^2=\frac{SS}{n-1}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;無偏樣本標準差平方根(不偏標本分散平方根)&lt;/strong&gt;： &lt;span class=&#34;math inline&#34;&gt;\(u=\sqrt{u^2}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;通在多元變量分析中，常常利用的思想是將&lt;strong&gt;變動(variability)&lt;/strong&gt;或者是&lt;strong&gt;波動(dispersion)&lt;/strong&gt;最大化，最小化。此處說的變動和波動是指上面提到的無偏樣本方差。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記4</title>
      <link>https://wangcc.me/post/2017-02-12-t/</link>
      <pubDate>Sun, 12 Feb 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/2017-02-12-t/</guid>
      <description>
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;連立方程式-simultaneous-equations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;連立方程式 (simultaneous equations)&lt;/h2&gt;
&lt;p&gt;連立方程式，將與第六章談的特徵值問題(固有値問題)有緊密聯系，此處我們一起觀察幾種不同的組合：&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;解同次連立1次方程式 &lt;span class=&#34;math inline&#34;&gt;\(\left\{ \begin{array}{ll}  (1)\;a_1+2a_2+3a_3 = 0 \\  (2)\;2a_1+4a_2+5a_3 = 0 \;\\  (3)\;3a_1+5a_2+6a_3 = 0 \\  \end{array} \right.\)&lt;/span&gt; &lt;br&gt;
由 &lt;span class=&#34;math inline&#34;&gt;\(2\times(1)-(2)\)&lt;/span&gt; 可得 &lt;span class=&#34;math inline&#34;&gt;\(a_3=0\)&lt;/span&gt; 。 代入 &lt;span class=&#34;math inline&#34;&gt;\((1),(2),(3)\)&lt;/span&gt; 式後，&lt;span class=&#34;math inline&#34;&gt;\((3)-(2)\)&lt;/span&gt; 可得 &lt;span class=&#34;math inline&#34;&gt;\(a_1=-a_2\)&lt;/span&gt; 。 代入 &lt;span class=&#34;math inline&#34;&gt;\((1)\)&lt;/span&gt; 式可得 &lt;span class=&#34;math inline&#34;&gt;\(a_2=0\)&lt;/span&gt; 。 再代入 &lt;span class=&#34;math inline&#34;&gt;\((4)\)&lt;/span&gt; 式可知 &lt;span class=&#34;math inline&#34;&gt;\(a_1=0\)&lt;/span&gt; 。最終可得 &lt;span class=&#34;math inline&#34;&gt;\(a_1=a_2=a_3=0\)&lt;/span&gt; &lt;br&gt;
其實上述問題不解自明 (trivial solution)。 那麼同次1次連立方程式 (homogeneous system) 除了自明解之外，還有別的解嗎? 我們再看下面一例。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;解 &lt;span class=&#34;math inline&#34;&gt;\(\left\{ \begin{array}{ll}  (1)\;4a_1+3a_2+6a_3 = 0 \\  (2)\;2a_1+a_2+4a_3 = 0 \;\\  (3)\;a_1+a_2+a_3 = 0 \\  \end{array} \right.\)&lt;/span&gt; &lt;br&gt;
上述方程表面上看有三個式子，實際上由於 &lt;span class=&#34;math inline&#34;&gt;\((3)=\left\{(1)-(2)\right\}\div2\)&lt;/span&gt; 只有2個有意義的方程式。如此這般，有3個未知數，卻只有兩個連立方程組，是無法求解的。如果將三個未知數中的一個例如 &lt;span class=&#34;math inline&#34;&gt;\(a_3\)&lt;/span&gt; 視爲常數(定数) (寫作：&lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; ) 即： &lt;br&gt;&lt;span class=&#34;math inline&#34;&gt;\((4)\;a_3=s\)&lt;/span&gt; &lt;br&gt;
整理方程組得到新的連立方程 &lt;span class=&#34;math inline&#34;&gt;\(\left\{ \begin{array}{ll}  (1^\prime)\;4a_1+3a_2 = -6s \\  (2^\prime)\;2a_1+a_2 = -4s \;\\  \end{array} \right.\)&lt;/span&gt; &lt;br&gt;
由 &lt;span class=&#34;math inline&#34;&gt;\((1^\prime)-2\times(2^\prime)\)&lt;/span&gt; 可得 &lt;span class=&#34;math inline&#34;&gt;\(a_2=2s\)&lt;/span&gt; 。代入 &lt;span class=&#34;math inline&#34;&gt;\((2^\prime)\)&lt;/span&gt; 可得 &lt;span class=&#34;math inline&#34;&gt;\(a_1=-3s\)&lt;/span&gt;。因此我們得到 &lt;span class=&#34;math inline&#34;&gt;\(a_1=-3s,a_2=2s,a_3=s\)&lt;/span&gt; 且 &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; 爲任意常數，故此連立方程組的解有無數組。當且僅當 &lt;span class=&#34;math inline&#34;&gt;\(s=0\)&lt;/span&gt; 時方程組有自明解， &lt;span class=&#34;math inline&#34;&gt;\(s\neq0\)&lt;/span&gt; 時此連立方程組的解爲非自明解 (non-trivial solution)。如果將其他未知數視爲常數(定数)時，求得的解會有變化嗎？&lt;br&gt;
若視 &lt;span class=&#34;math inline&#34;&gt;\(a_2=s\)&lt;/span&gt; 求解連立方程的解時，我們會獲得 &lt;span class=&#34;math inline&#34;&gt;\(a_1=-\frac{3}{2}s, a_2=s, a_3=-\frac{1}{2}s\)&lt;/span&gt;。若視 &lt;span class=&#34;math inline&#34;&gt;\(a_1=s\)&lt;/span&gt; 時，計算可得 &lt;span class=&#34;math inline&#34;&gt;\(a_1=s, a_2=-\frac{2}{3}s,a_3=-\frac{1}{3}s\)&lt;/span&gt;。&lt;br&gt;
由此可見，非自明解表面看去各不相同，但是都滿足了 &lt;span class=&#34;math inline&#34;&gt;\(a_1:a_2:a_3=-3:2:1\)&lt;/span&gt; 的本質條件。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;解 &lt;span class=&#34;math inline&#34;&gt;\(\left\{ \begin{array}{ll}  (1)\;4a_1+3a_2+6a_3 = 0 \\  (2)\;2a_1+a_2+4a_3 = 0 \;\\  (3)\;a_1^2+a_2^2+a_3^2 = 0 \\  \end{array} \right.\)&lt;/span&gt; &lt;br&gt;
上述方程組其實是將例題2.中的方程 &lt;span class=&#34;math inline&#34;&gt;\((3)\)&lt;/span&gt; 替換成了2次方程。&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(3\times(2)-(1)\)&lt;/span&gt; 可得 &lt;span class=&#34;math inline&#34;&gt;\(a_1=-3a_3\)&lt;/span&gt; &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\((1)-2\times(2)\)&lt;/span&gt; 可得 &lt;span class=&#34;math inline&#34;&gt;\(a_2=2a_3\)&lt;/span&gt; &lt;br&gt;
以上代入 &lt;span class=&#34;math inline&#34;&gt;\((3)\)&lt;/span&gt; 可得， &lt;span class=&#34;math inline&#34;&gt;\(a_3 = \pm \frac{1}{\sqrt{14}}\)&lt;/span&gt;。&lt;br&gt;
總結一下：　&lt;span class=&#34;math inline&#34;&gt;\(a_1=\mp \frac{3}{\sqrt{14}}, a_2=\pm \frac{2}{\sqrt{14}}, a_3=\pm\frac{1}{\sqrt{14}}\)&lt;/span&gt; (複号同順 double-sign corresponds)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;解 &lt;span class=&#34;math inline&#34;&gt;\(a_1+2a_2-3a_3=0\)&lt;/span&gt; &lt;br&gt;
上面的方程只有一個，並不是連立方程組，將其中兩個未知數視爲常數時就變成了只有一個未知數的方程。例如視，&lt;span class=&#34;math inline&#34;&gt;\(a_2=s, a_3=t\)&lt;/span&gt; 代入上述方程則可以得到: &lt;span class=&#34;math inline&#34;&gt;\(a_1=-2s+3t\)&lt;/span&gt;，因此，此方程的解爲： &lt;span class=&#34;math inline&#34;&gt;\(a_1=-2s+3t, a_2=s, a_3=t\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(s,t\)&lt;/span&gt; 爲任意常數，有無數組解。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;練習-解下列連立方程組&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;練習: 解下列連立方程組&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\left\{ \begin{array}{ll}  (1)\;2a_1-3a_2 = 0 \\  (2)\;-4a_1+6a_2 = 0 \;\\  \end{array} \right.\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\left\{ \begin{array}{ll}  (1)\;2a_1-3a_2 = 0 \\  (2)\;-4a_1+6a_2 = 0 \;\\  (3)\;a_1^2+a_2^2 = 0 \\  \end{array} \right.\)&lt;/span&gt; &lt;br&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;解&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;解&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\because\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\((2)=-2\times(1)\)&lt;/span&gt; 實質上方程組僅有一個方程。&lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\therefore a_1=\frac{3}{2}s, a_2=s\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;只需要求解例題1 中符合方程 &lt;span class=&#34;math inline&#34;&gt;\((3)\)&lt;/span&gt; 的解即可。 &lt;br&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\therefore a_1=\pm\frac{3}{\sqrt{13}}, a_2=\pm\frac{2}{\sqrt{13}}\)&lt;/span&gt; (複号同順 double-sign corresponds)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記3</title>
      <link>https://wangcc.me/post/2017-02-10/</link>
      <pubDate>Fri, 10 Feb 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/2017-02-10/</guid>
      <description>
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;函數的最大值最小值問題&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;函數的最大值最小值問題&lt;/h2&gt;
&lt;div id=&#34;沒有制約條件的情況&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;沒有制約條件的情況&lt;/h3&gt;
&lt;p&gt;函數 &lt;span class=&#34;math inline&#34;&gt;\(F(a_1,a_2,\dots,a_i,\dots,a_n)\)&lt;/span&gt; 取最大值或者最小值時，以下的連立方程
&lt;span class=&#34;math display&#34;&gt;\[\frac{\partial F}{\partial a_1}=0,\frac{\partial F}{\partial a_2}=0，\frac{\partial F}{\partial a_3}=0, \dots,\frac{\partial F}{\partial a_i}=0, \dots, \frac{\partial F}{\partial a_n}=0\]&lt;/span&gt;
要成立&lt;strong&gt;(必要條件)&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;1.已知下列方程有最小值，求當該方程取最小值時&lt;span class=&#34;math inline&#34;&gt;\(a_1,a_2\)&lt;/span&gt;的值 &lt;span class=&#34;math display&#34;&gt;\[F(a_1,a_2)=\left\{y_1-(a_1+a_2x_1)\right\}^2+\left\{y_2-(a_1+a_2x_2)\right\}^2+\cdots+\left\{y_n-(a_1+a_2x_n)\right\}^2\\
\;\;\;\;\;\;\;\;\;\;\;\;\;\;=\sum\limits_{i=1}^n\left\{y_i-(a_1+a_2x_i)\right\}^2\\\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
\frac{\partial F}{\partial a_1}&amp;amp;=-2\left\{y_1-(a_1+a_2x_1)\right\}-2\left\{y_2-(a_1+a_2x_2)\right\}-\cdots-2\left\{y_n-(a_1+a_2x_n)\right\}\\
&amp;amp;= -2\sum_{i=1}^n\left\{y_i-(a_1+a_2x_i)\right\}=0 \Leftrightarrow  \sum_{i=1}^n\left\{y_i-(a_1+a_2x_i)\right\}=0\\
\Leftrightarrow \sum_{i=1}^ny_i &amp;amp;= a_1\cdot n+a_2\sum_{i=1}^nx_i (1)\\
\\
\frac{\partial F}{\partial a_2}&amp;amp;=-2x_1\left\{y_1-(a_1+a_2x_1)\right\}-2x_2\left\{y_2-(a_1+a_2x_2)\right\}-\cdots-2x_3\left\{y_n-(a_1+a_2x_n)\right\}\\
&amp;amp;= -2\sum_{i=1}^nx_i\left\{y_i-(a_1+a_2x_i)\right\}=0\\
\Leftrightarrow \sum_{i=1}^nx_iy_i &amp;amp;=a_1\sum_{i=1}^nx_i+a_2\sum_{i=1}^nx_i^2 (2)\\

&amp;amp;將(1)(2)連立方程求解即可。在回歸分析中，\\
&amp;amp;這個連立方程組被稱作正規方程組(Normal \;equation)
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;求下列方程取最大或者最小值時的&lt;span class=&#34;math inline&#34;&gt;\(a_1,a_2,a_3\)&lt;/span&gt;的大小：
&lt;span class=&#34;math display&#34;&gt;\[F(a_1,a_2,a_3)=a_1^2+a_1a_2+a_1a_3+a_2^2+a_2a_3+a_3^2-6a_1-3a_2-7a_3\]&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
解連立方程：\\
\frac{\partial F}{\partial a_1} &amp;amp; = 2a_1+a_2+a_3-6=0\\
\frac{\partial F}{\partial a_2} &amp;amp; = a_1+2a_2+a_3-3=0\\
\frac{\partial F}{\partial a_3} &amp;amp; = a_1+a_2+2a_3-7=0\\
答：&amp;amp; a_1=2, a_2=-1,a_3=3
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記2</title>
      <link>https://wangcc.me/post/2017-02-08/</link>
      <pubDate>Wed, 08 Feb 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/2017-02-08/</guid>
      <description>
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;偏微分&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;偏微分&lt;/h2&gt;
&lt;div id=&#34;個變量的函數的微分&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;1個變量的函數的微分&lt;/h3&gt;
&lt;div id=&#34;公式&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;公式：&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;函數 &lt;span class=&#34;math inline&#34;&gt;\(f(a)\)&lt;/span&gt; 關於變量 &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; 的微分，被定義爲： &lt;span class=&#34;math inline&#34;&gt;\(\lim\limits_{h \to 0} \frac{f(a+h)-f(a)}{h}\)&lt;/span&gt; , 寫作 &lt;span class=&#34;math inline&#34;&gt;\(\frac{df}{da}\)&lt;/span&gt;, 具有下列性質：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(f(a) = a^n\)&lt;/span&gt; 時， &lt;span class=&#34;math inline&#34;&gt;\(\frac{df}{da} = na^{n-1}\)&lt;/span&gt; &lt;strong&gt;重要&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{d}{da}\left\{kf(a)+lg(a)\right\}=k\frac{df}{da}+l\frac{dg}{da}\)&lt;/span&gt; &lt;strong&gt;(&lt;span class=&#34;math inline&#34;&gt;\(k,l\)&lt;/span&gt; 是常數)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{d}{da}\left\{f(a) \cdot g(a)\right\}=\frac{df}{da}g(a)+f{a}\frac{dg}{da}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{d}{da}\left\{\frac{f(a)}{g(a)}\right\}=\frac{\frac{df}{da}g(a)-f(a)\frac{dg}{da}}{\left\{g(a)\right\}^2}\)&lt;/span&gt;, 特別的有，&lt;span class=&#34;math inline&#34;&gt;\(\frac{d}{da}\left\{\frac{1}{g(a)}\right\}=-\frac{\frac{dg}{da}}{\left\{g(a)\right\}^2}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(y=f(b), b=g(a)\)&lt;/span&gt; 時， &lt;span class=&#34;math inline&#34;&gt;\(\frac{dy}{da}=\frac{dy}{db}\frac{db}{da}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;2次（2階）微分 【二階導數】:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(f(a)\)&lt;/span&gt; 關於常數 &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; 的微分 &lt;span class=&#34;math inline&#34;&gt;\(\frac{df}{da}\)&lt;/span&gt; 的二次微分表示爲： &lt;span class=&#34;math inline&#34;&gt;\(\frac{d^2f}{da^2}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;多個變量的函數的微分&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;多個變量的函數的微分&lt;/h3&gt;
&lt;div id=&#34;偏微分-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;偏微分&lt;/h4&gt;
&lt;p&gt;包含了 &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; 個獨立變量 &lt;span class=&#34;math inline&#34;&gt;\(a_1, a_2,a_3,\cdots,a_i,\cdots,a_n\)&lt;/span&gt;的函數，即多變量函數 &lt;span class=&#34;math inline&#34;&gt;\(F(a_1, a_2,a_3,\cdots,a_i,\cdots,a_n)\)&lt;/span&gt; 關於 &lt;span class=&#34;math inline&#34;&gt;\(a_i (i=1,2,\cdots,n)\)&lt;/span&gt; 的偏微分 &lt;em&gt;(partial differentiation)&lt;/em&gt; 的定義是，把 &lt;span class=&#34;math inline&#34;&gt;\(a_i\)&lt;/span&gt; 以外的獨立變量當做常數（定数），將函數 &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; 對變量 &lt;span class=&#34;math inline&#34;&gt;\(a_i\)&lt;/span&gt; 求微分，寫作： &lt;span class=&#34;math inline&#34;&gt;\(\frac{\partial F}{\partial a_i}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;以下爲了便於說明，以三個變量爲例。&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;函數 &lt;span class=&#34;math inline&#34;&gt;\(F(a_1,a_2,a_3)=a_1+a_2+a_3=\sum\limits_{i=1}^3a_i\)&lt;/span&gt; 對於三個獨立變量分別求偏微分：
&lt;span class=&#34;math display&#34;&gt;\[\frac{\partial F}{\partial a_1}=1，\frac{\partial F}{\partial a_2}=1， \frac{\partial F}{\partial a_3}=1\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;函數 &lt;span class=&#34;math inline&#34;&gt;\(F(a_1,a_2,a_3)=a_1b_1+a_2b_2+a_3b_3=\sum\limits_{i=1}^3a_ib_i\)&lt;/span&gt; 對於三個獨立變量分別求偏微分：
&lt;span class=&#34;math display&#34;&gt;\[\frac{\partial F}{\partial a_1}=b_1，\frac{\partial F}{\partial a_2}=b_2， \frac{\partial F}{\partial a_3}=b_3\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;函數 &lt;span class=&#34;math inline&#34;&gt;\(F(a_1,a_2,a_3)=a_1^2+a_2^2+a_3^2=a_1\cdot a_1+a_2\cdot a_2+a_3\cdot a_3\\=\sum\limits_{i=1}^3a_i^2=\sum\limits_{i=1}^3a_i\cdot a_i \;對三個變量分別求偏微分：\)&lt;/span&gt;　
&lt;span class=&#34;math display&#34;&gt;\[\frac{\partial F}{\partial a_1}=2a_1，\frac{\partial F}{\partial a_2}=2a_2， \frac{\partial F}{\partial a_3}=2a_3\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;函數 &lt;span class=&#34;math inline&#34;&gt;\(F(a_1,a_2,a_3)=\lambda_1a_1^2+\lambda_2a_2^2+\lambda_3a_3^2=a_1\cdot\lambda_1\cdot a_1+a_2\cdot\lambda_2\cdot a_2+a_3\cdot\lambda_3\cdot a_3\\=\sum\limits_{i=1}^3\lambda_ia_i^2=\sum\limits_{i=1}^3a_i\cdot\lambda_i\cdot a_i \; 對三個變量分別求偏微分：\)&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\frac{\partial F}{\partial a_1}=2\lambda_1a_1，\frac{\partial F}{\partial a_2}=2\lambda_2a_2， \frac{\partial F}{\partial a_3}=2\lambda_3a_3\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;函數 &lt;span class=&#34;math inline&#34;&gt;\(F(a_1,a_2,a_3)=(b_1-\lambda a_1)^2+(b_2-\lambda a_2)^2+(b_3-\lambda a_3)^2\\=\sum\limits_{i=1}^3(b_i-\lambda a_i)^2=\sum\limits_{i=1}^3(b_i-\lambda a_i)(b_i-\lambda a_i)\;對三個變量求偏微分：\)&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\frac{\partial F}{\partial a_1}=-2\lambda(b_1-\lambda a_1)，\frac{\partial F}{\partial a_2}=-2\lambda(b_2-\lambda a_2)， \frac{\partial F}{\partial a_3}=-2\lambda(b_3-\lambda a_3)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;函數 &lt;span class=&#34;math inline&#34;&gt;\(F = a_{11}x_1y_1 + a_{12}x_1y_2 + a_{13}x_1y_3 \\ \;\;\;\;\;\;+a_{21}x_2y_1+a_{22}x_2y_2+a_{23}x_2y_3\\ \;\;\;\;\;\;+a_{31}x_3y_1+a_{32}x_3y_2+a_{33}x_3y_3\\ \;\;\;=\sum\limits_{i=1}^3\sum\limits_{i=1}^3a_{ij}x_iy_j\;對三個變量求偏微分：\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(F(x_1,x_2,x_3)\)&lt;/span&gt;, 即視爲 &lt;span class=&#34;math inline&#34;&gt;\(x_1,x_2,x_3\)&lt;/span&gt; 的函數的時候：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
 \frac{\partial F}{\partial x_1}=a_{11}y_1+a_{12}y_2+a_{13}y_3=\sum_{j=1}^3a_{1j}y_j \\
 \frac{\partial F}{\partial x_2}=a_{21}y_1+a_{22}y_2+a_{23}y_3=\sum_{j=1}^3a_{2j}y_j \\
 \frac{\partial F}{\partial x_3}=a_{31}y_1+a_{32}y_2+a_{33}y_3=\sum_{j=1}^3a_{3j}y_j \\
 將上面三個式子總結一下就是: \\
 \frac{\partial F}{\partial x_i}=\sum_{j=1}^3a_{ij}y_j (i=1,2,3)
 \]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(F(y_1,y_2,y_3)\)&lt;/span&gt;, 即視爲 &lt;span class=&#34;math inline&#34;&gt;\(y_1,y_2,y_3\)&lt;/span&gt; 的函數的時候：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
 \frac{\partial F}{\partial y_1}=a_{11}x_1+a_{21}x_2+a_{31}x_3=\sum_{i=1}^3a_{i1}x_i \\
 \frac{\partial F}{\partial y_2}=a_{12}x_1+a_{22}x_2+a_{32}x_3=\sum_{i=1}^3a_{i2}x_i \\
 \frac{\partial F}{\partial y_3}=a_{13}x_1+a_{32}x_2+a_{33}x_3=\sum_{i=1}^3a_{i3}x_i \\
 將上面三個式子總結一下就是: \\
 \frac{\partial F}{\partial x_i}=\sum_{i=1}^3a_{ij}x_i (j=1,2,3)
 \]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;函數 &lt;span class=&#34;math inline&#34;&gt;\(F(x_1,x_2,x_3)=a_{11}x_1x_1+a_{12}x_1x_2+a_{13}x_1x_3 \\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+a_{12}x_2x_1+a_{12}x_2x_2+a_{23}x_2x_3\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+a_{13}x_3x_1+a_{23}x_3x_2+a_{33}x_3x_3\\ \;\;\;\;\;\;\;\;\;\;\;\;=a_{11}x_1^2+2a_{12}x_1x_2+2a_{13}x_1x_3+a_{22}x_2^2+2a_{23}x_2x_3+a_{33}x_3^2\\ \;\;\;\;\;\;\;\;\;\;\;\;=\sum\limits_{i=1}^3a_{ii}x_i^2+2\mathop{\sum\limits^3\sum\limits^3}\limits_{i&amp;lt;j}a_{ij}x_ix_j\\ \;\;\;\;\;\;\;\;\;\;\;\;==\sum\limits_{i=1}^3x_ia_{ii}x_i+2\mathop{\sum\limits^3\sum\limits^3}\limits_{i&amp;lt;j}x_ia_{ij}x_j\;對三個變量求偏微分：\)&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\frac{\partial F}{\partial x_1}=2(a_{11}x_1+a_{12}x_2+a_{13}x_3)\\
\frac{\partial F}{\partial x_2}=2(a_{12}x_1+a_{22}x_2+a_{23}x_3)\\
\frac{\partial F}{\partial x_3}=2(a_{13}x_1+a_{23}x_2+a_{33}x_3)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;函數 &lt;span class=&#34;math inline&#34;&gt;\(F(x_1,x_2,x_3)=a_{11}x_1x_1+a_{12}x_1x_2+a_{13}x_1x_3 \\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+a_{12}x_2x_1+a_{12}x_2x_2+a_{23}x_2x_3\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+a_{13}x_3x_1+a_{23}x_3x_2+a_{33}x_3x_3\\ G(x_1,x_2,x_3)=b_{11}x_1x_1+b_{12}x_1x_2+b_{13}x_1x_3 \\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+b_{12}x_2x_1+b_{12}x_2x_2+b_{23}x_2x_3\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+b_{13}x_3x_1+b_{23}x_3x_2+b_{33}x_3x_3\\\;對三個變量求偏微分：\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\frac{\partial}{\partial x_1}(\frac{F}{G})=\frac{\frac{\partial F}{\partial x_1}G(x_1,x_2,x_3)-F(x_1,x_2,x_3)\frac{\partial G}{\partial x_1}}{\left\{G(x_1,x_2,x_3)\right\}^2}\\
=2\cdot \frac{(a_{11}x_1+a_{12}x_2+a_{13}x_3)\cdot G(x_1,x_2,x_3)-F(x_1,x_2,x_3)\cdot (b_{11}x_1+b_{12}x_2+b_{13}x_3)}{\left\{G(x_1,x_2,x_3)\right\}^2}\\
\\
\frac{\partial}{\partial x_2}(\frac{F}{G})=\frac{\frac{\partial F}{\partial x_2}G(x_1,x_2,x_3)-F(x_1,x_2,x_3)\frac{\partial G}{\partial x_2}}{\left\{G(x_1,x_2,x_3)\right\}^2}\\
=2\cdot \frac{(a_{12}x_1+a_{22}x_2+a_{23}x_3)\cdot G(x_1,x_2,x_3)-F(x_1,x_2,x_3)\cdot (b_{12}x_1+b_{22}x_2+b_{23}x_3)}{\left\{G(x_1,x_2,x_3)\right\}^2}\\
\frac{\partial}{\partial x_3}(\frac{F}{G})=\frac{\frac{\partial F}{\partial x_3}G(x_1,x_2,x_3)-F(x_1,x_2,x_3)\frac{\partial G}{\partial x_3}}{\left\{G(x_1,x_2,x_3)\right\}^2}\\
=2\cdot \frac{(a_{13}x_1+a_{23}x_2+a_{33}x_3)\cdot G(x_1,x_2,x_3)-F(x_1,x_2,x_3)\cdot (b_{13}x_1+b_{23}x_2+b_{33}x_3)}{\left\{G(x_1,x_2,x_3)\right\}^2}\\
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;次2階偏微分-二階導數&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;2次（2階）偏微分 【二階導數】:&lt;/h4&gt;
&lt;p&gt;函數 &lt;span class=&#34;math inline&#34;&gt;\(F(a_1,a_2,\cdots,a_n)\)&lt;/span&gt; 對 &lt;span class=&#34;math inline&#34;&gt;\(a_i\)&lt;/span&gt; 取偏微分 &lt;span class=&#34;math inline&#34;&gt;\(\frac{\partial F}{\partial a_i}\)&lt;/span&gt; 時，記作 &lt;span class=&#34;math inline&#34;&gt;\(\frac{\partial^2 F}{\partial a_i^2}\)&lt;/span&gt; ; 取變量 &lt;span class=&#34;math inline&#34;&gt;\(a_j\)&lt;/span&gt; 的偏微分時記作 &lt;span class=&#34;math inline&#34;&gt;\(\frac{\partial^2 F}{\partial a_i\partial a_j}\)&lt;/span&gt; 或者 &lt;span class=&#34;math inline&#34;&gt;\(\frac{\partial^2 F}{\partial a_j\partial a_i}\)&lt;/span&gt;。 這些都被稱爲是函數 &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; 的2次（2階）偏微分。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記1</title>
      <link>https://wangcc.me/post/2017-02-06/</link>
      <pubDate>Mon, 06 Feb 2017 00:00:00 +0000</pubDate>
      <guid>https://wangcc.me/post/2017-02-06/</guid>
      <description>
&lt;script src=&#34;https://wangcc.me/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;和記號sum&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;和記號&lt;span class=&#34;math inline&#34;&gt;\(\sum\)&lt;/span&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sum\)&lt;/span&gt; 的性質 (1)
&lt;strong&gt;下標(添字)&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(x_1 + x_2 + x_3 + \dots + x_n\)&lt;/span&gt; 記作如下:&lt;span class=&#34;math display&#34;&gt;\[\sum_{i=1}^{n}x_i\]&lt;/span&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^{n}x_i\)&lt;/span&gt; 中的&lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; 稱爲&lt;code&gt;dummy index&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;可以簡略寫爲：&lt;span class=&#34;math inline&#34;&gt;\(\sum x\)&lt;/span&gt; 或者 &lt;span class=&#34;math inline&#34;&gt;\(\sum_1 x_i\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\sum x_i\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sum\)&lt;/span&gt; 的性質 (2)
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:1&#34;&gt;\[\begin{equation}
\sum_{i=1}^{n}(ax_i + by_i)= a\sum_{i=1}^{n}x_i + b\sum_{i=1}^{n}y_i \tag{1}
\end{equation}\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^{n}ax_i = a\sum_{i=1}^{n}x_i\)&lt;/span&gt; &lt;code&gt;常數(定数)可以提前&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^{n}a = na\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^{n}1 = n\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^{n}(ax_i+b) = a\sum_{i=1}^{n}x_i + nb\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;公式&lt;a href=&#34;#eq:1&#34;&gt;(1)&lt;/a&gt;的應用:
&lt;span class=&#34;math display&#34;&gt;\[
  \begin{aligned}
  \sum_{i=1}^{n}(ax_i -by_i)^2 &amp;amp;= \sum_{i=1}^{n}(a^2x_i^2 - 2abx_iy_i + b^2y_i^2) \\
   &amp;amp;= \sum_{i=1}^{n}a^2x_i^2 -\sum_{i=1}^{n}2abx_iy_i + \sum_{i=1}^{n}b^2y_i^2 \\
   &amp;amp;= a^2\sum_{i=1}^{n}x_i^2 - 2ab\sum_{i=1}^{n}x_iy_i + b^2\sum_{i=1}^{n}y_i^2
  \end{aligned}
  \]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;但是，乘法或平方有如下性質，計算方差(分散)或者相關系數時需要注意：&lt;span class=&#34;math display&#34;&gt;\[\sum_{i=1}^{n}x_i^2 \neq (\sum_{i=1}^{n}x_i)^2\]&lt;/span&gt; 以及 &lt;span class=&#34;math display&#34;&gt;\[\sum_{i=1}^{n}x_iy_i \neq (\sum_{i=1}^{n}x_i)(\sum_{i=1}^{n}y_i)\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;自然數的冪運算之和(冪[べき]乗の和)的公式:
&lt;span class=&#34;math display&#34;&gt;\[
  \begin{aligned}
  1+2+3+\dots+n &amp;amp;= \sum_{t=1}^{n}t = \frac{n(n+1)}{2}\\
  1^2+2^2+3^2+\dots+n^2 &amp;amp;= \sum_{t=1}^{n}t^2 = \frac{n(n+1)(2n+1)}{6} \\
  1^3+2^3+3^3+\dots+n^3 &amp;amp;= \sum_{t=1}^{n}t^3 = {\frac{n(n+1)}{2}}^2 \\
  1^4+2^4+3^4+\dots+n^4 &amp;amp;= \sum_{t=1}^{n}t^4 = \frac{n(n+1)(2n+1)(3n^2+3n-1)}{30}
  \end{aligned}
  \]&lt;/span&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    上面的公式將會應用在時間序列分析，斯皮尔曼等级相关系数(スピアマンの順位相関係数)的定義公式的推導。
  &lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;sum式子變形成普通計算式&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sum\)&lt;/span&gt;式子變形成普通計算式：&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math display&#34;&gt;\[\sum_{i=1}^{n}f_{ij} = f_{i1} + f_{i2} + f_{i3} + \dots + f_{in}\]&lt;/span&gt; 此式子也常寫作&lt;span class=&#34;math inline&#34;&gt;\(f_{i+}\)&lt;/span&gt;, 或者&lt;span class=&#34;math inline&#34;&gt;\(f_{i\cdot}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math display&#34;&gt;\[\sum_{i=1}^{m}f_{ij} = f_{1i} + f_{2i} + f_{3i} + \dots + f_{mj}\]&lt;/span&gt; 此式子也常寫作&lt;span class=&#34;math inline&#34;&gt;\(f_{+j}\)&lt;/span&gt;, 或者&lt;span class=&#34;math inline&#34;&gt;\(f_{\cdot j}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math display&#34;&gt;\[\sum_{i=1}^{2}\sum_{j=1}^{3}x_{ij} = \sum_{i=1}^{2}(\sum_{i=j}^{3}x_{ij}) = \sum_{i=1}^{2}(x_{i1} + x_{i2} + x_{i3}) = (x_{11} + x_{12} + x_{13}) + (x_{21} + x_{22} + x_{23})\]&lt;/span&gt; 此式子也可以寫作&lt;span class=&#34;math inline&#34;&gt;\(x_{++}\)&lt;/span&gt;, 或者&lt;span class=&#34;math inline&#34;&gt;\(x_{\cdot\cdot}\)&lt;/span&gt;。另外，中間的式子如果是&lt;span class=&#34;math inline&#34;&gt;\(\sum_{j=1}^{3}(\sum_{i=1}^{2}x_{ij})\)&lt;/span&gt;也可以成立，過程如下：&lt;span class=&#34;math display&#34;&gt;\[\sum_{j=1}^{3}(\sum_{i=1}^{2}x_{ij})=\sum_{j=1}^{3}(x_{1j} + x_{2j}) = (x_{11} + x_{21}) + (x_{12} + x_{22}) + (x_{13} + x_{23})\]&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math display&#34;&gt;\[
  \begin{aligned}
  \sum_{i=1}^2\sum_{j=1}^2a_{ij}x_ix_j &amp;amp;= \sum_{i=1}^2(\sum_{j=1}^2a_{ij}x_ix_j) \\
  &amp;amp;= \sum_{i=1}^2({\sum_{j=1}^2a_{ij}x_j)x_i} \\
  &amp;amp;= \sum_{i=1}^2(a_{i1}x_1 + a_{i2}x_2)x_i \\
  &amp;amp;= (a_{11}x_1 + a_{12}x_2)x_1 + (a_{21}x_1 + a_{22}x_2)x_2 \\
  &amp;amp;= a_{11}x_1^2 + (a_{12} + a_{21})x_1x_2 + a_{22}x_2^2
  \end{aligned}
  \]&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math display&#34;&gt;\[
  \begin{aligned}
  \sum_{k=1}^3\left\{(\sum_{i=1}^2b_ix_{ik})(\sum_{j=1}^2b_jx_{jk})\right\} &amp;amp;= \sum_{k=1}^3\left\{(b_1x_{1k} + b_2x_{2k})(b_1x_{1k} + b_2x_{2k})\right\} \\
  &amp;amp;= \sum_{k=1}^3(b_1x_{1k} + b_2x_{2k})^2 \\
  &amp;amp;= (b_1x_{11} + b_2x_{21})^2 + (b_1x_{12} + b_2x_{22})^2 + (b_1x_{13} + b_2x_{23})^2
  \end{aligned}
  \]&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathop{\sum\limits^3\sum\limits^3}\limits_{i&amp;lt;j}e_{ij}\)&lt;/span&gt; 會變成怎樣的式子呢？ 滿足 &lt;span class=&#34;math inline&#34;&gt;\(i&amp;lt;j (i = 1,2,3; j = 1,2,3)\)&lt;/span&gt; 條件的 &lt;span class=&#34;math inline&#34;&gt;\(i,j\)&lt;/span&gt;, 有且僅有 &lt;span class=&#34;math inline&#34;&gt;\((1,2),(1,3),(2,3)\)&lt;/span&gt;,故 &lt;span class=&#34;math inline&#34;&gt;\(\mathop{\sum\limits^3\sum\limits^3}\limits_{i&amp;lt;j}e_{ij} = e_{12} + e_{13} + e_{23}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;
&lt;ol start=&#34;7&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;那麼&lt;span class=&#34;math inline&#34;&gt;\(\mathop{\sum\limits^3\sum\limits^3}\limits_{i\neq j}e_{ij}\)&lt;/span&gt;又會變成怎樣的式子呢？ 滿足 &lt;span class=&#34;math inline&#34;&gt;\(i\neq j (i = 1,2,3; j = 1,2,3)\)&lt;/span&gt; 的 &lt;span class=&#34;math inline&#34;&gt;\((i,j)\)&lt;/span&gt; 有6種組合:&lt;span class=&#34;math inline&#34;&gt;\((1,2),(2,1),(1,3),(3,1),(2,3),(3,2)\)&lt;/span&gt;, 故&lt;span class=&#34;math inline&#34;&gt;\(\mathop{\sum\limits^3\sum\limits^3}\limits_{i\neq j}e_{ij} = e_{12} + e_{21} + e_{13} + e_{31} + e_{23} + e_{32}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;加法算式變形爲sum&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;加法算式變形爲&lt;span class=&#34;math inline&#34;&gt;\(\sum\)&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;在多元變量分析(多変量解析)中，與前項相比，加法算式變形成爲&lt;span class=&#34;math inline&#34;&gt;\(\sum\)&lt;/span&gt;式子更加重要。也就是說，以前項計算爲例的話，作爲答案的計算式如果放在題幹，反向求解&lt;span class=&#34;math inline&#34;&gt;\(\sum\)&lt;/span&gt;式的過程更加常用。簡單練習一下吧：&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;將計算式&lt;span class=&#34;math inline&#34;&gt;\(a_1x_1^2 + a_2x_2^2 + a_3x_3^2 + a_4x_4^2 + a_5x_5^2\)&lt;/span&gt;改寫成&lt;span class=&#34;math inline&#34;&gt;\(\sum\)&lt;/span&gt;式：
&lt;ul&gt;
&lt;li&gt;先寫下：&lt;span class=&#34;math inline&#34;&gt;\(\sum\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;再寫各個單元的共通部分&lt;span class=&#34;math inline&#34;&gt;\((a,x^2)\)&lt;/span&gt;： &lt;span class=&#34;math inline&#34;&gt;\(\sum ax^2\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;各單元不同的僅爲下標： &lt;span class=&#34;math inline&#34;&gt;\(\sum a_ix_i^2\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;注意到下標的變化規律爲&lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;到&lt;span class=&#34;math inline&#34;&gt;\(5\)&lt;/span&gt;之間的整數，故在&lt;span class=&#34;math inline&#34;&gt;\(\sum\)&lt;/span&gt;符號的上部寫上&lt;span class=&#34;math inline&#34;&gt;\(5\)&lt;/span&gt;，下部寫上&lt;span class=&#34;math inline&#34;&gt;\(i=1\)&lt;/span&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\sum\limits_{i=1}^5a_ix_i^2\)&lt;/span&gt; (答)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;將計算式&lt;span class=&#34;math inline&#34;&gt;\(f_2(x_2 - \bar{x})^2 + f_3(x_3 - \bar{x})^2 + f_4(x_4 - \bar{x})^2 + f_5(x_5 - \bar{x})^2\)&lt;/span&gt;改寫成&lt;span class=&#34;math inline&#34;&gt;\(\sum\)&lt;/span&gt;式：
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sum\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sum f(x-\bar{x})^2\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sum f_i(x_i - \bar{x})^2\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sum\limits_{i=2}^5f_i(x_i - \bar{x})^2\)&lt;/span&gt; (答)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;提問&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;提問：&lt;/h2&gt;
&lt;div id=&#34;我們現在了解了可以使用簡單的sum符號來表達復雜有規律的加法算式請問有沒有相應的記號來代表乘法&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;我們現在了解了可以使用簡單的&lt;span class=&#34;math inline&#34;&gt;\(\sum\)&lt;/span&gt;符號來表達復雜有規律的加法算式。請問有沒有相應的記號來代表乘法？&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;當然有用希臘字母pi的大寫pi來表示例如-x_1x_2x_3x_4x_5-prod_i15x_i-x_1x_2x_3dots-x_n-prod_i1nx_i&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;當然有。用希臘字母&lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt;的大寫&lt;span class=&#34;math inline&#34;&gt;\(\Pi\)&lt;/span&gt;來表示。例如： &lt;span class=&#34;math display&#34;&gt;\[x_1x_2x_3x_4x_5 = \prod_{i=1}^5x_i\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[x_1x_2x_3\dots x_n = \prod_{i=1}^nx_i\]&lt;/span&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;乘法記號可以證明下面的等式成立-prod_i1nx_i2-prod_i1nx_i2-logprod_i1nx_i-prod_i1nlog-x_i&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;乘法記號可以證明下面的等式成立： &lt;span class=&#34;math display&#34;&gt;\[\prod_{i=1}^nx_i^2 = (\prod_{i=1}^nx_i)^2\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[\log(\prod_{i=1}^nx_i) = \prod_{i=1}^n\log x_i\]&lt;/span&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
