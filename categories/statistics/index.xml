<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Statistics on Be ambitious</title>
    <link>https://winterwang.github.io/categories/statistics/</link>
    <description>Recent content in Statistics on Be ambitious</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Chaochen Wang | 王超辰</copyright>
    <lastBuildDate>Sun, 24 Jun 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://winterwang.github.io/categories/statistics/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Summer Project Schedule</title>
      <link>https://winterwang.github.io/post/summer-project-schedule/</link>
      <pubDate>Sun, 24 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/summer-project-schedule/</guid>
      <description>Data analysis finish by 2018-07-2431 Paper structure confirm by 2018-08-01 Paper draft complete by 2018-08-16 2018-06-24 Read and try to repeat Rll&amp;rsquo;s method in R and familarize the dataset ASAP Two papers applying Repeated Measures LCA  2018-06-25 Meeting with supervisor and Susanna Confirm the cutoff of carborhydrate consumption Talk with Rll ask about the methodology and dataset  2018-06-26 Send the summarised memo of meeting to Supervisor and etc. Read the first part fundamentals of LCA.</description>
    </item>
    
    <item>
      <title>徒手打造一個假設檢驗</title>
      <link>https://winterwang.github.io/post/construction-of-a-hypothesis-test/</link>
      <pubDate>Wed, 08 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/construction-of-a-hypothesis-test/</guid>
      <description>什麼是假設檢驗 Hypothesis testing一般來說，我們的假設（或者叫假說）是對與我們實驗觀察數據來自的總體（或人羣）的概率分佈的描述。在參數檢驗的背景下，就是要檢驗描述這個總體（或人羣）的概率分佈的參數 (parameters)。最典型的情況是，我們提出兩個互補的假設，一個叫作零假設（或者叫原假設），null hypothesis (\(H_0\))；另一個是與之對應的（互補的）替代假設，althernative hypothesis (\(H_1/H_A\))。
例如，若 \(X\) 是一個服從二項分佈的隨機離散變量 \(X\sim Bin(5, \theta)\)。可以考慮如下的零假設和替代假設：\(H_0: \theta=\frac{1}{2}; H_1: \theta=\frac{2}{3}\)。
當建立了零假設和替代假設以後，假設檢驗就是要建立如下的規則以確定：
從樣本中計算所得的參數估計值爲多少時，拒絕零假設。（接受替代假設爲“真”）從樣本中計算所得的參數估計值爲多少時，零假設不被拒絕。（接受零假設爲“真”）注意：（這一段很繞）
上面的例子是零假設和替代假設均爲簡單假設的情況，實際操作中常常會設計更加複雜的（不對稱的）假設：即簡單的 \(H_0\)，複雜的 \(H_1\)。如此一來當零假設 \(H_0\) 不被拒絕時，我們並不一定就接受之。因爲無證據證明 \(H_1\) 不等於有證據證明 \(H_0\)。(Absence of evidence is not evidence of absence). 換句話說，無證據讓我們拒絕 \(H_0\) 本身並不成爲支持 \(H_0\) 爲“真”的證據。因爲在實際操作中，當我們設定的簡單的零假設沒有被拒絕，可能還存在其他符合樣本數據的零假設；相反地，當樣本數據的計算結果拒絕了零假設，我們只能接受替代假設。所以，反對零假設的證據，同時就是支持替代假設的證據。
在樣本空間 sample space 中，決定了零假設 \(H_0\) 會被拒絕的子集 subset，被命名爲拒絕域 rejection region 或者 判別區域 critical region，用 \(\mathfrak{R}\) 來標記。
錯誤概率和效能方程這一部分可以參考之前臨牀試驗樣本量計算的部分。
Table 1: Definition of Type I and Type II errorSAMPLE\(\underline{x} \notin \mathfrak{R}\) Accept \(H_0\)\(\underline{x} \in \mathfrak{R}\) Reject \(H_0\)TRUTH\(H_0\) is true\(\checkmark\)\(\alpha\) Type I error\(H_1\) is true\(\beta\) Type II error\(\checkmark\)假如一個假設檢驗是關於總體參數 \(\theta\) 的：</description>
    </item>
    
    <item>
      <title>二次方程近似法求對數似然比 approximate log-likelihood ratios</title>
      <link>https://winterwang.github.io/post/approximate-log-likelihood-ratios/</link>
      <pubDate>Tue, 07 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/approximate-log-likelihood-ratios/</guid>
      <description>爲什麼要用二次方程近似對數似然比方程？
上節也看到，我們會碰上難以用代數學計算獲得對數似然比信賴區間的情況 (binomial example)。我們同時知道，對數似然比方程會隨着樣本量增加而越來越漸進於二次方程，且左右對稱。所以，我們考慮當樣本量足夠大時，用二次方程來近似對數似然比方程從而獲得參數估計的信賴區間。正態近似法求對數似然 Normal approximation to the log-likelihood根據前一節，如果樣本均數的分佈符合正態分佈：\(\bar{X}\sim N(\mu, \sigma^2/n)\)。那麼樣本均數的對數似然比爲：
\[llr(\mu|\bar{X})=\ell(\mu|\bar{X})=-\frac{1}{2}(\frac{\bar{x}-\mu}{\sigma/\sqrt{n}})^2\]
其中， \(\bar{x}\) 是正態分佈總體均數 \(\mu\) 的極大似然估計 (maximum likelihood estimator, MLE)。如果已知總體的方差參數，那麼 \(\sigma/\sqrt{n}\) 是 \(\bar{x}\) 的標準誤 (standard error)。
因此，假設 \(\theta\) 是我們想尋找的總體參數。有些人提議可以使用下面的關於 \(\theta\) 的二次方程來做近似：
\[f(\theta|data)=-\frac{1}{2}(\frac{\theta-M}{S})^2\]
上述方程具有一個正態二次對數似然 (比) 的形式，而且該方程的極大似然估計(MLE)， \(M\) 的標準誤爲 \(S\)。如果我們正確地選用 \(M\) 和 \(S\)，那我們就可以用這樣的方程來近似求真實觀察數據的似然 \(\ell(\theta|data)\)。
通過近似正態對數似然比，\(M\) 應當選用使方程取最大值時，參數 \(\theta\) 的極大似然估計 \(M=\hat{\Theta}\)。
但是在選用標準誤 \(S\) 上必須滿足下列條件：
\(S\) 是極大似然估計 \(\hat{\Theta}\) 的標準誤。被選擇的 \(S\) 必須儘可能的使該二次方程形成一個十分接近真實的對數似然比方程。特別是在最大值的部分必須與之無限接近或者一致。所以二者在 MLE 的位置應當有相同的曲率（二階導數）。由於，一個方程的曲率是該方程的二階導數（斜線斜率變化的速度）。所以對數似然比方程在 MLE 取最大值時的曲率（二階導數）爲：
\[\left.\frac{d^2}{d\theta^2}\ell(\theta)\right\vert_{\theta=\hat{\theta}}=\ell^{\prime\prime}(\hat{\theta})=-\frac{1}{S^2}\\\Rightarrow S^2=\left.</description>
    </item>
    
    <item>
      <title>對數似然比 Log-likelihood ratio</title>
      <link>https://winterwang.github.io/post/log-likelihood-ratio/</link>
      <pubDate>Sun, 05 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/log-likelihood-ratio/</guid>
      <description>對數似然比 Log-likelihood ratio對數似然比的想法來自於將對數似然方程圖形的 \(y\) 軸重新調節 (rescale) 使之最大值爲零。這可以通過計算該分佈方程的對數似然比 (log-likelihood ratio) 來獲得：
\[llr(\theta)=\ell(\theta|data)-\ell(\hat{\theta}|data)\]
由於 \(\ell(\theta)\) 的最大值在 \(\hat{\theta}\) 時， 所以，\(llr(\theta)\) 就是個當 \(\theta=\hat{\theta}\) 時取最大值，且最大值爲零的方程。很容易理解我們叫這個方程爲對數似然比，因爲這個方程就是將似然比 \(LR(\theta)=\frac{L(\theta)}{L(\hat{\theta})}\) 取對數而已。
之前我們也確證了，不包含我們感興趣的參數的方程部分可以忽略掉。還是用上一節 10人中4人患病的例子：
\[L(\pi|X=4)=\binom{10}{4}\pi^4(1-\pi)^{10-4}\\\Rightarrow \ell(\pi)=log[\pi^4(1-\pi)^{10-4}]\\\Rightarrow llr(\pi)=\ell(\pi)-\ell(\hat{\pi})=log\frac{\pi^4(1-\pi)^{10-4}}{0.4^4(1-0.4)^{10-4}}\]
其實由上也可以看出 \(llr(\theta)\) 只是將對應的似然方程的 \(y\) 軸重新調節了一下而已。形狀是沒有改變的：
par(mfrow=c(1,2))x &amp;lt;- seq(0,1,by=0.001)y &amp;lt;- (x^4)*((1-x)^6)/(0.4^4*0.6^6)z &amp;lt;- log((x^4)*((1-x)^6))-log(0.4^4*0.6^6)plot(x, y, type = &amp;quot;l&amp;quot;, ylim = c(0,1.1),yaxt=&amp;quot;n&amp;quot;,frame.plot = FALSE, ylab = &amp;quot;LR(\U03C0)&amp;quot;, xlab = &amp;quot;\U03C0&amp;quot;)axis(2, at=seq(0,1, 0.2), las=2)title(main = &amp;quot;Binomial likelihood ratio&amp;quot;)abline(h=1.</description>
    </item>
    
    <item>
      <title>似然非然 Likelihood</title>
      <link>https://winterwang.github.io/post/likelihood/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/likelihood/</guid>
      <description>概率 vs. 推斷/Probability vs. Inference在概率論的環境下，我們常常被告知的前提是：某某事件發生的概率是多少。例如： 一枚硬幣正面朝上的概率是 \(0.5\; Prob(coin\;landing\;heads)=0.5\)。然後在這個前提下，我們又繼續去計算複雜的事件發生的概率（例如，10次投擲硬幣以後4次正面朝上的概率是多少？）。
\[\binom{10}{4}\times(0.5^4)\times(0.5^{10-4}) = 0.205\]
dbinom(4, 10, 0.5)## [1] 0.2050781# or you can calculate by hand:factorial(10)*(0.5^10)/(factorial(4)*(factorial(6)))## [1] 0.2050781在統計推斷的理論中，我們考慮實際的情況，這樣的實際情況就是，我們通過觀察獲得數據，然而我們並不知道某事件發生的概率到底是多少（神如果存在話，只有神知道）。故這個 \(Prob(coin\;landing\;heads)\) 的概率大小對於“人類”來說是未知的。我們可能觀察到投擲了10次硬幣，其中有4次是正面朝上的。那麼我們從這一次觀察實驗中，需要計算的是能夠符合觀察結果的“最佳”概率估計 (best estimate)。在這種情況下，似然法 (likelihood) 就是我們進行參數估計的最佳手段。
似然和極大似然估計此處用二項分佈的例子來理解似然法的概念：假設我們觀察到10個對象中有4個患病，我們假定這個患病的概率爲 \(\pi\)。於是我們就有了下面的模型：
模型： 我們假定患病與否是一個服從二項分佈的隨機變量，\(X\sim Bin(10,\pi)\)。同時也默認每個人之間是否患病是相互獨立的。
數據： 觀察到的數據是，10人中有4人患病。於是 \(x=4\)。
現在按照觀察到的數據，參數 \(\pi\) 變成了未知數：
\[Prob(X=4|\pi)=\binom{10}{4}\pi^4(1-\pi)^{10-4}\]
此時我們會很自然的考慮，當 \(\pi\) 是未知數的時候，它取值爲多大的時候才能讓這個事件（即：10人中4人患病）發生的概率最大？ 所以我們可以將不同的數值代入 \(\pi\) 來計算該事件在不同概率的情況下發生的可能性到底是多少：
Table 1: The probability of observing \(X=4\)\(\pi\)事件 \(X=4\) 發生的概率0.</description>
    </item>
    
    <item>
      <title>臨牀實驗的樣本量計算問題 Sample Size in Clinical Trial</title>
      <link>https://winterwang.github.io/post/sample-size-in-clinical-trial/</link>
      <pubDate>Sun, 29 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/sample-size-in-clinical-trial/</guid>
      <description>背景計劃臨牀實驗的時候，爲了避免偏倚和帶有偏見的結論，應當將注意力放在
如何將實驗對象隨機分配 (randomisation)設計對照組 (control group)合適（且必須）的貫徹盲法 (blinding)另外一個同樣重要的問題是–“我到底需要多少樣本?”
一項臨牀實驗，應該提供足夠的證據來證明新藥物（新治療方法）是否有效，是否安全。影響一個實驗設計的樣本量的因素可能有如下幾種：
統計學方案。從統計學上可以推算出，需要多少樣本來獲得一個堅實可信的證據來證明藥物的實際有效性。經濟上的因素。然而實際上可能還有經濟上，時間上，人力物力資源上的現實因素，會制約到底一個實驗能夠收集到多少樣本量。倫理道德上的因素。許多臨牀實驗還必須受制於醫學倫理因素。在倫理上一個實驗到底可以維持多久。或者說，要考慮當實驗中一些受試者的結果不理想，或者是有副作用的時候，我們何時該及時停止該實驗？實驗本身的可信度。如果一個臨牀實驗的規模在設計上就很小，可能它本身的可信度就很低。這裏我們只考慮沒有其他任何因素的影響下，1. 統計學方案上該如何計算準確的所需樣本量的大小。
比較下列兩個同樣比較了溶栓酶和安慰劑在預防心肌梗塞患者死亡的臨牀實驗：Table 1: Results from the 1st Australian and ISIS-2 trials for reducing mortality from post-MI治療組溶栓酶安慰劑p.values1st Australiann=264n=253死亡人數26 (9.8%)32 (12.</description>
    </item>
    
    <item>
      <title>卡方分佈 chi square distribution</title>
      <link>https://winterwang.github.io/post/chi-square-distribution/</link>
      <pubDate>Thu, 26 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/chi-square-distribution/</guid>
      <description>卡方分佈的期望和方差的證明：當 \(X\sim N(0,1)\) 時， \(X^2\sim \mathcal{X}_1^2\)
如果 \(X_1, \dots, X_n\stackrel{i.i.d}{\sim} N(0,1)\)，那麼 \(\sum_{i=1}^nX_i^2\sim\mathcal{X}_n^2\)
其中： \(\mathcal{X}_n^2\) 表示自由度爲 \(n\) 的卡方分佈。
且 \(X_m^2+X_n^2=\mathcal{X}_{m+n}^2\)
卡方分佈的期望：\[E(X_1^2)=Var(X)+[E(X)]^2=1+0=1\]
\[\Rightarrow E(X_n^2)=n\]
卡方分佈的方差：\[\begin{aligned}Var(X_1^2) &amp;amp;= E(X_1^{2^2}) - E(X_1^2)^2 \\&amp;amp;= E(X_1^4)-1\end{aligned}\]
下面來求 \(E(X_1^4)\)\[\begin{aligned}\because E(X_1) &amp;amp;= \int_{-\infty}^{+\infty} xf(x)dx \\\therefore E(X_1^4) &amp;amp;= \int_{-\infty}^{+\infty} x^4f(x)dx\end{aligned}\]
已知： \(f(x)=\frac{1}{\sqrt{2\pi}}e^{(-\frac{x^2}{2})}\) 代入上式：
\[\begin{aligned}E(X_1^4) &amp;amp;= \int_{-\infty}^{+\infty} x^4f(x)dx \\&amp;amp;= \int_{-\infty}^{+\infty} x^4\frac{1}{\sqrt{2\pi}}e^{(-\frac{x^2}{2})}dx\\&amp;amp;=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}x^4e^{(-\frac{x^2}{2})}dx\\&amp;amp;=\frac{-1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}x^3(-x)e^{(-\frac{x^2}{2})}dx\end{aligned}\]</description>
    </item>
    
    <item>
      <title>估計和精確度的概念</title>
      <link>https://winterwang.github.io/post/frequentist-statistical-inference02/</link>
      <pubDate>Tue, 24 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/frequentist-statistical-inference02/</guid>
      <description>估計量和他們的樣本分佈例子： 最大呼氣量 (Forced Expoiratory Volume in one second, FEV1) 用於測量一個人的肺功能，它的測量值是連續的。我們從前來門診的人中隨機抽取 \(n\) 人作爲樣本，用這個樣本的 FEV1 平均值來估計這個診所的患者的平均肺功能。
模型假設： 在這個例子中，我們的假設有如下：每個隨機抽取的 FEV1 測量值都是從同一個總體（人羣）中抽取，每一個觀察值 \(Y_i\) 都互相獨立互不影響。我們用縮寫 iid 表示這些隨機抽取的樣本是服從獨立同分佈 (independent and identically distributed)。另外，總體的分佈也假定爲正態分佈，且總體均值爲 \(\mu\)，總體方差爲 \(\sigma^2\)。那麼這個模型可以簡單的被寫成：
\[Y_i \stackrel{i.i.d}{\sim} N(\mu, \sigma^2), i=1,2,\dots,n\]
總體均值 \(\mu\) 的估計量： 顯然算術平均值: \(\bar{Y}=\frac{1}{n}\sum_{i=1}^ny_i\) 是我們用於估計總體均值的估計量。
估計量的樣本分佈：\[\bar{Y}\stackrel{i.i.d}{\sim}N(\mu, \frac{\sigma^2}{n})\]
證明\[\begin{aligned}E(\bar{Y}) &amp;amp;= E(\frac{1}{n}\sum Y_i) \\&amp;amp;= \frac{1}{n}E(\sum Y_i) \\&amp;amp;= \frac{1}{n}\sum E(Y_i) \\&amp;amp;= \frac{1}{n}n\mu = \mu \\Var(\bar{Y}) &amp;amp;= Var(\frac{1}{n}\sum Y_i) \\\because Y_i \;are &amp;amp;\; independent \\&amp;amp;= \frac{1}{n^2}\sum Var(Y_i) \\&amp;amp;= \frac{1}{n^2} n Var(Y_i) \\&amp;amp;= \frac{\sigma^2}{n}\end{aligned}\]</description>
    </item>
    
    <item>
      <title>概率論者統計推斷入門之-被門夾住</title>
      <link>https://winterwang.github.io/post/frequentist-statistical-inference01/</link>
      <pubDate>Tue, 24 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/frequentist-statistical-inference01/</guid>
      <description>人羣與樣本 (population and sample)討論樣本時，需考慮下面幾個問題：
樣本是否具有代表性？人羣被準確定義了嗎？我們感興趣的“人羣”是否可以是無限大（多）的？我們研究的樣本，是僅僅用來觀察，亦或是計劃對之進行某種干預呢？我們從所有可能的人羣中抽樣了嗎？樣本和統計量 (sample and statistic)通常我們在進行實驗或觀察時只是獲得了樣本的數據。而希望從樣本數據去推斷 (inference) 總體（或人羣）的一些特徵。我們也許只是想用樣本的平均值來估計整體人羣的某個特徵的平均值。不管是何種估計和推斷，都是基於對樣本數據的計算，從樣本中獲得想要推斷總體的統計量 (statistics)。我們用已知樣本去推斷未知總體的過程就叫做估計 (estimate)。這個想要被推斷的總體或人羣的值，被叫做參數 (parameter)，常常使用希臘字母來標記。用來估計總體或人羣的，從樣本數據計算得來的統計量，叫做估計量 (estimator)。
所有的統計量，都有樣本分佈 (sampling distributions，意爲重複無限次取樣後獲得的無限次統計量的分佈)。推斷的過程歸納如下：
從總體或人羣中抽樣 (樣本量 \(n\))計算這個樣本的合適統計量，從而用於估計它在整體或人羣中的值。我們還需要決定計算獲得的統計量的樣本分佈（假定會抽樣無數次）。一旦可以精確地確認樣本分佈，我們就可以定量地計算出使用步驟2中獲得的統計量估計總體或人羣的參數時的準確度。估計 Estimation從樣本的均值，推斷總體或人羣的均值是一種估計。我們的目的是，從已知樣本中計算一個儘可能接近那個未知的總體或人羣參數的值。一個估計量有兩個與生俱來的性質 (properties)：1) 偏倚 (bias); 2) 精確度 (precision)。這兩個性質都可以從樣本分佈和估計量獲得。
偏倚： 偏倚簡單說就是樣本分佈的均值，也就是我們從樣本中計算獲得的估計量，和我們想要拿它來估計的總體或人羣的參數之間的差距。(The bias is the difference between the mean of the sampling distribution – the expected or average value of the estimator – and the population parameter being estimated.</description>
    </item>
    
    <item>
      <title>中心極限定理的應用</title>
      <link>https://winterwang.github.io/post/central-limit-theorem-application/</link>
      <pubDate>Sat, 21 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/central-limit-theorem-application/</guid>
      <description>二項分佈的正態分佈近似假設我們有大量(\(n\rightarrow\infty\))的二項分佈實驗 \(X\sim Bin(n, \pi)\)根據二項分佈的概率公式，計算將會變得很繁瑣複雜。解決辦法：應用中心極限定理。中心極限定理告訴我們，當樣本量足夠大時:\[X\sim N（n\pi, n\pi(1-\pi))\]
問題在於，多大的 \(n\) 才能算大樣本呢？當且僅當 (only and if only) \(n&amp;gt;20\) AND \(n\pi&amp;gt;5\) AND \(n(1-\pi)&amp;gt;5\)泊松分佈的正態分佈近似假設時間 \(t\) 內某事件的發生次數服從泊松分佈 \(X\sim Po(\mu)\)。考慮將這段時間 \(t\) 等分成 \(n\) 個時間段。那麼第 \(i\) 時間段內事件發生次數依舊服從泊松分佈 \(X_i\sim Po(\frac{\mu}{n})\)。且 \(E(X_i)=\mu/n, Var(X_i)=\mu/n\)。那麼原先的 \(X\) 可以被視爲是將這無數的小時間段的 \(X_i\) 相加。應用中心極限定理：\[X=\sum_{i=1}^nX_i\sim N(\frac{n\mu}{n}, \frac{n\mu}{n})\]
需要注意的是，這段時間 (\(t\)) 內發生的事件次數 (\(\lambda\)) : \(\lambda t =\mu&amp;gt;10\) ，這樣的正態分佈模擬才能成立。
正態分佈模擬的校正：continuity corrections如果我們使用正態分佈來模擬離散變量的分佈，常常需要用到正態分佈模擬的矯正。例如：我們如果用正態分佈模擬來計算 \(P(X=15)\)，那麼實際上我們應該計算的是 \(P(14.</description>
    </item>
    
    <item>
      <title>偉大的中心極限定理</title>
      <link>https://winterwang.github.io/post/central-limit-theory/</link>
      <pubDate>Thu, 19 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/central-limit-theory/</guid>
      <description>最近明顯可以感覺到課程的步驟開始加速。看我的課表：
手機畫面太小了。早上都是9點半開始，下午基本都是到5點。週一更慘，到7點。週二-週五中午都被統計中心的講座佔據。簡直是非人的生活。
這周概率論基礎結束。中心極限定理講完以後我們正式進入了 Inference 統計推斷的課程。我們花了一天時間講什麼是樣本估計 (Estimation)，什麼是參數精確度 (Precision)，什麼是自由度 (degree of freedom)，怎樣進行不偏的估計 (unbiased inference)。然後還有似然方程 (likelihood function)。
今天的更新還是簡單的把概率論掃尾一下。感受一下中心極限定理的偉大。
協方差 Covariance之前我們定義過，兩個獨立連續隨機變量 \(X,Y\) 之和的方差 Variance ：
\[Var(X+Y)=Var(X)+Var(Y)\]
然而如果他們並不相互獨立的話：
\[\begin{aligned}Var(X+Y) &amp;amp;= E[((X+Y)-E(X+Y))^2] \\&amp;amp;= E[(X+Y)-(E(X)+E(Y))^2] \\&amp;amp;= E[(X-E(X)) - (Y-E(Y))^2] \\&amp;amp;= E[(X-E(X))^2+(Y-E(Y))^2 \\&amp;amp; \;\;\; +2(X-E(X))(Y-E(Y))] \\&amp;amp;= Var(X)+Var(Y)+2E[(X-E(X))(Y-E(Y))]\end{aligned}\]可以發現在兩者和的方差公式展開之後多了一部分 \(E[(X-E(X))(Y-E(Y))]\)。 這個多出來的一部分就說明了二者 \((X, Y)\) 之間的關係。它被定義爲協方差 (Covariance):\[Cov(X,Y) = E[(X-E(X))(Y-E(Y))]\]
所以：
\[Var(X+Y)=Var(X)+Var(Y)+2Cov(X,Y)\]
要記住，協方差只能用於評價(X,Y)之間的線性關係 (Linear Association)。
以下是協方差 (Covariance) 的一些特殊性質：
\(Cov(X,X)=Var(X)\)\(Cov(X,Y)=Cov(Y,X)\)\(Cov(aX,bY)=ab\:Cov(X,Y)\)\(Cov(aR+bS,cX+dY)=ac\:Cov(R,X)+ad\:Cov(R,Y)\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+bc\:Cov(S,X)+bd\:Cov(S,Y)\)\(Cov(aX+bY,cX+dY)=ac\:Var(X)+ad\:Var(Y)\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+(ad+bc)Cov(X,Y)\)\(Cov(X+Y,X-Y)=Var(X)-Var(Y)\)If \(X, Y\) are independent.</description>
    </item>
    
    <item>
      <title>你買的彩票中獎概率到底有多少？</title>
      <link>https://winterwang.github.io/post/probability3/</link>
      <pubDate>Wed, 11 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/probability3/</guid>
      <description>二項分佈的概念 Binomial distribution二項分佈在醫學研究中至關重要，一組二項分佈的數據，指的通常是 \(n\) 次相互獨立的成功率爲 \(\pi\) 的伯努利實驗 (\(n\) independent Bernoulli trials) 中成功的次數。
當 \(X\) 服從二項分佈，記爲 \(X \sim binomial(n, \pi)\) 或\(X \sim bin(n, \pi)\)。它的(第 \(x\) 次實驗的)概率被定義爲：
二項分佈的期望和方差期望 \(E(X)\)若 \(X \sim bin(n,\pi)\)，那麼 \(X\) 就是這一系列獨立伯努利實驗中成功的次數。用 \(X_i, i =1,\dots, n\) 標記每個相互獨立的伯努利實驗。那麼我們可以知道 \(X=\sum_{i=1}^nX_i\)。\[\begin{align} E(X) &amp;amp;= E(\sum_{i=1}^nX_i)\\&amp;amp;= E(X_1+X_2+\cdots+X_n) \\&amp;amp;= E(X_1)+E(X_2)+\cdots+E(X_n)\\&amp;amp;= \sum_{i=1}^nE(X_i)\\&amp;amp;= \sum_{i=1}^n\pi \\&amp;amp;= n\pi\end{align}\]方差 \(Var(X)\)\[\begin{align}Var(X) &amp;amp;= Var(\sum_{i=1}^nX_i) \\&amp;amp;= Var(X_i+X_2+\cdots+X_n) \\&amp;amp;= Var(X_i)+Var(X_2)+\cdots+Var(X_n) \\&amp;amp;= \sum_{i=1}^nVar(X_i) \\&amp;amp;= n\pi(1-\pi) \\\end{align}\]超幾何分佈 hypergeometric distribution假設我們從總人數爲 \(N\) 的人羣中，採集一個樣本 \(n\)。假如已知在總體人羣中(\(N\))有 \(M\) 人患有某種疾病。請問採集的樣本 \(X=n\) 中患有這種疾病的人，服從怎樣的分佈？</description>
    </item>
    
    <item>
      <title>正態分佈</title>
      <link>https://winterwang.github.io/post/normal-distribution/</link>
      <pubDate>Wed, 11 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/normal-distribution/</guid>
      <description>概率密度曲線 probability density function， PDF一個隨機連續型變量 \(X\) 它的性質由一個對應的概率密度方程 (probability density function, PDF) 決定。
在給定的範圍區間內，如 \(a\sim b, (a &amp;lt; b)\)，它的概率滿足:
\[P(a\leqslant X \leqslant b) = \int_a^bf(x)dx\]
這個相關的方程，在 \(a\sim b\) 區間內的積分，就是這個連續變量在這個區間內取值的概率。# R codes for drawing a standard normal distribution by using ggplot2library(ggplot2)p &amp;lt;- ggplot(data.frame(x=c(-3,3)), aes(x=x)) +stat_function(fun = dnorm)p + annotate(&amp;quot;text&amp;quot;, x=2, y=0.3, parse=TRUE, label=&amp;quot;frac(1, sqrt(2*pi)) * e ^(-z^2/2)&amp;quot;) +theme(plot.subtitle = element_text(vjust = 1),plot.</description>
    </item>
    
    <item>
      <title>概率論2</title>
      <link>https://winterwang.github.io/post/probability2-4/</link>
      <pubDate>Tue, 10 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/probability2-4/</guid>
      <description>Bayes 理論的概念許多時候，我們需要將概率中的條件相互對調。例如：在已知該人羣中有20%的人有吸菸習慣(\(P(S)\))，吸菸的人有9%的概率有哮喘(\(P(A|S)\))，不吸菸的人有7%的概率有哮喘(\(P(A|\bar{S})\))的前提下，有個人前來門診，發現是哮喘患者，那麼這個人有多大的概率是一個菸民？也就是要求 \(P(S|A)\)
這裏先引入貝葉斯的概念：
我們可以將 \(P(A\cap S)\) 寫成：\[P(A\cap S)=P(A|S)P(S)\\or\\P(A\cap S)=P(S|A)P(A)\]這兩個等式是完全等價的。我們將他們連起來：
\[P(S|A)P(A)=P(A|S)P(S)\\\Rightarrow P(S|A)=\frac{P(A|S)P(S)}{P(A)}\]
是不是看起來又像是寫了一堆廢話？沒錯，你看出來是一堆廢話的時候，證明你也同意這背後的簡單邏輯。
再繼續，我們可以利用另外一個廢話：\(\because S+\bar{S}=1\\ \therefore P(A)=P(A\cap S)+P(A\cap\bar{S})\)
用上面的公式替換掉 \(P(A\cap S)+P(A\cap\bar{S}） \\ \therefore P(A)=P(A|S)P(S)+P(A|\bar{S})P(\bar{S})\)
可以得到貝葉斯理論公式：
\[P(S|A)=\frac{P(A|S)P(S)}{P(A|S)P(S)+P(A|\bar{S})P(\bar{S})}\]
回到上面說到的哮喘人中有多少比例吸菸的問題。可以繼續使用概率樹來方便的計算：
所以我們的結論就是，在已知該人羣中有20%的人有吸菸習慣(\(P(S)\))，吸菸的人有9%的概率有哮喘(\(P(A|S)\))，不吸菸的人有7%的概率有哮喘(\(P(A|\bar{S})\))的前提下，有個人前來門診，發現是哮喘患者，那麼這個人有24% 的概率是一個菸民(\(P(S|A)\))。
期望 Expectation (或均值 or mean) 和 方差 Variance期望（或均值）是用來描述一組數據中心位置的指標（另一個是中位數 Median）。對於離散型隨機變量 \(X\) (discrete random variables)，它的期望被定義爲：
\[E(X)=\sum_x xP(X=x)\]
所以就是將所有 \(X\) 可能取到的值乘以相應的概率後求和。這個期望（或均值）常常用希臘字母 \(\mu\) 來標記。
方差 Variance 是衡量一組數據變化幅度(dispersion/variability)的指標之一。 方差的定義是：
\[Var(X)=E((X-\mu)^2)\\其中，\mu=E(x)\]
實際上我們更加常用的是它的另外一個公式：
\[Var(X)=E(X^2)-E(X)^2\]
證明 上面兩個方差公式相等方差的性質：\(Var(X+b)=Var(X)\)\(Var(aX)=a^2Var(X)\)\(Var(aX+b)=a^2Var(X)\)伯努利分佈 Bernoulli distribution伯努利分佈，說的就是一個簡單的二分變量 (1, 0)，它取1時的概率如果是 \(\pi\)。那麼我們可以計算這個分佈的期望值:</description>
    </item>
    
    <item>
      <title>“你會用概率論來賭博嗎？”之解答</title>
      <link>https://winterwang.github.io/post/probability-gambling-answers/</link>
      <pubDate>Mon, 09 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/probability-gambling-answers/</guid>
      <description>前情提要：
假設你在一個電視遊戲節目。有上圖一樣的三扇門。其中一扇門後面有一輛保時捷，另兩扇門後面則是(味道奇特的)山羊。遊戲規則是主持人會讓你先選擇其中一扇門（先不打開你選的這扇門）。主持人隨後打開另外兩扇門中的一扇沒有保時捷的門。主持人問你，你要堅持選擇之前選中的那扇門，還是要改變主意換一扇門去猜是否可以猜中保時捷。請問，堅持選擇之前選中的門猜中保時捷的概率高，還是主持人打開一扇門以後改變主意猜中保時捷的概率更高呢？
答案是：必須改變主意才能提高中獎概率。
上述情況下，最簡單的是用概率樹 (probability tree) 來做決定：
解說一下：
假定保時捷在1號門後，你第一次選擇了1號門，那麼此時主持人可以任意打開2號或者三號門（因爲他們後面都沒有保時捷）。假定保時捷在1號門後，你第一次選了2號門，那麼此時主持人只能打開3號門（因爲一號門後是保時捷，按照遊戲規則主持人不能打開）。假定保時捷在1號門後，你第一次選了3號門，那麼此時主持人只能打開2號門（因爲一號門後是保時捷，按照遊戲規則主持人不能打開）。所以按照圖中給出的計算概率樹的過程可以得到:
\[P[改變主意以後贏得保時捷的概率]\\=\frac{1}{3}+\frac{1}{3}=\frac{2}{3}\\P[不改主意，贏得保時捷的概率]\\=\frac{1}{6}+\frac{1}{6}=\frac{1}{3}\]
你是否選擇了改變主意了呢？</description>
    </item>
    
    <item>
      <title>你會用概率論來賭博嗎？</title>
      <link>https://winterwang.github.io/post/probability-gambling/</link>
      <pubDate>Sun, 08 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/probability-gambling/</guid>
      <description>轉眼我已經進入課程的第二週了，總體來說，我們一半的時間都在電腦房練習 Stata 的數據清理和簡單的描述統計 (descriptive statistics)。從我個人的經驗來說，數據分析的過程，其實一大半的時間是消耗在 data cleaning 上的，即使手頭拿到了所謂的乾淨的數據，到真正要分析的時候就會發現一大堆的問題在裏面，需要重新整理，重新添加標記以使之變得更加讓人類可以讀懂。電腦是機器，他是不管你的數據是否乾淨的。只要你放了數據進去，邏輯還可以，沒有編程上的語法錯誤，它總歸會出來一些報告和結果的。如果就這麼直接用的話，大部分的人就會掉進陷阱。畢竟數據不光會說出事實真相，更多的情況下還會把真相給掩蓋住了。
我的其餘大部分時間都用在了複習高等數學的微積分上了。感覺好似回到了高中時代。其實大學的時候線性代數得分還是接近滿分的。後來多年不用，生疏了。剛打開複習的書的時候，許多微分積分的規則都已經忘記。通過這一週的辛苦練習，終於是找回了一點狀態。如果你也想有空的時候複習以下高中數學知識，這本書可以推薦給你：
Quick Calculus: Short Manual of Self-instruction
上面這本書的內容可以一邊閱讀，一邊練習。實在是複習的一本好書。我花了一週的課餘時間，從頭到尾把裏面的習題和解答全部完成。收穫很大。感覺年輕時的數學思維又開始在大腦裏復甦了。一身輕鬆。
下面想介紹一下上週學習的概率的基礎問題。
首先是最基礎的三個概率的公理：對於任意事件 \(A\)，它發生的概率 \(P(A)\) 滿足這樣的不等式： \(0 \leqslant P(A) \leqslant 1\)\(P(\Omega)=1\) , \(\Omega\) 是全樣本空間 (total sample space)對於互斥（相互獨立）的事件 \(A_1, A_2, \dots, A_n\) 有如下的等式關係： \(P(A_1\cup A_2 \cup \cdots \cup A_n)=P(A_1)+P(A_2)+\cdots+P(A_n)\)你是不是覺得上面三條公理都是廢話。不用擔心，我也是這麼覺得的。因爲所有人都認同的道理，才能成爲公理 (axiom)，因爲它們是不需要證明的自然而然形成的人人都接受的觀念。(axiom: a saying that is widely accepted on its own merits; its truth is assumed to be self-evident)
然而，正是這樣顯而易見的道理，確是拿來建築理論的基石，千萬不能小看了他們。例如，我們看下面這個看似也應該成爲公理的公式，你能證明嗎：
\(P(A_1\cup A_2) = P(A_1) + P(A_2) - P(A_1 \cap A_2)\)</description>
    </item>
    
    <item>
      <title>Matrix Revisions</title>
      <link>https://winterwang.github.io/post/matrix-revision/</link>
      <pubDate>Sat, 30 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/matrix-revision/</guid>
      <description>Basic Definition and notations:An \(m\times n\) matrix \(A\) is a rectangular array of numbers with \(m\) rows and \(n\) columns.The elements of a matrix \(A_{m\times n}\) are \(a_{ij}\)The order of a matrix is the number of rows by the number of columns, i.e. \(m\times n\)A column vector with \(m\) elements, \(y = \left( \begin{array}{c} y_1\\ y_2\\ \vdots\\ y_n \end{array} \right)\), is a matrix with only one column i.</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記 28</title>
      <link>https://winterwang.github.io/post/plus-equations-and-matrix-multiplication/</link>
      <pubDate>Wed, 16 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/plus-equations-and-matrix-multiplication/</guid>
      <description>行向量乘以矩陣，乘以列向量可得標量\(\underset{1\times m}{\underline{x}^t}\underset{m\times m}{M}\underset{m\times1}{\underline{x}}\) 或者 \(\underset{1\times m}{\underline{x}^t}\underset{m\times n}{N}\underset{n\times1}{\underline{y}}\) 的形式其實質上均爲 \(1\times1\)的標量，即最早我們接觸到的加法算式。
這樣的乘法計算通過矩陣（包括向量）的積的定義很容易進行。然而，反過來的話，（即從乘法算式反寫變形成爲行向量，矩陣，列向量相乘的形式），如果沒有練習的話，常常讓人覺得很困難。
在這裏，我們將多元變量分析中常常遭遇的加法算式拿出來舉例，練習變形成爲矩陣的積的形式。當然，爲了簡便起見，我們用三個元素的向量來練習：\[\underline{x}=\left(\begin{array}{c}x_1 \\x_2 \\x_3\end{array}\right), \underline{a}=\left(\begin{array}{c}a_1 \\a_2 \\a_3\end{array}\right)\]
\(a_1x_1^2+a_2x_2^2+a_3x_3^3\\ =x_1\times a_1x_1+x_2\times a_2x_2+x_3\times a_3x_3\\ =(x_1,x_2,x_3)\left( \begin{array}{c} a_1x_1 \\ a_2x_2 \\ a_3x_3 \end{array}\right)\\ =(x_1,x_2,x_3)\left( \begin{array}{c} a_1 &amp;amp; 0 &amp;amp; 0 \\ 0 &amp;amp; a_2 &amp;amp; 0 \\ 0 &amp;amp; 0 &amp;amp; a_3 \end{array}\right)\left( \begin{array}{c} x_1 \\ x_2 \\ x_3 \end{array}\right)\\ =\underline{x}^tD\underline{x}\) 其中\[D=\left(\begin{array}{c}a_1 &amp;amp; 0 &amp;amp; 0 \\0 &amp;amp; a_2 &amp;amp; 0 \\0 &amp;amp; 0 &amp;amp; a_3\end{array}\right)\]</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記 27</title>
      <link>https://winterwang.github.io/post/homogeneouse-linear-equations/</link>
      <pubDate>Fri, 11 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/homogeneouse-linear-equations/</guid>
      <description>\[X=\left(\begin{array}{c}x_{1} &amp;amp; x_{12} &amp;amp; \cdots &amp;amp; x_{1n}\\x_{21} &amp;amp; x_{22} &amp;amp; \cdots &amp;amp; x_{2n}\\\vdots &amp;amp; \vdots &amp;amp; \cdots &amp;amp; \vdots \\x_{n1} &amp;amp; x_{n2} &amp;amp; \cdots &amp;amp; x_{nn}\end{array}\right), \underline{a}=\left(\begin{array}{c}a_1 \\a_2 \\\vdots \\a_n\end{array}\right), \underline{0}=\left(\begin{array}{c}\underline{0}\\\underline{0}\\\vdots\\\underline{0}\\\end{array}\right)\]
用上述來表達的同次連立一次方程式 (system of homogeneouse linear equations)：
\[X\underline{a}=\underline{0}\]
即：\[\begin{align}\left\{\begin{array}{ll}x_{11}a_1+x_{12}a_2+\cdots+x_{1n}a_n = 0\\x_{21}a_1+x_{22}a_2+\cdots+x_{2n}a_n = 0\\\cdots\\x_{n1}a_1+x_{n2}a_2+\cdots+x_{nn}a_n = 0\\\end{array}\right.</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記 26</title>
      <link>https://winterwang.github.io/post/elementary-row-operations/</link>
      <pubDate>Thu, 10 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/elementary-row-operations/</guid>
      <description>擴大係數矩陣 \((X \underline{y})\) 通過行的基本變形，轉化成爲 \((E \underline{y}^*)\) 的時候，寫在右側的 \(\underline{y}^*\) 就是所求的 \(\underline{a}\)。
練習 解下列連立一次方程式\[\begin{align}\left\{\begin{array}{ll}a_1+2a_2+a_3 = 2\\2a_1+a_2+a_3 = 3\\a_1+a_2+2a_3 = 3\end{array}\right.\end{align}\]
解此連立方程組的擴大係數矩陣爲：\[(X \underline{y})=\left(\begin{array}{c}1 &amp;amp; 2 &amp;amp; 1 &amp;amp; 2\\2 &amp;amp; 1 &amp;amp; 1 &amp;amp; 3\\1 &amp;amp; 1 &amp;amp; 2 &amp;amp; 3\end{array}\right)\]下面開始行變形：\[\left(\begin{array}{c}1&amp;amp; 2&amp;amp; 1 &amp;amp; \vdots &amp;amp; 2\\2&amp;amp; 1&amp;amp; 1 &amp;amp; \vdots &amp;amp; 3\\1&amp;amp; 1&amp;amp; 2 &amp;amp; \vdots &amp;amp; 3\\\end{array}\right) \begin{align}\left\{\begin{array}{rr}(1)\\(2)\\(3)\end{array}\right.</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記 25</title>
      <link>https://winterwang.github.io/post/cramers-formula/</link>
      <pubDate>Wed, 09 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/cramers-formula/</guid>
      <description>克萊姆法則 Cramer’s Formula當 \(X\) 爲正則矩陣（\(|X|\neq0\)）時 連立一次方程式：\(X\underline{a}=\underline{y}\) 的解可以寫作：
\[a_j=\frac{|X_j|}{|X|} (j=1,2,\cdots, n)\]
其中： \(|X_j|\) 爲矩陣 \(X\) 的第 \(j\) 列替換爲 \(\underline{y}\) 以後的矩陣的行列式。
練習 解下列連立一次方程式\[\begin{align}\left\{\begin{array}{ll}a_1+2a_2+a_3 = 2\\2a_1+a_2+a_3 = 3\\a_1+a_2+2a_3 = 3\end{array}\right.\end{align}\]
解\[X=\left(\begin{array}{c}1 &amp;amp; 2 &amp;amp; 1 \\2 &amp;amp; 1 &amp;amp; 1 \\1 &amp;amp; 1 &amp;amp; 2\end{array}\right), \underline{a}=\left(\begin{array}{c}a_1 \\a_2 \\a_3 \\\end{array}\right), \underline{y}=\left(\begin{array}{c}2 \\3 \\3 \\\end{array}\right)\]</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記 24</title>
      <link>https://winterwang.github.io/post/inverse-matrix-method/</link>
      <pubDate>Sun, 06 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/inverse-matrix-method/</guid>
      <description>逆矩陣法解連立一次方程式\(X\) 為正則矩陣時(\(|X|\neq0\))，給 \(X\underline{a}=\underline{y}\) 等式兩邊同時乘以 \(X^{-1}\)，可以得到 \(X^{-1}X\underline{a}=X^{-1}\underline{y}\rightarrow E\underline{a}=X^{-1}\underline{y}\)。由此方法可以得到 \(\underline{a}=X^{-1}\underline{y}\)。
練習 解下列連立一次方程式\[\begin{align}\left\{\begin{array}{ll}a_1+2a_2+a_3 = 2\\2a_1+a_2+a_3 = 3\\a_1+a_2+2a_3 = 3\end{array}\right.\end{align}\]
解元連立方程式可以寫作\(X\underline{a}=\underline{y}\)，其中\[X=\left(\begin{array}{c}1 &amp;amp; 2 &amp;amp; 1 \\2 &amp;amp; 1 &amp;amp; 1 \\1 &amp;amp; 1 &amp;amp; 2\end{array}\right), \underline{a}=\left(\begin{array}{c}a_1 \\a_2 \\a_3 \\\end{array}\right), \underline{y}=\left(\begin{array}{c}2 \\3 \\3 \\\end{array}\right)\]之前我們已經用行的基本變形法和逆矩陣法分別計算過了 \(X^{-1}\) ：\[X^{-1}=\left(\begin{array}{c}-1/4 &amp;amp; 3/4 &amp;amp; -1/4\\3/4 &amp;amp; -1/4 &amp;amp; -1/4\\-1/4 &amp;amp; -1/4 &amp;amp; -3/4\\\end{array}\right)\]</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記 23</title>
      <link>https://winterwang.github.io/post/linear-simultaneous-equation/</link>
      <pubDate>Sat, 05 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/linear-simultaneous-equation/</guid>
      <description>連立一次方程式：\(\begin{align} \left\{ \begin{array}{ll} x_{11}a_1+x_{12}a_2+\cdots+x_{1n}a_n = y_1\\ x_{21}a_1+x_{22}a_2+\cdots+x_{2n}a_n = y_2\\ \cdots \\ x_{n1}a_1+x_{n2}a_2+\cdots+x_{nn}a_n = y_n \end{array} \right. \end{align}\)
可以看成是利用：
\(X=\left( \begin{array}{c} x_{11} &amp;amp; x_{12} &amp;amp; \cdots &amp;amp; x_{1n} \\ x_{21} &amp;amp; x_{22} &amp;amp; \cdots &amp;amp; x_{2n} \\ \vdots &amp;amp; \vdots &amp;amp; \cdots &amp;amp; \vdots \\ x_{n1} &amp;amp; x_{n2} &amp;amp; \cdots &amp;amp; x_{nn} \end{array} \right), \underline{a}=\left( \begin{array}{c} a_1 \\ a_2 \\ \vdots \\ a_n \\ \end{array} \right), \underline{y}=\left( \begin{array}{c} y_1 \\ y_2 \\ \vdots \\ y_n \\ \end{array} \right)\)</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記22</title>
      <link>https://winterwang.github.io/post/inverse-matrix/</link>
      <pubDate>Thu, 03 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/inverse-matrix/</guid>
      <description>正方形矩陣 $A$ 的行列式滿足 $|A| \neq 0$ 時，逆矩陣可以表達爲(當 $|A|=0$ 時，正方形矩陣 $A$ 沒有逆矩陣)： $$A^{-1}=\frac{1}{|A|}adj(A)=\frac{1}{|A|}(A_{ij})^t$$
$$=\frac{1}{|A|}\lbrace(-1)^{i+j}D_{ij}\rbrace^t$$
其中:
 $adj(A)$ 爲餘因子矩陣 $A_{ij}$ 爲餘因子 $D_{ij}$ 爲小行列式  (1) 之前舉過的例子再拿來試試看：
$$X=\left( \begin{array}{c} 1 &amp;amp; 2 &amp;amp; 1 \newline 2 &amp;amp; 1 &amp;amp; 1 \newline 1 &amp;amp; 1 &amp;amp; 2 \end{array} \right)=\left(\begin{array}{c} x_{11} &amp;amp; x_{12} &amp;amp; x_{13} \newline x_{21} &amp;amp; x_{22} &amp;amp; x_{23} \newline x_{31} &amp;amp; x_{32} &amp;amp; x_{33} \end{array}\right)$$ 元素 $x_{ij}$ 的餘因子 $X_{ij}(i,j=1,2,3)$ 爲：
$$X_{11}=(-1)^{1+1}\left| \begin{array}{c} 1 &amp;amp; 1 \newline 1 &amp;amp; 2 \end{array}\right|=1$$</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記21</title>
      <link>https://winterwang.github.io/post/2017-07-07/</link>
      <pubDate>Fri, 07 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/2017-07-07/</guid>
      <description>行的基本變形Theorem 1 (行的基本變形) 對矩陣進行下列操作的過程，被稱爲是行的基本變形（行的基本操作, elementary row operations）。
給任意一行乘以/除以一個非零的數。給任意一行加上/減去另外任意行的倍數。將任意兩行的對應元素互換。練習基本變形：用行的基本變形求矩陣 \(X=\left(\begin{array}{c} 1&amp;amp; 2&amp;amp; 1\\ 2&amp;amp; 1&amp;amp; 1\\ 1&amp;amp; 1&amp;amp; 2\\ \end{array}\right)\) 的逆矩陣 \(X^{-1}\) 首先，將矩陣 \(X\) 和同次單位矩陣 \(E_3\) 的元素寫成如下的左右並列的形式（用點隔開）\((X, E)\)。數字 (1) (2) (3) 表示行數：
\[\left(\begin{array}{c}1&amp;amp; 2&amp;amp; 1 &amp;amp; \vdots &amp;amp; 1 &amp;amp; 0 &amp;amp; 0\\2&amp;amp; 1&amp;amp; 1 &amp;amp; \vdots &amp;amp; 0 &amp;amp; 1 &amp;amp; 0\\1&amp;amp; 1&amp;amp; 2 &amp;amp; \vdots &amp;amp; 0 &amp;amp; 0 &amp;amp; 1\\\end{array}\right) \begin{align}\left\{\begin{array}{rr}(1)\\(2)\\(3)\end{array}\right.</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記20</title>
      <link>https://winterwang.github.io/post/2017-07-06/</link>
      <pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/2017-07-06/</guid>
      <description>逆矩陣逆矩陣定義Theorem 1 如果對於正方形矩陣 \(A\)，存在一個正方形矩陣 \(X\) 滿足 \(AX=XA=E\) (\(E\) 爲單位矩陣) 時，這個正方形矩陣 \(X\) 被叫做 \(A\) 的逆矩陣，寫作 \(A^{-1}\)。
存在逆矩陣 \((A^{-1})\) 的 \(A\) ，被叫做正則矩陣 (regular matrix, nonsingular matrix)。
不存在逆矩陣的 \(A\)，被叫做奇異矩陣 (singular matrix)。
滿足 \(|A|\neq 0\) 的矩陣 \(A\) 被叫做正則矩陣。滿足 \(|A|=0\) 的矩陣 \(A\) 被叫做奇異矩陣。
\(A\) 爲正則矩陣時，滿足：\(A^{-1}A=AA^{-1}=E\) 。
顯然，單位矩陣的逆矩陣也是一個單位矩陣: \[E^{-1}E=EE^{-1}=E, E^{-1}=E\]逆矩陣的性質對於正則矩陣 \(A, B\) 有以下性質：
\((AB)^{-1}=B^{-1}A^{-1}\)
注意此處矩陣 \(A，B\) 的順序對調了。\((A^{-1})^{-1}=A\)\((A^{t})^{-1}=(A^{-1})^t\)\((\lambda A)^{-1}=\frac{1}{\lambda}A^{-1} (\lambda \ne 0)\)對角矩陣 \(D_n=diag(a_{11},a_{22},\dotsm,a_{nn})\) 的逆矩陣寫作： \(D_n^{-1}=diag(1/a_{11}, 1/a_{22},\dotsm,1/a_{nn})\)；
注意此處的條件爲所有對角成分均非零: \(a_{11}a_{22}\dotsm a_{nn}\neq 0\)證明\((AB)(AB)^{-1}=E\) 等式兩邊從左往右乘以 \(A^{-1}\)</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記19</title>
      <link>https://winterwang.github.io/post/2017-04-02/</link>
      <pubDate>Sun, 02 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/2017-04-02/</guid>
      <description>行列式的性質具體的行列式的值，可以通過以下介紹的行列式性質，儘量簡潔地求解。本節也是爲了簡易示範，僅僅使用3次行列式作例子。4次以上的行列式性質依然相同，依此類推即可。
轉置矩陣的行列式，與轉置前的行列式一致。即：\(|A^t|=|A|\)。 \(|A|=\begin{vmatrix} 1 &amp;amp; 2 &amp;amp; 3 \\ 4 &amp;amp; 5 &amp;amp; 6 \\ 7 &amp;amp; 8 &amp;amp; 9 \\ \end{vmatrix}\)任意一列（或者任意一行）若乘以 \(\lambda\) 倍，那麼這個矩陣的行列式結果也將是乘以 \(\lambda\) 倍。
\(|A|=\begin{vmatrix} a_{11} &amp;amp; a_{12} &amp;amp; a_{13}\\ \lambda a_{21} &amp;amp;\lambda a_{22} &amp;amp; \lambda a_{23}\\ a_{31} &amp;amp; a_{32} &amp;amp; a_{33}\\ \end{vmatrix}\\ \;\;\;\;=|A|=\lambda \begin{vmatrix} a_{11} &amp;amp; a_{12} &amp;amp; a_{13}\\ a_{21} &amp;amp; a_{22} &amp;amp; a_{23}\\ a_{31} &amp;amp; a_{32} &amp;amp; a_{33}\\ \end{vmatrix}\)
\(|A|=\begin{vmatrix} a_{11} &amp;amp; a_{12} &amp;amp; \lambda a_{13}\\ a_{21} &amp;amp; a_{22} &amp;amp; \lambda a_{23}\\ a_{31} &amp;amp; a_{32} &amp;amp; \lambda a_{33}\\ \end{vmatrix}\\ \;\;\;\;=|A|=\lambda \begin{vmatrix} a_{11} &amp;amp; a_{12} &amp;amp; a_{13}\\ a_{21} &amp;amp; a_{22} &amp;amp; a_{23}\\ a_{31} &amp;amp; a_{32} &amp;amp; a_{33}\\ \end{vmatrix}\)任意一列（或者任意一行）的各成分乘以 \(\lambda\) 倍，與其他任意一列（或者任意一行）的各成分進行加運算（或者減運算）獲得的矩陣的行列式與原矩陣的行列式相同。</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記18</title>
      <link>https://winterwang.github.io/post/2017-03-15/</link>
      <pubDate>Wed, 15 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/2017-03-15/</guid>
      <description>行列式的定義與計算Theorem 1 (determinant) \(n\) 次正方形矩陣 \(A= (a_{ij})=\left( \begin{array}{c} a_{11}&amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n}\\ a_{21}&amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n}\\ \vdots&amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ a_{n1}&amp;amp; a_{n2} &amp;amp; \cdots &amp;amp; a_{nn} \end{array} \right)\) 的行列式(determinant)被定義爲是，\(A\) 的全部成分 \(a_{11},a_{12},\cdots,a_{nn}\) 的函數，這個函數是一個標量(scalar)。\(n\)次正方形矩陣 \(A\) 的行列式(\(n\)次行列式)，被記作：
\(|A|, |a_{ij}|, \det(A), \det(a_{ij})， \begin{vmatrix} a_{11}&amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n}\\ a_{21}&amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n}\\ \vdots&amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ a_{n1}&amp;amp; a_{n2} &amp;amp; \cdots &amp;amp; a_{nn} \notag \end{vmatrix}\)</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記17</title>
      <link>https://winterwang.github.io/post/2017-03-13/</link>
      <pubDate>Mon, 13 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/2017-03-13/</guid>
      <description>正定，半正定 (正值，半正值)對於任意的非零向量 \(\underline{x}(\neq\underline{0})\) ，如果2次型 \(\underline{x}^tA\underline{x}\) 始終滿足 \(\underline{x}^tA\underline{x} &amp;gt; 0\) 注意此處無等號。我們稱這個2次型爲正定(positive definite)，\(A\)爲正定矩陣(positive definite matrix)。另外，如果任意非零向量 \(\underline{x}(\neq\underline{0})\) 始終滿足2次型 \(\underline{x}^tA\underline{x} \geqslant 0\)， 這個2次型被叫做半正定(positive semi-definite)，\(A\)爲半正定矩陣(positive semi-definite matrix)。
\(x=\left( \begin{array}{} x_1\\ x_2\\ x_3 \end{array} \right), \ A=\left( \begin{array}{} 5 &amp;amp; 2 &amp;amp; 4\\ 2 &amp;amp; 2 &amp;amp; 3\\ 4 &amp;amp; 3 &amp;amp; 25 \end{array} \right)\)，2次型 \(\underline{x}^tA\underline{x}\) 是正定。因爲：
\(\underline{x}^tA\underline{x}=5x_1^2+2x_2^2+25x_3^2\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+4x_1x_2+8x_1x_3+6x_2x_3\\\;\;\;\;\;\;\;\;\;\:=(2x_1+x_2)^2+(x_2+3x_3)^2+(x_1+4x_3)^2\\ \because \underline{x}\neq\underline{0}=\left( \begin{array}{} 0\\ 0\\ 0 \end{array} \right)\\ \therefore \underline{x}^tA\underline{x}&amp;gt;0\)
\(x=\left( \begin{array}{} x_1\\ x_2\\ x_3 \end{array} \right), \ A=\left( \begin{array}{} 5 &amp;amp; -6 &amp;amp; 3\\ -6 &amp;amp; 25 &amp;amp; 32\\ 3 &amp;amp; 32 &amp;amp; 73 \end{array} \right)\)，2次型 \(\underline{x}^tA\underline{x}\) 是半正定。因爲：</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記16</title>
      <link>https://winterwang.github.io/post/2017-03-11/</link>
      <pubDate>Sat, 11 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/2017-03-11/</guid>
      <description>二次型(形式)對於 \(\underline{x}=\left( \begin{array}{c} x_{1}\\ x_{2}\\ \vdots\\ x_{n} \end{array} \right), A=\left( \begin{array}{c} a_{11}&amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n}\\ a_{21}&amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n}\\ \vdots&amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots\\ a_{n1}&amp;amp; a_{n2} &amp;amp; \cdots &amp;amp; a_{nn} \end{array} \right)\) 那麼：
\(\underline{x}^tA\underline{x}=\sum\limits_{i=1}^n\sum\limits_{j=1}^na_{ij}x_ix_j\\ \;\;\;\;\;\;\;\;\:\:=\sum\limits_{i=1}^na_{ii}x_i^2+\mathop{\sum\limits^n\sum\limits^n}_{i \neq j}a_{ij}x_ix_j\\ \;\;\;\;\;\;\;\;\:\:=\sum\limits_{i=1}^na_{ii}x_i^2+\mathop{\sum\limits^n\sum\limits^n}_{i\ &amp;lt;\ j}(a_{ij}+a_{ji})x_ix_j\)
被稱爲 \(\underline{x}\) 的同次2次式。又被叫做關於 \(x_1,x_2,\cdots,x_n\) 的2次型(quadratic form)。特別的，當 \(A\) 爲對稱矩陣時的2次型：\(\underline{x}^tA\underline{x}=\sum\limits_{i=1}^na_{ii}x_i^2+2\mathop{\sum\limits^n\sum\limits^n}_{i\ &amp;lt;\ j}a_{ij}x_ix_j\) 在多元變量分析中十分重要。
\(x=\left( \begin{array}{} x_1\\ x_2 \end{array} \right),\ A=\left( \begin{array}{} a_{11} &amp;amp; a_{12}\\ a_{12} &amp;amp; a_{22} \end{array} \right)\), 那麼： \(\underline{x}^tA\underline{x}=a_{11}x_1^2+a_{22}x_2^2+2a_{12}x_1x_2\)</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記15</title>
      <link>https://winterwang.github.io/post/2017-03-08/</link>
      <pubDate>Wed, 08 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/2017-03-08/</guid>
      <description>單位矩陣對角成分全部都是 \(1\) (此時我們假定有 \(n\) 個)，的對角矩陣被叫做單位矩陣(identity matrix, unit matrix)。寫作：\(\left( \begin{array}{c} 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\ 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\ 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; \vdots \\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \ddots &amp;amp; 0 \\ 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 1 \end{array} \right)=E_n=I_n\) 下標 \(n\) 常被省略。一般的，將 \(E_n\) 從左往右乘以 \(n\) 次正方形矩陣 \(A\)，的結果和從右往左相乘的結果是相等的： \(E_nA=AE_n=A\)。</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記14</title>
      <link>https://winterwang.github.io/post/2017-03-01/</link>
      <pubDate>Wed, 01 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/2017-03-01/</guid>
      <description>updated: 2017-03-07對稱矩陣Theorem 1 (symmetric matrix) 矩陣 \(A\) 如果完全和它的轉置矩陣 \(A^t\) 相同，即：\(A=A^t\) 成立時，這樣的正方形矩陣被稱爲對稱矩陣(symmetric matrix)。對稱矩陣的成分是以主對角線(main diagonal)對稱的。\(A=\left( \begin{array}{c} 4 &amp;amp; 3 &amp;amp; 2 &amp;amp; 1 \\ 3 &amp;amp; 5 &amp;amp; 6 &amp;amp; 7 \\ 2 &amp;amp; 6 &amp;amp; 8 &amp;amp; 9 \\ 1 &amp;amp; 7 &amp;amp; 9 &amp;amp; 0 \end{array} \right)\) 是典型的4次對稱矩陣。數學物理化學數學\(1\)\(0.72\)\(0.62\)物理\(0.</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記13</title>
      <link>https://winterwang.github.io/post/2017-02-28/</link>
      <pubDate>Tue, 28 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/2017-02-28/</guid>
      <description>連立一次方程式與矩陣向量的積連立一次方程式可以改寫爲矩陣與向量的積形成的向量的形式。特別的，以連立方程式的系數作成分的矩陣被叫做系數矩陣(coefficient matrix)。當我們看到連立方程式，應該能立刻條件反射地聯想到其對應的矩陣和向量的積。
\(\begin{align} \left\{ \begin{array}{rr} a_1+2a_2+3a_3 = 3\\ 2a_1+4a_2+5a_3 = 5\\ 3a_1+5a_2+6a_3 = 7 \end{array} \right. \end{align}\) 可以改寫成 \(\left( \begin{array}{c} 1 &amp;amp; 2 &amp;amp; 3 \\ 2 &amp;amp; 4 &amp;amp; 5 \\ 3 &amp;amp; 5 &amp;amp; 6 \end{array} \right)\left( \begin{array}{c} a_1\\ a_2\\ a_3 \end{array} \right)=\left( \begin{array}{c} 3\\ 5\\ 7 \end{array} \right)\) 的形式。
如果把等號右邊的列向量寫到系數矩陣的右側，形成的矩陣被叫做擴大系數矩陣(augmented coefficient)：
\(\left( \begin{array}{c} 1 &amp;amp; 2 &amp;amp; 3 &amp;amp; 3 \\ 2 &amp;amp; 4 &amp;amp; 5 &amp;amp; 5 \\ 3 &amp;amp; 5 &amp;amp; 6 &amp;amp; 7 \end{array} \right)\)</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記12</title>
      <link>https://winterwang.github.io/post/2017-02-22/</link>
      <pubDate>Wed, 22 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/2017-02-22/</guid>
      <description>矩陣乘法運算矩陣乘法定義Theorem 1 (matrix multiplication) 兩個矩陣 \(A, B\) ，只有 \(A\) 的列數和 \(B\) 的行數相等(這種特徵又被稱爲：矩陣 \(A,B\) 可整合的，conformable)時，才有定義：\(AB\)。\(AB\) 則爲新的矩陣，類型爲 \(A\) 的行數， \(B\)的列數。即：\(A_{k\times l}, \; B_{m\times n}\) 且 \(l=m\) 時才能計算乘積: \(AB_{k\times n}\)。\(A_{2\times3}=\left( \begin{array}{c} 4 &amp;amp; 6 &amp;amp; 8\\ 2 &amp;amp; 1 &amp;amp; 3\\ \end{array} \right),\; B_{3\times2}=\left( \begin{array}{c} 0 &amp;amp; 8\\ 2 &amp;amp; -1\\ 9 &amp;amp; 4 \\ \end{array} \right)\) 時，
“\(A\)的列數” \(=\) “\(B\) 的行數” \(= 3\)，因此積 \(AB\) 被定義，類型是 \((2,2)\) “\(B\)的列數” \(=\) “\(A\) 的行數” \(= 2\)，因此積 \(BA\) 被定義，類型是 \((3,3)\)</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記11</title>
      <link>https://winterwang.github.io/post/2017-02-21/</link>
      <pubDate>Tue, 21 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/2017-02-21/</guid>
      <description>矩陣的定義Theorem 1 (matrix) 將\(m\times n\) 個數 \(a_{ij} (i=1,2,\cdots,m; j=1,2,\cdots,n)\), 寫成縱 \(m\) 行， 橫 \(n\) 列的長方形或者正方形，左右用圓括號或者方括號包含在內。我們稱之爲 \(m\times n\) 矩陣(matrix)，或者 \((m, n)\) 矩陣。 \(m\times n\) 或者 \((m,n)\) 被稱爲是這個矩陣的類型。我們常用大寫字母來標記一個矩陣，如下面的矩陣我們標記爲 \(A\)。 如果要特別明示矩陣的類型，可以寫作 \(\mathop{A}_{m\times n}, \mathop{A}_{(m, n)}, \; A(m\times n)\)。兩個矩陣如果行數相等，列數也相等，我們稱他們爲類型相同的矩陣。構成矩陣的一個個數 \(a_{11},a_{12},\cdots,a_{mn}\) 被叫做矩陣的成分(component, element, entry)。
第\(i\)行，第\(j\)列交叉的地方的成分，\(a_{ij}\) 被叫做 \((i,j)\) 成分。矩陣有時候也會寫成 \(A=(a_{ij})\)\(\left( \begin{array}{c} a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1j} &amp;amp; \cdots &amp;amp; a_{1n}\\ a_{21} &amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2j} &amp;amp; \cdots &amp;amp; a_{2n}\\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ a_{i1} &amp;amp; a_{i2} &amp;amp; \cdots &amp;amp; a_{ij} &amp;amp; \cdots &amp;amp; a_{in}\\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ a_{m1} &amp;amp; a_{m2} &amp;amp; \cdots &amp;amp; a_{mj} &amp;amp; \cdots &amp;amp; a_{mn}\\ \end{array} \right), \\ \left[ \begin{array}{c} a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1j} &amp;amp; \cdots &amp;amp; a_{1n}\\ a_{21} &amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2j} &amp;amp; \cdots &amp;amp; a_{2n}\\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ a_{i1} &amp;amp; a_{i2} &amp;amp; \cdots &amp;amp; a_{ij} &amp;amp; \cdots &amp;amp; a_{in}\\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ a_{m1} &amp;amp; a_{m2} &amp;amp; \cdots &amp;amp; a_{mj} &amp;amp; \cdots &amp;amp; a_{mn}\\ \end{array} \right]\)</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記10</title>
      <link>https://winterwang.github.io/post/2017-02-19/</link>
      <pubDate>Mon, 20 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/2017-02-19/</guid>
      <description>向量的內積 (inner product)Theorem 1 (vectors inner product) 向量的內積運算，僅限定於維度相同的兩個向量之間。一個向量爲橫向量寫在左側，一個向量爲列向量寫在右側，兩個向量的相對應成分一一相乘，然後將各成分乘積相加的過程，我們稱之爲內積(inner product, scalar product)運算。內積運算結果通常不會是向量，而是標量(scalar)，或正或負，或爲零。向量 \(\underline{a}\) 與向量 \(\underline{b}\) 的內積寫作：\(\underline{a}^t\underline{b}, \underline{b}^t\underline{a}\) 或者寫作： \(\underline{a}\cdot\underline{b}, (\underline{a},\underline{b}), &amp;lt;\underline{a},\underline{b}&amp;gt;\)。內積爲 \(0\) 的向量我們稱他們爲正交向量(orthogonal)，寫作：\(\underline{a}\perp\underline{b}\)。內積，與和記號: \(\sum\) 有緊密聯系。我們常常會把 \(\sum\) 式子/量寫成向量的內積形式。練習列向量 \(\underline{a}=\left( \begin{array}{c} a_1\\ a_2\\ a_3 \end{array} \right), \underline{b}=\left( \begin{array}{c} b_1\\ b_2\\ b_3 \end{array} \right)\) 的內積：
\(\underline{a}^t\underline{b}=(a_1,a_2,a_3)\left( \begin{array}{c} b_1\\ b_2\\ b_3 \end{array} \right)=a_1b_1+a_2b_2+a_3b_3\\=\sum\limits_{i=1}^3a_ib_i=\sum\limits_{i=1}^3b_ia_i=\underline{b}^t\underline{a}\)
橫向量 \(\underline{a}=(a_1,a_2,a_3), \underline{b}=(b_1,b_2,b_3)\) 的內積：
\(\underline{a}\underline{b}^t=(a_1,a_2,a_3)\left( \begin{array}{c} b_1\\ b_2\\ b_3 \end{array} \right)=a_1b_1+a_2b_2+a_3b_3\\=\sum\limits_{i=1}^3a_ib_i=\sum\limits_{i=1}^3b_ia_i=\underline{b}\underline{a}^t\)
完全相同的兩個列向量 \(\underline{x}=\left( \begin{array}{c} x_1\\ x_2\\ x_3 \end{array} \right),\;\underline{x}=\left( \begin{array}{c} x_1\\ x_2\\ x_3 \end{array} \right)\) 的內積：</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記9</title>
      <link>https://winterwang.github.io/post/2017-02-18/</link>
      <pubDate>Sat, 18 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/2017-02-18/</guid>
      <description>特殊向量零向量 (zero vector, null vector)全部的成分均爲\(0\)的向量，我們稱之爲零向量(zero vector, null vector), 寫作： \(\underline{0}\)注意與標量(scalar) \(0\) 相區分。如果想要加注零向量的維度，我們可以在右下角加上 \(n\)：\(\underline{0}_n\) ，意爲 \(n\) 維度的零向量。不是零向量的向量又被叫做，非零向量(non-zero vector, non-null vector)。例如： 列向量：\(\underline{0}_3=\left( \begin{array}{c} 0\\ 0\\ 0\\ \end{array} \right)\)， 行向量：\(\underline{0}_3^t=(0,0,0)\)
\(1\) 向量 (vector with all elements 1)當一個向量的全部成分都是數字 \(1\)，我們稱這個向量爲 \(1\) 向量。 \(\underline{1}\)這裏也需要注意與標量 \(1\) 相區分。如果想要加注\(1\)向量的維度，我們可以在右下角加上 \(n\)：\(\underline{1}_n\) ，意爲 \(n\) 維度的\(1\)向量。例如：列向量：\(\underline{1}_4=\left( \begin{array}{c} 1\\ 1\\ 1\\ 1 \end{array} \right)\)， 行向量：\(\underline{1}_4^t=(1,1,1,1)\)
第 \(i\) 基本向量Theorem 1 (fundamental vector) \(n\) 維度的向量，假如它的第 \(i\) 個成分是自然數 \(1\)，其他的成分全部都是 \(0\)， 我們稱這樣的向量爲第 \(\textbf{i}\) 基本向量 (fundamental vector)。寫作 \(\underline{\smash{e}}_i\)。平時我們較少用到一個單獨的基本向量。大多情況下我們用的是由 \(n\) 個單獨向量組成的一組向量。這個類型的向量與坐標軸的關系緊密。例如：維度爲4的第 \(1\sim4\) 基本向量：\(\underline{e}_1=\left( \begin{array}{c} 1\\ 0\\ 0\\ 0 \end{array} \right), \; \underline{e}_2=\left( \begin{array}{c} 0\\ 1\\ 0\\ 0 \end{array} \right), \; \underline{e}_3=\left( \begin{array}{c} 0\\ 0\\ 1\\ 0 \end{array} \right), \; \underline{e}_4=\left( \begin{array}{c} 0\\ 0\\ 0\\ 1 \end{array} \right)\)</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記8</title>
      <link>https://winterwang.github.io/post/2017-02-17/</link>
      <pubDate>Fri, 17 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/2017-02-17/</guid>
      <description>向量 vector列向量 column vector在等號的右側，將數字寫成一列，左右用圓括號或者方括號包含在內的形式，被叫做列向量(column vector)：
\(\underline{a}=\left( \begin{array}{c} a_1\\ a_2\\ \vdots\\ a_i\\ \vdots\\ a_n \end{array} \right), \;\; \textbf{a}=\left[ \begin{array}{c} a_1\\ a_2\\ \vdots\\ a_i\\ \vdots\\ a_n \end{array} \right]\)
我們接下來將會繼續定義，向量的加減法，標量乘法(scalar multiplication)。把上述的向量用一個文字表示的時候，通常會記爲下劃線 \(\underline{a}\)，或者是加粗的小寫字母： \(\bf{a}\)。
構成向量的各個數字，被命名爲成分(component, element, entry)，從上往下第 \(i\) 個成分稱爲第 \(i\) 成分。
成分的個數爲 \(n\)，就被稱爲這個向量具有 \(n\) 個維度(次元，dimension)，或者說這個向量的維度爲 \(n\)。成分可以是數字，也可以是函數，或者式子。如果兩個列向量的維度一致，我們稱這兩個列向量的型(size, order),或者 類型(type) 一致。
成分只有一個的向量，被特別稱爲標量(scalar)，原則上不加括號。
將向量成分全部羅列出來，寫成上面的形式的過程，被稱爲成分表示。在多元變量分析中，我們說到向量，多默認指的就是列向量。
\(\underline{b}=\left( \begin{array}{c} 16\\ 59\\ 80\\ \end{array} \right)=\left[ \begin{array}{c} 16\\ 59\\ 80\\ \end{array} \right]=\textbf{b}\)
今後我們都用字母帶下劃線，圓括號包含數字的方式表示向量。</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記7</title>
      <link>https://winterwang.github.io/post/2017-02-16/</link>
      <pubDate>Thu, 16 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/2017-02-16/</guid>
      <description>分解平方和 1樣本量均爲 \(n\) 的兩變量 \(z, \hat{z}\) 如下表，已知這兩個變量滿足條件：
\(\bar{z}=\frac{1}{n}\sum\limits_{i=1}^nz_i=\frac{1}{n}\sum\limits_{i=1}^n\hat{z}_i=\bar{\hat{z}},\) \(\sum\limits_{i=1}^n(z_i-\hat{z_i})(\hat{z_i}-\bar{z})=0\)
個体の番号変量 \(z\)変量 \(\hat{z}\)\(1\)\(z_1\)\(\hat{z}_1\)\(2\)\(z_2\)\(\hat{z}_2\)\(\vdots\)\(\vdots\)\(\vdots\)\(i\)\(z_i\)\(\hat{z}_i\)\(\vdots\)\(\vdots\)\(\vdots\)\(n\)\(z_n\)\(\hat{z}_n\)此時我們有：
全平方和(全変動，總平方和，總變動， Total sum of Squares)：
\(S_T=(z_i-\bar{z})^2+(z_2-\bar{z})^2+\cdots+(z_n-\bar{z})^2\\ \;\;\;\;=\sum\limits_{i=1}^n(z_i-\bar{z})^2\)回歸平方和(回歸變動，Regression sum of Squares)
\(S_R=(\hat{z_1}-\bar{\hat{z}})^2+(\hat{z_2}-\bar{\hat{z}})^2+\cdots+(\hat{z_n}-\bar{\hat{z}})^2\\ \;\;\;\;=\sum\limits_{i=1}^n(\hat{z_i}-\bar{\hat{z}})^2=\sum\limits_{i=1}^n(\hat{z_i}-\bar{z})^2\)殘差平方和(誤差平方和，殘差變動，誤差變動，residual sum of Squares)
\(S_e=(z_1-\hat{z_1})^2+(z_2-\hat{z_2})^2+\cdots+(z_n-\hat{z_n})^2\\ \;\;\;\;=\sum\limits_{i=1}^n(z_i-\hat{z_i})^2\)上面三個平方和之間，有如下的關系：\[\begin{equation}S_T=S_R+S_e\tag{1}\end{equation}\]既：全平方和等於殘差平方和與回歸平方和之和。(1)式被稱爲平方和的分解(decomposition of sum of squares)證明(1)式解：\[\begin{equation}\begin{split}S_T &amp;amp; = \sum\limits_{i=1}^n(z_i-\bar{z})^2 \\&amp;amp; = \sum\limits_{i=1}^n\left\{(z_i-\hat{z_i})+(\hat{z_i}-\bar{z})\right\}^2\\&amp;amp; = \sum\limits_{i=1}^n\left\{(z_i-\hat{z_i})^2+(\hat{z_i}-\bar{z})^2+2(z_i-\hat{z_i})(\hat{z_i}-\bar{z})\right\}\\&amp;amp; = \sum\limits_{i=1}^n(z_i-\hat{z_i})^2+\sum\limits_{i=1}^n(\hat{z_i}-\bar{z})^2 + 0\\&amp;amp; = S_e + S_R\end{split}\end{equation}\]最後一步等式，利用了一開始給出的條件 \(\sum\limits_{i=1}^n(z_i-\hat{z_i})(\hat{z_i}-\bar{z})=0\)</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記6</title>
      <link>https://winterwang.github.io/post/2017-02-15/</link>
      <pubDate>Wed, 15 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/2017-02-15/</guid>
      <description>數據的變換平均值附近的偏差:各個數值 \(x_i\) 與樣本平均值 \(\bar{x}\) 的差 \[x_i^\prime=x_i-\bar{x} (i = 1,2,\cdots,n)\] 稱爲數據 \(x_i\) 在它的平均值 \(\bar{x}\) 附近的偏差(deviation)。通常我們說求偏差，指的是，對數據 \(x_i\) 進行偏差轉換。這個過程又被稱作是中心變換(centering)關於偏差，我們列舉如下兩個有特徵的的概括統計：樣本平均值：\[\begin{equation}\bar{x}^\prime=\frac{1}{n}\sum_{i=1}^nx_i^\prime=0\tag{1}\end{equation}\]樣本偏差平方和：\[\begin{equation}SS^\prime=\sum_{i=1}^n(x^\prime)^2=SS\tag{2}\end{equation}\]練習：證明(1)解：證明(1):\[\bar{x}^\prime=\frac{\sum\limits_{i=1}^n(x_i-\bar{x})}{n}\\\;\;\;=\frac{\sum\limits_{i=1}^nx_i-n\bar{x}}{n}\\\;\;\;=\frac{\sum\limits_{i=1}^nx_i}{n}-\bar{x}\\\;\;\;=\bar{x}-\bar{x}=0\]
數據的標準化：將數據 \(x_i\) 的平均值 \(\bar{x}\) 附近的偏差除以樣本標準偏差 \(s\) 從而獲得下面式子所表示的數據 \(z_i\) 的過程，被叫做數據的標準化 (standardization)：\[\begin{equation}z_i=\frac{x_i-\bar{x}}{s}\tag{3}\end{equation}\]標準化後的數據 \(z_i\) 的概括統計有下列特徵：樣本平均值：\[\begin{equation}\bar{z}=\frac{1}{n}\sum_{i=1}^nz_i=0\tag{4}\end{equation}\]樣本方差:\[\begin{equation}s_{z}^2=\frac{1}{n}\sum_{i=1}^nz_i^2=1\tag{5}\end{equation}\]由於標準化數據具有上述兩個非常顯著的特徵，均值爲 \(0\)，方差爲 \(1\)，因此我們實際分析數據過程中常常對數據進行標準化。標準化以後的數據，單位消失，變成了一組無名數\(\divideontimes\) 數據的標準化，有時你會看到被定義爲:\[\begin{equation}z_i=\frac{x_i-\bar{x}}{u}\tag{6}\end{equation}\] 此時的不偏樣本方差爲：\[\begin{equation}u_z^2=\frac{1}{n-1}\sum_{i=1}{n}z_i^2=1\tag{7}\end{equation}\]2變量數據的概括統計：樣本量同爲 \(n\) 的 \(2\) 變量 \(x_1,x_2\) 的數據，表示爲如下表格：個体の番号変量 \(x_1\)変量 \(x_2\)\(1\)\(x_{11}\)\(x_{12}\)\(2\)\(x_{21}\)\(x_{22}\)\(\vdots\)\(\vdots\)\(\vdots\)\(i\)\(x_{i1}\)\(x_{i2}\)\(\vdots\)\(\vdots\)\(\vdots\)\(n\)\(x_{n1}\)\(x_{n2}\)按照變量 \(x_1,x_2\) 各自的定義：樣本平均值：\(\bar{x_1}=\frac{1}{n}\sum\limits_{i=1}^nx_{i1}, \; \bar{x_2}=\frac{1}{n}\sum\limits_{i=1}^nx_{i2}\)樣本偏差平方和: \(SS_1=\sum\limits_{i=1}^n(x_{i1}-\bar{x_1})^2, \; SS_2=\sum\limits_{i=1}^n(x_{i2}-\bar{x_2})^2\)樣本方差： \(s_1^2=\frac{SS_1}{n}, \; s_2^2=\frac{SS_2}{n}\)樣本標準偏差： \(s_1=\sqrt{s_1^2}, \; s_2=\sqrt{s_2^2}\)不偏樣本方差： \(u_1^2=\frac{SS}{n-1}, \; u_2^2=\frac{SS_2}{n-1}\)不偏樣本方差平方根: \(u_1=\sqrt{u_1^2}, \; u_2=\sqrt{u_2^2}\)對於這樣一對變量 \(x_1,x_2\) 來說，我們又可以追加如下的概括統計：樣本總體平均值： \(\bar{x}=\frac{1}{n+n}(\sum\limits_{i-1}^nx_{i1}+\sum\limits_{i-1}^nx_{i2})\)樣本方差積和(cross-product)：\[\begin{equation}\begin{split}S_{12} &amp;amp; = \sum_{i=1}^n(x_{i1}-\bar{x_1})\cdot(x_{i2}-\bar{x_2})\\&amp;amp; = \sum_{i=1}^n(x_{i1}x_{i2}-\bar{x_1}x_{i2}-x_{i1}\bar{x_2}+\bar{x_1}\bar{x_2})\\&amp;amp; = \sum_{i=1}^nx_{i1}x_{i2}-\bar{x_1}\sum_{i=1}^nx_{i2}-{\sum_{i=1}^nx_{i1}}\bar{x_2}+n\bar{x_1}\bar{x_2}\\&amp;amp; = \sum_{i=1}^nx_{i1}x_{i2}-\bar{x_1}\cdot n\bar{x_2}-n\bar{x_1}\cdot\bar{x_2}+n\bar{x_1}\bar{x_2}\\&amp;amp; = \sum_{i=1}^nx_{i1}x_{i2}-n\cdot\bar{x_1}\cdot\bar{x_2}\\&amp;amp; = \sum_{i=1}^nx_{i1}x_{i2}-n\cdot\frac{\sum\limits_{i=1}^nx_{i1}}{n}\cdot\frac{\sum\limits_{i=1}^nx_{i2}}{n}\\&amp;amp; = \sum_{i=1}^nx_{i1}x_{i2}-\frac{1}{n}(\sum_{i=1}^nx_{i1})(\sum_{i=1}^nx_{i2}) = S_{21}\end{split}\tag{8}\end{equation}\]</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記5</title>
      <link>https://winterwang.github.io/post/2017-02-13/</link>
      <pubDate>Tue, 14 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/2017-02-13/</guid>
      <description>2017-02-15 updated.數據的種類和尺度表1. 20歳の若者9名のデータ性別健康状態体温身長男女極良良好普通不良極悪°Ccm1101000036.</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記4</title>
      <link>https://winterwang.github.io/post/2017-02-12-t/</link>
      <pubDate>Sun, 12 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/2017-02-12-t/</guid>
      <description>連立方程式 (simultaneous equations)連立方程式，將與第六章談的特徵值問題(固有値問題)有緊密聯系，此處我們一起觀察幾種不同的組合：
解同次連立1次方程式 \(\left\{ \begin{array}{ll} (1)\;a_1+2a_2+3a_3 = 0 \\ (2)\;2a_1+4a_2+5a_3 = 0 \;\\ (3)\;3a_1+5a_2+6a_3 = 0 \\ \end{array} \right.\) 由 \(2\times(1)-(2)\) 可得 \(a_3=0\) 。 代入 \((1),(2),(3)\) 式後，\((3)-(2)\) 可得 \(a_1=-a_2\) 。 代入 \((1)\) 式可得 \(a_2=0\) 。 再代入 \((4)\) 式可知 \(a_1=0\) 。最終可得 \(a_1=a_2=a_3=0\) 其實上述問題不解自明 (trivial solution)。 那麼同次1次連立方程式 (homogeneous system) 除了自明解之外，還有別的解嗎? 我們再看下面一例。
解 \(\left\{ \begin{array}{ll} (1)\;4a_1+3a_2+6a_3 = 0 \\ (2)\;2a_1+a_2+4a_3 = 0 \;\\ (3)\;a_1+a_2+a_3 = 0 \\ \end{array} \right.\) 上述方程表面上看有三個式子，實際上由於 \((3)=\left\{(1)-(2)\right\}\div2\) 只有2個有意義的方程式。如此這般，有3個未知數，卻只有兩個連立方程組，是無法求解的。如果將三個未知數中的一個例如 \(a_3\) 視爲常數(定数) (寫作：\(s\) ) 即： \((4)\;a_3=s\) 整理方程組得到新的連立方程 \(\left\{ \begin{array}{ll} (1^\prime)\;4a_1+3a_2 = -6s \\ (2^\prime)\;2a_1+a_2 = -4s \;\\ \end{array} \right.</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記3</title>
      <link>https://winterwang.github.io/post/2017-02-10/</link>
      <pubDate>Fri, 10 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/2017-02-10/</guid>
      <description>函數的最大值最小值問題沒有制約條件的情況函數 \(F(a_1,a_2,\dots,a_i,\dots,a_n)\) 取最大值或者最小值時，以下的連立方程\[\frac{\partial F}{\partial a_1}=0,\frac{\partial F}{\partial a_2}=0，\frac{\partial F}{\partial a_3}=0, \dots,\frac{\partial F}{\partial a_i}=0, \dots, \frac{\partial F}{\partial a_n}=0\]要成立(必要條件)。
1.已知下列方程有最小值，求當該方程取最小值時\(a_1,a_2\)的值 \[F(a_1,a_2)=\left\{y_1-(a_1+a_2x_1)\right\}^2+\left\{y_2-(a_1+a_2x_2)\right\}^2+\cdots+\left\{y_n-(a_1+a_2x_n)\right\}^2\\\;\;\;\;\;\;\;\;\;\;\;\;\;\;=\sum\limits_{i=1}^n\left\{y_i-(a_1+a_2x_i)\right\}^2\\\]
求下列方程取最大或者最小值時的\(a_1,a_2,a_3\)的大小：\[F(a_1,a_2,a_3)=a_1^2+a_1a_2+a_1a_3+a_2^2+a_2a_3+a_3^2-6a_1-3a_2-7a_3\]</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記2</title>
      <link>https://winterwang.github.io/post/2017-02-08/</link>
      <pubDate>Wed, 08 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/2017-02-08/</guid>
      <description>偏微分1個變量的函數的微分公式：函數 \(f(a)\) 關於變量 \(a\) 的微分，被定義爲： \(\lim\limits_{h \to 0} \frac{f(a+h)-f(a)}{h}\) , 寫作 \(\frac{df}{da}\), 具有下列性質：\(f(a) = a^n\) 時， \(\frac{df}{da} = na^{n-1}\) 重要\(\frac{d}{da}\left\{kf(a)+lg(a)\right\}=k\frac{df}{da}+l\frac{dg}{da}\) (\(k,l\) 是常數)\(\frac{d}{da}\left\{f(a) \cdot g(a)\right\}=\frac{df}{da}g(a)+f{a}\frac{dg}{da}\)\(\frac{d}{da}\left\{\frac{f(a)}{g(a)}\right\}=\frac{\frac{df}{da}g(a)-f(a)\frac{dg}{da}}{\left\{g(a)\right\}^2}\), 特別的有，\(\frac{d}{da}\left\{\frac{1}{g(a)}\right\}=-\frac{\frac{dg}{da}}{\left\{g(a)\right\}^2}\)\(y=f(b), b=g(a)\) 時， \(\frac{dy}{da}=\frac{dy}{db}\frac{db}{da}\)2次（2階）微分 【二階導數】:
\(f(a)\) 關於常數 \(a\) 的微分 \(\frac{df}{da}\) 的二次微分表示爲： \(\frac{d^2f}{da^2}\)
多個變量的函數的微分偏微分包含了 \(n\) 個獨立變量 \(a_1, a_2,a_3,\cdots,a_i,\cdots,a_n\)的函數，即多變量函數 \(F(a_1, a_2,a_3,\cdots,a_i,\cdots,a_n)\) 關於 \(a_i (i=1,2,\cdots,n)\) 的偏微分 (partial differentiation) 的定義是，把 \(a_i\) 以外的獨立變量當做常數（定数），將函數 \(F\) 對變量 \(a_i\) 求微分，寫作： \(\frac{\partial F}{\partial a_i}\)。</description>
    </item>
    
    <item>
      <title>「統計解析のための線形代数」復習筆記1</title>
      <link>https://winterwang.github.io/post/2017-02-06/</link>
      <pubDate>Mon, 06 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/2017-02-06/</guid>
      <description>和記號\(\sum\)\(\sum\) 的性質 (1)下標(添字) \(x_1 + x_2 + x_3 + \dots + x_n\) 記作如下:\[\sum_{i=1}^{n}x_i\]\(\sum_{i=1}^{n}x_i\) 中的\(i\) 稱爲dummy index可以簡略寫爲：\(\sum x\) 或者 \(\sum_1 x_i\), \(\sum x_i\)\(\sum\) 的性質 (2)\[\begin{equation}\sum_{i=1}^{n}(ax_i + by_i)= a\sum_{i=1}^{n}x_i + b\sum_{i=1}^{n}y_i \tag{1}\end{equation}\]\(\sum_{i=1}^{n}ax_i = a\sum_{i=1}^{n}x_i\) 常數(定数)可以提前\(\sum_{i=1}^{n}a = na\)\(\sum_{i=1}^{n}1 = n\)\(\sum_{i=1}^{n}(ax_i+b) = a\sum_{i=1}^{n}x_i + nb\)公式(1)的應用:\[\begin{aligned}\sum_{i=1}^{n}(ax_i -by_i)^2 &amp;amp;= \sum_{i=1}^{n}(a^2x_i^2 - 2abx_iy_i + b^2y_i^2) \\&amp;amp;= \sum_{i=1}^{n}a^2x_i^2 -\sum_{i=1}^{n}2abx_iy_i + \sum_{i=1}^{n}b^2y_i^2 \\&amp;amp;= a^2\sum_{i=1}^{n}x_i^2 - 2ab\sum_{i=1}^{n}x_iy_i + b^2\sum_{i=1}^{n}y_i^2\end{aligned}\]但是，乘法或平方有如下性質，計算方差(分散)或者相關系數時需要注意：\[\sum_{i=1}^{n}x_i^2 \neq (\sum_{i=1}^{n}x_i)^2\] 以及 \[\sum_{i=1}^{n}x_iy_i \neq (\sum_{i=1}^{n}x_i)(\sum_{i=1}^{n}y_i)\]自然數的冪運算之和(冪[べき]乗の和)的公式:\[\begin{aligned}1+2+3+\dots+n &amp;amp;= \sum_{t=1}^{n}t = \frac{n(n+1)}{2}\\1^2+2^2+3^2+\dots+n^2 &amp;amp;= \sum_{t=1}^{n}t^2 = \frac{n(n+1)(2n+1)}{6} \\1^3+2^3+3^3+\dots+n^3 &amp;amp;= \sum_{t=1}^{n}t^3 = {\frac{n(n+1)}{2}}^2 \\1^4+2^4+3^4+\dots+n^4 &amp;amp;= \sum_{t=1}^{n}t^4 = \frac{n(n+1)(2n+1)(3n^2+3n-1)}{30}\end{aligned}\]上面的公式將會應用在時間序列分析，斯皮尔曼等级相关系数(スピアマンの順位相関係数)的定義公式的推導。</description>
    </item>
    
  </channel>
</rss>