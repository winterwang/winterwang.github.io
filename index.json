[{"authors":["admin"],"categories":null,"content":"I was a faculty member in Aichi Medical University until 2021-09-30. Here is the home page of the department I worked before. I finished my MSc of Medical Statistics from the London School of Hygiene \u0026amp; Tropical Medicine in 2018 and PhD in Epidemiology from the Department of Public Health, Graduate School of Medicine, Nagoya University in 2016. You can find my cv here. I am capable of doing statistical analyses/computer programming using a variety of softwares including Python, MATLAB, STATA, SAS. However,I am a big fan of R as well as the open-source integrated development environment (IDE) Rstudio and many other open-source softwares, which might suggest that my daily life looks like this:\nBut I am only a fan, not an expert. My personal interests are Bayesian Statistics, statistical methodology, nutritional epidemiology and real world evidence (RWE) generation using existing databases (suchs as electronic medical records, health insurance database, etc). This site you are visiting is the place to hold the materials I personally used or going to use for lectures/conference presentations, and a place to keep my memories.\n這裏是王超辰的個人網站。2016年3月底我終於取得日本名古屋大学的醫學博士學位，我的研究方向是慢性疾病的流行病學研究, 這是我的個人簡歷。我從2015年4月至2021年9月在日本愛知医科大学医学部公衆衛生学教室做教員。如果你不能打開左邊的鏈接，說明你生活在東土大唐GFW淪陷區內，請學會(肉身)翻牆訪問我們研究室的主頁。我支持開源軟件，工作，生活和學習中主要在Ubuntu，及Rstudio環境下進行，堅持一邊學習，一邊使用R分析數據。平时業餘愛好是折騰折騰电脑，讀讀經濟學人/the Economist,智商平庸，痛恨GFW，每次回中國大陸超过一周時間就會出現渾身不適，惡心嘔吐等被牆症狀，每日19点症狀最爲嚴重。2017年9月我踏上留英一年之路，於2018年9月完成醫學統計學的碩士學位。我留學的地方是倫敦衛生與熱帶醫學院 LSHTM，如果你對公共衛生學，流行病學，或者是傳染病學防控，國際醫療保健，或者是對我所學的醫學統計學有興趣，牆裂推薦來 LSHTM 學習。這裡的教學和學習生活一定不會讓你失望。也歡迎參考我留下的留學學習筆記。這本小書只是我學習中的部分筆記，大量的知識無法及時，也很難用中文詳細解釋。不過我相信對於想入門數據科學，或者是想從事醫學研究的人來說，能起到一定的參考作用。也許世界上認為統計學最難的人，就是我們學習統計學的人了。書山有路勤為徑，學海無涯苦作舟。沒有紮實的功底，論文發表得再多，也會問出標準差和標準誤有什麼差別這樣的問題。\n這是我第二次嘗試利用github+hugo託管個人網站，感謝開源社區的人們，你們才是人類進步的階梯。\nWe are drowning in information and starving for knowledge.\n\u0026mdash; Rutherford D. Roger\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://wangcc.me/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I was a faculty member in Aichi Medical University until 2021-09-30. Here is the home page of the department I worked before. I finished my MSc of Medical Statistics from the London School of Hygiene \u0026amp; Tropical Medicine in 2018 and PhD in Epidemiology from the Department of Public Health, Graduate School of Medicine, Nagoya University in 2016. You can find my cv here. I am capable of doing statistical","tags":null,"title":"王　超辰","type":"authors"},{"authors":null,"categories":["Journal Club","Reading notes","RWE","RWD"],"content":"論文標題 Title of the paper 今日閱讀此文： Hiramatsu, K., Barrett, A., Miyata, Y. et al. Current Status, Challenges, and Future Perspectives of Real-World Data and Real-World Evidence in Japan. Drugs - Real World Outcomes 8, 459–480 (2021). https://doi.org/10.1007/s40801-021-00266-3\n關鍵信息 Key message 支持美國 FDA 對於 RWD 的定義: 真實世界數據，簡稱 RWD，指的是和患者健康狀況相關聯的任何數據，包含在爲患者提供醫療服務過程中產生的日常收集獲得的各種來源的數據：\n電子病例，electronic health records (EHR) 醫療保險給付資料，claims and billing activities 產品，疾病的註冊登記記錄，product and disease registries 患者在任何環境下包括居家治療時產生的數據，patient-generated data including in home-use settings 其他能夠體現患者健康狀態的數據包括移動醫療儀器設備記錄的數據，data gathered from other sources that can inform on health status, such as mobile devices RWE 被定義爲：通過分析RWD獲得的真實世界研究證據，evidence derived from analysis of RWD\n在日本，該領域的挑戰是獲得相關數據的途徑 access，還有各種數據庫之間的聯動 linkage。此外，目前爲止並沒有公認的分析這些RWD的方法學手段。\n","date":1652832000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652832000,"objectID":"04c9b397f00a33d0c888578b31610323","permalink":"https://wangcc.me/post/japan-rwd-rwe/","publishdate":"2022-05-18T00:00:00Z","relpermalink":"/post/japan-rwd-rwe/","section":"post","summary":"論文標題 Title of the paper 今日閱讀此文： Hiramatsu, K., Barrett, A., Miyata, Y. et al. Current Status, Challenges, and Future Perspectives of Real-World Data and Real-World Evidence in Japan. Drugs - Real World Outcomes 8, 459–480 (2021). https://doi.org/10.1007/s40801-021-00266-3 關鍵信息 Key message 支持美國 FDA 對於 RWD 的定義:","tags":["Journal Club","Reading notes","RWE","RWD"],"title":"文獻閱讀 - 日本真實世界數據研究和真實世界證據的現狀，挑戰，和未來的期望","type":"post"},{"authors":[],"categories":["thoughts","iPad","handwriting"],"content":"\n","date":1647820800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647997053,"objectID":"d138ca3f1e807a08f271fc027bb8905d","permalink":"https://wangcc.me/post/ichiran/","publishdate":"2022-03-21T00:00:00Z","relpermalink":"/post/ichiran/","section":"post","summary":"","tags":["偶爾感慨","thoughts","手寫","日記"],"title":"吃麵","type":"post"},{"authors":[],"categories":["thoughts","iPad","handwriting"],"content":"\n","date":1647648000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647997053,"objectID":"aab4a96341114f72203557178023f21d","permalink":"https://wangcc.me/post/kaoya/","publishdate":"2022-03-19T00:00:00Z","relpermalink":"/post/kaoya/","section":"post","summary":"","tags":["偶爾感慨","thoughts","手寫","日記"],"title":"烤鴨","type":"post"},{"authors":[],"categories":["thoughts","iPad","handwriting"],"content":"\n","date":1647561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647997053,"objectID":"7b831606881c22d271ea58f52335e654","permalink":"https://wangcc.me/post/saygoodbyeagain2/","publishdate":"2022-03-18T00:00:00Z","relpermalink":"/post/saygoodbyeagain2/","section":"post","summary":"","tags":["偶爾感慨","thoughts","手寫","日記"],"title":"瞎BB","type":"post"},{"authors":[],"categories":["thoughts","iPad","handwriting"],"content":"\n","date":1647475200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647997053,"objectID":"df756bbf80f775385e6a1dc24947846b","permalink":"https://wangcc.me/post/saygoodbyeagain/","publishdate":"2022-03-17T00:00:00Z","relpermalink":"/post/saygoodbyeagain/","section":"post","summary":"","tags":["偶爾感慨","thoughts","手寫","日記"],"title":"徹底告別","type":"post"},{"authors":[],"categories":["thoughts","iPad","handwriting"],"content":"\n","date":1647216000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647997053,"objectID":"cb665c1141b5818238690457dc9cb054","permalink":"https://wangcc.me/post/buwuzhengye2/","publishdate":"2022-03-14T00:00:00Z","relpermalink":"/post/buwuzhengye2/","section":"post","summary":"","tags":["偶爾感慨","thoughts","手寫","日記"],"title":"不務正業2","type":"post"},{"authors":[],"categories":["thoughts","iPad","handwriting"],"content":"\n","date":1647129600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647997053,"objectID":"f7e599dfb435c1b77247649f9616ffae","permalink":"https://wangcc.me/post/buwuzhengye/","publishdate":"2022-03-13T00:00:00Z","relpermalink":"/post/buwuzhengye/","section":"post","summary":"","tags":["偶爾感慨","thoughts","手寫","日記"],"title":"不務正業","type":"post"},{"authors":[],"categories":["thoughts","iPad","handwriting"],"content":"\n","date":1646956800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647910653,"objectID":"f22bce929237f5793e901a8ef7d6be07","permalink":"https://wangcc.me/post/hand-writing/","publishdate":"2022-03-11T00:00:00Z","relpermalink":"/post/hand-writing/","section":"post","summary":"","tags":["偶爾感慨","thoughts","手寫","日記"],"title":"手寫日記","type":"post"},{"authors":["Yingsong Lin","**Chaochen Wang**","Shogo Kikuchi","Tomoyuki Akita","Junko Tanaka","Sarah K. Abe","Mayo Hirabayashi","Eiko Saito","Megumi Hori","Kota Katanoda","Tomohiro Matsuda","Manami Inoue","the Cancer PAF Japan Collaborators"],"categories":null,"content":"Abstract Population attributable fraction (PAF) offers a means to quantify cancer burden that is attributable to a specific etiological factor. To better characterize the current cancer burden due to infection in the Japanese population, we estimated the PAF for cancer incidence and mortality in 2015 that could be attributable to infectious agents, including Helicobacter pylori (H. pylori), Hepatitis B and C (HBV/HCV), Human papillomavirus virus (HPV), Epstein-Barr virus, and human T-lymphotropic virus type 1. We estimated the PAFs for each infectious agent on the basis of representative data on prevalence and risk-outcome associations assuming a latency period of 10 years. Overall, 16.6% of cancer cases in 2015 in Japan were attributable to the infectious agents included in this analysis. The estimated PAF was slightly higher in men (18.1%) than in women (14.7%). The highest proportion of cancer deaths attributable to infectious agents was observed for H. pylori infection, followed by HBV/HCV, and HPV infection. Our findings corroborated with previous estimates that H. pylori and HBV/HCV infections were the two most important infectious agents in the Japanese population. Strategies focusing on eradication of infectious agents among infected individuals or primary prevention through vaccination could decrease the burden of infection-related cancers.\n","date":1639353600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639353600,"objectID":"8c0dc1b9c352734947d20223a50c17fa","permalink":"https://wangcc.me/publication/journal-article/2021infectionpaf/","publishdate":"2021-12-13T00:00:00Z","relpermalink":"/publication/journal-article/2021infectionpaf/","section":"publication","summary":"Abstract Population attributable fraction (PAF) offers a means to quantify cancer burden that is attributable to a specific etiological factor. To better characterize the current cancer burden due to infection in the Japanese population, we estimated the PAF for cancer incidence and mortality in 2015 that could be attributable to infectious agents, including Helicobacter pylori (H. pylori), Hepatitis B and C (HBV/HCV), Human papillomavirus virus (HPV), Epstein-Barr virus, and human T-lymphotropic virus type 1.","tags":null,"title":"Burden of cancer attributable to infection in Japan in 2015","type":"publication"},{"authors":["Maki Inoue-Choi","Neal D Freedman","Eiko Saito","Shiori Tanaka","Mayo Hirabayashi","Norie Sawada","Shoichiro Tsugane","Yoshiaki Usui","Hidemi Ito","**Chaochen Wang**","Akiko Tamakoshi","Taro Takeuchi","Yuri Kitamura","Mai Utada","Kotaro Ozasa","Yumi Sugawara","Ichiro Tsuji","Keiko Wada","Chisato Nagata","Taichi Shimazu","Tetsuya Mizoue","Keitaro Matsuo","Mariko Naito","Keitaro Tanaka","Kota Katanoda","Manami Inoue","for the Research Group for the Development and Evaluation of Cancer Prevention Strategies in Japan"],"categories":null,"content":"Abstract Background: Increasing proportions of smokers in Japan smoke less than 10 cigarettes per day (CPD). Yet, the health risks of low-intensity smoking in Asia are poorly understood.\nMethods: We performed a pooled analysis of 410,294 adults from nine population-based prospective cohort studies participating in the Japan Cohort Consortium. Cigarette use data were collected at each study baseline in 1983-1994. Study specific hazard ratios (HRs) and 95% confidence intervals (CIs) for all-cause and cause-specific mortality were calculated using multivariable-adjusted Cox regression by CPD among current smokers and by age at cessation among former smokers with never smokers as the referent group. Pooled HRs and CIs were computed using a random effect model.\nResults: The smoking prevalence was 54.5% in men and 7.4% in women. About 15.5% of male and 50.4% of female current smokers smoked 1-10 CPD (low-intensity). Both male and female low-intensity smokers had higher all-cause mortality risks than never smokers. Risks were further higher with increasing CPD in a dose-response manner. HRs (95% CIs) were 1.27 (0.97-1.66), 1.45 (1.33-1.59), and 1.49 (1.38-1.62) for 1-2, 3-5, and 6-10 CPD, respectively, in men; 1.28 (1.01-1.62), 1.49 (1.34-1.66), and 1.68 (1.55-1.81) for 1-2, 3-5, and 6-10 CPD, respectively, in women. Similar associations were observed for smoking-related causes of death. Among former low-intensity smokers, younger age at cessation was associated with lower mortality risk.\nConclusions: Smoking very low amounts was associated with increased mortality risks in Japan. All smokers should quit, even if they smoke very few cigarettes per day.\n","date":1631491200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631491200,"objectID":"a82de6144a5255170a6747c70cbae481","permalink":"https://wangcc.me/publication/journal-article/2021lowintensity_pool/","publishdate":"2021-09-13T00:00:00Z","relpermalink":"/publication/journal-article/2021lowintensity_pool/","section":"publication","summary":"Abstract Background: Increasing proportions of smokers in Japan smoke less than 10 cigarettes per day (CPD). Yet, the health risks of low-intensity smoking in Asia are poorly understood.\nMethods: We performed a pooled analysis of 410,294 adults from nine population-based prospective cohort studies participating in the Japan Cohort Consortium. Cigarette use data were collected at each study baseline in 1983-1994. Study specific hazard ratios (HRs) and 95% confidence intervals (CIs) for all-cause and cause-specific mortality were calculated using multivariable-adjusted Cox regression by CPD among current smokers and by age at cessation among former smokers with never smokers as the referent group.","tags":null,"title":"Low-intensity cigarette smoking and mortality risks: a pooled analysis of prospective cohort studies in Japan","type":"publication"},{"authors":null,"categories":null,"content":"","date":1630540800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630540800,"objectID":"cec48faf777b8d1f22a4900dd5119183","permalink":"https://wangcc.me/project/hpylori/","publishdate":"2021-09-02T00:00:00Z","relpermalink":"/project/hpylori/","section":"project","summary":"","tags":["pylori"],"title":"H. pylori related researches","type":"project"},{"authors":["Kawai, Sayo","**Wang, Chaochen**","Lin, Yingsong","Sasakabe, Tae","Okuda, Masumi","Kikuchi, Shogo"],"categories":null,"content":"Abstract Helicobacter pylori (H. pylori) infection is considered the leading cause of gastric cancer. Gastric cancer is currently a common cancer with high incidence and mortality rates, but it is expected that the incidence rate will gradually decrease as the H. pylori infection prevalence decreases in the future. When evaluating the effectiveness of gastric cancer prevention strategies, it is essential to note the differences in long-term cumulative risks between H. pylori-infected and uninfected populations, but this has not yet been precisely evaluated. In our study, we aimed to estimate the cumulative incidence risks of developing gastric cancer from birth to 85 years among H. pylori-infected and uninfected populations by using population-based cancer registry data and birth year-specific H. pylori infection prevalence rates. Death from gastric cancer and other causes of death were considered in the estimations of the adjusted cumulative incidence risks stratified by sex and H. pylori infection status. After performing 5000 Monte Carlo simulations with repeated random sampling using observed cancer incidence in selected three prefectures (Fukui, Nagasaki, Yamagata) of prefectural population-based cancer registry in Japan, the mean adjusted cumulative incidence risk for gastric cancer in the H. pylori-infected population was 17.0% for males and 7.7% for females and 1.0% for males and 0.5% for females in the uninfected population. These results calculated with Japanese cancer registry data may be useful in considering and evaluating future prevention strategies for gastric cancer in Japan.\n","date":1629072000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629072000,"objectID":"7affcbaed12252117f57bc7324bef018","permalink":"https://wangcc.me/publication/journal-article/lifetimegastriccancer/","publishdate":"2021-08-16T00:00:00Z","relpermalink":"/publication/journal-article/lifetimegastriccancer/","section":"publication","summary":"Abstract Helicobacter pylori (H. pylori) infection is considered the leading cause of gastric cancer. Gastric cancer is currently a common cancer with high incidence and mortality rates, but it is expected that the incidence rate will gradually decrease as the H. pylori infection prevalence decreases in the future. When evaluating the effectiveness of gastric cancer prevention strategies, it is essential to note the differences in long-term cumulative risks between H.","tags":null,"title":"Lifetime incidence risk for gastric cancer in the Helicobacter pylori-infected and uninfected population in Japan: A Monte Carlo simulation study","type":"publication"},{"authors":["Chaochen Wang","Suzana Almoosawi","Luigi Palla"],"categories":null,"content":"Abstract Time of eating has been shown to be associated with diabetes and obesity but little is known about less healthy foods and specific time of their intake over the 24 hours of the day. In this study, we aimed to identify potential relationships between foods and their eating time, and see whether these associations may vary by diabetes status. The National Diet and Nutrition Survey (NDNS) including 6802 adults (age \u0026gt;= 19 years old) collected 749,026 food recordings by a 4-day-diary. The contingency table cross-classifying 60 food groups with 7 pre-defined eating time slots (6-9am, 9am-12pm, 12-2pm, 2-5pm, 8-10pm, 10pm-6am) was analyzed by Correspondence Analysis (CA). CA biplots displaying the associations were generated for all adults and separately by diabetes status (self-reported, pre-diabetes, undiagnosed-diabetes, and non-diabetics) to visually explore the associations between food groups and time of eating across diabetes strata. For selected food groups, odds ratios (OR, 99% confidence intervals, CI) were derived of consuming unhealthy foods at evening/night (8 pm-6 am) vs. earlier time in the day, by logistic regression models with generalized estimating equations. The biplots suggested positive associations between evening/night and consumption of puddings, regular soft drinks, sugar confectioneries, chocolates, beers, ice cream, biscuits, and crisps for all adults in the UK. The OR (99% CIs) of consuming these foods at evening/night were respectively 1.43 (1.06, 1.94), 1.72 (1.44, 2.05), 1.84 (1.31, 2.59), 3.08 (2.62, 3.62), 7.26 (5.91, 8.92), 2.45 (1.84, 3.25), 1.90 (1.68, 2.16), 1.49 (1.22, 1.82) vs. earlier time in the day adjusted for age, sex, body mass index, and social-economic levels. Stratified biplots found that sweetened beverages, sugar-confectioneries appeared more strongly associated with evening/night among un-diagnosed diabetics. Foods consumed in the evening/night time tend to be highly processed, easily accessible, and rich in added sugar or saturated fat. Individuals with undiagnosed diabetes are more likely to consume unhealthy foods at night. Further longitudinal studies are required to ascertain the causal direction of the association between late-eating and diabetes status.\n","date":1628467200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1628467200,"objectID":"190d05b29c66705453e322d3edf3d51d","permalink":"https://wangcc.me/publication/journal-article/ca_ndnsrp/","publishdate":"2021-08-09T00:00:00Z","relpermalink":"/publication/journal-article/ca_ndnsrp/","section":"publication","summary":"Abstract Time of eating has been shown to be associated with diabetes and obesity but little is known about less healthy foods and specific time of their intake over the 24 hours of the day. In this study, we aimed to identify potential relationships between foods and their eating time, and see whether these associations may vary by diabetes status. The National Diet and Nutrition Survey (NDNS) including 6802 adults (age \u0026gt;= 19 years old) collected 749,026 food recordings by a 4-day-diary.","tags":null,"title":"Relationships between food groups and eating time slots according to diabetes status in adults from the UK National Diet and Nutrition Survey (2008-2017)","type":"publication"},{"authors":["Iwase Madoka","Matsuo Keitaro","Koyanagi Yuriko N.","Ito Hidemi","Tamakoshi Akiko","**Wang Chaochen**","Utada Mai","Ozasa Kotaro","Sugawara Yumi","Tsuji Ichiro","Sawada Norie","Tanaka Shiori","Nagata Chisato","Kitamura Yuri","Shimazu Taichi","Mizoue Tetsuya","Naito Mariko","Tanaka Keitaro","Inoue Manami"],"categories":null,"content":"Abstract Although alcohol consumption is reported to increase the incidence of breast cancer in European studies, evidence for an association between alcohol and breast cancer in Asian populations is insufficient. We conducted a pooled analysis of eight large-scale population-based prospective cohort studies in Japan to evaluate the association between alcohol (both frequency and amount) and breast cancer risk with categorization by menopausal status at baseline and at diagnosis. Estimated hazard ratios (HR) and 95% confidence intervals were calculated in the individual cohorts and combined using random-effects models. Among 158,164 subjects with 2,369,252 person-years of follow-up, 2,208 breast cancer cases were newly diagnosed. Alcohol consumption had a significant association with a higher risk of breast cancer in both women who were premenopausal at baseline (regular drinker compared to non-drinker: HR 1.37, 1.04-1.81, ≥23g/day compared to 0g/day: HR 1.74, 1.25-2.43, P for trend per frequency category: P=0.017) and those who were premenopausal at diagnosis (≥23g/day compared to 0g/day: HR 1.89, 1.04-3.43, P for trend per frequency category: P=0.032). In contrast, no significant association was seen in women who were postmenopausal at baseline or at diagnosis, despite a substantial number of subjects and long follow-up period. Our results revealed that frequent and high alcohol consumption are both risk factors for Asian premenopausal breast cancer, similarly to previous studies in Western countries. The lack of a clear association in postmenopausal women in this study warrants larger investigation in Asia.\n","date":1611619200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611619200,"objectID":"e7787905329997e2b739ba4c4a040b4f","permalink":"https://wangcc.me/publication/journal-article/2021alcbreast_pool/","publishdate":"2021-01-26T00:00:00Z","relpermalink":"/publication/journal-article/2021alcbreast_pool/","section":"publication","summary":"Abstract Although alcohol consumption is reported to increase the incidence of breast cancer in European studies, evidence for an association between alcohol and breast cancer in Asian populations is insufficient. We conducted a pooled analysis of eight large-scale population-based prospective cohort studies in Japan to evaluate the association between alcohol (both frequency and amount) and breast cancer risk with categorization by menopausal status at baseline and at diagnosis. Estimated hazard ratios (HR) and 95% confidence intervals were calculated in the individual cohorts and combined using random-effects models.","tags":null,"title":"Alcohol consumption and breast cancer risk in Japan: a pooled analysis of eight population-based cohort studies","type":"publication"},{"authors":null,"categories":["Gallery","thoughts","diary","life"],"content":"我家門口下雪了。\n我們驅車去山裏玩雪。\n路上一片白茫茫的美景。\n","date":1610582400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610613576,"objectID":"c42648f1a124c09577ccb158a0e06e55","permalink":"https://wangcc.me/gallery/ski/","publishdate":"2021-01-14T00:00:00Z","relpermalink":"/gallery/ski/","section":"gallery","summary":"岐阜めいほうスキー場","tags":["Gallery","自娛自樂","偶爾感慨","休閒"],"title":"滑雪場的初學者","type":"gallery"},{"authors":["Chaochen Wang","Hiroshi Yatsuya","Yingsong Lin","Tae Sasakabe","Sayo Kawai","Shogo Kikuchi","Hiroyasu Iso","Akiko Tamakoshi"],"categories":null,"content":"Abstract The aim of this study was to further examine the relationship between milk intake and stroke mortality among the Japanese population. We used data from the Japan Collaborative Cohort (JACC) Study (total number of participants = 110,585, age range: 40–79) to estimate the posterior acceleration factors (AF) as well as the hazard ratios (HR) comparing individuals with different milk intake frequencies against those who never consumed milk at the study baseline. These estimations were computed through a series of Bayesian survival models that employed a Markov Chain Monte Carlo simulation process. In total, 100,000 posterior samples were generated separately through four independent chains after model convergency was confirmed. Posterior probabilites that daily milk consumers had lower hazard or delayed mortality from strokes compared to non-consumers was 99.0% and 78.0% for men and women, respectively. Accordingly, the estimated posterior means of AF and HR for daily milk consumers were 0.88 (95% Credible Interval, CrI: 0.81, 0.96) and 0.80 (95% CrI: 0.69, 0.93) for men and 0.97 (95% CrI: 0.88, 1.10) and 0.95 (95% CrI: 0.80, 1.17) for women. In conclusion, data from the JACC study provided strong evidence that daily milk intake among Japanese men was associated with delayed and lower risk of mortality from stroke especially cerebral infarction.\n","date":1599609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599609600,"objectID":"ed27a0b363aa9e8e44c1b38e5fdf39c9","permalink":"https://wangcc.me/publication/journal-article/jaccbayes/","publishdate":"2020-09-09T00:00:00Z","relpermalink":"/publication/journal-article/jaccbayes/","section":"publication","summary":"Abstract The aim of this study was to further examine the relationship between milk intake and stroke mortality among the Japanese population. We used data from the Japan Collaborative Cohort (JACC) Study (total number of participants = 110,585, age range: 40–79) to estimate the posterior acceleration factors (AF) as well as the hazard ratios (HR) comparing individuals with different milk intake frequencies against those who never consumed milk at the study baseline.","tags":null,"title":"Milk Intake and Stroke Mortality in the Japan Collaborative Cohort Study—A Bayesian Survival Analysis.","type":"publication"},{"authors":null,"categories":["Gallery","thoughts","diary","life"],"content":"\n","date":1597622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597653576,"objectID":"90d2ed8964ddeeacb09e6e0a3c267eb3","permalink":"https://wangcc.me/gallery/water/","publishdate":"2020-08-17T00:00:00Z","relpermalink":"/gallery/water/","section":"gallery","summary":"和鄰居家的孩子一起玩水","tags":["Gallery","自娛自樂","偶爾感慨","休閒"],"title":"玩水樂在其中","type":"gallery"},{"authors":["Shamima Akter","Zobida Islam","Tetsuya Mizoue","Norie Sawada","Hikaru Ihira","Shoichiro Tsugane","Yuriko N. Koyanagi","Hidemi Ito","**Chaochen Wang**","Akiko Tamakoshi","Keiko Wada","Chisato Nagata","Kenta Tanaka","Yuri Kitamura","Mai Utada","Kotaro Ozasa","Yumi Sugawara","Ichiro Tsuji","Taichi Shimazu","Keitaro Matsuo","Mariko Naito","Keitaro Tanaka","Manami Inoue"],"categories":null,"content":"Abstract Smoking has been consistently associated with the risk of colorectal cancer (CRC) in Western populations; however, evidence is limited and inconsistent in Asian people. To assess the association of smoking status, smoking intensity and smoking cessation with colorectal risk in the Japanese population, we performed a pooled analysis of 10 population‐based cohort studies. Study‐specific hazard ratios (HRs) and 95% confidence intervals (CIs) were estimated using Cox\u0026rsquo;s proportional hazards model and then pooled using a random‐effects model. Among 363409 participants followed up for 2666004 person‐years, 9232 incident CRCs were identified. In men, compared with never smokers, ever smokers showed higher risk of CRC. The HRs (95% CI) were 1.19 (1.10‐1.29) for CRC, 1.19 (1.09‐1.30) for colon cancer, 1.28 (1.13‐1.46) for distal colon cancer and 1.21 (1.07‐1.36) for rectal cancer. Smoking was associated with risk of CRC in a dose‐response manner. In women, compared with never smokers, ever smokers showed increased risk of distal colon cancer (1.47 [1.19‐1.82]). There was no evidence of a significant gender difference in the association of smoking and CRC risk. Our results confirm that smoking is associated with an increased risk of CRC, both overall and subsites, in Japanese men and distal colon cancer in Japanese women.\n","date":1596585600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596585600,"objectID":"192ed40b4dfd77944f0e356b725c3faf","permalink":"https://wangcc.me/publication/journal-article/smkcolocan_pool/","publishdate":"2020-08-05T00:00:00Z","relpermalink":"/publication/journal-article/smkcolocan_pool/","section":"publication","summary":"Abstract Smoking has been consistently associated with the risk of colorectal cancer (CRC) in Western populations; however, evidence is limited and inconsistent in Asian people. To assess the association of smoking status, smoking intensity and smoking cessation with colorectal risk in the Japanese population, we performed a pooled analysis of 10 population‐based cohort studies. Study‐specific hazard ratios (HRs) and 95% confidence intervals (CIs) were estimated using Cox\u0026rsquo;s proportional hazards model and then pooled using a random‐effects model.","tags":null,"title":"Smoking and colorectal cancer: A pooled analysis of 10 population‐based cohort studies in Japan","type":"publication"},{"authors":null,"categories":["Gallery","thoughts","diary","life"],"content":"\n新買了一個dyson的吸塵器，這盒子好大。\n見證歷史的我們，是否意識到，序幕已經拉開，烈日即將升起。你看那滿地的魑魅魍魎和蛆蟲軟體生物即將被曝光於世間。正如同是你眼看它起高樓，眼看它宴賓客，最終也一定會眼看它樓塌了。\n抓到了兩種不同的知了好開心。\n","date":1596585600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596616776,"objectID":"972dc9315fd68242078fa8fe22c8731c","permalink":"https://wangcc.me/gallery/icecream/","publishdate":"2020-08-05T00:00:00Z","relpermalink":"/gallery/icecream/","section":"gallery","summary":"見證歷史的夏天","tags":["Gallery","自娛自樂","偶爾感慨","休閒"],"title":"盛夏要吃冰激凌","type":"gallery"},{"authors":null,"categories":["Gallery","thoughts","diary","life"],"content":"\n妹妹過生日，哥哥蹭吃蹭喝蹭玩具。\n妹妹穿上新的小裙子好可愛。\n超市買的紅色色素把雞蛋只能染成粉紅色。\n小傢伙自己穿襪襪。\n兩個小活寶。抓着粉色的雞蛋。\n全家都給妹妹唱生日歌。快快幸福的長大吧。\n七夕節，保育園給小朋友們一人一條竹葉（笹の葉）。男孩子掛王子，掛心願，女孩子掛公主，也掛心願。\n公園遛娃很開心。\n向日葵含苞待放第一天。\n向日葵含苞待放第二天\n向日葵雨後初綻似陽光般豔麗。\n繼續遛娃。\n妹妹穿上小螃蟹花的夏日短褲，和綻放的向日葵一起合照。\n五株開了兩株。\n小男孩去看貓頭鷹，興味津々。\n妹妹滑滑梯的慢鏡頭。蘋果手機自帶功能。慢速視頻。\nGitHub 竟然啓動了把開源數據存儲在北極凍土1000年的計劃。驚覺我的代碼也被放入全部人類 21 TB的代碼中，1000年以後，世界會是什麼樣的呢？又會有誰會有興趣在這21TB的數據中閱讀到我寫下的那些信息呢？突然有種自己的一部分被存儲了1000年的感覺。將來閱讀的人類（那時候還有沒有人類？），會知道1000年前的世界是多麼的操蛋嗎？快告訴我，1000年後的中國（還叫不叫拆那？）是不是已經擁有了免於恐懼的自由了呢？\n","date":1595203200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595234376,"objectID":"3de1b6c13e8fbcee03c0a811bf32d6e1","permalink":"https://wangcc.me/gallery/sunflower/","publishdate":"2020-07-20T00:00:00Z","relpermalink":"/gallery/sunflower/","section":"gallery","summary":"向日葵在陽光下盛開","tags":["Gallery","自娛自樂","偶爾感慨","休閒"],"title":"妹妹生日過完了向日葵開啦","type":"gallery"},{"authors":null,"categories":null,"content":"","date":1594944000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594944000,"objectID":"ac742868b7c232d99624b2203fec8c8e","permalink":"https://wangcc.me/project/pool_meta/","publishdate":"2020-07-17T00:00:00Z","relpermalink":"/project/pool_meta/","section":"project","summary":"Pooled/meta analyses or systematic reviews I conducted or joined","tags":["pool_meta"],"title":"Pooled/meta analysis or systematic review papers","type":"project"},{"authors":null,"categories":["Gallery","thoughts","diary","life"],"content":"端午佳節我們家今年終於買到了地道的粽子。鹹味粽子，跟廈門的燒肉粽有些神似，包着鹹蛋黃和香菇和栗子，味道特別正。點讚一下我認識的孫師傅。 還要感謝還在這裏陪伴的奶奶做的一手好菜。孩子們（包括我）都吃得圓圓的。 帶小孩在蛋糕店訂妹妹的生日蛋糕，導致妹妹以爲自己今天就過生日。。。 女兒把一個大大紅氣球夾在屁股後面跑步，真可愛。\n小孩好多錢。開心。 帶小孩去抓蟲子玩。抓到一隻好大的天牛蟲（カミキリムシ）。 雖然第二天就拿去放生了，小孩還是很興奮地享受抓蟲子的過程。 大夏天帶個草帽更帥！ 週末帶女兒出門玩，買個蘋果汁就被收買了。 週日這天，買鋼琴的名古屋調律中心的調音師來家裏給之前買來的鋼琴做了一次全身體檢。第一次看到這雅馬哈鋼琴內部的結構，把金屬和木材放在一起就可以釋放出天籟一般的旋律。太神奇了。\n雅馬哈鋼琴logo的近照一張。 全部拆開了以後的內部結構，很霸氣是不是。 調音師的工具箱，什麼都有。 調音師做事很認真，仔細地給我們講解構造和發生的機制，包括消音和延長音的方法。應該說是我們對鋼琴瞭解的太少了。\n感覺家裏買了鋼琴以後，生活品質頓時提升了一個檔次。特別是家裏的空氣中彷彿有一股鋼琴的木質結構的香氣，混合著美妙的鋼琴韻律，感覺彷彿人生又開啓了一個新的世界。\n","date":1593302400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593333576,"objectID":"6b168fac564e17aaa72bc28721db25bb","permalink":"https://wangcc.me/gallery/nagoya-tyouritsu/","publishdate":"2020-06-28T00:00:00Z","relpermalink":"/gallery/nagoya-tyouritsu/","section":"gallery","summary":"購入鋼琴第一個月。雖然是二手的雅馬哈，年紀也比我大，但是它的音質真的實在是太讓人迷戀了，今天調音師來家裏第一次調音，一切都沒問題。","tags":["Gallery","自娛自樂","偶爾感慨","休閒"],"title":"端午節吃粽子-鋼琴調音師來訪","type":"gallery"},{"authors":["Yingsong Lin","Masahiro Nakatochi","Yasuyuki Hosono","Hidemi Ito","Yoichiro Kamatani","Akihito Inoko","Hiromi Sakamoto","Fumie Kinoshita","Yumiko Kobayashi","Hiroshi Ishii","Masato Ozaka","Takashi Sasaki","Masato Matsuyama","Naoki Sasahira","Manabu Morimoto","Satoshi Kobayashi","Taito Fukushima","Makoto Ueno","Shinichi Ohkawa","Naoto Egawa","Sawako Kuruma","Mitsuru Mori","Haruhisa Nakao","Yasushi Adachi","Masumi Okuda","Takako Osaki","Shigeru Kamiya","**Chaochen Wang**","Kazuo Hara","Yasuhiro Shimizu","Tatsuo Miyamoto","Yuko Hayashi","Hiromichi Ebi","Tomohiro Kohmoto","Issei Imoto","Yumiko Kasugai","Yoshinori Murakami","Masato Akiyama","Kazuyoshi Ishigaki","Koichi Matsuda","Makoto Hirata","Kazuaki Shimada","Takuji Okusaka","Takahisa Kawaguchi","Meiko Takahashi","Yoshiyuki Watanabe","Kiyonori Kuriki","Aya Kadota","Rieko Okada","Haruo Mikami","Toshiro Takezaki","Sadao Suzuki","Taiki Yamaji","Motoki Iwasaki","Norie Sawada","Atsushi Goto","Kengo Kinoshita","Nobuo Fuse","Fumiki Katsuoka","Atsushi Shimizu","Satoshi S. Nishizuka","Kozo Tanno","Ken Suzuki","Yukinori Okada","Momoko Horikoshi","Toshimasa Yamauchi","Takashi Kadowaki","Herbert Yu","Jun Zhong","Laufey T. Amundadottir","Yuichiro Doki","Hideshi Ishii","Hidetoshi Eguchi","David Bogumil","Christopher A. Haiman","Loic Le Marchand","Masaki Mori","Harvey Risch","Veronica W. Setiawan","Shoichiro Tsugane","Kenji Wakai","Teruhiko Yoshida","Fumihiko Matsuda","Michiaki Kubo","Shogo Kikuchi","Keitaro Matsuo"],"categories":null,"content":"Abstract Pancreatic cancer is the fourth leading cause of cancer-related deaths in Japan. To identify risk loci, we perform a meta-analysis of three genome-wide association studies comprising 2,039 pancreatic cancer patients and 32,592 controls in the Japanese population. Here, we identify 3 (13q12.2, 13q22.1, and 16p12.3) genome-wide significant loci ($P \u0026lt; 5.0 \\times 10^{−8}$), of which 16p12.3 has not been reported in the Western population. The lead single nucleotide polymorphism (SNP) at 16p12.3 is rs78193826 (odds ratio = 1.46, 95% confidence interval = 1.29-1.66, $P = 4.28 \\times 10^{−9}$), an Asian-specific, nonsynonymous glycoprotein 2 (GP2) gene variant. Associations between selected GP2 gene variants and pancreatic cancer are replicated in 10,822 additional cases and controls of East Asian origin. Functional analyses using cell lines provide supporting evidence of the effect of rs78193826 on KRAS activity. These findings suggest that GP2 gene variants are probably associated with pancreatic cancer susceptibility in populations of East Asian ancestry.\n","date":1592956800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592956800,"objectID":"17db1a77bf022c6c5d2daa5be99b02b6","permalink":"https://wangcc.me/publication/journal-article/gwasgp2/","publishdate":"2020-06-24T00:00:00Z","relpermalink":"/publication/journal-article/gwasgp2/","section":"publication","summary":"Abstract Pancreatic cancer is the fourth leading cause of cancer-related deaths in Japan. To identify risk loci, we perform a meta-analysis of three genome-wide association studies comprising 2,039 pancreatic cancer patients and 32,592 controls in the Japanese population. Here, we identify 3 (13q12.2, 13q22.1, and 16p12.3) genome-wide significant loci ($P \u0026lt; 5.0 \\times 10^{−8}$), of which 16p12.3 has not been reported in the Western population. The lead single nucleotide polymorphism (SNP) at 16p12.","tags":null,"title":"Genome-wide association meta-analysis identifies GP2 gene risk variants for pancreatic cancer","type":"publication"},{"authors":[],"categories":["thoughts","sorrow"],"content":"你是否還記得一年前的這一天，我們目睹了這個城市的集體哭泣。 還有那「你是否聽見人民的歌聲」的小女孩的清甜歌聲。\n想當初，他們出惡法禁止行人蒙面，結果武漢肺炎讓整個大陸的所有人人心惶惶，個個蒙面直至今日。香港人爭取的反對送終條款經過他們在街頭巷尾的人肉搏鬥暫時被成功撤回。結果這2020年的7月1日竟然在蹂躪他們自己提出的讓港人高度自治原則的橡皮圖章會議上通過了比送終條款更壞一百倍的共安條款，親手葬送了曾經繁榮自由的香港。他們到底是有多脆弱，在街頭舉個旗子，推特上點個讚政權就能被顛覆，他們難道竟然是玻璃做的。這個除了謊言就是暴力的組織，綁架了整個民族近100年，罪惡深重。如今的香港，暴徒穿上了警服，為每一個人爭取自由的人被這些暴徒用紫色恐怖踩在腳底下不能呼吸。我們如何竟無恥，無知，卑賤，懦弱到這樣的地步。\n憤怒，只剩下憤怒了。\n可憐的中國人，何時才能擁有免於恐懼的自由。\n一定要記住這個充滿了被自己人羞辱而不得自由的時代，幾個世紀以後的人回頭看我們，是否會心生同情和憐憫？還是只有對我們感到悲哀，哀其不幸，怒其不爭？悲哀。\n香港安息。\n","date":1592784000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593824253,"objectID":"b60a83799a13b6764b652e912de6ff15","permalink":"https://wangcc.me/post/rememberhongkong-sfight/","publishdate":"2020-06-22T00:00:00Z","relpermalink":"/post/rememberhongkong-sfight/","section":"post","summary":"RIP HK","tags":["偶爾感慨","thoughts"],"title":"香港安息","type":"post"},{"authors":null,"categories":["Gallery","thoughts","diary","life"],"content":"土撥鼠出沒 土撥鼠又出沒 大烏龜在吃草草 小屁孩興高采烈 妹妹也興味津々 非洲犀牛壯得像坦克 長頸鹿頭上的天更高更籃 後面的 flamingo 火紅火紅的 神奇鳥類在哪裏 乘坐公園裏面的單軌小火車之前，公園職員辛苦地把每個座位都消毒一遍 小火車途經小池塘邊 小火車裏的小朋友 三個小朋友高興極了 然後小小朋友累壞了 ","date":1592697600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592728776,"objectID":"006490cd48532032551d03645f689381","permalink":"https://wangcc.me/gallery/higashiyama-zoo/","publishdate":"2020-06-21T00:00:00Z","relpermalink":"/gallery/higashiyama-zoo/","section":"gallery","summary":"解禁以後的東山公園其實也挺好玩","tags":["Gallery","自娛自樂","偶爾感慨","休閒"],"title":"東山動物園一日遊","type":"gallery"},{"authors":null,"categories":["Gallery","thoughts","diary","life"],"content":"家裏的大玻璃瓶又開始準備釀自家的果酒了。一個已經泡了1公斤的梅子，1.8升的焼酎，500克的冰糖，等待半年後的美味。最近入手了新鮮的小楊梅，吃了其實也沒有太多味道，於是用了相同的配方又突發奇想泡了一缸楊梅的果酒。期待下楊梅燒酒的味道。\n去保育園接兒子時，開心給我看他自己拼好的恐龍。 手拉手下樓梯。 終於安全把妹妹接下來啦。 某天找來蓋房子當時的建築公司（一条工務店）的人幫忙看看浴室地板發出聲音的原因，從樓梯下的入口下到地面和地基之間的空間，真是辛苦了。那裏面現在的溫度估計可想而知啊。最終問題解決了。太感謝一条。 帶娃在家旁邊的小樹林裏賽跑玩，妹妹輸了就哭了。你們看她哭的假不假。\n","date":1592697600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592642376,"objectID":"e8d2e89d5e31ad4fa177f9edded75ed7","permalink":"https://wangcc.me/gallery/yamamomo/","publishdate":"2020-06-21T00:00:00Z","relpermalink":"/gallery/yamamomo/","section":"gallery","summary":"開啓自家釀酒模式","tags":["Gallery","自娛自樂","偶爾感慨","休閒","Japan"],"title":"楊梅準備釀酒","type":"gallery"},{"authors":null,"categories":["diary","Gallery","life","thoughts"],"content":" 我家的妹妹開心地背上 Hello Kitty 的小書包。\n上週六帶着大兒子去看鋼琴，名古屋調律中心的鋼琴擺滿了三四棟樓的每一個房間。簡直是目不暇接。兒子開心極了。不知道要選哪個。\n訂了鋼琴回家的路上，不小心被警察蜀黍抓到違章右轉。唉，全黨是破財消災。 鋼琴入住我們家這天，天氣很好，搬鋼琴的夥計還能說出我家小黑板上寫的“白日依山盡，黃河入海流”。人才真是不可貌相。不過看他們搬的過程，深感這位新成員的體量還是相當沉重。\n選中了這臺標準的雅馬哈鋼琴，小孩玩耍的房間頓時格調倍增。 孩子們多了一個可以發出美妙旋律的大夥伴，妹妹興奮地給認真彈琴的哥哥搗亂哈哈。\n","date":1592006400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592020900,"objectID":"c58e829c38181fe703efe9c2a3bb9a29","permalink":"https://wangcc.me/gallery/piano-come-to-home/","publishdate":"2020-06-13T00:00:00Z","relpermalink":"/gallery/piano-come-to-home/","section":"gallery","summary":"家裏有了鋼琴頓時感覺高大上了許多","tags":["Gallery","自娛自樂","自我隔離","偶爾感慨","休閒","experience","Aichi"],"title":"歡迎家庭新成員-雅馬哈鋼琴","type":"gallery"},{"authors":[],"categories":["diary","Gallery","life","thoughts"],"content":"假裝在自家陽臺玩野餐： 到了鐵板鍋，可以在自家烤牛肉和香腸的感覺真不賴： 爲了不讓鳥兒把院子裏的果子全部吃光，奶奶搶救了一碗六月梅（June Berry）。顏色很好看，可惜果子太小沒什麼味道。 快要回到過去的日常，唉，小朋友們在家待時間太長也是很無奈。在保育園可以有好多小夥伴一起玩耍呢。 兒子的表情太有意思： 女兒摟在懷裏像貼心的小棉襖： 看，人都呆傻掉了： 跟女兒一起玩滑梯，看天上飛過的直升飛機。 是的，你沒有看錯，同一個烤肉鍋，還可以用來做章魚小丸子喲： 章魚小丸子一邊翻還一邊嗞嗞地冒油呢：\n日本小朋友指定的小學生背包真是像炸藥包一樣好大： 安倍不知道跟哪個財團私下搞的發口罩活動，每家不分人口多少，一家兩片布口罩，這緊急狀態都解除了才送到，是留着等第二波再用的吧： ","date":1591142400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591158791,"objectID":"6e8d4fd15872ec54aea553f442556f62","permalink":"https://wangcc.me/gallery/abenomask/","publishdate":"2020-06-03T00:00:00Z","relpermalink":"/gallery/abenomask/","section":"gallery","summary":"終於可以送小朋友去上學了哈哈哈","tags":["Gallery","experience","Japan","偶爾感慨","自娛自樂"],"title":"安倍のマスク（安倍口罩到家）","type":"gallery"},{"authors":null,"categories":null,"content":" 在家裏悶了太多日子，今日海邊的風分外清爽，海水也分外的清澈，天空湛藍而美好。\n","date":1589760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589760000,"objectID":"fdf866054ba04efed24e8710701ce936","permalink":"https://wangcc.me/gallery/20200518/","publishdate":"2020-05-18T00:00:00Z","relpermalink":"/gallery/20200518/","section":"gallery","summary":"跑去海邊浪了一浪","tags":["海邊","休閒","Gallery"],"title":"藍天，白雲，沙灘","type":"gallery"},{"authors":[],"categories":["diary","life","Gallery"],"content":"表演霸王龍的兒子超可愛： 超市到打折時間，看大家根本不在乎 social-distance 嘛：\nYour browser does not support the video tag. 上次從中國大陸帶來的蘇泊爾高壓鍋還沒到一年就退役了，這是新成員，負責填飽全家人肚皮的大白： 早晨起來興奮地吃小兔兔麪包，兩口就把兔子耳朵吃掉啦： 下面是一連串的小朋友動圖。超級暖心：\n瘟疫流行期間，來來亭的拉麵提供了外賣，冷凍的湯底拿回家自己鍋裏煮一遍，可以彌補一下無法堂食的味覺損失（不是武漢肺炎導致的啦）：\n小朋友也是吃得熱火朝天。喜滋滋。 價格比平時便宜15%呢。\n吃完拉麵去公園釋放一下： ","date":1589500800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589770094,"objectID":"a4386fac1d2488e87ba659c0c465eaab","permalink":"https://wangcc.me/gallery/nagakute/","publishdate":"2020-05-15T00:00:00Z","relpermalink":"/gallery/nagakute/","section":"gallery","summary":"宅在家裏自娛自樂","tags":["休閒","偶爾感慨","Japan","Gallery","自我隔離","自娛自樂"],"title":"在家自我隔離第好幾天","type":"gallery"},{"authors":null,"categories":["diary"],"content":"2020已經快過去了5個月。不知道有多少人寧願這5個月可以重新來過。該做的事，很多希望在2020做的事變成了泡影，甚至於可能已經成爲了永遠的遺憾。當然，大多數人如我，不得不每天都窩在家裏遠程辦公，還好有可愛（又頭疼）的孩子們左右陪伴。\n原來之前2月份，武漢瘟疫嚴重的時候，每日被牽動人心的那些可愛可恨的人和事折騰地無法平靜。如今已經感覺每天都不想再打開手機去看那些重複了快5個月的新聞頭條。日本如今（4月中旬時）情勢也都不容樂觀，我們大學的醫院也出現了未知傳染路徑的感染者，包括2名護士和1名已經出院的患者。於是從五一長假前的兩週開始，我們研究室就開始了原則上在家辦公的工作狀態。\n巴西這座耶穌雕像被打上了醫生和護士的影像，致敬全世界爲人類健康奮鬥在生死邊緣的醫務工作者。\n令人感到唯一值得期待的事現在就是安倍允諾的一個人10萬日元的補助金。我們家目前5口人，所以，金額還是挺可觀的。至於之後每個人可能又要多負擔的復興稅務，大家現在也只好暫時先拋諸腦後。待過去了再緩緩從各自的工資中扣除。我也是這次才搞懂了工資單上的復興特別税的來龍去脈。當初這個復興特別稅還在徵集的途中，雖說一般人沒有獲得當時特別預算的補助，但是出於對東北地區遇難者受災者的支持，這也是無可厚非的。25年的計劃現在才剛到第8個年頭，這一次的肺炎特殊預算，估計也會在之後的工資中以特別稅的形式體現。不知道領了這10萬的補助金，之後的特別稅是不是要交到退休都不一定交的完。\n於是我開始在家裏給小朋友們折紙飛機玩，用於消磨不想寫論文的時光。\n看這兩個小朋友也是百無聊賴。如果天氣不好連附近的公園都去不了。只能在家兩兄妹鬥嘴吵架玩。。。\n","date":1588032000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588048959,"objectID":"a79af4ba16feb2e08c53586a0859b8e7","permalink":"https://wangcc.me/post/coronainjapan/","publishdate":"2020-04-28T00:00:00Z","relpermalink":"/post/coronainjapan/","section":"post","summary":"某一年在一百年裏也只佔了百分之一，可是2020或許在人類歷史上永遠也無法忽略不計。","tags":["corona","偶爾感慨"],"title":"2020可以重啓嗎","type":"post"},{"authors":["Chaochen Wang","Tatsunori Ikemoto","Atsuhiko Hirasawa","Young-Chang Arai","Shogo Kikuchi","Masataka Deie"],"categories":null,"content":"Abstract Background. The 25-question Geriatric Locomotive Function Scale (GLFS-25) is widely used in daily clinical practice in evaluating locomotive syndrome (LS). The questionnaire contains 25 questions aiming to describe 6 aspects, including body pain, movement-related difficulty, usual care, social activities, cognitive status, and daily activities. However, its underlying latent factor structure of the questionnaire has not been fully examined so far.\nMethods. Five hundred participants who were 60 years or older and were able to walk independently with or without a cane but had complaints of musculoskeletal disorders were recruited face to face at the out-patient ward of Aichi Medical University Hospital between April 2018 and June 2019. All participants completed the GLFS-25. Confirmatory factor analysis (CFA) models (single-factor model, 6-factor model as designed by the developers of the GLFS-25) were fitted and compared using Mplus 8.3 with a maximum likelihood minimization function. Modification indices, standardized expected parameter change were used, standard strategy for scale development was followed in the search for an alternative and simpler model that could well fit the collected data. Cronbach\u0026rsquo;s $\\alpha$ and its 95% confidence interval (CI) were also calculated.\nResults. Mean (standard deviation) participants age was 72.6 (7.4) years old; 63.6% of them were women. Under the current criteria, 132 (26.4%) and 262 (52.4%) of the study subjects would be classified as LS stage 1 and stage 2, respectively. Overall, the Cronbach\u0026rsquo;s $\\alpha$ for GLFS-25 evaluated using these data was 0.959 (95%CI: 0.953, 0.964). The single- and 6-factor models were rejected due to poor fit. The alternative models with either full 25 questions or a shortened GLFS-16 were found to fit the data better. These alternative models included three latent factors (body pain, movement-related difficulty, and psycho-social complication) and allowed for cross-loading and residual correlations.\nDiscussion. The findings of the CFA models provided evidence that the factor structure of the GLFS-25 might be simpler than the 6-factor model as suggested by the designers. The complex relationships between the latent factors and the observed items may also indicate that individual sub-scale use or simply combining the raw scores for evaluation is likely to be inadequate or unsatisfactory. Thus, future revisions of the scoring algorithm or questions of the GLFS-25 may be required.\n","date":1585612800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585612800,"objectID":"9ee1e9701d9c370413fc2b8125ed93e6","permalink":"https://wangcc.me/publication/journal-article/cfa_locomo/","publishdate":"2020-03-31T00:00:00Z","relpermalink":"/publication/journal-article/cfa_locomo/","section":"publication","summary":"Abstract Background. The 25-question Geriatric Locomotive Function Scale (GLFS-25) is widely used in daily clinical practice in evaluating locomotive syndrome (LS). The questionnaire contains 25 questions aiming to describe 6 aspects, including body pain, movement-related difficulty, usual care, social activities, cognitive status, and daily activities. However, its underlying latent factor structure of the questionnaire has not been fully examined so far.\nMethods. Five hundred participants who were 60 years or older and were able to walk independently with or without a cane but had complaints of musculoskeletal disorders were recruited face to face at the out-patient ward of Aichi Medical University Hospital between April 2018 and June 2019.","tags":null,"title":"Assessment of locomotive syndrome among older individuals: a confirmatory factor analysis of the 25-question Geriatric Locomotive Function Scale","type":"publication"},{"authors":null,"categories":["thoughts","diary"],"content":"\n其實我也不知道到底誰在造謠。政令之混亂，只能反映出這個政權是多麼地無能。\n日本的話，估計病毒是防控不住了額，好在，安倍是會被控制住的，他事後被問責是逃不掉的。\n其實輸血漿的治療方法並不是很理想。\n一個月前。\n病毒傳遍天涯。比瘟疫更可怕的是謊言，是毫無羞恥的謊言。\n他也喊過蔣委員長萬歲，然後又逼着4億人喊他萬歲。\n金將軍還可以用石子擊落敵機呢。\n這不是住在奇幻世界的人的體溫嗎？\n恐怕真的不在少數。\n老婆呢?\n下列視頻可能引起不適，如果你想看喜訊請自覺調到CCTV進行自我調理。\nYour browser does not support the video tag. Your browser does not support the video tag. Your browser does not support the video tag. 你知道義勇軍進行曲作詞者田漢的結局嗎？ Your browser does not support the video tag. 無情抓捕。\nYour browser does not support the video tag. 他們是最可憐的。\nYour browser does not support the video tag. 只是不知道萬一發生火災，樓上隔離的人有沒有辦法逃生呢？\nYour browser does not support the video tag. 秦暉老師說的好，救災是最基本的職責，沒有哪個國家的公民會讚美政府的救災，只有批評救災不及時，只有問責災害發生的責任，但是在奇葩魔幻世界的神奇國度裏，你會看見無數讚美之詞，而且有些百姓還會起身咒罵那些拒絕舔菊，甚至是讚美得不夠起勁的人，把他們遊街示衆。這不是奇幻世界是什麼。\nYour browser does not support the video tag. Your browser does not support the video tag. 奇幻國家的上海新聞臺不小心說漏嘴了噢。 你還記得多少？\n推薦閱讀：\n《官僚体系与公民社会：谁是肺炎危机的答案》笔记\u0026amp;商榷\n为什么我厌恶疫情防控中的“抄作业”比喻？\n译文 | 柳叶刀主编评论文《不止于真相》\n疫情日记2020.02.23. 病在瘟疫蔓延时，丧事勿当成喜事做\n","date":1582502400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582523085,"objectID":"c9af6d6ed3d5fb1d562e6ed0d6eff6e4","permalink":"https://wangcc.me/post/qihuanrizhi/","publishdate":"2020-02-24T00:00:00Z","relpermalink":"/post/qihuanrizhi/","section":"post","summary":"有人說武漢解封了，然後三個小時之後就被闢謠了。(究竟誰在這三個小時內離開了武漢呢，這是一個值得思考的問題)","tags":["偶爾感慨"],"title":"道聽途說的奇幻錄--20200224","type":"post"},{"authors":null,"categories":["thoughts","diary"],"content":"![](/post/2020-02-22-rizhi_files/2020-02-22 15.18.31.jpg)\n韓國一夜之間變成了疫區。武漢那邊正在高奏凱歌噢。\n感謝方方\n如果你也明白這首詩紀念的是誰，請留言和我做朋友。\n永別了。\nYour browser does not support the video tag. 湖南這天报的新增病例只有1例。\n![](/post/2020-02-22-rizhi_files/2020-02-22 15.13.06.jpg)\n指鹿爲馬，秦人不暇自哀。崇禎最後亡國的時候，全部的臣子都在等待他的最高指示。\n![](/post/2020-02-22-rizhi_files/2020-02-22 15.13.51.jpg)\n圍觀各種奇葩標語。這些算不算是對漢字的侮辱。\n![](/post/2020-02-22-rizhi_files/2020-02-22 15.15.59.jpg)\n![](/post/2020-02-22-rizhi_files/2020-02-22 15.16.06.jpg)\n![](/post/2020-02-22-rizhi_files/2020-02-22 15.16.10.jpg)\n![](/post/2020-02-22-rizhi_files/2020-02-22 15.16.16.jpg)\n![](/post/2020-02-22-rizhi_files/2020-02-22 15.16.25.jpg)\n![](/post/2020-02-22-rizhi_files/2020-02-22 15.16.30.jpg)\n![](/post/2020-02-22-rizhi_files/2020-02-22 15.16.35.jpg)\n![](/post/2020-02-22-rizhi_files/2020-02-22 16.24.29.jpg)\n連刪除它的網絡警察也都明白衆望所歸之事指的是什麼。\n岩田教授說，他認爲他的視頻已經起到了驚醒大家的作用。\n一定打贏防疫站，的病毒，黨性覺悟大概緊跟一尊其後。\n404 的頁面，他們已經遠離這羣無法被醫治的行屍走肉。\n其實我也不知道誰才是負面輿論，誰才是這個世界邪惡的化身。\n領導開會可以延遲，屁民趕工上班死活算什麼。\n2月30日，這是在說疫情結束遙遙無期麼。如果你說的是社會主義官狀瘟疫，那可能真的是遙遙無期。\n這天，我還自己驅車155公里從名古屋來到京都參加日本流行病學會的口頭發表。結束回家的第二天，就爆出名古屋高速上收費站的職員有人被確診了。。確診了。。。\n所謂治癒患者，可能只是病毒選擇潛伏一段時間，暫時陰性而已。\nYour browser does not support the video tag. 上面是許志永給BBC的採訪視頻。\nYour browser does not support the video tag. 網絡上流傳的歌曲。\nYour browser does not support the video tag. 白色恐怖，隨時都會降臨在你我頭上。我們都是臨時工。\n推薦閱讀：\n防疫死角：武汉养老院多名老人感染后死亡，有人去世前感叹“死了算啦”\n疫情之下，诸众联合的新可能性：在市场–国家的对立之外，我们还应看到什么?\n受歧视遭退租被辞工，湖北籍农民工流落深圳烂尾楼\n回国杂记完整版\n","date":1582329600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582347435,"objectID":"936c83389bcdb9eb7c8f3d20e8673f04","permalink":"https://wangcc.me/post/rizhi20200222/","publishdate":"2020-02-22T00:00:00Z","relpermalink":"/post/rizhi20200222/","section":"post","summary":"全球爆發只是時間問題，日本奧運會還能辦嗎，韓國淪陷速度比日本還快","tags":["偶爾感慨"],"title":"道聽途說的奇幻錄--20200222","type":"post"},{"authors":null,"categories":["thoughts","diary"],"content":"\n不能不明白，你明白了嗎？\n披上17年前的戰袍，致敬愛的醫護工作者。一定要安全歸來。\n院長犧牲。\n微信什麼時候才會自掛東南枝？\n俄國終於找到理由驅逐黃禍了。\nWHO他們已經絞盡腦汁了唉。。。太拆臺了。\n王朔說的話，不能再同感。\n啪\n歌舞昇平，白骨如山\n日本媒體終於醒悟，現在日本也已經處在傳染病爆發前期階段。日本人是否會開始後悔把口罩毫無保留地都捐助給了西邊的鄰居？\n二十天的婴儿开口说话后，卧床多年的植物人也笑了\n其實我也不知道是誰在造謠，他們中有人領訓誡書了嗎？\n有些人說了些實話，然後他們就失蹤了。\n政治正確是最令人作嘔的遮羞布，殺人不眨眼的制度。\n在鑽石公主號新診斷了79人感染者的今天，總理大臣撒完了謊，也沒有出現在國會，而是去品嚐了美味的河豚魚，現在NHK正在介紹他吃的山口縣的河豚魚。這就是美麗的日本。\u0026mdash;清水潔\n安倍晉三的內閣這次出醜出大了，弱智政府總是那麼相似。\n你知道該寫什麼嗎？\n卑鄙是卑鄙者的通行證。\n不要臉的最高境界。\n演員演技不行。警察叔叔快點來寫訓誡書。\n中國好故事，怎麼看起來似曾相識。\n日本厚生省提出的4個自查用的就診條件：\n感冒症狀，或者37.5以上發燒持續4天以上 感到呼吸困難或者重度倦怠感 有以上兩條者，請聯絡歸國者/接觸者聯繫中心，如果是高齡患者，或者同時具有糖尿病，心衰竭等基礎疾病，或者是需要接受腎臟透析的患者，患者正在服用免疫抑制劑/抗癌藥物的人，那麼上述1，2兩條症狀持續2天以上就需要聯繫求助電話。 孕婦也請聯繫求助電話 電話聯繫之前，請避免外出，上班或者上學。每天請測量體溫，並做好記錄。 这条幽暗无光的隧道，我们还要走多久才能到头？\n思念曲，紀念李文亮，希望他不會白白逝去：\n推薦閱讀：\n大家｜比病毒可怕：被谣言操纵的人类暴力史\n武汉肺炎50天，全体中国人都在承受媒体死亡的代价\n","date":1582156800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582209427,"objectID":"321a42fa86e70eb165879faaf642dcf0","permalink":"https://wangcc.me/post/rizhi20200220/","publishdate":"2020-02-20T00:00:00Z","relpermalink":"/post/rizhi20200220/","section":"post","summary":"2020-02-20 日本已經失守","tags":["偶爾感慨"],"title":"道聽途說的奇幻錄--20200220","type":"post"},{"authors":[],"categories":["thoughts","diary"],"content":"\n可憐的醫務工作者們，光致敬太廉價了，她們的付出，是那麼地不值得。原本他們可以不用這樣“視死如歸，殊死搏鬥”的。\n我對這張照片和上面的評論類似，感到極爲不齒。這個所謂的全責政府，就是這樣對待一線工作的“戰士”的嗎？下大雪之前爲什麼不準備好帳篷和取暖設備，這哪裏是裝給上級看的，以中國大陸給人的信任度來說，有理由相信說這可能和雷鋒叔叔一樣都是擺拍的。\n曾經SARS病毒也有過泄漏的前科，你說武漢的實驗室萬無一失？呵呵。\n林語堂此話，我深以爲然。\n去世了的“學生”如果能本人提出申請，估計受理申請的人會被嚇死。\n日本可能要失守，我對安倍政府這次反應遲鈍和對中國大陸的過度信任表示遺憾，如果會影響到奧運會的舉辦，估計日本人腸子都要悔青了。大家都要好好保護自己和家人。\nYour browser does not support the video tag. 希望秋實這樣正直勇敢的人，可以多一點，祝願他被強制隔離期間平安。\nYour browser does not support the video tag. 歡迎轉載上面打臉視頻。\nYour browser does not support the video tag. 真正的病毒，是產生紅衛兵的萬惡的制度，至今無反思，無認罪，永遠都不可能進步，無正義可以在這篇土地上得到申張。\nYour browser does not support the video tag. 鼓勵大家對全責政府無限追責，幹不好，請自己走人。\nYour browser does not support the video tag. 中國速度，中國質量。\n推薦下列值得一讀的文章：\n非典幸存医护者现状：不是说不会忘了我们吗？\n秦晖：不能真把「防疫」当作「战争」\n愤怒的人民已不再恐惧\n武汉广发肿瘤医院被临时征用，家属口述癌症患者被强制出院后的遭遇\n永不消逝的哨音\n","date":1581552000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581863827,"objectID":"74bcf381ce4457eb2f7c09f8c8dd6c85","permalink":"https://wangcc.me/post/rizhi/","publishdate":"2020-02-13T00:00:00Z","relpermalink":"/post/rizhi/","section":"post","summary":"2020-02-16 日本可能即將失守","tags":["偶爾感慨"],"title":"比電影更加奇幻的是現實--道聽途說的記錄20200216","type":"post"},{"authors":null,"categories":["sorrow","thoughts"],"content":" Your browser does not support the video tag. 日本援助時送的物品上寫的那些詩詞，讀起來真是暖暖的感動。\n兒子剛學會的紙飛機，一下子折了這麼多。\n看見有人傳現在在中國大陸各個居民小區門禁時使用口令，有這樣小清新的也算是有點文化了： 鍾南山其實也是這個制度的受害者： 又嘗試了一下 Plague 遊戲，果然成功消滅人類只需要一個病毒：\n看看我可愛的女兒壓壓驚： ","date":1581465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581465600,"objectID":"63a0ef446112d22361a8d3027c3434bc","permalink":"https://wangcc.me/post/daily-thinking0212/","publishdate":"2020-02-12T00:00:00Z","relpermalink":"/post/daily-thinking0212/","section":"post","summary":"2020-02-12","tags":["sorrow","偶爾感慨"],"title":"日誌","type":"post"},{"authors":[],"categories":["sorrow","thoughts"],"content":"\n其實這些日子，太多人都是以淚洗面地度過的吧。\n多年前，我參觀廣島和平紀念館的時候，內心感到極爲震撼的是看到整面整面的牆壁刻下的受難者的名字。如果你去看廣島市政府網站上關於受難者名錄的解說，你會發現，至今受難者名字都沒有錄全。但是這並不妨礙她們每日每月每年努力的尋找可能沒有被記錄的受難者。整面牆下每個人經過的時候都會產生敬畏之情。因爲這個民族學會了尊重每個受難者，因爲他們深深的明白，建造這座紀念館，是爲了記住這一人類造成的災難，爲了警醒世人莫要再重蹈戰爭覆轍，而不是爲了培養無端的仇恨。\n這就是廣島和平紀念館內刻有受難者名字的禱告牆。\n即使是純粹的自然災害，311地震後死者名錄，也是可以公開查閱得到的，失蹤者，固然是無法知曉（但是也不能放棄尋找線索），那麼已知的死難者，就更應該留下她們曾經來過的痕跡，名字，性別，居住的城市，年齡。這是對死者最起碼的尊重。如果要成爲一個有人性的值得尊重的文明，而不是只有（可能還是編造的）外表光鮮亮麗，內心只有空洞甚至是（讓人覺得無比）邪惡的野蠻國家的話，我呼籲，認真反省並追尋發生疫情災害的真實原因，爲犯下的錯誤道歉，謝罪，爲將來不再犯同樣的錯誤作準備，也爲每一名死難者留下在這個世界來過的痕跡，刻一面受難者名字的碑文。即使這樣，可能也難以撫慰逝去的人的遺憾，和破碎的那些家庭的悲傷。那些苦難的人們，永遠也回不來了，他們永遠也無法再對你我訴說也無法再哭泣。請不要再在他們受難的軀體上表演不可能存在的醫學奇跡。\nYour browser does not support the video tag. 請記住下面這份訓誡書。這是一份野蠻世界給爲我們敲響警鐘的醫生的墓誌銘。一個連“出生年月”都能寫錯別字的黨衛警察，竟然可以這樣以居高臨下的口氣質問一名醫生，“你能做的到嗎？你明白了嗎？”。我無法想象，不知道李醫生的靈魂離開他的軀體的時候，是否看見這一出比哈姆雷特還慘的悲劇，是否後悔當初寫下了“能，明白”。也許那時他才真正地明白魯迅說的那句，學醫，是救不了這些人的，他最終連自己都沒辦法拯救。\n在野蠻世界的局域網，有個叫做weibo的東西上，掀起了一番小小的波瀾。彷彿死水裏被丟入一塊石子，其實不可能有期待有什麼驚濤駭浪。\nYour browser does not support the video tag. 就像我贊同的下面的說法：\n嗚呼哀哉。\n","date":1581292800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581311649,"objectID":"bf632b33657e7dc1671d3a1360be2210","permalink":"https://wangcc.me/post/daily-thinking/","publishdate":"2020-02-10T00:00:00Z","relpermalink":"/post/daily-thinking/","section":"post","summary":"這是怎樣一個操蛋的社會","tags":["sorrow","偶爾感慨"],"title":"誰在救度，誰在欺騙","type":"post"},{"authors":[],"categories":["diary","thoughts"],"content":"更新一些我們住進新家以後的照片。還有我們做院子進度(監工)的照片。\n這是剛住進來的時候，可以看到門口玄關地磚上的保護用紙還沒有撕掉，因爲前面做停車場和院子的地方還都是泥土。。\n去送別響子，我們在名古屋站吃了一次晚餐，小朋友們興奮地在名站門口巨大的聖誕樹前跳起來。\n院子開工了，一個白髮老爺爺開着小卡車載着一臺挖掘機駕到。\nYour browser does not support the video tag. 用攝像頭拍到的挖掘機作業，老爺爺很認真。（請忽略背景音😂）\n上班開會時接到奶奶從家裏撥來緊急聯絡電話，說，停車場的下水管被老爺爺用挖掘機挖斷了。。。看，這都還是一個新管子。奶奶一整個下午都沒辦法使用洗手間。還好負責院子施工的人及時聯繫了水道公司，很快緊急修復好了。\n門口一大堆廢土被挖掉之後的樣子，接下來就要開始往上鋪水泥，砌磚塊啦。你可以看到這個時候（2019年12月初），南邊那塊土地上還什麼都沒有，只是堆積了一堆建築材料而已。\n房間寬敞，小朋友們歡樂地學習寫字。\nYour browser does not support the video tag. 用鋼筋作出了停車場和院子的輪廓之後，專業搬磚工人開始認真地固定這些水泥磚塊。\nYour browser does not support the video tag. 家附近有一條磁懸浮的電車，大家叫他Linimo，很親切的稱呼，在車站候車時拍到的夕陽時Linimo進站的鏡頭。\nYour browser does not support the video tag. 繼續鋪磚塊。\n停車場的雛形出來了。\n停車場的水泥需要保養。\nYour browser does not support the video tag. 小朋友喜歡的冬至湯圓在鍋裏滾。\n紅色白色的湯圓好看，還是女兒的笑臉好看呢？\n在工作單位前的池塘拍到藍天與白雲。\n我們居然買到了大龍蝦。奶奶也好興奮。\n這隻大龍蝦非常飽滿，奶奶的手藝真是沒得挑剔。\nYour browser does not support the video tag. 點播放這個監視器視頻的時候請關閉音量，原諒這拍攝時角度麼有調試好，是倒置的。不過可以看見院子的柵欄正在一點點被固定好。\n噹噹噹噹。院子完工，停車場水泥也養護完畢可以停上我家的小西了。好興奮。\n兒子打開待煮的火鍋，開心溢於言表。\n第一次試用電磁爐的烤箱烤了一隻雞，味道淡了一些，不過還是很有成就感。\nYour browser does not support the video tag. 家附近的永旺（AEON）有十分有趣的電子黑板，小朋友們（還有童心未泯的大哥）在玩耍。\n第一次嘗試日本回転寿司的大表哥。\n調皮搗蛋的小朋友捏雪人一樣的紅色湯圓。\n女兒呆萌的表情真有愛，話筒的方向都拿反了。哈哈。\n帶小朋友們去圖書館借繪本的時候，竟然發現了講獨裁統治的小人書。感慨萬千。\n看看這些榜上有名的萬歲爺們是否有你認識的，也許這個名單還會再更新。\n這天（1月21日），我被派去大阪負責大學入學考試的一部分監考工作。酒店的頂樓居酒屋裏沒有什麼客人，窗口看見的是大阪繁華的燈光與樓房。那天，我和幾個同行的監考老師一起晚餐，談到了可能會爆發（事後證明，我是錯的，因爲那個時候是已經爆發）的SARS。本來，一切都可能繼續着平靜祥和，或者叫歲月靜好？那天那個酒店裏，其實我看到了不少來自中國大陸的遊客，現在想起來，還是有點令人憂心的。\n收拾疲憊回到家以後，看到這隻蜘蛛俠這麼風騷的姿勢，不禁啞然失笑，家裏還是最溫馨的地方。\n農曆春節奶奶辛苦地準備一大桌美食，其實我的內心還牽掛着萬里之外那些還被蒙在鼓裏的朋友和她們的親人們。自己和家人的平安之外，今年我並沒有其他更多的願望了。\n最後，請我們家的新成員日產Dayz來和大家拜個年，希望你們都還好，你們都還平安。\n","date":1580774400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580784785,"objectID":"a3cb133a71f2c47eed27709db5cd531b","permalink":"https://wangcc.me/post/newhome3month/","publishdate":"2020-02-04T00:00:00Z","relpermalink":"/post/newhome3month/","section":"post","summary":"一條的房子真心舒服","tags":["偶爾感慨"],"title":"在新家住了3個月","type":"post"},{"authors":null,"categories":["thoughts","life"],"content":"11月28日，左盼右盼，終於盼到了搬家的日子，我提前一天打開了新家的地暖。\n搬家當天，還有光纖網絡的施工：\n施工的人說因爲附近的NTT的網絡線路距離家門口有點遠，結果這個網絡施工竟然持續了2個多小時。\n裝完網絡之後，搬家公司的車就來了：\n一輛中型卡車竟然就把全家5口人的箱子都裝走了：\n搬入新家的作業時，可以看出這家0123的搬家公司還是很細緻的： 去年購買的大冰箱竟然剛剛好經過玄關的過道，玄關感覺壓力山大： 清空後的舊房子，想不到在這裏住了4年多，快5年。大寶寶的0-5歲，小寶寶的0-2歲都是在這裏度過的。雖然有不捨，但是這裏的冬天實在是太冷，夏天實在是太熱了。再見拉，希望你在不久的將來繼續成爲別的幸福家庭過渡時期的溫馨小屋。 ","date":1575158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575158400,"objectID":"ede26c5dcae82239316aa2821ad85862","permalink":"https://wangcc.me/post/move-to-new-home/","publishdate":"2019-12-01T00:00:00Z","relpermalink":"/post/move-to-new-home/","section":"post","summary":"新家感覺不到冬天","tags":["experience","偶爾感慨","Japan"],"title":"搬家","type":"post"},{"authors":null,"categories":["thoughts","life"],"content":"10月12日，拆完腳手架之後，開始貼牆紙的工作： 10月18日，牆紙貼好了以後第一次去看，發現房子內部的設備也撕掉了他們神祕的面紗，廚房的台和壁櫥簡直不能再寬敞：\n地板的顏色也很有格調： 洗手間也做好了： 藍色的網也拆除了之後，從南側看整個建築還是挺大的： 小朋友們在玄關門口看見天上飛過直升飛機，興奮地打招呼： 太陽能發電試運行，一切看起來都準備妥當： 一條還贈送了放在室內外的溫度溼度計看，可以隨時監測房間內外的溫度溼度： 另外還有專門爲電動車充電使用的開關，酷酷的： 再上一張穿堂透光部分的照片： 11月12日，領鑰匙的日子，好激動： 新買的沙發，牀鋪也搬進來了，好讚： 接下來就剩下搬家啦\n","date":1574812800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574812800,"objectID":"dc91eed16494442232cb4b575c3907fe","permalink":"https://wangcc.me/post/home-almost-complete/","publishdate":"2019-11-27T00:00:00Z","relpermalink":"/post/home-almost-complete/","section":"post","summary":"期待在新家過冬天","tags":["experience","偶爾感慨","Japan"],"title":"馬上搬家，腳手架拆除之後","type":"post"},{"authors":null,"categories":["thoughts","life"],"content":"Before 2019-12-03 日本蓋房子的速度還是很快的。\n我這一轉眼馬上就要搬新家了，結果發現自己太多都還沒有準備。\n這篇日誌本來上個月12日就想更新上傳的，結果遇到去愛爾蘭發表，參加日本疫学会若手の会，等等都被我用來當作自己懶惰拖延更新的藉口。\n其實最大的障礙是，Blogdown 上最流行的學術主題 Academic 經歷了數次重大更新之後，增加了夜間和日間模式的主題樣式，讓人十分心動，所以更新主題搬家花去了太多時間（因爲我對一些CSS文件和進行了修改然而我自己又不完全記得具體改了哪些，所以需要一個個再在更新後的主題中加上去）。最想吐嘈的是新的學術主題 Academic和之前的版本在整體結構，和許多設計上雖然做了很多改進，但是某些語言也進行了本質的修改，例如在個人主頁中介紹自己研究項目 project 的部分的開頭，原先都是用下面的加號的形式包括進來各種選項。\n+++ +++ 但是現在最新的主題中不再支持過去的格式轉而要求全部以標準 Rmarkdown 的開頭部分一樣的形式用三個減號 --- 來包含全部的選項。類似這樣的巨大修改，但是又需要對每個文檔進行調整的部分實在是多的數不勝數。將來更新的學術主題可能只會愈來越複雜繁瑣，年紀大了實在是經不起這麼大的折騰。但是欣慰的是如今默認增加了一個 update_academic.sh 文件，下次更新或許能用上 (T_T)。\nUPdated 2019-12-3 其實到這裏我才開始打算放重點圖片哈哈。 過去了三個多月了，從夏天一直忙活到冬天。我家的房子終於平安建成。\n8月份地基好了以後，一樓的地板下面的空間開始鋪設：\n這是工人們鋪設時的錄像，可以看到地板下面有較大的隔空空間，還有厚厚的防熱層。\nYour browser does not support the video tag. 之後就到了上棟的日子（8月22日）了：\n高高的腳手架前，用大吊車把工廠運來的房屋部件大大小小的送進工地。看這會飛的牆壁：\nYour browser does not support the video tag. 一樓：\n8月24日，二樓搭起來了，還沒有房頂，很擔心下雨(T_T)：\n站在二樓西側時的樣子：\n二樓堆滿了建築資材：\n一樓也堆滿了接下來需要的各種材料和工具：\n南側可以看到東邊吐出來一塊是陽臺： 9月7日，房間內地板下開始鋪設地暖的水管： 樓頂上的太陽能板也已鋪設完畢： 屋檐下還沒貼磚頭的牆壁長這樣： 這天還在機場的好市多買了這臺65寸的電視，有機LED屏，支持4K，好期待，暫時存放在辦公室： 從西側看這三七開的房頂，有點小尷尬： 9月14日，廚房的台架起來： 站在廚房，客廳裏的地暖也鋪起來： 屋檐下白色的隔板裝上了： 粉紅色的東側陽臺有點羞澀： 加上房頂以後，一條在腳手架外面掛上了他們自家的廣告和彩旗： 二樓陽臺內側，可以看到地板已經鋪設成功，地暖的水管看不見了： 二樓的穿堂透光部分（吹き抜け）也差不多完成了，窗戶很明亮： 10月1日，穿堂部分後來竟然架起了腳手架，應該是爲了貼牆紙用的，這裏可以看見我們選的白色滑動門，用於隔開玄關和客廳： 浴室前的洗臉檯： 至此，房子外圍的腳手架可以拆除了，看拆掉了腳手架之後的家：\n","date":1570838400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570838400,"objectID":"9a807922775edd297e4af6b3ca5dba96","permalink":"https://wangcc.me/post/home-before-completion/","publishdate":"2019-10-12T00:00:00Z","relpermalink":"/post/home-before-completion/","section":"post","summary":"從新家的陽臺可以俯瞰整個長久手","tags":["experience","偶爾感慨","Japan"],"title":"馬上竣工，腳手架拆除之前","type":"post"},{"authors":["Chaochen Wang","Suzana Almoosawi","Luigi Palla"],"categories":null,"content":"Abstract This study aims at combining time and quantity of carbohydrate (CH) intake in the definition of eating patterns in UK adults and investigating the association of the derived patterns with type 2 diabetes (T2D). The National Diet and Nutrition Survey (NDNS) Rolling Program included 6155 adults in the UK. Time of the day was categorized into 7 pre-defined time slots: 6–9 am, 9–12 noon, 12–2 pm, 2–5 pm, 5–8 pm, 8–10 pm, and 10 pm–6 am. Responses for CH intake were categorized into: no energy intake, CH \u0026lt;50% or ≥50% of total energy. Non-parametric multilevel latent class analysis (MLCA) was applied to identify eating patterns of CH consumption across day-time, as a novel method accounting for the repeated measurements of intake over 3–4 days nested within individuals. Survey-designed multivariable regression was used to assess the associations of CH eating patterns with T2D. Three CH eating day patterns (low, high CH percentage and regular meal CH intake day) emerged from 24,483 observation days; based on which three classes of CH eaters were identified and characterized as: low (28.1%), moderate (28.8%) and high (43.1%) CH eaters. On average, low-CH eaters consumed the highest amount of total energy intake (7985.8 kJ) and had higher percentages of energy contributed by fat and alcohol, especially after 8 pm. Moderate-CH eaters consumed the lowest amount of total energy (7341.8 kJ) while they tended to have their meals later in the day. High-CH eaters consumed most of their carbohydrates and energy earlier in the day and within the time slots of 6–9 am, 12–2 pm and 5–8 pm, which correspond to traditional mealtimes. The high-CH eaters profile had the highest daily intake of CH and fiber and the lowest intake of protein and fat. Low-CH eaters had greater odds than high-CH eaters of having T2D in self-reported but not in previously undiagnosed diabetics. Further research using prospective longitudinal studies is warranted to ascertain the direction of causality in the association of CH patterns with type 2 diabetes.\nThe compositions (%) and absolute (kJ) energy consumption within each time slot by individual level carbohydrate eating classes: A, low carbohydrate eaters (LCE); B, moderate carbohydrate eaters (MCE); C, high carbohydrate eaters (HCE).\n","date":1570665600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570665600,"objectID":"a893d83d8eb225faa0ebbcf8880c6219","permalink":"https://wangcc.me/publication/journal-article/ch_ndnsrp/","publishdate":"2019-10-10T00:00:00Z","relpermalink":"/publication/journal-article/ch_ndnsrp/","section":"publication","summary":"Abstract This study aims at combining time and quantity of carbohydrate (CH) intake in the definition of eating patterns in UK adults and investigating the association of the derived patterns with type 2 diabetes (T2D). The National Diet and Nutrition Survey (NDNS) Rolling Program included 6155 adults in the UK. Time of the day was categorized into 7 pre-defined time slots: 6–9 am, 9–12 noon, 12–2 pm, 2–5 pm, 5–8 pm, 8–10 pm, and 10 pm–6 am.","tags":null,"title":"Day-Time Patterns of Carbohydrate Intake in Adults by Non-Parametric Multi-Level Latent Class Analysis—Results from the UK National Diet and Nutrition Survey (2008/09–2015/16)","type":"publication"},{"authors":["Sayo Kawai","Kensuke Arai","Yingsong Lin","Takeshi Nishiyama","Tae Sasakabe","**Chaochen Wang**","Hiroto Miwa","Shogo Kikuchi"],"categories":null,"content":"Abstract Background Serum Helicobacter pylori (H. pylori) antibody kits (LZ and LIA) using the latex agglutination immunoassay method are commercially available, but few studies have been performed to determine their diagnostic accuracy or to compare their results with those of enzyme-linked immunosorbent assay (ELISA) kits (EP and EIA).\nMethods Sera were obtained from 213 hospital outpatients with dyspeptic symptoms. The serological results were compared with the result of the 13C-urea breath test (UBT) which seems to be reliable.\nResults Of the 213 subjects, 154 were diagnosed as positive for H. pylori infection according to the UBT. The sensitivities and specificities of these tests were 97.4% and 76.3%, 98.1% and 78.0%, 99.4% and 74.6%, and 98.1% and 71.2% for the EP, LZ, EIA and LIA tests, respectively. When the 13 subjects whose seropositive results of the four kits were completely opposite to the negative results of the UBT were excluded, the specificities of evaluated kits were all higher than 90%. The concordance rate between the EP and EIA tests was 98.1% (Spearman\u0026rsquo;s rank correlation coefficient = 0.83) and that between the LZ and LIA tests was 97.1% (correlation coefficient = 0.91). The LZ gave higher antibody titer value than EP (p \u0026lt; 0.0001, Z = 9.82; Wilcoxon signed-rank test), and EIA gave higher value than LIA (p \u0026lt; 0.0001, Z = 6.43; Wilcoxon signed-rank test).\nConclusions The latex immunoassay method provided the same reliability to ELISA in terms of the diagnostic accuracy for current H. pylori infection, although we should take into account the titer value differences by each test method in practical use.\n","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888000,"objectID":"ca8a6e4280de4edbd35a68801da6d58a","permalink":"https://wangcc.me/publication/journal-article/2019kawai/","publishdate":"2019-10-01T00:00:00Z","relpermalink":"/publication/journal-article/2019kawai/","section":"publication","summary":"Abstract Background Serum Helicobacter pylori (H. pylori) antibody kits (LZ and LIA) using the latex agglutination immunoassay method are commercially available, but few studies have been performed to determine their diagnostic accuracy or to compare their results with those of enzyme-linked immunosorbent assay (ELISA) kits (EP and EIA).\nMethods Sera were obtained from 213 hospital outpatients with dyspeptic symptoms. The serological results were compared with the result of the 13C-urea breath test (UBT) which seems to be reliable.","tags":null,"title":"Comparison of the detection of Helicobacter pylori infection by commercially available serological testing kits and the 13C-urea breath test","type":"publication"},{"authors":null,"categories":["Bayesian","R techniques","statistics"],"content":" 多層(等級)線性回歸模型/混合效應模型 multilevel/mixed effect regression model 適用於等級線性回歸模型的數據 確認數據分佈 如果不考慮組間(公司間)差異 如果要考慮組間差異 等級線性回歸的貝葉斯實現 模型機制 mechanism 多層(等級)線性回歸模型/混合效應模型 multilevel/mixed effect regression model 關於等級線性回歸的基本知識和概念，請參考讀書筆記58-60章節。簡單來說，等級線性回歸通過給數據內部可能存在或者已知存在的結構或者層級增加隨機截距或者隨機斜率的方式來輔助解釋組間差異和組內的差異。\n適用於等級線性回歸模型的數據 本章節使用的數據是四家大公司40名社員的年齡和年收入數據：\nd \u0026lt;- read.csv(file=\u0026#39;../../static/files/data-salary-2.txt\u0026#39;) d ## X Y KID ## 1 7 457 1 ## 2 10 482 1 ## 3 16 518 1 ## 4 25 535 1 ## 5 5 427 1 ## 6 25 603 1 ## 7 26 610 1 ## 8 18 484 1 ## 9 17 508 1 ## 10 1 380 1 ## 11 5 453 1 ## 12 4 391 1 ## 13 19 559 1 ## 14 10 453 1 ## 15 21 517 1 ## 16 12 553 2 ## 17 17 653 2 ## 18 22 763 2 ## 19 9 538 2 ## 20 18 708 2 ## 21 21 740 2 ## 22 6 437 2 ## 23 15 646 2 ## 24 4 422 2 ## 25 7 444 2 ## 26 10 504 2 ## 27 2 376 2 ## 28 15 522 3 ## 29 27 623 3 ## 30 14 515 3 ## 31 18 542 3 ## 32 20 529 3 ## 33 18 540 3 ## 34 11 411 3 ## 35 26 666 3 ## 36 22 641 3 ## 37 25 592 3 ## 38 28 722 4 ## 39 24 726 4 ## 40 22 728 4 X: 社員年齡減去23獲得的數據（23歲是大部分人大學畢業入職時的年齡） Y: 年收入（萬日元） KID: 公司編號 我們認爲，年收入 Y，是基本平均年收入和隨機誤差（服從均值爲零，方差是 \\(\\sigma^2\\) 的正態分佈）之和。且基本平均年收入和年齡成正比（年功序列型企業）。但是呢，因爲不同的公司入職時的基本收入可能不同，且可能隨着年齡增加而增長薪水的速度可能也不一樣。那麼由於不同公司所造成的差異，可以被認爲是組間差異。\n確認數據分佈 這次分析的目的是要瞭解「每個公司KID內隨着年齡的增加而增長的薪水幅度是多少」，那麼我們要在結果報告中體現的就是每家公司的基本年收入，新入職時的年收入，以及隨着年齡增長而上升的薪水的事後分佈。\n我們先來看把四家公司職員放在一起時的整體圖形：\nlibrary(ggplot2) d$KID \u0026lt;- as.factor(d$KID) res_lm \u0026lt;- lm(Y ~ X, data=d) coef \u0026lt;- as.numeric(res_lm$coefficients) p \u0026lt;- ggplot(d, aes(X, Y, shape=KID)) p \u0026lt;- p + theme_bw(base_size=18) p \u0026lt;- p + geom_abline(intercept=coef[1], slope=coef[2], size=2, alpha=0.3) p \u0026lt;- p + geom_point(size=2) p \u0026lt;- p + scale_shape_manual(values=c(16, 2, 4, 9)) p \u0026lt;- p + labs(x=\u0026#39;X (age-23)\u0026#39;, y=\u0026#39;Y (10,000 Yen/year)\u0026#39;) p Figure 1: 年齡和年收入的散點圖，不同點的形狀代表不同的公司編號。 從總體的散點圖 1 來看，似乎年收入確實是隨着年齡增長而呈現直線增加的趨勢。但是公司編號 KID = 4 的三名社員薪水似乎是在同一水平的並無明顯變化。這一點可以把四家公司社員的數據分開來看更加清晰:\np \u0026lt;- ggplot(d, aes(X, Y, shape=KID)) p \u0026lt;- p + theme_bw(base_size=20) p \u0026lt;- p + geom_abline(intercept=coef[1], slope=coef[2], size=2, alpha=0.3) p \u0026lt;- p + facet_wrap(~KID) p \u0026lt;- p + geom_line(stat=\u0026#39;smooth\u0026#39;, method=\u0026#39;lm\u0026#39;, se=FALSE, size=1, color=\u0026#39;black\u0026#39;, linetype=\u0026#39;31\u0026#39;, alpha=0.8) p \u0026lt;- p + geom_point(size=3) p \u0026lt;- p + scale_shape_manual(values=c(16, 2, 4, 9)) p \u0026lt;- p + labs(x=\u0026#39;X (age-23)\u0026#39;, y=\u0026#39;Y (10,000 Yen/year)\u0026#39;) p Figure 2: 年齡和年收入的散點圖，不同的公司在四個平面中展示,黑色點線是每家公司數據單獨使用線性回歸時獲得的直線。 如果不考慮組間(公司間)差異 模型的數學描述 \\[ \\begin{aligned} Y[n] \u0026amp; = y_{\\text{base}}[n] + \\varepsilon[n] \u0026amp; n = 1, \\dots, N \\\\ y_{\\text{base}}[n] \u0026amp; = a + bX[n] \u0026amp; n = 1, \\dots, N \\\\ \\varepsilon[n] \u0026amp; \\sim \\text{Normal}(0, \\sigma_Y^2) \u0026amp; n = 1, \\dots, N \\\\ \\end{aligned} \\]\n當然，如果你想，模型可以直接簡化成：\n\\[ Y[n] \\sim \\text{Normal}(a + bX[n], \\sigma^2_Y) \\;\\;\\;\\;\\;\\; n = 1, \\dots, N \\\\ \\]\n上述簡化版的模型，翻譯成Stan語言如下：\ndata { int N; real X[N]; real Y[N]; } parameters{ real a; real b; real\u0026lt;lower = 0\u0026gt; s_Y; } model { for (n in 1 : N) Y[n] = normal(a + b * X[n], s_Y); } 下面的 R 代碼用來實現對上面 Stan 模型的擬合:\nlibrary(rstan) d \u0026lt;- read.csv(file=\u0026#39;../../static/files/data-salary-2.txt\u0026#39;) d$KID \u0026lt;- as.factor(d$KID) data \u0026lt;- list(N=nrow(d), X=d$X, Y=d$Y) fit \u0026lt;- stan(file=\u0026#39;stanfiles/model8-1.stan\u0026#39;, data=data, seed=1234) ## ## SAMPLING FOR MODEL \u0026#39;model8-1\u0026#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 1.3e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.076557 seconds (Warm-up) ## Chain 1: 0.043822 seconds (Sampling) ## Chain 1: 0.120379 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL \u0026#39;model8-1\u0026#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 5e-06 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 2: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 2: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 2: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 2: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 2: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 2: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 2: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 2: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 2: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 2: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 2: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.080947 seconds (Warm-up) ## Chain 2: 0.043589 seconds (Sampling) ## Chain 2: 0.124536 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL \u0026#39;model8-1\u0026#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 3e-06 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 3: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 3: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 3: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 3: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 3: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 3: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 3: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 3: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 3: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 3: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 3: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.077026 seconds (Warm-up) ## Chain 3: 0.043674 seconds (Sampling) ## Chain 3: 0.1207 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL \u0026#39;model8-1\u0026#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 5e-06 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 4: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 4: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 4: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 4: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 4: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 4: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 4: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 4: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 4: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 4: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 4: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.082446 seconds (Warm-up) ## Chain 4: 0.035819 seconds (Sampling) ## Chain 4: 0.118265 seconds (Total) ## Chain 4: fit ## Inference for Stan model: model8-1. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## a 376.15 0.63 24.41 328.86 359.80 376.18 392.66 423.88 1483 1 ## b 11.05 0.04 1.41 8.27 10.12 11.02 12.02 13.78 1527 1 ## s_Y 68.42 0.21 8.26 54.41 62.59 67.64 73.53 86.73 1538 1 ## lp__ -184.12 0.03 1.31 -187.60 -184.70 -183.76 -183.18 -182.66 1391 1 ## ## Samples were drawn using NUTS(diag_e) at Mon May 18 16:58:23 2020. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). 現在有更加方便的 rstanarm 包可以幫助我們省去寫 Stan 模型的過程：\nlibrary(rstanarm) rstanarm_results = stan_glm(Y ~ X, data=d, iter=2000, warmup=1000, cores=4) summary(rstanarm_results, probs=c(.025, .975), digits=3) ## ## Model Info: ## function: stan_glm ## family: gaussian [identity] ## formula: Y ~ X ## algorithm: sampling ## sample: 4000 (posterior sample size) ## priors: see help(\u0026#39;prior_summary\u0026#39;) ## observations: 40 ## predictors: 2 ## ## Estimates: ## mean sd 2.5% 97.5% ## (Intercept) 376.371 24.615 328.532 423.842 ## X 11.030 1.401 8.387 13.793 ## sigma 68.159 8.136 54.474 86.401 ## ## Fit Diagnostics: ## mean sd 2.5% 97.5% ## mean_PPD 548.026 15.100 517.682 577.179 ## ## The mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help(\u0026#39;summary.stanreg\u0026#39;)). ## ## MCMC diagnostics ## mcse Rhat n_eff ## (Intercept) 0.397 1.000 3836 ## X 0.022 1.001 3940 ## sigma 0.146 1.000 3112 ## mean_PPD 0.245 1.000 3795 ## log-posterior 0.029 1.001 1788 ## ## For each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1). 可以看到強制不同公司社員的年收入來自同一個正態分佈時，方差顯得非常的大。\n如果要考慮組間差異 我們認爲每家公司社員新入職時的起點薪水不同(截距不同-隨機截距)，進入公司之後隨年齡增加的薪水幅度也不同(斜率不同-隨機斜率)。因此，用 \\(a[1]\\sim a[K], K = 1, 2, 3, 4\\) 表示每家公司的截距，用 \\(b[1] \\sim b[K], K = 1, 2, 3, 4\\) 表示每家公司薪水上升的斜率。那麼每家公司的薪水年齡線性回歸模型可以寫作是 \\(a[K] + b[K] X, K = 1, 2, 3, 4\\)\n模型數學描述 \\[ Y[n] \\sim \\text{Normal}(a[\\text{KID[n]}] + b[\\text{KID}[n]] X[n], \\sigma^2_Y) \\\\ n = 1, \\dots, N \\]\n上述模型的 Stan 譯文如下：\ndata { int N; int K; real X[N]; real Y[N]; int\u0026lt;lower = 1, upper = K\u0026gt; KID[N]; } parameters { real a[K]; real b[K]; real\u0026lt;lower = 0\u0026gt; s_Y; } model { for (n in 1:N) Y[n] ~ normal(a[KID[n]] + b[KID[n]] * X[n], s_Y); } 下面的 R 代碼用來實現上面貝葉斯多組不同截距不同斜率線性回歸模型的擬合:\nlibrary(rstan) d \u0026lt;- read.csv(file=\u0026#39;../../static/files/data-salary-2.txt\u0026#39;) data \u0026lt;- list(N=nrow(d), X=d$X, Y=d$Y, KID = d$KID, K = 4) fit \u0026lt;- stan(file=\u0026#39;stanfiles/model8-2.stan\u0026#39;, data=data, seed=1234) ## ## SAMPLING FOR MODEL \u0026#39;model8-2\u0026#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 1.7e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.17 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.450551 seconds (Warm-up) ## Chain 1: 0.267262 seconds (Sampling) ## Chain 1: 0.717813 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL \u0026#39;model8-2\u0026#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 5e-06 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 2: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 2: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 2: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 2: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 2: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 2: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 2: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 2: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 2: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 2: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 2: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.414798 seconds (Warm-up) ## Chain 2: 0.262094 seconds (Sampling) ## Chain 2: 0.676892 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL \u0026#39;model8-2\u0026#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 5e-06 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 3: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 3: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 3: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 3: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 3: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 3: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 3: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 3: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 3: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 3: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 3: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.440107 seconds (Warm-up) ## Chain 3: 0.288008 seconds (Sampling) ## Chain 3: 0.728115 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL \u0026#39;model8-2\u0026#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 5e-06 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 4: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 4: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 4: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 4: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 4: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 4: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 4: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 4: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 4: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 4: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 4: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.447923 seconds (Warm-up) ## Chain 4: 0.259137 seconds (Sampling) ## Chain 4: 0.70706 seconds (Total) ## Chain 4: fit ## Inference for Stan model: model8-2. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## a[1] 387.16 0.28 14.18 359.00 377.67 387.06 396.34 415.21 2560 1 ## a[2] 328.45 0.32 16.66 295.74 317.60 328.68 339.52 361.24 2649 1 ## a[3] 314.06 0.66 34.62 246.26 290.04 314.33 337.91 381.39 2725 1 ## a[4] 751.09 3.10 157.77 440.06 643.83 752.64 857.32 1063.97 2598 1 ## b[1] 7.51 0.02 0.87 5.79 6.94 7.51 8.10 9.19 2381 1 ## b[2] 19.88 0.02 1.23 17.51 19.07 19.88 20.70 22.32 2545 1 ## b[3] 12.45 0.03 1.69 9.16 11.28 12.45 13.60 15.75 2661 1 ## b[4] -1.04 0.12 6.36 -13.54 -5.34 -1.06 3.39 11.43 2616 1 ## s_Y 27.32 0.07 3.57 21.48 24.77 26.95 29.43 35.38 2858 1 ## lp__ -148.07 0.07 2.38 -153.72 -149.41 -147.68 -146.36 -144.57 1311 1 ## ## Samples were drawn using NUTS(diag_e) at Mon May 18 16:59:02 2020. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). 等級線性回歸的貝葉斯實現 模型機制 mechanism 如果我們認爲每家公司的起點薪水 \\(a[k]\\) 服從正態分佈，且該正態分佈的平均值是全體公司的起點薪水的均值 \\(a_\\mu\\)，方差是 \\(\\sigma^2_a\\)。類似地，假設每家公司內隨着年齡增長而增加薪水的幅度 \\(b[k]\\) 也服從某個正態分佈，均值和方差分別是 \\(b_\\mu, \\sigma^2_b\\)。這樣我們就不僅僅是允許了各家公司的薪水年齡回歸直線擁有不同的斜率和截距，還對這些隨機斜率和截距的前概率分佈進行了設定。\n此時，隨機效應模型的數學表達式就可以寫成下面這樣:\n\\[ \\begin{aligned} Y[n] \u0026amp;\\sim \\text{Normal}(a[\\text{KID[n]}] + b[\\text{KID}[n]] X[n], \\sigma^2_Y) \u0026amp; n = 1, \\dots, N \\\\ a[k] \u0026amp;= a_\\mu + a_\\varepsilon[k] \u0026amp; k = 1, \\dots, K \\\\ a_\\varepsilon[k] \u0026amp; \\sim \\text{Normal}(0, \\sigma^2_a) \u0026amp; k = 1, \\dots, K \\\\ b[k] \u0026amp; = b_\\mu + b_\\varepsilon[k] \u0026amp; k = 1, \\dots, K \\\\ b_\\varepsilon[k] \u0026amp;\\sim \\text{Normal}(0, \\sigma^2_b) \u0026amp; k = 1, \\dots, K \\end{aligned} \\]\n使用 rstanarm 包可以使用下面的代碼實現\nlibrary(rstanarm) M1_stanlmer \u0026lt;- stan_lmer(formula = Y ~ X + (X | KID), data = d, seed = 1234) ## ## SAMPLING FOR MODEL \u0026#39;continuous\u0026#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 0.000116 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.16 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 7.72361 seconds (Warm-up) ## Chain 1: 3.71292 seconds (Sampling) ## Chain 1: 11.4365 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL \u0026#39;continuous\u0026#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 2.5e-05 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.25 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 2: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 2: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 2: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 2: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 2: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 2: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 2: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 2: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 2: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 2: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 2: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 5.67096 seconds (Warm-up) ## Chain 2: 2.07953 seconds (Sampling) ## Chain 2: 7.75049 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL \u0026#39;continuous\u0026#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 4.2e-05 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.42 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 3: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 3: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 3: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 3: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 3: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 3: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 3: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 3: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 3: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 3: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 3: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 7.81475 seconds (Warm-up) ## Chain 3: 3.88969 seconds (Sampling) ## Chain 3: 11.7044 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL \u0026#39;continuous\u0026#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 3.7e-05 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.37 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 4: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 4: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 4: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 4: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 4: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 4: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 4: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 4: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 4: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 4: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 4: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 11.2964 seconds (Warm-up) ## Chain 4: 2.51584 seconds (Sampling) ## Chain 4: 13.8122 seconds (Total) ## Chain 4: print(M1_stanlmer, digits = 2) ## stan_lmer ## family: gaussian [identity] ## formula: Y ~ X + (X | KID) ## observations: 40 ## ------ ## Median MAD_SD ## (Intercept) 358.76 15.31 ## X 12.71 3.11 ## ## Auxiliary parameter(s): ## Median MAD_SD ## sigma 29.93 3.79 ## ## Error terms: ## Groups Name Std.Dev. Corr ## KID (Intercept) 24.36 ## X 9.26 -0.15 ## Residual 30.32 ## Num. levels: KID 4 ## ## ------ ## * For help interpreting the printed output see ?print.stanreg ## * For info on the priors used see ?prior_summary.stanreg summary(M1_stanlmer, pars = c(\u0026quot;(Intercept)\u0026quot;, \u0026quot;X\u0026quot;,\u0026quot;sigma\u0026quot;, \u0026quot;Sigma[KID:(Intercept),(Intercept)]\u0026quot;, \u0026quot;Sigma[KID:X,(Intercept)]\u0026quot;, \u0026quot;Sigma[KID:X,X]\u0026quot;), probs = c(0.025, 0.975), digits = 2) ## ## Model Info: ## function: stan_lmer ## family: gaussian [identity] ## formula: Y ~ X + (X | KID) ## algorithm: sampling ## sample: 4000 (posterior sample size) ## priors: see help(\u0026#39;prior_summary\u0026#39;) ## observations: 40 ## groups: KID (4) ## ## Estimates: ## mean sd 2.5% 97.5% ## (Intercept) 357.39 18.40 312.29 390.14 ## X 12.40 4.06 2.50 19.81 ## sigma 30.32 3.91 23.68 38.98 ## Sigma[KID:(Intercept),(Intercept)] 593.52 1322.20 1.72 3794.51 ## Sigma[KID:X,(Intercept)] -34.16 170.47 -375.20 302.16 ## Sigma[KID:X,X] 85.70 170.48 6.74 481.62 ## ## MCMC diagnostics ## mcse Rhat n_eff ## (Intercept) 1.20 1.01 233 ## X 0.25 1.02 267 ## sigma 0.08 1.00 2545 ## Sigma[KID:(Intercept),(Intercept)] 65.01 1.01 414 ## Sigma[KID:X,(Intercept)] 8.90 1.01 367 ## Sigma[KID:X,X] 10.07 1.01 287 ## ## For each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1). 和非貝葉斯版本的概率論隨機效應線性回歸模型的結果相對比一下：\nlibrary(lme4) M1 \u0026lt;- lmer(formula = Y ~ X + (X | KID), data = d, REML = TRUE) summary(M1) ## Linear mixed model fit by REML [\u0026#39;lmerMod\u0026#39;] ## Formula: Y ~ X + (X | KID) ## Data: d ## ## REML criterion at convergence: 387.2 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.3697 -0.5184 -0.0355 0.7635 1.8788 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## KID (Intercept) 504.00 22.450 ## X 28.54 5.342 -1.00 ## Residual 833.94 28.878 ## Number of obs: 40, groups: KID, 4 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 358.207 15.385 23.283 ## X 13.067 2.741 4.767 ## ## Correlation of Fixed Effects: ## (Intr) ## X -0.848 ## convergence code: 0 ## boundary (singular) fit: see ?isSingular ","date":1565913600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565913600,"objectID":"0790a5e11b0ff714686c30cabbe75028","permalink":"https://wangcc.me/post/multilevel-model-rstan/","publishdate":"2019-08-16T00:00:00Z","relpermalink":"/post/multilevel-model-rstan/","section":"post","summary":"Rstan 學習筆記 Chapter 8","tags":["Bayesian","Medical Statistics"],"title":"等級線性回歸模型的 Rstan 貝葉斯實現","type":"post"},{"authors":null,"categories":["thoughts","life"],"content":"\n","date":1565827200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565827200,"objectID":"785892c46e901729dfd155fa93a08389","permalink":"https://wangcc.me/post/entrance-done/","publishdate":"2019-08-15T00:00:00Z","relpermalink":"/post/entrance-done/","section":"post","summary":"更新最近去的一次岐阜山裏烤肉體驗，以及蓋房進度的照片。","tags":["experience","偶爾感慨","Japan"],"title":"房子還是只有地基","type":"post"},{"authors":null,"categories":["thoughts","life"],"content":"來日本第8個年頭，一次也未去過沖繩實在是有些說不過去。\n一家人自上次冬天去了北海道感受零下極寒的風雪之後，疲於處理找土地蓋房子的事情，終於得空再出門度假休息，這次我們登上飛機飛往離日本本土較遠的沖繩本島。\n(有趣的日本航空在機翼兩側頑皮地繪製了可愛的小動物)\n(日本航空飛行時可以連接機艙內無線網路，非常貼心，好評)\n從名古屋離開時天氣預報顯示颱風的尾部可能還在對沖繩造成風雨的影響，有點擔心短短的三天時間不要全部都是下雨天。兩小時之後抵達那霸機場，剛出艙門，一陣悶熱的空氣迎面而來，讓我想起多年不曾再感受過的南國海濱城市的酷暑。\n(日本最西端的車站\u0026ndash;那霸空港站)\n那霸機場往市區有一條單軌電車，我們上電車去國際通附近預訂的酒店住下。夜裏小朋友睡着了以後。我們還去拉麵館犯了罪(T_T)：\n第二天想起早去吃個早點，結果幾家有早餐的沖繩餐廳已經人滿爲患，最後我們在買完早餐去取車時站在國際通的十字路口碰見大晴天：\n(我還買了一頂草帽臭美)\n去浮潛 然後我們開着日產的紅色Serena開始一路向北去體驗第一站\u0026mdash;-青の洞窟浮潛。\n我們預約的浮潛店裏有懂國語的潛水員，你可能還能看見我的中文評論：\n下水之前穿上潛水服是一件有些困難且需要技巧的事情：\n(還留着鼻涕的KY，興奮地牽着潛水員姐姐的手往海邊走)\n我帶着像素很差，勉強可以在水中成像的小米小蟻運動相機。水下拍照也挺有趣的：\n還能拍到背着氧氣瓶在深處潛水的朋友：\n但是和潛水員背着的水下相機相比，小蟻相機真的僅僅只是能拍出東西而已：\n(專業水下拍攝裝備的照片頓時給出高大上的圖)\n(我在拍魚，有人在拍我) (藍色洞窟的得名原因\n\u0026mdash;洞窟裏水下透出的陽光讓這海水顯得無比蔚藍)\n後來奶奶對潛水體驗也念念不忘，這是我的小蟻相機在水裏拍到的魚和奶奶： (上岸之後，陽光明媚笑容燦爛的奶奶) 去水族館 第二天我們來到沖繩旅行必去的景點之一\u0026ndash;美ら海水族館。天氣又不遺餘力地晴空萬里。\n海豚在興奮地跳躍，天空和大海比賽誰更藍：\n沖繩水族館展示巨大鯊魚的玻璃牆聽說是吉尼斯世界紀錄上水族館裏最大的玻璃牆。 這個大水缸裏著名的兩頭大鯊魚分別有5噸重和4.5噸重。真是兩個龐然大物了。\n大海龜游泳都顯得那麼自得其樂。\n第一次體驗海邊游泳，看到這麼大的魚，我家的兩個KY小寶寶都開心得不得了。\n(南國不可少的芒果冰)\n(沖繩特色紫薯蕎麥麵)\n(在離開古宇利島不遠處經過一家不顯眼的壽司店，價格實惠又意外地美味的壽司。)\n依依不捨地離開 (和藍天碧海說再見)\n(海邊撿到的寄居蟹) (疲倦地睡到大天亮) 中午趕到那霸機場，帶着小朋友們登上塗着藍色鯨魚的圖案的日航飛機。沖繩真是適合帶小朋友來旅行的好地方。想起三天前下飛機時還擔心颱風擾亂旅行計劃的我們，這次運氣真的好。希望下次還能再來體驗其他的離島。再見啦，沖繩。\n","date":1563926400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563926400,"objectID":"4ab7100cba31ded2aaa600735d3b4b51","permalink":"https://wangcc.me/post/okinawa-trip/","publishdate":"2019-07-24T00:00:00Z","relpermalink":"/post/okinawa-trip/","section":"post","summary":"來日本第8個年頭，一次也未去過沖繩實在是有些說不過去","tags":["experience","偶爾感慨","Japan"],"title":"沖繩旅行小記","type":"post"},{"authors":null,"categories":null,"content":"We applied unsupervised learning technique on the NDNS RP dataset. More details will be released asap.\n","date":1563321600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563321600,"objectID":"e777f2fc02332c6e2e16bcbe11bba53d","permalink":"https://wangcc.me/project/chronon/","publishdate":"2019-07-17T00:00:00Z","relpermalink":"/project/chronon/","section":"project","summary":"Understand the time when you eat-chrononutrition","tags":["chrononutrition"],"title":"Chrononutrition","type":"project"},{"authors":null,"categories":["sorrow"],"content":"如果你想看這本書，可以在王友琴教授的網站上下載。\nP18: 所謂“無法顯示”，完全是謊言。這個網站一直存在並且運作良好。當然，說謊的並不是電腦。\n盡管不是出乎意料，我仍然感到震驚。受難者們已經死亡三十多年。當年他們死亡的時候，大多數人的骨灰都沒有保留，更談不上安葬。三十年後，在電腦網絡的虛擬空間裏，都不容許有他們的安息之地，是爲了什麼？是誰，做了決定禁止受難者的名字在網上？\nP21: 流落人間者，泰山一毫芒\nP23: 中國是一個最講尊師重道的古老文明古國，而且尊師的傳統從未斷絕過。\nP26: 對於一個患了嚴重失憶症的民族，王友琴博士這部文革受難者真是一劑及時良藥。\nP28: 毛首先罷黜了那些試圖約束青年的同僚，以此掃清了道路，致使很多地方陷入霍布斯式的自然狀態（即相互爭鬥，人人自危的野蠻狀態），中學生和大學生實施暴力和恐怖整整兩年，最先鬥老師，然後鬥黨內幹部，最後自己互相鬥。。。。　但是真正的研究可能形成對整個一代人的指控－－那些參與者和觀看者。他們正在掌握國家的領導權。\nP29: 她是受難者的一個活資料庫。她一個人抗拒着數億人的遺忘。\nP40: 其實，看看事實，就知道這不但不是什麼向權勢者“造反”，而且從開始就是極權勢力的一次直接擴張。\nP46: 作爲一個中學校長，她從來沒有也幾乎不可能在上級指示之外做什麼標新立異之事，也沒有違抗過他們的命令。高層領導人的孩子，都在她主管的學校上學。然而，當革命需要打擊目標的時候，上級們就可以翻臉不認人，把一個個活人當做靶子拋出來，批判鬥爭，處分懲罰。他們根本不把他們的下級當人來看待，而只是一些數字和百分比，一些可以服務於革命目標的工具甚至靶子。冷酷是文革的一個重要特徵。文革不但嚴厲打擊反對革命的人，而且嚴厲打擊未曾反對革命的人。\nP48: 我們永遠無法知道，在卞仲耘死前的幾個小時裡，當她遭到這樣殘酷的毆打和折磨的時候，她想了些什麼。雖然她一直被人群包圍，她死在絕對的孤獨之中。當她被打的時候，沒有一個人出來制止暴行。當她快要死去的時候，沒有一個人在身邊表示同情。她從來沒有與這些打死她的人為敵，但是這些人不但打死了她，而且，在打她的時候毫不猶豫，在她被打死後也沒有覺得任何後悔或者羞愧。她孤立無援地死在紅衛兵學生的亂棒之下，甚至沒有可能作一點但反抗來保護自己。從一個活人的世界上，她被無情無義地背叛了，被拋棄了被犧牲了。\nP51: 1966年的夏天，全中國的學校變成了刑訊室，監獄，甚至殺人場。大批老師被迫害致死。\nP52: 1966年10月召開的“中共中央工作會議”發放了一個題為《把舊世界打個落花流水》的文章，其中被列為紅衛兵功績之一的，是1968年8月20日到9月底北京有1772人被打死。有理由認為實際死亡數據大於此數。但是，此數已經是極其巨大的數字。卞仲耘的死尚不在此數之中。1966年8月5日發生的卞仲耘之死，是8月殺戮的開端，經過三個星期的發展，在8月底前後達到每日被害人數的最高峰。由最高權力者號召鼓動，用中學生紅衛兵為打手，打死手無寸鐵的教育工作者如卞仲耘，以及大批沒有防衛能力的和平居民，還視為偉大功績，這實在是二十世紀統治者所作的最為殘忍和無恥的行為之一。\nP62: 卞仲耘，一個教育工作者的死，標誌了這個血腥時代的開始。讓我們記住這個名字和這個日期，記住在文明的進程中可能發生什麼樣的逆轉和災難。\nP63: 1993年，筆者到校中攝下一張宿舍樓的照片。卞仲耘被打死在這座宿舍樓門口的台階上。四個住在樓裡的高中三年級的學生問我：“20多年前有人在這兒被打死，這是真的嗎？我們什麼都沒聽說過。”\nP101: 文革前，清華大學有108名教授，曾經被人開玩笑說好像《水滸傳》裏有“108將”。陳祖東就是這“108將”之一。陳祖東的家人聽說，到1978年，這108人只死剩下40多人了。\nP104: 我在文革後考進北京大學中文系讀書，從來沒有聽到人提起程賢策的名字和他在文革中自殺的事情。雖然這個大學剛剛發生過文革這樣的重大歷史事件，文革歷史還未得到記錄和分析，但是，有着著名文科科系的北京大學，卻不教學生去認識和分析這些發生在自己學校的重要歷史事實，這顯然不恰當也相當具有諷刺性。不過，這也是普遍現象。其中主要的原因，是最高權力當局的嚴格禁止。\nP110: 1968年，北京大學建立了一所校園監獄，命名爲\u0026quot;黑幫監改大院\u0026quot;，把二百多名教職員工關在裏面。那年6月18日，關在\u0026quot;監改大院\u0026quot;裏的人被拉出來\u0026quot;鬥爭\u0026quot;，當他們排隊穿過校園的時候，甬道兩側站滿了學生，手持棍棒皮鞭，爭相痛打他們。然後，他們被拉到各系，施以種種酷刑。那一天，北大校園裏充滿了狂熱的殘忍於惡毒。\nP110: 北京大學建立於1898年的維新運動中。大學本該是中國現代化，科學和文明的代表。但是在文革中，北京大學卻變成最野蠻殘酷的行爲發生的地方。暴力性的\u0026quot;鬥爭會\u0026quot;，包括毆打侮辱掛黑牌戴高帽子等等，校園\u0026quot;勞改隊\u0026quot;，校園監獄，都在北京大學領先開始，更不要說道德方面的墮落如誣陷，謊言，讒佞等等普遍發生。北京大學發生的這種巨大變化，是文革真正劇烈改變社會傳統以及行爲規範最\u0026quot;成功\u0026quot;的例子。這種成功，令人震驚，也令人思考。\nP112: 一批一批的人成爲\u0026quot;敵人\u0026quot;，一旦被指控，就被無情地清洗出去，既不能自我辯護，也逃脫不了殘酷的處罰。革命的巨爪不但在農村，也在這所中國最早建立的現代大學裏面，把人一把一把抓起來，糟蹋丟棄。\nP112: 在1966年，程賢策從\u0026quot;革命者\u0026quot;變成爲\u0026quot;革命\u0026quot;打擊的對象。看起來，文革好像是非邏輯的。但是實際上這一切有其內在的邏輯。檢視往事，現在可以看出，一批人在參與迫害的同時，也鋪就了迫害他們自己的道路。因爲他們參與的迫害，不只是對一些個人的否定，而且是對法治，對程序正義，對一個公民應該具有的公民權利的根本否定。\nP139: 在1991年範長江的名字被用來命名“新聞獎”，但是，他本人爲什麼從1952年就不能再做新聞工作，爲什麼他在1970年悲慘地死於井中，這些卻沒有報道和分析。\nP139: 樊西曼，女，1915年生，鐵道部中共黨校黨委副書記，1966年8月25日被兒子的同學，北京師範大學附屬第二中學紅衛兵綁架到學校，在學校內一個磚砌的乒乓球臺子上被打死。兒子曹濱海從此精神失常。同一天在校中被打死的，還有這個中學的語文老師斳正宇和學校負責人姜培良。\nP142: 從此，鬥打，亂殺事件日益嚴重，由開始打鬥個別“表現不好”的“四類分子”（地主，富農，反革命分子，壞分子），發展到打鬥一般的四類分子；由一個大隊消滅一兩個，兩三個四類分子，發展到亂殺家屬子女和有一般問題的人，最後發展到全家被滅絕。子8月27日至9月1日，該縣的13個公社，48個大隊，先後殺害“四類分子”及其家屬供325人。最大的80歲，最小的僅38天，有22戶被殺絕。\nP147: 傅雷，男，1908年生，上海居民，翻譯家，翻譯大量法語作品，在1957年被劃成“右派分子”，1966年8月下旬被抄家和“鬥爭”，9月3日在寓所中和妻子朱梅馥一起留下遺書自殺身亡。傅雷時年53歲。文革後傅雷得到“平反”。《傅雷家書》出版後，成爲受歡迎的暢銷書。“家書”是他和兒子的通信。他有兩個兒子，一名“聰”，一名“敏”，都出生與1930年代。1966年時，傅敏是北京第一女子中學的英文教員。1966年8月北京的中學教員和校長們遭到紅衛兵學生的野蠻攻擊，傅敏在學校附近投水自殺，幸而未死。他的哥哥是鋼琴家傅聰，1958年在公派波蘭學習畢業的時候，不回中國，去了英國，當時被稱作“叛國分子”。\nP150: 乒乓球和政治和思想觀念沒有直接的關聯。然而，文革不但整死作家，教員和演員，還把這些乒乓球運動員整死，這是怎樣的殘酷和瘋狂？在毛澤東之前和之後，還沒有一個暴君做過這樣的事情。\nP151: 高斌，男，湖北人，1940年代留學英國，曾任北京外國語學院俄國文學教授，調陝西師範大學後，在“反右運動”中被定位“右派分子”，送農場“勞動改造”，“摘帽”後恢復授課，但降薪降級。1966年遭到“批鬥”後自縊身亡。\nP158: 夜裏，龔維泰就躺在“一教”的地板上，靜悄悄地殺死了自己。很難想象，什麼樣的絕望會讓人這樣結束自己的生命！他自殺，沒有抗議，沒有抱怨，甚至在流血中漸漸死去的過程中沒有呻吟，沒有響動，以至躺在他身邊的人們都不知道發生了什麼。 。。。 龔維泰的事情聽起來確實很“奇怪”。龔維泰參加共產黨組織的“民青”反對國民黨政府，他被逮捕以後並沒有治他的罪。沒有證據說明他被捕後爲國民黨政府做過任何傷害共產黨的事情，他卻在20年後的“清理階級隊伍運動”中，爲此被捕事件遭到嚴酷的審查，最後這樣可怕的死去。實際上，從法律的角度看，除非是殺了人，不管龔維泰在那時候做了什麼，都已經過了法律的追溯期限，不能再作追究。但是“革命”壓倒一切的時候，法律是紙上空文。非常諷刺的是，文革後有人控告文革中的殺人兇手，北京的檢察院卻以“法律追溯時限已過”拒絕。\nP169: “中華人民共和國憲法”上冠冕堂皇地寫着中國公民享有“言論自由”，甚至在文革中修改過的新憲法中，雖然去掉了“遷徙自由”，卻依然留有“言論自由”。然而，在實際上，言論竟被當作判處死刑的根據。 \u0026hellip; 顧文選的第二條罪狀是逃離中國，在材料上被稱爲“叛國投敵”。在共產黨宣傳中，人民是“國家的主人”，但是在1950，1960，1970年代，普通人民根本不被允許得到護照出國。如果他們想要離開中國，只有祕密離開。祕密離開中國是十分危險的事情。他們可能在中途被打死。如果被抓住，竟然可以成爲判處死刑的根據。\n在歷史上，從來沒有過這樣殘酷的執法。在德國有過“柏林牆”。在1989年柏林牆被推倒以前，試圖偷越“柏林牆”的人，有一百多人被哨兵打死，這是非常兇殘的事情，因此在柏林牆被推倒之後，下令開槍射擊的東德領導人被法庭起訴。\nP170: 在文革後很多年，仍然有很人認爲文革是一個“大民主”。這種看法的“根據”是，普通人可以對各級領導幹部“造反”。且不說當時的“鬥爭會”等形式是多麼野蠻和違法的手段，也不說可以“造反”的內容僅僅是那些人“反對毛澤東思想”，這樣的看法無視顧文選這樣的人被殘酷殺害的事實，無視聞佳這樣的人被判重型的事實，創造了一個遠離事實的文革神話。\nP172: 至今一些人還在肯定文革“反對官僚制度”的正面貢獻，因爲毛澤東說了要“精兵簡政”以及“打破重疊的行政機構”（1968年3月）。然而文革中實際上工作了的，是取消了公安局，檢察院，法院三家分開並且獨立這樣的社會組織形式，取消了定罪和審判的法律程序，在製造對人的迫害方面大大提高了效率。這是一種多麼可怕的提高效率啊。 \u0026hellip;\n文革中掌管北京的“公檢法”長達十年的，是原爲南京軍區某部軍委副政委的劉傳新。1976年毛澤東死去以及“四人幫”隨之被逮捕。在文革中被劉傳新關押迫害過的一批老幹部重新回到權力位置上。1977年1月27日，劉傳新被免去北京市公安局長的職務。他被“隔離”在東交民巷他居住的院落裏受“審查”。1977年5月18日，他接到了北京市公安局第二天要開“聲討劉傳新大會”的通知。半夜，他在一棵樹上上吊自殺。\n劉傳新自殺，當然是因爲他的失勢，也可能是由於懼怕他用於別人身上的殘酷做法會被用到他自己身上。但是他懼怕的人中間，不會有顧文選。這不但因爲顧文選已經被殺死，也因爲顧文選本來也只是一個沒有權力的普通人。\nP186: 把社會中的一個很大的人羣，劃出來進行“審查”，隔離審訊，再從中劃出一部分作爲受到永久性處罰的“敵人”，這樣的做法幾十年來不斷實行，以致有的中國人已經把這樣的做法視爲像颳風下雨一樣的常態，從不從根本上去質疑和反對。\nP187: 也使人吃驚的是，被裝載籮筐裏遭到“鬥爭”的人，對於他自己被安的罪名不承認，但是對他自己曾經發動的對幾百萬人進行過的於此類似的“鬥爭”，至死也並未覺得不安。在1990年代出版的他的女兒寫的關於他的書裏（《紅色家族檔案：羅瑞卿的女兒的點點回憶》，羅點點，南海出版公司，海口1999年），細膩深情地寫到他在文革中的遭遇多麼不公平，卻一字也沒有提到他曾多麼殘酷地對待千千萬萬別的人。\nP187: 在歷史上，也還從來沒有一個時代和一個政權，可以把普通人民控制到這樣嚴密的程度。他們不告訴人民這些人的“反革命”活動到底是些什麼，卻要求每一個人都“表態”來支持殺死這些人。他們不但殺人，還要造成一個“衆口一詞”的形勢。他們用恐怖來塑造輿論，這輿論反過來又來支持恐怖。 。。。 一位被訪者說：你能想象那時候北京人有多壞嗎？他們根本不把別人的命當回事兒。他們喊過“槍斃槍斃”，就趕快回家吃飯去了。\n魯迅寫的阿Q，在他自己被殺之前，曾經很興奮地去看殺別人的頭。文革年代，普通人也活得像魯迅筆下的阿Q，會興高采烈地參加“公判大會”，把別人被槍斃當作好戲看。\nP189： 以意識形態的名義和革命的名義，把人類的一部分宣佈爲必須消滅的“敵人”，通過一系列預先設計的所謂“政治運動”，一個政權把社會中的一個羣體，不是一個人兩個人，也不是幾十個或幾百個人，而是一個及其巨大的人數\u0026ndash;人羣中的一個百分比，有計劃有組織有系統地予以打擊和消滅，這就是毛澤東對顧文選和億萬中國人所作的。\nP192: 筆者曾經問清華附中的受訪者，你們是怎麼知道紅衛兵不準醫院搶救郭蘭蕙的。兩位認識郭蘭蕙的學生說，紅衛兵曾經在學校當衆宣佈，由於郭蘭蕙是自殺的，**醫院打電話到清華附中詢問她是什麼人，是否有“問題”，清華附中紅衛兵接了電話告訴醫院，郭蘭蕙是“右派學生”，於是醫院不給搶救，讓郭蘭蕙在醫院的地板上死去。**紅衛兵不但對郭蘭蕙自殺毫無憐憫之心，而且用得意洋洋的口氣在學校裏告訴其他學生這些情況，顯示他們主宰生死的權力和威風。\n郭蘭蕙死時只有19歲。她曾經病休一年，所以1966年她上高二，而不是高三。\nP192： 在郭蘭蕙死亡兩個星期以前，該校高一（二）班學生楊愛倫也因相同的原因試圖自殺。楊愛倫的父親在1949年以前的政府海關做事，於是被認定爲“壞家庭出身”。她在1966年7月底就開始在班裏被紅衛兵“鬥爭”。清華附中紅衛兵的領導人之一曾經到她的班上詳細指示如何整她。她被緊閉在一間小屋裏，被強迫寫“檢查交代”。1966年8月8日，楊愛倫到“清華園”火車站附近臥軌自殺。火車頭把她鏟出了軌道。她沒有死，但是臉部和身體受到重傷，並且失去了三個手指，成爲永久性傷殘。\nP197-199: 韓光第，男，牙醫，家住四川省漢源縣富林鎮第二居民段。在1968年夏天因說毛澤東送給“首都工農毛澤東思想宣傳隊”的芒果“像一條紅薯沒什麼看頭”，被逮捕，長期關押之後，1970年被以“現行反革命”罪判處死刑，在富林鎮郊被槍斃。 。。。\n實際上，送到四川漢源鎮上的這個芒果，也根本不是真的芒果。真的芒果要保存那麼長的時間，早已經腐爛了。在漢源鎮上展示的，只可能是個蠟製的複製件。當時在全國各地作了無數這類的複製品或者芒果照片，強制八億人崇拜。1968年，是文革中害死人最多的“清理階級隊伍運動”進行的時候，也是對毛澤東的個人崇拜最嚴重的時候之一。 。。。\n漢源鎮上的人，包括很多小學生，看到了韓光第被槍殺的場面。這樣的場面，無疑對他們的一生都影響深遠。它們從此再不會敢對任何和毛澤東有關的東西，說任何自己想作的評論。它們變得非常謹慎小心。甚至在35年之後，它們還得顧慮是否要把這樣的事實說出來。 。。。\n文革時期曾經秩序相當混亂，當時的“專政機關”審批死刑的手續也被簡化（文革前死刑要經過最高法院批准，文革中這個權力被下放了），但是文革並不是一個失控的時期，被判處死刑的人，都是當時上面下令開始的政治運動高潮的結果，有一套逮捕人和處分人的規定。\nP201: 南京第13中學位於南京市“西家大塘”，離開市中心不願。韓康是13中學的圖書館員。韓康的家就在學校附近。因1949年以前曾參加過國民黨，紅衛兵抄了他的家，把他家的東西雜碎毀壞。\n1966年9月5日早上韓康被十三中的紅衛兵揪到學校操場上“批鬥”。另有十三中體育老師夏忠謀，也因歷史上參加過三青團等“問題”，一起被揪到操場批鬥。批鬥中，紅衛兵對二人拳打腳踢，用皮帶抽，用磚頭砸。由於紅衛兵說韓康的態度不好，對他打得更是厲害。直至下午三四點中，韓康兩三次昏死過去，但都被紅衛兵用冷水潑醒後再接着批鬥，到傍晚，韓康終於被活活打死。\n夏忠謀因一直低頭任由紅衛兵批鬥，所以僥倖沒有被當場打死，晚上他被關押到學校實驗室，外面有紅衛兵看守。夏忠謀看到韓康被打死，自己被關押第二天要接着被批鬥，肯定是難逃一死，因此晚上用衣服撕成布繩上吊自殺身亡。兩人都死於1966年9月5日一天。\nP202: 韓志穎，西安市第五中學校長，男，中國民主同盟盟員。1966年8月，第五中學的紅衛兵把凳子壘成高臺，讓韓志穎站上去接受“批鬥”。他不吸菸，紅衛兵把數隻香菸點燃後分別插入韓的耳朵，鼻孔和嘴中，用煙熏韓。韓志穎在1966年被“批鬥”至死。\nP206: 這種“主席大手筆”的說法正是文革思維的產物。實際上，在文明設會裏，儘管人們之間有貧富之分，有地位高低的區別，但是，沒有一個人有權力在法庭之外剝奪另一個人的生命，是普遍的基本原則。文革以“革命”的名義，打破了這一條原則。而且，不但這樣做了，而且，樹立了這樣的“大手筆”觀念，即認爲是可以做的，特別是對“偉大”人物來說。\n在建設“紀念園”的時候，不止一人向筆者提過這樣的問題：寫這種普通人受難者的故事有什麼意義？這種質疑和這種“主席大手筆”說法有實質相連。如果說這種話的是有權勢的人，那是權勢使它們眼裏沒有普通人的位置。如果說這種話的是普通人，那麼是長期精神奴役的結果。\nP224: 文革後進入北京航空學院讀書的一位被訪者說，那時候學校裏依然流傳着文革時代留下的一句話：該校主樓的每一扇窗戶，差不多都曾經有人跳下來自殺。\n","date":1563235200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563235200,"objectID":"2f00ae9833273f09032af30e335b0743","permalink":"https://wangcc.me/post/notes/","publishdate":"2019-07-16T00:00:00Z","relpermalink":"/post/notes/","section":"post","summary":"Victim","tags":["sorrow"],"title":"受難者列傳筆記","type":"post"},{"authors":null,"categories":["thoughts"],"content":"過了一週時間，我又忍不住跑去我家的地裏去看看進展如何。發現上週的所謂貌似已經完成地基完全是未經證實的假消息。 原來這周結束才算是真正完成了房屋的地基結構，不信你看：\n對比前一個週五同一個角度拍下的地基模樣，是不是相差很多？（不要告訴我只是多了一個熊孩子\u0026hellip;.）\n原來只有鋼筋的地方現在都灌上了水泥。還能看見這周下過雨留下的積水。。。所以，前一週僅僅只是按照圖紙把鋼筋骨骼的結構鋪設好了。這周加上了水泥，就像骨骼上長滿了肌肉一樣，一下子順眼很多。房間結構也更加清晰了。\n快來看將來的玄關： 感覺進門之後的空間略有狹小，前進四五步之後左手邊會是一個類似壁櫥的位置可以放置進門帶着的隨手行李，手提包，還可以掛不想帶入房間內的外套。同一位置的右手邊則是一樓的洗手間。再進去就是客廳了。這張圖沒有照進去的是左手邊，也就是壁櫥後面的一個連着客廳的房間，我們暫時準備放置給小朋友練習的鋼琴，以及平時玩耍的玩具之類。大概格局是這樣子的，也可以用作臨時客房：\n琴房的背後連着客廳：\n鏡頭中照進去的會是一樓通往二樓的樓梯的位置，樓梯上面會有一個透光採光用的大窗，樓梯前面則可以放置電視臺。拍這張照片時準備放置沙發的位置放了一堆不明鐵板（期待下星期再來時鐵板會被拼成什麼樣子）：\n可以看到鐵板被防雨的帆布包裹着，上面有一籃不明鐵器：\n客廳連着廚房和餐廳的位置：\n這裏的右邊將來會是全家人洗漱的洗臉檯房間，感覺這裏會比玄關還要寬敞：\n洗臉房間往裏走連着浴室，不清楚這裏途中右手邊黑色的一塊是什麼。 靠近拍了一張： 發現有一行字被印在左下角，放大了才看清楚寫的是\u0026quot;防蟻処理済\u0026quot;，原來這竟是一塊木頭放在洗臉房間和浴室連接的門框下面的地基位置。很好奇這是一種什麼處理方式。 另外，浴室外面西北角的土地上現在被打入了三個小小的洞，也不太清楚目的是什麼：\n另外三個小洞這張圖的右側水泥牆壁的部分，保留了一個和外面相同的洞，估計能通過水管之類的用於排水？\n從外側再給廚房來一張特寫： 另外讓人有點遺憾的是整個房子的南側，由於土地形狀的限制，其實是有個梯形的結構的。這使得餐廳連接客廳的位置被收窄。這裏垂直上方會是二樓的陽臺，因此這裏將來準備鋪木質的平臺，連着客廳的大落地窗。南部的採光會不會被這個陽臺應響了呢。這裏正好往北推進了大約20~30公分，所以，希望上面即使會有陽臺也不至於把南側進入客廳的陽光擋住了。\n這天我們其實是先帶了小朋友們去參觀體操俱樂部，看他倆多開心：\n我們離開新家的時候，還在家旁邊的萬博紀念公園的樹上發現了褪了殼的知了，是不是安靜完整得像一隻真的蟲？\n炎熱夏日就要到來，希望工地的工人們不要中暑。這週末我們全家會啓程去沖繩度假。也但願天公作美，沖繩的海要藍藍地等我們來玩耍呀。\n","date":1563148800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563148800,"objectID":"b884bfd8fcd8fcf893f6fe5a0b4e411e","permalink":"https://wangcc.me/post/base-of-the-house-done/","publishdate":"2019-07-15T00:00:00Z","relpermalink":"/post/base-of-the-house-done/","section":"post","summary":"地基完成啦","tags":["experience","偶爾感慨"],"title":"地基完成啦","type":"post"},{"authors":null,"categories":["Bayesian","R techniques","statistics"],"content":" 分析目的，數據，和選擇 Poisson 回歸模型的原因 想象模型機制 寫下數學模型表達式 把數學模型翻譯成 Stan 模型代碼 運行結果的解釋 分析目的，數據，和選擇 Poisson 回歸模型的原因 我們這裏使用之前擬合貝葉斯邏輯回歸模型時使用的相同的數據來展示如何跑貝葉斯泊松回歸模型。\nd \u0026lt;- read.table(\u0026quot;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap05/input/data-attendance-2.txt\u0026quot;, sep = \u0026quot;,\u0026quot;, header = T) head(d) ## PersonID A Score M Y ## 1 1 0 69 43 38 ## 2 2 1 145 56 40 ## 3 3 0 125 32 24 ## 4 4 1 86 45 33 ## 5 5 1 158 33 23 ## 6 6 0 133 61 60 其中，\nPersonID: 是學生的編號； A, Score: 用來預測出勤率的兩個預測變量，分別是表示是否喜歡打工的 A，和表示對學習本身是否喜歡的評分 (滿分200)； M: 過去三個月內，該名學生一共需要上課的總課時數； Y: 過去三個月內，該名學生實際上出勤的課時數。 這一次我們希望通過分析泊松回歸來回答「A 和 Score 對總課時數 M 具體有多大的影響？」這個問題。之前擬合貝葉斯邏輯回歸模型時，使用的結果變量是 Y，也就是實際出勤課時數。但是本小節我們用 M 作爲結果變量。因爲總課時數是學生自己選課時的結果，也就是說學生本身的態度（是否喜歡打工，是否熱愛學習），可能本身左右了他/她到底會選多少課。背景知識假設是：喜歡多去打工的學生，選課可能態度消極，總課時數從開始可能就選的少。那麼像總選課時數這樣的非負（計數型）離散變量作爲結果變量的時候，泊松回歸模型是我們的第一選擇。\n想象模型機制 如果使用上上節介紹的多重線性回歸模型，那麼模型的預測變量的分佈便可能取到負數，這樣就不符合實際情況下“總選課時數”是非負（計數型）離散變量這一事實。這就需要把預測變量 A 和 Score 相加的線性模型 \\((b_1 + b_2A + b_3Score)\\)，通過數學轉換限制在非負數範圍。假設平均總課時數是 \\(\\lambda\\)，我們認爲它服從均值是 \\(\\lambda\\) 的泊松分佈。關於泊松分佈的詳細知識，期望值和方差的推導可以參考學習筆記。另外，非貝葉斯版本的一般性傳統泊松回歸模型可以參照學習筆記的廣義線性回歸的泊松回歸模型章節。\n對泊松回歸模型略有瞭解的話應該很自然地想到，把結果變量限制在非負數範圍的標準鏈接方程是 \\(\\log(\\lambda)\\)，或者在 Stan 模型中，我們更自然地把線性模型部分寫在指數模型中: \\(\\exp(b_1 + b_2A + b_3Score)\\)。\n寫下數學模型表達式 \\[ \\begin{aligned} \\lambda[n] \u0026amp; = \\exp(b_1 + b_2A[n] + b_3Score[n]) \u0026amp; n = 1, \\dots, N \\\\ M[n] \u0026amp; \\sim \\text{Poisson}(\\lambda[n]) \u0026amp; n = 1, \\dots, N \\end{aligned} \\]\n其中，\n\\(N\\)，是該數據中學生的人數； \\(n\\)，是每名學生的標籤/編號（下標）； \\(b_1, b_2, b_3\\) 是我們感興趣的參數。 把數學模型翻譯成 Stan 模型代碼 data { int N; int\u0026lt;lower=0, upper=1\u0026gt; A[N]; real\u0026lt;lower=0, upper=1\u0026gt; Score[N]; int\u0026lt;lower=0\u0026gt; M[N]; } parameters { real b[3]; } transformed parameters { real lambda[N]; for (n in 1:N) { lambda[n] = exp(b[1] + b[2]*A[n] + b[3]*Score[n]); } } model { for (n in 1:N) { M[n] ~ poisson(lambda[n]); } } generated quantities { int m_pred[N]; for (n in 1:N) { m_pred[n] = poisson_rng(M[n], q[n]); } } 值得一提的是，在 Stan 中，提供了 poisson_log(x) 分佈函數，其實它等價於使用 poisson(exp(x))。除了更加接近我們熟悉的泊松回歸模型的數學表達式，避免了 exp 指數運算，計算結果穩定。於是我們還可以把上面的模型修改成：\ndata { int N; int\u0026lt;lower=0, upper=1\u0026gt; A[N]; real\u0026lt;lower=0, upper=1\u0026gt; Score[N]; int\u0026lt;lower=0\u0026gt; M[N]; } parameters { real b[3]; } transformed parameters { real lambda[N]; for (n in 1:N) { lambda[n] = b[1] + b[2]*A[n] + b[3]*Score[n]； } } model { for (n in 1:N) { M[n] ~ poisson_log(lambda[n]); } } generated quantities { int m_pred[N]; for (n in 1:N) { m_pred[n] = poisson_log_rng(M[n], q[n]); } } 運行它的代碼如下：\nlibrary(rstan) ## Loading required package: StanHeaders ## Loading required package: ggplot2 ## rstan (Version 2.19.3, GitRev: 2e1f913d3ca3) ## For execution on a local, multicore CPU with excess RAM we recommend calling ## options(mc.cores = parallel::detectCores()). ## To avoid recompilation of unchanged Stan programs, we recommend calling ## rstan_options(auto_write = TRUE) data \u0026lt;- list(N=nrow(d), A=d$A, Score=d$Score/200, M=d$M) # fit \u0026lt;- stan(file=\u0026#39;model/model5-6.stan\u0026#39;, data=data, seed=1234) fit \u0026lt;- stan(file=\u0026#39;stanfiles/model5-6b.stan\u0026#39;, data=data, seed=1234, pars = c(\u0026quot;b\u0026quot;, \u0026quot;lambda\u0026quot;)) ## ## SAMPLING FOR MODEL \u0026#39;model5-6b\u0026#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 2e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.2 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.113057 seconds (Warm-up) ## Chain 1: 0.111411 seconds (Sampling) ## Chain 1: 0.224468 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL \u0026#39;model5-6b\u0026#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 7e-06 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 2: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 2: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 2: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 2: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 2: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 2: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 2: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 2: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 2: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 2: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 2: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.107243 seconds (Warm-up) ## Chain 2: 0.125844 seconds (Sampling) ## Chain 2: 0.233087 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL \u0026#39;model5-6b\u0026#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 6e-06 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 3: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 3: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 3: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 3: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 3: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 3: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 3: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 3: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 3: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 3: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 3: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.111618 seconds (Warm-up) ## Chain 3: 0.122729 seconds (Sampling) ## Chain 3: 0.234347 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL \u0026#39;model5-6b\u0026#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 7e-06 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 4: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 4: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 4: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 4: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 4: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 4: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 4: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 4: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 4: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 4: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 4: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.116266 seconds (Warm-up) ## Chain 4: 0.114301 seconds (Sampling) ## Chain 4: 0.230567 seconds (Total) ## Chain 4: fit ## Inference for Stan model: model5-6b. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff ## b[1] 3.58 0.00 0.09 3.41 3.52 3.58 3.64 3.77 1323 ## b[2] 0.26 0.00 0.04 0.18 0.23 0.26 0.29 0.34 1775 ## b[3] 0.29 0.00 0.14 0.00 0.19 0.28 0.39 0.57 1332 ## lambda[1] 3.68 0.00 0.05 3.59 3.65 3.68 3.71 3.77 1470 ## lambda[2] 4.05 0.00 0.03 3.98 4.03 4.05 4.07 4.11 2285 ## lambda[3] 3.76 0.00 0.03 3.70 3.74 3.76 3.78 3.81 2639 ## lambda[4] 3.97 0.00 0.04 3.89 3.94 3.97 3.99 4.04 1687 ## lambda[5] 4.07 0.00 0.04 3.99 4.04 4.07 4.10 4.14 1976 ## lambda[6] 3.77 0.00 0.03 3.71 3.75 3.77 3.79 3.83 2604 ## lambda[7] 3.74 0.00 0.03 3.68 3.72 3.74 3.76 3.79 2297 ## lambda[8] 4.05 0.00 0.03 3.98 4.03 4.05 4.08 4.12 2232 ## lambda[9] 3.79 0.00 0.03 3.72 3.77 3.79 3.81 3.85 2312 ## lambda[10] 3.79 0.00 0.03 3.72 3.77 3.79 3.81 3.85 2337 ## lambda[11] 4.04 0.00 0.03 3.98 4.02 4.04 4.07 4.11 2392 ## lambda[12] 3.78 0.00 0.03 3.72 3.76 3.78 3.80 3.83 2530 ## lambda[13] 4.01 0.00 0.03 3.95 3.99 4.01 4.03 4.07 2518 ## lambda[14] 3.74 0.00 0.03 3.68 3.72 3.74 3.76 3.79 2297 ## lambda[15] 3.74 0.00 0.03 3.68 3.72 3.74 3.76 3.79 2230 ## lambda[16] 3.98 0.00 0.03 3.92 3.96 3.99 4.01 4.05 1959 ## lambda[17] 3.74 0.00 0.03 3.69 3.72 3.74 3.76 3.80 2396 ## lambda[18] 3.70 0.00 0.04 3.62 3.67 3.70 3.72 3.77 1587 ## lambda[19] 3.84 0.00 0.05 3.74 3.81 3.84 3.88 3.95 1687 ## lambda[20] 4.07 0.00 0.04 3.99 4.04 4.07 4.09 4.14 2017 ## lambda[21] 3.97 0.00 0.04 3.89 3.94 3.97 3.99 4.04 1687 ## lambda[22] 4.00 0.00 0.03 3.94 3.98 4.00 4.02 4.06 2226 ## lambda[23] 3.99 0.00 0.03 3.93 3.97 4.00 4.02 4.06 2164 ## lambda[24] 4.05 0.00 0.03 3.98 4.02 4.05 4.07 4.11 2339 ## lambda[25] 4.01 0.00 0.03 3.95 3.99 4.01 4.03 4.07 2493 ## lambda[26] 4.03 0.00 0.03 3.97 4.01 4.03 4.05 4.08 2626 ## lambda[27] 3.75 0.00 0.03 3.69 3.73 3.75 3.77 3.80 2540 ## lambda[28] 3.75 0.00 0.03 3.69 3.73 3.75 3.77 3.80 2540 ## lambda[29] 3.81 0.00 0.04 3.73 3.78 3.81 3.84 3.89 1979 ## lambda[30] 3.74 0.00 0.03 3.69 3.72 3.74 3.76 3.80 2363 ## lambda[31] 4.08 0.00 0.04 3.99 4.05 4.08 4.11 4.16 1853 ## lambda[32] 3.95 0.00 0.05 3.85 3.92 3.95 3.98 4.04 1544 ## lambda[33] 4.04 0.00 0.03 3.98 4.02 4.04 4.06 4.10 2469 ## lambda[34] 4.02 0.00 0.03 3.96 4.00 4.02 4.04 4.08 2616 ## lambda[35] 3.76 0.00 0.03 3.70 3.74 3.76 3.78 3.81 2645 ## lambda[36] 3.77 0.00 0.03 3.71 3.75 3.77 3.79 3.82 2630 ## lambda[37] 3.99 0.00 0.03 3.93 3.97 3.99 4.01 4.06 2134 ## lambda[38] 3.74 0.00 0.03 3.68 3.72 3.74 3.76 3.79 2263 ## lambda[39] 3.71 0.00 0.04 3.64 3.68 3.71 3.73 3.78 1689 ## lambda[40] 3.71 0.00 0.04 3.63 3.68 3.71 3.73 3.78 1672 ## lambda[41] 3.76 0.00 0.03 3.71 3.75 3.76 3.78 3.82 2645 ## lambda[42] 3.77 0.00 0.03 3.71 3.75 3.77 3.79 3.83 2604 ## lambda[43] 3.75 0.00 0.03 3.70 3.74 3.75 3.77 3.81 2602 ## lambda[44] 3.79 0.00 0.03 3.72 3.77 3.79 3.82 3.86 2264 ## lambda[45] 3.84 0.00 0.05 3.74 3.81 3.84 3.88 3.94 1711 ## lambda[46] 3.73 0.00 0.03 3.67 3.71 3.73 3.75 3.79 2100 ## lambda[47] 3.65 0.00 0.06 3.54 3.61 3.65 3.69 3.77 1390 ## lambda[48] 3.79 0.00 0.03 3.73 3.77 3.79 3.82 3.86 2216 ## lambda[49] 3.72 0.00 0.03 3.66 3.70 3.72 3.74 3.78 1911 ## lambda[50] 3.98 0.00 0.03 3.92 3.96 3.99 4.01 4.05 1959 ## lp__ 6896.60 0.03 1.18 6893.47 6896.04 6896.91 6897.47 6897.96 1368 ## Rhat ## b[1] 1 ## b[2] 1 ## b[3] 1 ## lambda[1] 1 ## lambda[2] 1 ## lambda[3] 1 ## lambda[4] 1 ## lambda[5] 1 ## lambda[6] 1 ## lambda[7] 1 ## lambda[8] 1 ## lambda[9] 1 ## lambda[10] 1 ## lambda[11] 1 ## lambda[12] 1 ## lambda[13] 1 ## lambda[14] 1 ## lambda[15] 1 ## lambda[16] 1 ## lambda[17] 1 ## lambda[18] 1 ## lambda[19] 1 ## lambda[20] 1 ## lambda[21] 1 ## lambda[22] 1 ## lambda[23] 1 ## lambda[24] 1 ## lambda[25] 1 ## lambda[26] 1 ## lambda[27] 1 ## lambda[28] 1 ## lambda[29] 1 ## lambda[30] 1 ## lambda[31] 1 ## lambda[32] 1 ## lambda[33] 1 ## lambda[34] 1 ## lambda[35] 1 ## lambda[36] 1 ## lambda[37] 1 ## lambda[38] 1 ## lambda[39] 1 ## lambda[40] 1 ## lambda[41] 1 ## lambda[42] 1 ## lambda[43] 1 ## lambda[44] 1 ## lambda[45] 1 ## lambda[46] 1 ## lambda[47] 1 ## lambda[48] 1 ## lambda[49] 1 ## lambda[50] 1 ## lp__ 1 ## ## Samples were drawn using NUTS(diag_e) at Mon May 18 16:57:44 2020. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). 運行結果的解釋 ...{省略}... mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat b[1] 3.58 0.00 0.09 3.38 3.51 3.58 3.64 3.76 1373 1 b[2] 0.26 0.00 0.04 0.18 0.24 0.26 0.29 0.35 1797 1 b[3] 0.29 0.00 0.15 0.00 0.20 0.29 0.39 0.59 1422 1 lambda[1] 3.68 0.00 0.05 3.58 3.65 3.68 3.71 3.77 1510 1 ...{省略}... 我們把計算獲得的事後概率分佈均值放入前面寫下的數學表達式:\n\\[ \\begin{aligned} \\lambda[n] \u0026amp; = \\exp(3.58 + 0.26A[n] + 0.29Score[n]/200) \u0026amp; n = 1, \\dots, N \\\\ M[n] \u0026amp; \\sim \\text{Poisson}(\\lambda[n]) \u0026amp; n = 1, \\dots, N \\end{aligned} \\]\n例如說，Score = 150 和 Score = 50 的兩名學生，如果對打工喜好態度相同的話，他們之間選課的總課時數之比爲：\n\\[ \\begin{aligned} \\frac{M_\\text{Score = 150}}{M_\\text{Score = 50}} \u0026amp; = \\frac{\\exp(3.58 + 0.26A + 0.29\\times\\frac{150}{200})}{\\exp(3.58 + 0.26A + 0.29\\times\\frac{50}{200})} \\\\ \u0026amp; = \\exp(0.29\\times\\frac{150-50}{200}) \\approx 1.16 \\end{aligned} \\]\n也就是熱愛學習分數 Score 達到150的人和只有50的人相比，選課總課時數平均多 16%。相似地，喜歡打工 A = 1 的學生和不喜歡打工 A = 0 的學生選課總課時數之比爲 \\(\\exp(0.26)\\approx1.30\\)。\n","date":1562803200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562803200,"objectID":"2bdaa681c689eed4126d72e97c14aef5","permalink":"https://wangcc.me/post/poisson-stan/","publishdate":"2019-07-11T00:00:00Z","relpermalink":"/post/poisson-stan/","section":"post","summary":"Rstan 學習筆記 Chapter 5.4","tags":["Bayesian","Medical Statistics"],"title":"泊松回歸模型的貝葉斯Stan實現","type":"post"},{"authors":null,"categories":["diary","thoughts"],"content":"上週6月29日結束開工的祈福儀式(地鎮祭)過後，又是一星期的陰雨天不斷。\n週五下午接到工地負責師傅打來電話報告進度說，這周的工事基本已經完成，地基的輪廓大致做好了。我很好奇這陰雨天中他們是怎麼見縫插針地找時間灌好水泥的。直到看到建築公司的銷售小哥發了下面的挖土作業的照片給我，心終於放了下來：\n![](/post/2019-07-07-construction_files/shovel car.jpg)\n看來他們還是能找到不下雨的空隙來作業的。顯然我的擔心有點多餘。但畢竟可能這輩子就蓋這一次房子。真是不想有任何不順利的事發生呢。\n週六白天去菜市場買菜回來的路上又拐過去工地看了一下，看見他們已經在門口立起來一塊牌子，牌子上有一条工務店給自己打的廣告，還有建築工地負責人，設計師的名字以及一条的公司老闆的名字：\n可以看見預計的工事期間是到12月10日。也就是說，我們的元旦和新年就會在新房子裏度過啦！十分期待。\n擡腳跑到西邊空地上（稍高）給這個地基來了一張整體照（封面圖片）： 可以清楚的看到離鏡頭最近的部分將會是浴室和洗手臺的房間。右邊很寬敞的則是廚房及餐廳。稍遠一點的地方將會是未來的大客廳。還有停在路邊的就是我自家開的豐田Sienta。估計這樣每週每週過來看看進度，會是一件很有意思，也很有盼頭的事呢。\n同一天，其實正好也是女兒的2歲生日。\n去年女兒一歲的時候，我人還在倫敦苦逼地寫着畢業論文。今年我們就已經在一起期盼着年底住新房的喜悅了。希望在新的房子裏，孩子們能健康快樂地長大。等8月份上樑了以後，房子的輪廓就能看見了，到時候可能又會是新的不安新的期待，還有好多傢俱和生活所需的電器的要準備，要花銀子的地方實在是數不玩。。。。\n","date":1562457600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562457600,"objectID":"520715902a998c111fce47971baab66f","permalink":"https://wangcc.me/post/construction/","publishdate":"2019-07-07T00:00:00Z","relpermalink":"/post/construction/","section":"post","summary":"地基貌似已經完成","tags":["偶爾感慨","experience"],"title":"地基貌似已經完成","type":"post"},{"authors":null,"categories":["diary","thoughts"],"content":"今日在即將開工建房的土地上，建築公司請來神社的神職人，花了大約一個小時的時間，幫助我們完成了 地鎮祭的儀式。\n地鎮祭儀式主要是為了祈福接下來的建築過程以及主人家人居住之後的平安與幸福的祈禱儀式。聽說中國農村如果自己土地上蓋房子，也會有相似的儀式來祈求神靈守護。\n本來儀式本身我們並不擔心會有什麼意外的事情發生。是這一週的天氣預報總是雷雨和陰雨不斷。於是家人都很擔心週六的地鎮祭儀式時的天氣，從週一開始每天都在手機查閱天氣情況。還好到了地鎮祭舉行的週六這天，天公作美在早上11點儀式開始前雨停下。我們抵達土地時，神職人也為了以防萬一搭好了臨時擋風遮雨的帳篷。\n可以看到神職人已經為我們準備好了供奉的水果蔬菜，清酒，驅邪的食鹽和潔淨的大米： 之後儀式順利舉行，神職人帶著我們在土地建房的東南西北各個方向上把祭拜後的食鹽和淨米，還有清酒灑在地面上。\n兒子也很聽話地參與了祭拜和祈禱的整個過程：\n全家一起在未開工的新家地址上第一張合照產生啦：\n感謝上蒼，我們順利在無風無雨中結束儀式，建築現場負責的工人安排說明了接下來直至我們能夠搬新家之前的流程之後，天又一下子陰沈下來，轉眼又大雨傾盆。\n祝願我們的新家可以平安無事地建成，建成後，也保佑我們會幸福美滿地在這裏繼續我們的生活。封面是從神職人手裏領受的守護。聽說會被一直埋在地基下面，從今天起，保佑這片土地。\n","date":1561766400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561766400,"objectID":"82b9adb636c17fe47c7255467d1ec8c2","permalink":"https://wangcc.me/post/jichinsai/","publishdate":"2019-06-29T00:00:00Z","relpermalink":"/post/jichinsai/","section":"post","summary":"即將開工的祈禱儀式","tags":["experience","偶爾感慨"],"title":"地鎮祭","type":"post"},{"authors":null,"categories":[],"content":"這些年習慣了折騰Ubuntu之後，突然切換回高大上的 Macbook Pro，真心不太適應。\n首先吐槽花了兩天時間才搞定家裏和學校的打印機聯接設定。真是不曉得昨天早上和晚上為啥死都連不上的佳能打印機驅動，今天重新打開電腦之後就突然能接上了。果然\u0026quot;重新啟動\u0026quot;永遠是最有有效的招數。其實完全不知道未重啟之前到底電腦的哪根筋沒有接好。\n接下來讓人不可思議的是 MacOS 沒有OpenBUGS。經過放狗搜索之後找到了在蘋果電腦上安裝 OpenBUGS 的方法。簡單總結就是下載 Wine 之後用 wine 來跑OpenBUGES，介面風格像80年代的筆電一樣：\n即使最終能夠使用OpenBUGS的介面進行運算之後，在Ubuntu和Windows都能自動識別並連結OpenBUGS引擎的 Brugs 包竟然還是無法下載識別MCMC引擎。所以下載了勉強能夠使用 R2OpenBUGS 包計算，但是還需要用下面繁瑣的設定：\nWINE=\u0026#34;/usr/local/bin/wine\u0026#34; WINEPATH=\u0026#34;/usr/local/bin/winepath\u0026#34; OpenBUGS.pgm=\u0026#34;/Users/{user.name}/Wine\\ Files/drive_c/Program\\ Files/OpenBUGS/OpenBUGS323/OpenBUGS.exe\u0026#34; 於是，我的學習筆記大概率估計是只能在Ubuntu上更新了。\n還有要吐槽的一個事情就是，StataIC 版本在Ubuntu下可以使用命令行直接在終端呼叫Stata引擎進行統計分析並且輸出結果，十分方便。而且設定了路徑之後可以直接在Rstudio裡面加入Stata引擎，使得Rmarkdown的代碼部分可以自由地在R和Stata之間切換。但是Mac版本的StataIC竟然沒有命令行的安裝方式。繼續放狗查詢才知道，必須是StataSE或者StataMP版本才可以。這是另一條讓我滾回Ubuntu的理由嗎？\nMac電腦現在在我眼裡已經爛得只剩下顏值了。\n這兩天折騰安裝各種軟件之時，發現Texlive 2019的Mac版本安裝完畢之後佔據將近6個G的硬盤空間。於是狠狠心刪除了之後換上輕巧簡潔的 TinyTex，發現整個世界都純淨了。\n只需要在Rmd文檔的YAML部分加入下面這段代碼，一份日語（中文）英文混合的PDF文檔即可在Rstudio中編譯完成，從安裝到完成第一個日語文檔的編譯，基本上僅僅耗時十分鐘左右。好評！\n--- title: \u0026#34;日本語\u0026#34; header-includes: - \\usepackage{xltxtra} - \\usepackage{zxjatype} - \\usepackage[ipaex]{zxjafont} output: pdf_document: latex_engine: xelatex --- ","date":1561680000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561680000,"objectID":"0393a0410f73d8968fabcef303c0fead","permalink":"https://wangcc.me/post/2019-06-28-macbook/","publishdate":"2019-06-28T00:00:00Z","relpermalink":"/post/2019-06-28-macbook/","section":"post","summary":"Mac筆記本折騰之路","tags":["experience","偶爾感慨"],"title":"菜鳥的Mac筆記本折騰之路","type":"post"},{"authors":null,"categories":["diary","sorrow","thoughts"],"content":" Your browser does not support the video tag. 試問誰還未發聲\n都捨我其誰衛我城\n天生有權還有心可作主\n誰要認命噤聲\n試問誰能未覺醒\n聽真那自由在奏鳴\n激起再難違背的那份良知和應\n為何美夢仍是個夢\n還想等恩賜泡影\n為這黑與白這非與是\n真與偽來做證\n為這世代有未來\n要及時擦亮眼睛\n無人有權沉默\n看著萬家燈火變了色\n問我心再用我手\n去為選我命途力拼\n人既是人\n有責任有自由決定遠景\n這是一個值得銘記的日子。這是一個需要我們所有人都認識到自己所處的世代，要面臨劇變的時刻。\n在那個廣場上，你要麼是王維林，要麼就是那臺坦克。\n","date":1560297600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560297600,"objectID":"4dfa55ce0fe7c00e01c86beab278148c","permalink":"https://wangcc.me/post/do-you-hear-people-sing/","publishdate":"2019-06-12T00:00:00Z","relpermalink":"/post/do-you-hear-people-sing/","section":"post","summary":"Your browser does not support the video tag. 試問誰還未發聲 都捨我其誰衛我城 天生有權還有心可作主 誰要認命噤聲 試問誰能未覺醒 聽真那自由在奏鳴 激起再難違背的那份良知和應 為何美","tags":["偶爾感慨"],"title":"誰還未覺醒 | Do you hear people sing","type":"post"},{"authors":null,"categories":["thoughts"],"content":"原文鏈接\n我不知道一個過去年代的廣場\n從何而始，從何而終\n有的人用一小時穿過廣場\n有的人用一生 \u0026mdash;\u0026mdash;\n早晨是孩子，傍晚已是垂暮之人\n我不知道還要在陽光中走出多遠\n才能停住腳步？\n還要在陽光中眺望多久才能\n閉上眼睛？\n當高速行駛的汽車打開刺目的車燈\n哪些曾在一個明媚早晨穿過廣場的人\n我從汽車的後視鏡看見過他們一閃即逝\n的面孔\n傍晚他們乘車離去\n一個無人離去的地方不是廣場\n一個無人倒下的地方也不是\n離去的重新歸來\n倒下的卻永遠倒下了\n一種叫做石頭的東西\n迅速地堆積，屹立\n不像骨頭的生長需要一百年的時間\n也不像骨頭那麼軟弱\n每個廣場都有一個用石頭壘起來的\n腦袋，使兩手空空的人們感到生存的\n分量。以巨大的石頭腦袋去思考和仰望\n對任何人都不是一件輕鬆的事\n石頭的重量\n減輕了人們肩上的責任，愛情和犧牲\n或許人們會在一個明媚的早晨穿過廣場\n張開手臂在四面來風中柔情地擁抱\n但當黑夜降臨\n雙手就變得沉重\n唯一的發光體是腦袋裏的石頭\n唯一刺向石頭的利劍悄然墜地\n黑暗和寒冷在上升\n廣場周圍的高層建築穿上了瓷和玻璃的時裝\n一切變得矮小了。石頭的世界\n在玻璃反射出來的世界中輕輕浮起\n像是塗在孩子們作業本上的\n一個隨時會被撕下來揉成一團的陰沉念頭\n汽車疾駛而過，把流水的速度\n傾瀉到有着鋼鐵筋骨的龐大混凝土制度中\n賦予寂靜以喇叭的形狀\n一個過去年代的廣場從後視鏡消失了\n永遠消失了 \u0026mdash;\u0026mdash;\n一個青春期的，初戀的，佈滿粉刺的廣場\n一個從未在賬單和死亡通知書上出現的廣場\n一個路初胸膛，挽起衣袖，紮緊腰帶\n一個雙手使勁搓洗的帶補丁的廣場\n一個通過年輕的血液流到身體之外\n用舌頭去舔，用前額去下磕，用旗幟去掩蓋\n的廣場\n空想的，消失的，不復存在的廣場\n像下了一夜的大雪在早晨停住\n一種純潔而神祕的融化\n在良心和眼睛裏交替閃爍\n一部分成爲叫做淚水的東西\n零一部分在叫做石頭的東西裏變得堅硬起來\n石頭的世界崩潰了\n一個軟組織的世界爬到高處\n整個過程就像泉水從吸管離開礦物\n進入密閉的，蒸餾過的，有着精美包裝的空間\n我乘坐高速電梯在雨天的傘柄裏上升\n回到地面時，我看到雨傘一樣張開的\n一座圓形餐廳在城市上空旋轉\n像一頂從魔法變出來的帽子\n它的尺寸並不適合\n用石頭壘起來的巨人的腦袋\n那些曾托起廣場的手臂放了下來\n如今巨人僅靠一柄短劍來支撐\n它會不會刺破什麼呢？比如，一場曾經有過的\n從紙上掀起，在牆上張貼的脆弱革命？\n從來沒有一種力量\n能把兩個不同世界長久地粘在一起\n一個反覆張貼的腦袋最終將被撕去\n反覆粉刷的牆壁\n被露出大腿的混血女郎佔據了一半\n另一半時頭髮再生，假肢安裝之類的誘人廣告\n一輛嬰兒車靜靜地停在傍晚的廣場上\n靜靜地，和這個快要發瘋的世界沒有關係\n我猜嬰兒和落日之間的距離有一百年之遙\n這是近乎無限的尺度，足以測量\n穿過廣場所要經歷的一個幽閉時代有多麼漫長\n對幽閉的普遍恐懼，使人們從各自的棲居\n雲集廣場，把一生中的孤獨時刻變成熱烈的節日\n但在棲居深處，在愛與死的默默的注目禮中\n一個空無人際的影子廣場被珍藏着\n像緊閉的懺悔室只屬於內心的祕密\n是否穿越廣場之前必須穿越內心的黑暗\n現在黑暗中最黑的兩個世界合爲一體\n堅硬的石頭腦袋被劈開\n利劍在黑暗中閃閃發光\n如果我能用被劈成兩半的神祕黑夜\n去解釋一個雙腳踏在大地上的明媚早晨 \u0026mdash;\u0026mdash;\n如果我能沿着灑滿晨曦的臺階\n去登上虛無之巔的巨人的肩膀\n不是爲了升起，而是爲了隕落 \u0026mdash;\u0026mdash;\n如果黃金鐫刻的銘文不是爲了被傳頌\n而是爲了被抹去，被遺忘，被踐踏 \u0026mdash;\u0026mdash;\n正如一個被踐踏的廣場遲早要落到踐踏者頭上\n哪些曾在一個明媚早晨穿過廣場的人\n他們的黑色皮鞋也遲早要落到利劍之上\n像必將落下的棺蓋落到棺材上那麼沉重\n躺在裏面的不是我，也不是\n行走在劍刃上的人\n我沒想到這麼多人會在一個明媚的早晨\n穿過廣場，避開孤獨和永生\n他們時幽閉時代的倖存者\n我沒想到他們會在傍晚時離去或倒下\n一個無人倒下的地方不是廣場\n一個無人站立的地方也不是\n我曾時站着的嗎？還要站立多久？\n畢竟我和哪些倒下去的人一樣\n從來不是一個永生者\n","date":1559606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559606400,"objectID":"c2129e00991903f0a0abb070adb0c3dd","permalink":"https://wangcc.me/post/2019-06-04/","publishdate":"2019-06-04T00:00:00Z","relpermalink":"/post/2019-06-04/","section":"post","summary":"Victim-the great forgotten","tags":["偶爾感慨"],"title":"君與長江|傍晚穿過廣場","type":"post"},{"authors":null,"categories":["diary","thoughts"],"content":"在幫別人做論文質量評價的時候，也是個自我學習的過程。在下不才，目前無償爲幾個雜誌做審稿的工作。\n今天發來的稿件是某財大氣粗，曾經因爲篡改患者治療數據而臭名昭著的製藥公司員工寫的文章。那文章讀起來不長，卻實在是狗屎一堆。他們對歐洲高血壓患者中，直腸息肉的發生和直腸癌的發病率進行了數據庫分析。其中作者提到它們爲了避免伯克森偏倚(Berksonian Bias)，還對未執行直腸指檢篩查的高血壓患者的數據進行了分析。聽說高血壓患者中，直腸癌發病率也高（不知是真是假）。該文章在背景中大言不慚說別人的研究都搞不定到底是因爲治療高血壓的藥物導致了直腸癌，還是高血壓本身導致了直腸癌。我讀到這裏不禁已經覺得此文下半身應該已經涼涼。忍住不笑看完它們華麗麗地對每個risk factor單獨進行了一個回歸模型之後我的下巴已經掉了一半。心中暗想，他家就這水平還生產全世界最著名的高血壓藥物真的沒關係嗎？怪不得要去篡改患者的數據了。\n在鄙人的觀察下，該論文估計是沒戲了，但是這個偏倚的類型卻是我這個流行病學博士僧未曾聽說過的。算是這篇屎一樣的論文中唯一的發光點。\n原來這種偏倚最常見於樣本均採集自醫院樣本的病例對照研究。\n如果我們使用均來自於醫院的樣本作爲病例對照研究的對象，那麼我們需要假設，我們研究A疾病患者被收治到醫院內的概率，不會被該疾病可能的 risk factor 影響。但是，這種前提很多時候是無法被滿足的。特別是當A疾病的 risk factor 本身是另一種疾病的時候。理由很簡單，一名同時患有兩種疾病的患者去醫院報道的概率，顯然是要高於只有一種疾病的患者的。\nWhen we take the sample we have to assume that the chance of admission to hospital for the disease is not affected by the presence or absence of the risk factor for that disease. This may not be the case, especially if the risk factor is another disease. This is because people are more likely to be hospitalized if they have two diseases, rather than only one. 如本文開頭使用的圖片中所提示的那樣，Sacket (1979)^[Sackett, D.L. (1979). Bias in analytic research. Journal of Chronic Diseases 32, 51-63.] 當年從一般社區人羣中採集了2784名樣本進行調查，看這些研究對象中呼吸道系統疾病，和身體機能嚴重下降(locomotor disease)之間是否存在相關性。然後，他又對同一樣本中，過去半年內住院過的患者進行了相同的分析。結果顯示，如果只看住院的患者數據，作者會作出\u0026quot;患有呼吸道疾病，有較高概率導致身體機能嚴重下降。\u0026ldquo;這樣的結論。也就是兩種疾病之間存在相關性。但是如果看右半側全體社區人羣數據的話，兩種疾病之間並無顯著的關係。顯然後者才是正確的結論。前者之所以會推導出錯誤的結論，純粹是因爲同時患有兩種疾病的人，比起只有呼吸系統疾病，或者只有身體機能下降的患者有更高的概率住院。\n回頭想起我們曾經做過那麼多院內病例對照研究，不禁感到細思極恐。。。。。。\n","date":1549670400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549670400,"objectID":"3d76c9657d792ac844dc12f2cf9fec30","permalink":"https://wangcc.me/post/berksonian-bias/","publishdate":"2019-02-09T00:00:00Z","relpermalink":"/post/berksonian-bias/","section":"post","summary":"伯克森住院偏倚","tags":["偶爾感慨"],"title":"Berksonian Bias","type":"post"},{"authors":null,"categories":["Bayesian","R techniques","statistics"],"content":" 另一種形式的貝葉斯邏輯回歸 分析的目的 確認數據分佈 思考數據模型 寫下 Stan 模型代碼 檢查模型參數的收斂情況 檢查模型的擬合情況 另一種形式的貝葉斯邏輯回歸 前面一節使用的數據是以學生爲單位，將每名學生的實際課時數和實際出勤數進行了彙總之後的總結性數據，本章我們來看看相同數據的另一種形式。由於分析中有人建議說，天氣狀況對出勤率也是有較大的影響的，所以希望在前一節已有的邏輯回歸模型中增加對天氣狀況的調整。那麼這時候需要使用的就是彙總之前的數據，也就是要是用實際記錄了每名學生每一次課時的出勤與否的原始數據。值得注意的是，這時候原始數據中每名學生的記錄有許多行，因爲每行記錄的是該名學生每次上課時的天氣狀況和他/她是否出勤(0,1)的結果。\nd \u0026lt;- read.table(\u0026quot;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap05/input/data-attendance-3.txt\u0026quot;, sep = \u0026quot;,\u0026quot;, header = T) head(d, 10) ## PersonID A Score Weather Y ## 1 1 0 69 B 1 ## 2 1 0 69 A 1 ## 3 1 0 69 C 1 ## 4 1 0 69 A 1 ## 5 1 0 69 B 1 ## 6 1 0 69 B 1 ## 7 1 0 69 C 0 ## 8 1 0 69 B 1 ## 9 1 0 69 A 1 ## 10 1 0 69 A 1 其中，\n\\(Weather\\)，天氣數據 (A = 晴天，B = 多雲，C = 下雨)； \\(Y\\)，該次課時學生是否出勤 (0 = 缺勤，1 = 出勤)。 其他的數據和前一節中使用的數據相同。\n分析的目的 本次數據分析的目的依然是瞭解幾個預測變量，天氣，是否喜歡打工，是否熱愛學習，對學生出勤率的影響。\n確認數據分佈 你可以用先進的 tidyverse 進行簡單的數據彙總，看看天氣狀況不同時實際出勤率是否有差別:\nlibrary(tidyverse) d %\u0026gt;% group_by(Weather, Y) %\u0026gt;% summarise (n= n()) %\u0026gt;% mutate(rel.freq = paste0(round(100 * n/sum(n), 2), \u0026quot;%\u0026quot;)) ## # A tibble: 6 x 4 ## # Groups: Weather [3] ## Weather Y n rel.freq ## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; ## 1 A 0 306 24.31% ## 2 A 1 953 75.69% ## 3 B 0 230 31.51% ## 4 B 1 500 68.49% ## 5 C 0 138 33.91% ## 6 C 1 269 66.09% 如果你不想學習 tidyverse，也可以用下面的方法獲得類似的效果，\naggregate(Y ~ Weather, data = d, FUN = table) ## Weather Y.0 Y.1 ## 1 A 306 953 ## 2 B 230 500 ## 3 C 138 269 無論是哪種方法，我們都能大概猜出，天氣是晴天的時候 (Weather = A)，出勤率相對較高。\n在作者的原著中，使用的是給分類型變量強制賦予連續值的方法，這點確實有點噁心，爲了正常的模型，我們需要把天氣轉換成爲更加常見的啞變量 (dummy variable) 如下:\nd \u0026lt;- fastDummies::dummy_cols(d, select_columns = \u0026quot;Weather\u0026quot;) head(d) ## PersonID A Score Weather Y Weather_A Weather_B Weather_C ## 1 1 0 69 B 1 0 1 0 ## 2 1 0 69 A 1 1 0 0 ## 3 1 0 69 C 1 0 0 1 ## 4 1 0 69 A 1 1 0 0 ## 5 1 0 69 B 1 0 1 0 ## 6 1 0 69 B 1 0 1 0 思考數據模型 我們設想的數學模型應該是這樣子的:\n\\[ \\begin{aligned} \\text{logit}(q[i]) \u0026amp; = b_{1} + b_{2}A_{i} + b_{3}\\text{Score}_{i} + b_{4}\\text{WeatherB} + b_{5}\\text{WeatherC} \\\\ \\text{where} \u0026amp; \\\\ \u0026amp; \\text{ WeatherB} = 0, \\text{ WeatherC} = 0 \\text{ indicates weather = A} \\\\ \u0026amp; \\text{ WeatherB} = 1, \\text{ WeatherC} = 0 \\text{ indicates weather = B} \\\\ \u0026amp; \\text{ WeatherB} = 0, \\text{ WeatherC} = 1 \\text{ indicates weather = C} \\\\ Y[i] \u0026amp;\\sim \\text{Bernulli}(q[i]) \\end{aligned} \\]\n寫下 Stan 模型代碼 下面是相應的 Stan 模型:\ndata { int I; int\u0026lt;lower=0, upper=1\u0026gt; A[I]; real\u0026lt;lower=0, upper=1\u0026gt; Score[I]; int\u0026lt;lower=0, upper=1\u0026gt; W_B[I]; int\u0026lt;lower=0, upper=1\u0026gt; W_C[I]; int\u0026lt;lower=0, upper=1\u0026gt; Y[I]; } // The parameters accepted by the model. parameters { real b[5]; } // The model to be estimated. model { for (i in 1:I) Y[i] ~ bernoulli_logit(b[1] + b[2]*A[i] + b[3]*Score[i] + b[4]*W_B[i] + b[5]*W_C[i]); } 和跑它們的 R 代碼\nlibrary(rstan) ## Loading required package: StanHeaders ## rstan (Version 2.19.3, GitRev: 2e1f913d3ca3) ## For execution on a local, multicore CPU with excess RAM we recommend calling ## options(mc.cores = parallel::detectCores()). ## To avoid recompilation of unchanged Stan programs, we recommend calling ## rstan_options(auto_write = TRUE) ## ## Attaching package: \u0026#39;rstan\u0026#39; ## The following object is masked from \u0026#39;package:tidyr\u0026#39;: ## ## extract data \u0026lt;- list(I=nrow(d), A=d$A, Score=d$Score/200, W_A=d$Weather_A, W_B = d$Weather_B, W_C = d$Weather_C, Y=d$Y) fit1 \u0026lt;- stan(file=\u0026#39;stanfiles/myex4.stan\u0026#39;, data=data, pars=c(\u0026#39;b\u0026#39;, \u0026quot;OR1\u0026quot;, \u0026quot;OR2\u0026quot;, \u0026quot;OR3\u0026quot;, \u0026quot;OR4\u0026quot;, \u0026quot;q\u0026quot;), seed=1234) ## ## SAMPLING FOR MODEL \u0026#39;myex4\u0026#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 0.000761 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 7.61 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 7.48792 seconds (Warm-up) ## Chain 1: 7.76994 seconds (Sampling) ## Chain 1: 15.2579 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL \u0026#39;myex4\u0026#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 0.000538 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 5.38 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 2: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 2: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 2: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 2: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 2: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 2: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 2: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 2: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 2: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 2: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 2: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 7.6327 seconds (Warm-up) ## Chain 2: 8.47747 seconds (Sampling) ## Chain 2: 16.1102 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL \u0026#39;myex4\u0026#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 0.000533 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 5.33 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 3: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 3: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 3: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 3: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 3: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 3: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 3: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 3: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 3: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 3: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 3: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 7.62369 seconds (Warm-up) ## Chain 3: 8.71986 seconds (Sampling) ## Chain 3: 16.3436 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL \u0026#39;myex4\u0026#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 0.000367 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 3.67 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 4: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 4: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 4: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 4: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 4: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 4: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 4: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 4: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 4: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 4: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 4: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 7.33882 seconds (Warm-up) ## Chain 4: 8.12735 seconds (Sampling) ## Chain 4: 15.4662 seconds (Total) ## Chain 4: fit1 ## Inference for Stan model: myex4. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% ## b[1] 0.27 0.01 0.23 -0.19 0.11 0.26 0.43 0.73 ## b[2] -0.63 0.00 0.09 -0.81 -0.69 -0.63 -0.57 -0.45 ## b[3] 1.97 0.01 0.37 1.23 1.72 1.97 2.21 2.72 ## b[4] -0.38 0.00 0.10 -0.58 -0.45 -0.38 -0.30 -0.18 ## b[5] -0.50 0.00 0.13 -0.74 -0.59 -0.50 -0.41 -0.25 ## OR1 0.54 0.00 0.05 0.45 0.50 0.53 0.57 0.64 ## OR2 7.65 0.07 2.96 3.42 5.60 7.15 9.10 15.15 ## OR3 0.69 0.00 0.07 0.56 0.64 0.68 0.74 0.84 ## OR4 0.61 0.00 0.08 0.48 0.56 0.61 0.66 0.78 ## q[1] 0.64 0.00 0.03 0.58 0.62 0.64 0.66 0.69 ## q[2] 0.72 0.00 0.02 0.67 0.70 0.72 0.74 0.77 ## q[3] 0.61 0.00 0.04 0.54 0.59 0.61 0.63 0.68 ## q[4] 0.72 0.00 0.02 0.67 0.70 0.72 0.74 0.77 ## q[5] 0.64 0.00 0.03 0.58 0.62 0.64 0.66 0.69 ## q[6] 0.64 0.00 0.03 0.58 0.62 0.64 0.66 0.69 ## q[7] 0.61 0.00 0.04 0.54 0.59 0.61 0.63 0.68 ## q[8] 0.64 0.00 0.03 0.58 0.62 0.64 0.66 0.69 ## q[9] 0.72 0.00 0.02 0.67 0.70 0.72 0.74 0.77 ## q[10] 0.72 0.00 0.02 0.67 0.70 0.72 0.74 0.77 ## q[11] 0.72 0.00 0.02 0.67 0.70 0.72 0.74 0.77 ## q[12] 0.61 0.00 0.04 0.54 0.59 0.61 0.63 0.68 ## q[13] 0.72 0.00 0.02 0.67 0.70 0.72 0.74 0.77 ## q[14] 0.72 0.00 0.02 0.67 0.70 0.72 0.74 0.77 ## q[15] 0.72 0.00 0.02 0.67 0.70 0.72 0.74 0.77 ## q[16] 0.72 0.00 0.02 0.67 0.70 0.72 0.74 0.77 ## q[17] 0.72 0.00 0.02 0.67 0.70 0.72 0.74 0.77 ## q[18] 0.64 0.00 0.03 0.58 0.62 0.64 0.66 0.69 ## q[19] 0.72 0.00 0.02 0.67 0.70 0.72 0.74 0.77 ## q[20] 0.72 0.00 0.02 0.67 0.70 0.72 0.74 0.77 ## q[21] 0.64 0.00 0.03 0.58 0.62 0.64 0.66 0.69 ## q[22] 0.64 0.00 0.03 0.58 0.62 0.64 0.66 0.69 ## q[23] 0.61 0.00 0.04 0.54 0.59 0.61 0.63 0.68 ## q[24] 0.61 0.00 0.04 0.54 0.59 0.61 0.63 0.68 ## q[25] 0.72 0.00 0.02 0.67 0.70 0.72 0.74 0.77 ## q[26] 0.72 0.00 0.02 0.67 0.70 0.72 0.74 0.77 ## q[27] 0.64 0.00 0.03 0.58 0.62 0.64 0.66 0.69 ## q[28] 0.72 0.00 0.02 0.67 0.70 0.72 0.74 0.77 ## q[29] 0.64 0.00 0.03 0.58 0.62 0.64 0.66 0.69 ## q[30] 0.72 0.00 0.02 0.67 0.70 0.72 0.74 0.77 ## q[31] 0.64 0.00 0.03 0.58 0.62 0.64 0.66 0.69 ## q[32] 0.72 0.00 0.02 0.67 0.70 0.72 0.74 0.77 ## q[33] 0.64 0.00 0.03 0.58 0.62 0.64 0.66 0.69 ## q[34] 0.72 0.00 0.02 0.67 0.70 0.72 0.74 0.77 ## q[35] 0.61 0.00 0.04 0.54 0.59 0.61 0.63 0.68 ## q[36] 0.72 0.00 0.02 0.67 0.70 0.72 0.74 0.77 ## q[37] 0.72 0.00 0.02 0.67 0.70 0.72 0.74 0.77 ## q[38] 0.72 0.00 0.02 0.67 0.70 0.72 0.74 0.77 ## q[39] 0.64 0.00 0.03 0.58 0.62 0.64 0.66 0.69 ## q[40] 0.72 0.00 0.02 0.67 0.70 0.72 0.74 0.77 ## q[41] 0.72 0.00 0.02 0.67 0.70 0.72 0.74 0.77 ## q[42] 0.64 0.00 0.03 0.58 0.62 0.64 0.66 0.69 ## q[43] 0.64 0.00 0.03 0.58 0.62 0.64 0.66 0.69 ## q[44] 0.74 0.00 0.02 0.71 0.73 0.74 0.76 0.78 ## q[45] 0.74 0.00 0.02 0.71 0.73 0.74 0.76 0.78 ## q[46] 0.74 0.00 0.02 0.71 0.73 0.74 0.76 0.78 ## q[47] 0.64 0.00 0.03 0.58 0.62 0.64 0.66 0.69 ## q[48] 0.74 0.00 0.02 0.71 0.73 0.74 0.76 0.78 ## q[49] 0.66 0.00 0.02 0.62 0.65 0.66 0.68 0.71 ## q[50] 0.66 0.00 0.02 0.62 0.65 0.66 0.68 0.71 ## q[51] 0.66 0.00 0.02 0.62 0.65 0.66 0.68 0.71 ## q[52] 0.66 0.00 0.02 0.62 0.65 0.66 0.68 0.71 ## q[53] 0.74 0.00 0.02 0.71 0.73 0.74 0.76 0.78 ## q[54] 0.74 0.00 0.02 0.71 0.73 0.74 0.76 0.78 ## q[55] 0.74 0.00 0.02 0.71 0.73 0.74 0.76 0.78 ## q[56] 0.74 0.00 0.02 0.71 0.73 0.74 0.76 0.78 ## q[57] 0.74 0.00 0.02 0.71 0.73 0.74 0.76 0.78 ## q[58] 0.64 0.00 0.03 0.58 0.62 0.64 0.66 0.69 ## q[59] 0.74 0.00 0.02 0.71 0.73 0.74 0.76 0.78 ## q[60] 0.64 0.00 0.03 0.58 0.62 0.64 0.66 0.69 ## q[61] 0.74 0.00 0.02 0.71 0.73 0.74 0.76 0.78 ## q[62] 0.74 0.00 0.02 0.71 0.73 0.74 0.76 0.78 ## q[63] 0.74 0.00 0.02 0.71 0.73 0.74 0.76 0.78 ## q[64] 0.74 0.00 0.02 0.71 0.73 0.74 0.76 0.78 ## q[65] 0.66 0.00 0.02 0.62 0.65 0.66 0.68 0.71 ## q[66] 0.74 0.00 0.02 0.71 0.73 0.74 0.76 0.78 ## q[67] 0.66 0.00 0.02 0.62 0.65 0.66 0.68 0.71 ## q[68] 0.64 0.00 0.03 0.58 0.62 0.64 0.66 0.69 ## q[69] 0.66 0.00 0.02 0.62 0.65 0.66 0.68 0.71 ## q[70] 0.64 0.00 0.03 0.58 0.62 0.64 0.66 0.69 ## q[71] 0.64 0.00 0.03 0.58 0.62 0.64 0.66 0.69 ## q[72] 0.74 0.00 0.02 0.71 0.73 0.74 0.76 0.78 ## q[73] 0.66 0.00 0.02 0.62 0.65 0.66 0.68 0.71 ## q[74] 0.74 0.00 0.02 0.71 0.73 0.74 0.76 0.78 ## q[75] 0.66 0.00 0.02 0.62 0.65 0.66 0.68 0.71 ## q[76] 0.64 0.00 0.03 0.58 0.62 0.64 0.66 0.69 ## q[77] 0.74 0.00 0.02 0.71 0.73 0.74 0.76 0.78 ## q[78] 0.74 0.00 0.02 0.71 0.73 0.74 0.76 0.78 ## q[79] 0.64 0.00 0.03 0.58 0.62 0.64 0.66 0.69 ## q[80] 0.66 0.00 0.02 0.62 0.65 0.66 0.68 0.71 ## q[81] 0.66 0.00 0.02 0.62 0.65 0.66 0.68 0.71 ## q[82] 0.64 0.00 0.03 0.58 0.62 0.64 0.66 0.69 ## q[83] 0.66 0.00 0.02 0.62 0.65 0.66 0.68 0.71 ## q[84] 0.74 0.00 0.02 0.71 0.73 0.74 0.76 0.78 ## q[85] 0.74 0.00 0.02 0.71 0.73 0.74 0.76 0.78 ## q[86] 0.74 0.00 0.02 0.71 0.73 0.74 0.76 0.78 ## q[87] 0.74 0.00 0.02 0.71 0.73 0.74 0.76 0.78 ## q[88] 0.66 0.00 0.02 0.62 0.65 0.66 0.68 0.71 ## q[89] 0.74 0.00 0.02 0.71 0.73 0.74 0.76 0.78 ## q[90] 0.74 0.00 0.02 0.71 0.73 0.74 0.76 0.78 ## q[91] 0.66 0.00 0.02 0.62 0.65 0.66 0.68 0.71 ## q[92] 0.74 0.00 0.02 0.71 0.73 0.74 0.76 0.78 ## q[93] 0.74 0.00 0.02 0.71 0.73 0.74 0.76 0.78 ## q[94] 0.66 0.00 0.02 0.62 0.65 0.66 0.68 0.71 ## q[95] 0.66 0.00 0.02 0.62 0.65 0.66 0.68 0.71 ## q[96] 0.66 0.00 0.02 0.62 0.65 0.66 0.68 0.71 ## q[97] 0.74 0.00 0.02 0.71 0.73 0.74 0.76 0.78 ## q[98] 0.74 0.00 0.02 0.71 0.73 0.74 0.76 0.78 ## q[99] 0.64 0.00 0.03 0.58 0.62 0.64 0.66 0.69 ## q[100] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.84 ## q[101] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.84 ## q[102] 0.73 0.00 0.02 0.68 0.71 0.73 0.75 0.77 ## q[103] 0.75 0.00 0.02 0.72 0.74 0.75 0.76 0.79 ## q[104] 0.75 0.00 0.02 0.72 0.74 0.75 0.76 0.79 ## q[105] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.84 ## q[106] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.84 ## q[107] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.84 ## q[108] 0.73 0.00 0.02 0.68 0.71 0.73 0.75 0.77 ## q[109] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.84 ## q[110] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.84 ## q[111] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.84 ## q[112] 0.75 0.00 0.02 0.72 0.74 0.75 0.76 0.79 ## q[113] 0.75 0.00 0.02 0.72 0.74 0.75 0.76 0.79 ## q[114] 0.73 0.00 0.02 0.68 0.71 0.73 0.75 0.77 ## q[115] 0.75 0.00 0.02 0.72 0.74 0.75 0.76 0.79 ## q[116] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.84 ## q[117] 0.73 0.00 0.02 0.68 0.71 0.73 0.75 0.77 ## q[118] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.84 ## q[119] 0.73 0.00 0.02 0.68 0.71 0.73 0.75 0.77 ## q[120] 0.75 0.00 0.02 0.72 0.74 0.75 0.76 0.79 ## q[121] 0.75 0.00 0.02 0.72 0.74 0.75 0.76 0.79 ## q[122] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.84 ## q[123] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.84 ## q[124] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.84 ## q[125] 0.75 0.00 0.02 0.72 0.74 0.75 0.76 0.79 ## q[126] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.84 ## q[127] 0.75 0.00 0.02 0.72 0.74 0.75 0.76 0.79 ## q[128] 0.75 0.00 0.02 0.72 0.74 0.75 0.76 0.79 ## q[129] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.84 ## q[130] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.84 ## q[131] 0.73 0.00 0.02 0.68 0.71 0.73 0.75 0.77 ## q[132] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[133] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[134] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[135] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.58 ## q[136] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.58 ## q[137] 0.50 0.00 0.03 0.43 0.47 0.50 0.52 0.56 ## q[138] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[139] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[140] 0.50 0.00 0.03 0.43 0.47 0.50 0.52 0.56 ## q[141] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[142] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[143] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[144] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[145] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[146] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.58 ## q[147] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[148] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[149] 0.50 0.00 0.03 0.43 0.47 0.50 0.52 0.56 ## q[150] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.58 ## q[151] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.58 ## q[152] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.58 ## q[153] 0.50 0.00 0.03 0.43 0.47 0.50 0.52 0.56 ## q[154] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.58 ## q[155] 0.50 0.00 0.03 0.43 0.47 0.50 0.52 0.56 ## q[156] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[157] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[158] 0.50 0.00 0.03 0.43 0.47 0.50 0.52 0.56 ## q[159] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[160] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.58 ## q[161] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[162] 0.50 0.00 0.03 0.43 0.47 0.50 0.52 0.56 ## q[163] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.58 ## q[164] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[165] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[166] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.58 ## q[167] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[168] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[169] 0.50 0.00 0.03 0.43 0.47 0.50 0.52 0.56 ## q[170] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[171] 0.50 0.00 0.03 0.43 0.47 0.50 0.52 0.56 ## q[172] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.58 ## q[173] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[174] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[175] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[176] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[177] 0.67 0.00 0.03 0.61 0.65 0.67 0.69 0.72 ## q[178] 0.69 0.00 0.02 0.64 0.68 0.69 0.71 0.74 ## q[179] 0.69 0.00 0.02 0.64 0.68 0.69 0.71 0.74 ## q[180] 0.67 0.00 0.03 0.61 0.65 0.67 0.69 0.72 ## q[181] 0.69 0.00 0.02 0.64 0.68 0.69 0.71 0.74 ## q[182] 0.77 0.00 0.02 0.73 0.75 0.77 0.78 0.80 ## q[183] 0.77 0.00 0.02 0.73 0.75 0.77 0.78 0.80 ## q[184] 0.69 0.00 0.02 0.64 0.68 0.69 0.71 0.74 ## q[185] 0.77 0.00 0.02 0.73 0.75 0.77 0.78 0.80 ## q[186] 0.77 0.00 0.02 0.73 0.75 0.77 0.78 0.80 ## q[187] 0.69 0.00 0.02 0.64 0.68 0.69 0.71 0.74 ## q[188] 0.69 0.00 0.02 0.64 0.68 0.69 0.71 0.74 ## q[189] 0.69 0.00 0.02 0.64 0.68 0.69 0.71 0.74 ## q[190] 0.77 0.00 0.02 0.73 0.75 0.77 0.78 0.80 ## q[191] 0.69 0.00 0.02 0.64 0.68 0.69 0.71 0.74 ## q[192] 0.77 0.00 0.02 0.73 0.75 0.77 0.78 0.80 ## q[193] 0.69 0.00 0.02 0.64 0.68 0.69 0.71 0.74 ## q[194] 0.69 0.00 0.02 0.64 0.68 0.69 0.71 0.74 ## q[195] 0.77 0.00 0.02 0.73 0.75 0.77 0.78 0.80 ## q[196] 0.77 0.00 0.02 0.73 0.75 0.77 0.78 0.80 ## q[197] 0.69 0.00 0.02 0.64 0.68 0.69 0.71 0.74 ## q[198] 0.77 0.00 0.02 0.73 0.75 0.77 0.78 0.80 ## q[199] 0.77 0.00 0.02 0.73 0.75 0.77 0.78 0.80 ## q[200] 0.77 0.00 0.02 0.73 0.75 0.77 0.78 0.80 ## q[201] 0.77 0.00 0.02 0.73 0.75 0.77 0.78 0.80 ## q[202] 0.77 0.00 0.02 0.73 0.75 0.77 0.78 0.80 ## q[203] 0.69 0.00 0.02 0.64 0.68 0.69 0.71 0.74 ## q[204] 0.77 0.00 0.02 0.73 0.75 0.77 0.78 0.80 ## q[205] 0.67 0.00 0.03 0.61 0.65 0.67 0.69 0.72 ## q[206] 0.67 0.00 0.03 0.61 0.65 0.67 0.69 0.72 ## q[207] 0.77 0.00 0.02 0.73 0.75 0.77 0.78 0.80 ## q[208] 0.69 0.00 0.02 0.64 0.68 0.69 0.71 0.74 ## q[209] 0.77 0.00 0.02 0.73 0.75 0.77 0.78 0.80 ## q[210] 0.83 0.00 0.01 0.80 0.82 0.83 0.84 0.85 ## q[211] 0.83 0.00 0.01 0.80 0.82 0.83 0.84 0.85 ## q[212] 0.83 0.00 0.01 0.80 0.82 0.83 0.84 0.85 ## q[213] 0.75 0.00 0.02 0.70 0.73 0.75 0.76 0.79 ## q[214] 0.83 0.00 0.01 0.80 0.82 0.83 0.84 0.85 ## q[215] 0.77 0.00 0.02 0.73 0.76 0.77 0.78 0.80 ## q[216] 0.77 0.00 0.02 0.73 0.76 0.77 0.78 0.80 ## q[217] 0.75 0.00 0.02 0.70 0.73 0.75 0.76 0.79 ## q[218] 0.77 0.00 0.02 0.73 0.76 0.77 0.78 0.80 ## q[219] 0.77 0.00 0.02 0.73 0.76 0.77 0.78 0.80 ## q[220] 0.75 0.00 0.02 0.70 0.73 0.75 0.76 0.79 ## q[221] 0.83 0.00 0.01 0.80 0.82 0.83 0.84 0.85 ## q[222] 0.83 0.00 0.01 0.80 0.82 0.83 0.84 0.85 ## q[223] 0.83 0.00 0.01 0.80 0.82 0.83 0.84 0.85 ## q[224] 0.75 0.00 0.02 0.70 0.73 0.75 0.76 0.79 ## q[225] 0.83 0.00 0.01 0.80 0.82 0.83 0.84 0.85 ## q[226] 0.83 0.00 0.01 0.80 0.82 0.83 0.84 0.85 ## q[227] 0.83 0.00 0.01 0.80 0.82 0.83 0.84 0.85 ## q[228] 0.83 0.00 0.01 0.80 0.82 0.83 0.84 0.85 ## q[229] 0.83 0.00 0.01 0.80 0.82 0.83 0.84 0.85 ## q[230] 0.77 0.00 0.02 0.73 0.76 0.77 0.78 0.80 ## q[231] 0.83 0.00 0.01 0.80 0.82 0.83 0.84 0.85 ## q[232] 0.83 0.00 0.01 0.80 0.82 0.83 0.84 0.85 ## q[233] 0.83 0.00 0.01 0.80 0.82 0.83 0.84 0.85 ## q[234] 0.77 0.00 0.02 0.73 0.76 0.77 0.78 0.80 ## q[235] 0.83 0.00 0.01 0.80 0.82 0.83 0.84 0.85 ## q[236] 0.75 0.00 0.02 0.70 0.73 0.75 0.76 0.79 ## q[237] 0.77 0.00 0.02 0.73 0.76 0.77 0.78 0.80 ## q[238] 0.83 0.00 0.01 0.80 0.82 0.83 0.84 0.85 ## q[239] 0.77 0.00 0.02 0.73 0.76 0.77 0.78 0.80 ## q[240] 0.77 0.00 0.02 0.73 0.76 0.77 0.78 0.80 ## q[241] 0.83 0.00 0.01 0.80 0.82 0.83 0.84 0.85 ## q[242] 0.83 0.00 0.01 0.80 0.82 0.83 0.84 0.85 ## q[243] 0.77 0.00 0.02 0.73 0.76 0.77 0.78 0.80 ## q[244] 0.75 0.00 0.02 0.70 0.73 0.75 0.76 0.79 ## q[245] 0.77 0.00 0.02 0.73 0.76 0.77 0.78 0.80 ## q[246] 0.75 0.00 0.02 0.70 0.73 0.75 0.76 0.79 ## q[247] 0.83 0.00 0.01 0.80 0.82 0.83 0.84 0.85 ## q[248] 0.83 0.00 0.01 0.80 0.82 0.83 0.84 0.85 ## q[249] 0.77 0.00 0.02 0.73 0.76 0.77 0.78 0.80 ## q[250] 0.75 0.00 0.02 0.70 0.73 0.75 0.76 0.79 ## q[251] 0.83 0.00 0.01 0.80 0.82 0.83 0.84 0.85 ## q[252] 0.83 0.00 0.01 0.80 0.82 0.83 0.84 0.85 ## q[253] 0.83 0.00 0.01 0.80 0.82 0.83 0.84 0.85 ## q[254] 0.83 0.00 0.01 0.80 0.82 0.83 0.84 0.85 ## q[255] 0.83 0.00 0.01 0.80 0.82 0.83 0.84 0.85 ## q[256] 0.83 0.00 0.01 0.80 0.82 0.83 0.84 0.85 ## q[257] 0.75 0.00 0.02 0.70 0.73 0.75 0.76 0.79 ## q[258] 0.75 0.00 0.02 0.70 0.73 0.75 0.76 0.79 ## q[259] 0.77 0.00 0.02 0.73 0.76 0.77 0.78 0.80 ## q[260] 0.77 0.00 0.02 0.73 0.76 0.77 0.78 0.80 ## q[261] 0.83 0.00 0.01 0.80 0.82 0.83 0.84 0.85 ## q[262] 0.83 0.00 0.01 0.80 0.82 0.83 0.84 0.85 ## q[263] 0.83 0.00 0.01 0.80 0.82 0.83 0.84 0.85 ## q[264] 0.77 0.00 0.02 0.73 0.76 0.77 0.78 0.80 ## q[265] 0.77 0.00 0.02 0.73 0.76 0.77 0.78 0.80 ## q[266] 0.83 0.00 0.01 0.80 0.82 0.83 0.84 0.85 ## q[267] 0.77 0.00 0.02 0.73 0.76 0.77 0.78 0.80 ## q[268] 0.83 0.00 0.01 0.80 0.82 0.83 0.84 0.85 ## q[269] 0.75 0.00 0.02 0.70 0.73 0.75 0.76 0.79 ## q[270] 0.75 0.00 0.02 0.70 0.73 0.75 0.76 0.79 ## q[271] 0.80 0.00 0.01 0.77 0.79 0.80 0.80 0.82 ## q[272] 0.73 0.00 0.02 0.69 0.71 0.73 0.74 0.76 ## q[273] 0.73 0.00 0.02 0.69 0.71 0.73 0.74 0.76 ## q[274] 0.70 0.00 0.02 0.65 0.69 0.70 0.72 0.75 ## q[275] 0.80 0.00 0.01 0.77 0.79 0.80 0.80 0.82 ## q[276] 0.80 0.00 0.01 0.77 0.79 0.80 0.80 0.82 ## q[277] 0.70 0.00 0.02 0.65 0.69 0.70 0.72 0.75 ## q[278] 0.80 0.00 0.01 0.77 0.79 0.80 0.80 0.82 ## q[279] 0.73 0.00 0.02 0.69 0.71 0.73 0.74 0.76 ## q[280] 0.80 0.00 0.01 0.77 0.79 0.80 0.80 0.82 ## q[281] 0.80 0.00 0.01 0.77 0.79 0.80 0.80 0.82 ## q[282] 0.73 0.00 0.02 0.69 0.71 0.73 0.74 0.76 ## q[283] 0.80 0.00 0.01 0.77 0.79 0.80 0.80 0.82 ## q[284] 0.73 0.00 0.02 0.69 0.71 0.73 0.74 0.76 ## q[285] 0.80 0.00 0.01 0.77 0.79 0.80 0.80 0.82 ## q[286] 0.80 0.00 0.01 0.77 0.79 0.80 0.80 0.82 ## q[287] 0.73 0.00 0.02 0.69 0.71 0.73 0.74 0.76 ## q[288] 0.70 0.00 0.02 0.65 0.69 0.70 0.72 0.75 ## q[289] 0.73 0.00 0.02 0.69 0.71 0.73 0.74 0.76 ## q[290] 0.80 0.00 0.01 0.77 0.79 0.80 0.80 0.82 ## q[291] 0.73 0.00 0.02 0.69 0.71 0.73 0.74 0.76 ## q[292] 0.73 0.00 0.02 0.69 0.71 0.73 0.74 0.76 ## q[293] 0.70 0.00 0.02 0.65 0.69 0.70 0.72 0.75 ## q[294] 0.73 0.00 0.02 0.69 0.71 0.73 0.74 0.76 ## q[295] 0.70 0.00 0.02 0.65 0.69 0.70 0.72 0.75 ## q[296] 0.80 0.00 0.01 0.77 0.79 0.80 0.80 0.82 ## q[297] 0.80 0.00 0.01 0.77 0.79 0.80 0.80 0.82 ## q[298] 0.70 0.00 0.02 0.65 0.69 0.70 0.72 0.75 ## q[299] 0.80 0.00 0.01 0.77 0.79 0.80 0.80 0.82 ## q[300] 0.73 0.00 0.02 0.69 0.71 0.73 0.74 0.76 ## q[301] 0.80 0.00 0.01 0.77 0.79 0.80 0.80 0.82 ## q[302] 0.73 0.00 0.02 0.69 0.71 0.73 0.74 0.76 ## q[303] 0.70 0.00 0.02 0.65 0.69 0.70 0.72 0.75 ## q[304] 0.73 0.00 0.02 0.69 0.71 0.73 0.74 0.76 ## q[305] 0.80 0.00 0.01 0.77 0.79 0.80 0.80 0.82 ## q[306] 0.80 0.00 0.01 0.77 0.79 0.80 0.80 0.82 ## q[307] 0.80 0.00 0.01 0.77 0.79 0.80 0.80 0.82 ## q[308] 0.80 0.00 0.01 0.77 0.79 0.80 0.80 0.82 ## q[309] 0.80 0.00 0.01 0.77 0.79 0.80 0.80 0.82 ## q[310] 0.73 0.00 0.02 0.69 0.71 0.73 0.74 0.76 ## q[311] 0.80 0.00 0.01 0.77 0.79 0.80 0.80 0.82 ## q[312] 0.73 0.00 0.02 0.69 0.71 0.73 0.74 0.76 ## q[313] 0.80 0.00 0.01 0.77 0.79 0.80 0.80 0.82 ## q[314] 0.70 0.00 0.02 0.65 0.69 0.70 0.72 0.75 ## q[315] 0.73 0.00 0.02 0.69 0.71 0.73 0.74 0.76 ## q[316] 0.70 0.00 0.02 0.65 0.69 0.70 0.72 0.75 ## q[317] 0.80 0.00 0.01 0.77 0.79 0.80 0.80 0.82 ## q[318] 0.80 0.00 0.01 0.77 0.79 0.80 0.80 0.82 ## q[319] 0.80 0.00 0.01 0.77 0.79 0.80 0.80 0.82 ## q[320] 0.67 0.00 0.02 0.62 0.65 0.67 0.69 0.71 ## q[321] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[322] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[323] 0.64 0.00 0.03 0.59 0.62 0.64 0.66 0.70 ## q[324] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[325] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[326] 0.67 0.00 0.02 0.62 0.65 0.67 0.69 0.71 ## q[327] 0.67 0.00 0.02 0.62 0.65 0.67 0.69 0.71 ## q[328] 0.67 0.00 0.02 0.62 0.65 0.67 0.69 0.71 ## q[329] 0.64 0.00 0.03 0.59 0.62 0.64 0.66 0.70 ## q[330] 0.67 0.00 0.02 0.62 0.65 0.67 0.69 0.71 ## q[331] 0.64 0.00 0.03 0.59 0.62 0.64 0.66 0.70 ## q[332] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[333] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[334] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[335] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[336] 0.64 0.00 0.03 0.59 0.62 0.64 0.66 0.70 ## q[337] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[338] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[339] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[340] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[341] 0.64 0.00 0.03 0.59 0.62 0.64 0.66 0.70 ## q[342] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[343] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[344] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[345] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[346] 0.67 0.00 0.02 0.62 0.65 0.67 0.69 0.71 ## q[347] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[348] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[349] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[350] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[351] 0.67 0.00 0.02 0.62 0.65 0.67 0.69 0.71 ## q[352] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[353] 0.67 0.00 0.02 0.62 0.65 0.67 0.69 0.71 ## q[354] 0.64 0.00 0.03 0.59 0.62 0.64 0.66 0.70 ## q[355] 0.67 0.00 0.02 0.62 0.65 0.67 0.69 0.71 ## q[356] 0.67 0.00 0.02 0.62 0.65 0.67 0.69 0.71 ## q[357] 0.67 0.00 0.02 0.62 0.65 0.67 0.69 0.71 ## q[358] 0.67 0.00 0.02 0.62 0.65 0.67 0.69 0.71 ## q[359] 0.64 0.00 0.03 0.59 0.62 0.64 0.66 0.70 ## q[360] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[361] 0.67 0.00 0.02 0.62 0.65 0.67 0.69 0.71 ## q[362] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[363] 0.64 0.00 0.03 0.59 0.62 0.64 0.66 0.70 ## q[364] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[365] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[366] 0.64 0.00 0.03 0.59 0.62 0.64 0.66 0.70 ## q[367] 0.67 0.00 0.02 0.62 0.65 0.67 0.69 0.71 ## q[368] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[369] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[370] 0.67 0.00 0.02 0.62 0.65 0.67 0.69 0.71 ## q[371] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[372] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[373] 0.64 0.00 0.03 0.59 0.62 0.64 0.66 0.70 ## q[374] 0.67 0.00 0.02 0.62 0.65 0.67 0.69 0.71 ## q[375] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[376] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[377] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[378] 0.67 0.00 0.02 0.62 0.65 0.67 0.69 0.71 ## q[379] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[380] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[381] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[382] 0.64 0.00 0.03 0.59 0.62 0.64 0.66 0.70 ## q[383] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[384] 0.64 0.00 0.03 0.59 0.62 0.64 0.66 0.70 ## q[385] 0.67 0.00 0.02 0.62 0.65 0.67 0.69 0.71 ## q[386] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[387] 0.67 0.00 0.02 0.62 0.65 0.67 0.69 0.71 ## q[388] 0.67 0.00 0.02 0.62 0.65 0.67 0.69 0.71 ## q[389] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[390] 0.67 0.00 0.02 0.62 0.65 0.67 0.69 0.71 ## q[391] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[392] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[393] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[394] 0.64 0.00 0.03 0.59 0.62 0.64 0.66 0.70 ## q[395] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[396] 0.79 0.00 0.02 0.75 0.78 0.79 0.80 0.82 ## q[397] 0.85 0.00 0.01 0.82 0.84 0.85 0.85 0.87 ## q[398] 0.85 0.00 0.01 0.82 0.84 0.85 0.85 0.87 ## q[399] 0.77 0.00 0.02 0.72 0.75 0.77 0.78 0.81 ## q[400] 0.85 0.00 0.01 0.82 0.84 0.85 0.85 0.87 ## q[401] 0.85 0.00 0.01 0.82 0.84 0.85 0.85 0.87 ## q[402] 0.85 0.00 0.01 0.82 0.84 0.85 0.85 0.87 ## q[403] 0.79 0.00 0.02 0.75 0.78 0.79 0.80 0.82 ## q[404] 0.79 0.00 0.02 0.75 0.78 0.79 0.80 0.82 ## q[405] 0.79 0.00 0.02 0.75 0.78 0.79 0.80 0.82 ## q[406] 0.79 0.00 0.02 0.75 0.78 0.79 0.80 0.82 ## q[407] 0.77 0.00 0.02 0.72 0.75 0.77 0.78 0.81 ## q[408] 0.79 0.00 0.02 0.75 0.78 0.79 0.80 0.82 ## q[409] 0.77 0.00 0.02 0.72 0.75 0.77 0.78 0.81 ## q[410] 0.85 0.00 0.01 0.82 0.84 0.85 0.85 0.87 ## q[411] 0.85 0.00 0.01 0.82 0.84 0.85 0.85 0.87 ## q[412] 0.85 0.00 0.01 0.82 0.84 0.85 0.85 0.87 ## q[413] 0.85 0.00 0.01 0.82 0.84 0.85 0.85 0.87 ## q[414] 0.85 0.00 0.01 0.82 0.84 0.85 0.85 0.87 ## q[415] 0.85 0.00 0.01 0.82 0.84 0.85 0.85 0.87 ## q[416] 0.79 0.00 0.02 0.75 0.78 0.79 0.80 0.82 ## q[417] 0.77 0.00 0.02 0.72 0.75 0.77 0.78 0.81 ## q[418] 0.85 0.00 0.01 0.82 0.84 0.85 0.85 0.87 ## q[419] 0.85 0.00 0.01 0.82 0.84 0.85 0.85 0.87 ## q[420] 0.85 0.00 0.01 0.82 0.84 0.85 0.85 0.87 ## q[421] 0.85 0.00 0.01 0.82 0.84 0.85 0.85 0.87 ## q[422] 0.85 0.00 0.01 0.82 0.84 0.85 0.85 0.87 ## q[423] 0.79 0.00 0.02 0.75 0.78 0.79 0.80 0.82 ## q[424] 0.85 0.00 0.01 0.82 0.84 0.85 0.85 0.87 ## q[425] 0.85 0.00 0.01 0.82 0.84 0.85 0.85 0.87 ## q[426] 0.79 0.00 0.02 0.75 0.78 0.79 0.80 0.82 ## q[427] 0.79 0.00 0.02 0.75 0.78 0.79 0.80 0.82 ## q[428] 0.85 0.00 0.01 0.82 0.84 0.85 0.85 0.87 ## q[429] 0.85 0.00 0.01 0.82 0.84 0.85 0.85 0.87 ## q[430] 0.79 0.00 0.02 0.75 0.78 0.79 0.80 0.82 ## q[431] 0.79 0.00 0.02 0.75 0.78 0.79 0.80 0.82 ## q[432] 0.79 0.00 0.02 0.75 0.78 0.79 0.80 0.82 ## q[433] 0.79 0.00 0.02 0.75 0.78 0.79 0.80 0.82 ## q[434] 0.85 0.00 0.01 0.82 0.84 0.85 0.85 0.87 ## q[435] 0.77 0.00 0.02 0.72 0.75 0.77 0.78 0.81 ## q[436] 0.79 0.00 0.02 0.75 0.78 0.79 0.80 0.82 ## q[437] 0.85 0.00 0.01 0.82 0.84 0.85 0.85 0.87 ## q[438] 0.79 0.00 0.02 0.75 0.78 0.79 0.80 0.82 ## q[439] 0.85 0.00 0.01 0.82 0.84 0.85 0.85 0.87 ## q[440] 0.79 0.00 0.02 0.75 0.78 0.79 0.80 0.82 ## q[441] 0.85 0.00 0.01 0.82 0.84 0.85 0.85 0.87 ## q[442] 0.79 0.00 0.02 0.75 0.78 0.79 0.80 0.82 ## q[443] 0.79 0.00 0.02 0.75 0.78 0.79 0.80 0.82 ## q[444] 0.85 0.00 0.01 0.82 0.84 0.85 0.85 0.87 ## q[445] 0.85 0.00 0.01 0.82 0.84 0.85 0.85 0.87 ## q[446] 0.85 0.00 0.01 0.82 0.84 0.85 0.85 0.87 ## q[447] 0.79 0.00 0.02 0.75 0.78 0.79 0.80 0.82 ## q[448] 0.85 0.00 0.01 0.82 0.84 0.85 0.85 0.87 ## q[449] 0.85 0.00 0.01 0.82 0.84 0.85 0.85 0.87 ## q[450] 0.85 0.00 0.01 0.82 0.84 0.85 0.85 0.87 ## q[451] 0.85 0.00 0.01 0.82 0.84 0.85 0.85 0.87 ## q[452] 0.85 0.00 0.01 0.82 0.84 0.85 0.85 0.87 ## q[453] 0.85 0.00 0.01 0.82 0.84 0.85 0.85 0.87 ## q[454] 0.85 0.00 0.01 0.82 0.84 0.85 0.85 0.87 ## q[455] 0.85 0.00 0.01 0.82 0.84 0.85 0.85 0.87 ## q[456] 0.85 0.00 0.01 0.82 0.84 0.85 0.85 0.87 ## q[457] 0.79 0.00 0.02 0.75 0.78 0.79 0.80 0.82 ## q[458] 0.85 0.00 0.01 0.82 0.84 0.85 0.85 0.87 ## q[459] 0.77 0.00 0.02 0.72 0.75 0.77 0.78 0.81 ## q[460] 0.85 0.00 0.01 0.82 0.84 0.85 0.85 0.87 ## q[461] 0.77 0.00 0.02 0.72 0.75 0.77 0.78 0.81 ## q[462] 0.79 0.00 0.02 0.75 0.78 0.79 0.80 0.82 ## q[463] 0.85 0.00 0.01 0.82 0.84 0.85 0.85 0.87 ## q[464] 0.85 0.00 0.01 0.82 0.84 0.85 0.85 0.87 ## q[465] 0.77 0.00 0.02 0.72 0.75 0.77 0.78 0.81 ## q[466] 0.79 0.00 0.02 0.75 0.78 0.79 0.80 0.82 ## q[467] 0.85 0.00 0.01 0.82 0.84 0.85 0.85 0.87 ## q[468] 0.79 0.00 0.02 0.75 0.78 0.79 0.80 0.82 ## q[469] 0.85 0.00 0.01 0.82 0.84 0.85 0.85 0.87 ## q[470] 0.85 0.00 0.01 0.82 0.84 0.85 0.85 0.87 ## q[471] 0.85 0.00 0.01 0.82 0.84 0.85 0.85 0.87 ## q[472] 0.77 0.00 0.02 0.72 0.75 0.77 0.78 0.81 ## q[473] 0.85 0.00 0.01 0.82 0.84 0.85 0.85 0.87 ## q[474] 0.79 0.00 0.02 0.75 0.78 0.79 0.80 0.82 ## q[475] 0.84 0.00 0.01 0.82 0.84 0.84 0.85 0.87 ## q[476] 0.84 0.00 0.01 0.82 0.84 0.84 0.85 0.87 ## q[477] 0.84 0.00 0.01 0.82 0.84 0.84 0.85 0.87 ## q[478] 0.79 0.00 0.02 0.75 0.78 0.79 0.80 0.82 ## q[479] 0.79 0.00 0.02 0.75 0.78 0.79 0.80 0.82 ## q[480] 0.77 0.00 0.02 0.72 0.75 0.77 0.78 0.81 ## q[481] 0.77 0.00 0.02 0.72 0.75 0.77 0.78 0.81 ## q[482] 0.77 0.00 0.02 0.72 0.75 0.77 0.78 0.81 ## q[483] 0.84 0.00 0.01 0.82 0.84 0.84 0.85 0.87 ## q[484] 0.84 0.00 0.01 0.82 0.84 0.84 0.85 0.87 ## q[485] 0.84 0.00 0.01 0.82 0.84 0.84 0.85 0.87 ## q[486] 0.84 0.00 0.01 0.82 0.84 0.84 0.85 0.87 ## q[487] 0.77 0.00 0.02 0.72 0.75 0.77 0.78 0.81 ## q[488] 0.84 0.00 0.01 0.82 0.84 0.84 0.85 0.87 ## q[489] 0.84 0.00 0.01 0.82 0.84 0.84 0.85 0.87 ## q[490] 0.84 0.00 0.01 0.82 0.84 0.84 0.85 0.87 ## q[491] 0.79 0.00 0.02 0.75 0.78 0.79 0.80 0.82 ## q[492] 0.84 0.00 0.01 0.82 0.84 0.84 0.85 0.87 ## q[493] 0.84 0.00 0.01 0.82 0.84 0.84 0.85 0.87 ## q[494] 0.84 0.00 0.01 0.82 0.84 0.84 0.85 0.87 ## q[495] 0.84 0.00 0.01 0.82 0.84 0.84 0.85 0.87 ## q[496] 0.79 0.00 0.02 0.75 0.78 0.79 0.80 0.82 ## q[497] 0.84 0.00 0.01 0.82 0.84 0.84 0.85 0.87 ## q[498] 0.79 0.00 0.02 0.75 0.78 0.79 0.80 0.82 ## q[499] 0.84 0.00 0.01 0.82 0.84 0.84 0.85 0.87 ## q[500] 0.84 0.00 0.01 0.82 0.84 0.84 0.85 0.87 ## q[501] 0.84 0.00 0.01 0.82 0.84 0.84 0.85 0.87 ## q[502] 0.77 0.00 0.02 0.72 0.75 0.77 0.78 0.81 ## q[503] 0.79 0.00 0.02 0.75 0.78 0.79 0.80 0.82 ## q[504] 0.79 0.00 0.02 0.75 0.78 0.79 0.80 0.82 ## q[505] 0.79 0.00 0.02 0.75 0.78 0.79 0.80 0.82 ## q[506] 0.79 0.00 0.02 0.75 0.78 0.79 0.80 0.82 ## q[507] 0.84 0.00 0.01 0.82 0.84 0.84 0.85 0.87 ## q[508] 0.84 0.00 0.01 0.82 0.84 0.84 0.85 0.87 ## q[509] 0.77 0.00 0.02 0.72 0.75 0.77 0.78 0.81 ## q[510] 0.77 0.00 0.02 0.72 0.75 0.77 0.78 0.81 ## q[511] 0.84 0.00 0.01 0.82 0.84 0.84 0.85 0.87 ## q[512] 0.77 0.00 0.02 0.72 0.75 0.77 0.78 0.81 ## q[513] 0.77 0.00 0.02 0.72 0.75 0.77 0.78 0.81 ## q[514] 0.84 0.00 0.01 0.82 0.84 0.84 0.85 0.87 ## q[515] 0.84 0.00 0.01 0.82 0.84 0.84 0.85 0.87 ## q[516] 0.84 0.00 0.01 0.82 0.84 0.84 0.85 0.87 ## q[517] 0.84 0.00 0.01 0.82 0.84 0.84 0.85 0.87 ## q[518] 0.79 0.00 0.02 0.75 0.78 0.79 0.80 0.82 ## q[519] 0.79 0.00 0.02 0.75 0.78 0.79 0.80 0.82 ## q[520] 0.77 0.00 0.02 0.72 0.75 0.77 0.78 0.81 ## q[521] 0.84 0.00 0.01 0.82 0.84 0.84 0.85 0.87 ## q[522] 0.84 0.00 0.01 0.82 0.84 0.84 0.85 0.87 ## q[523] 0.84 0.00 0.01 0.82 0.84 0.84 0.85 0.87 ## q[524] 0.84 0.00 0.01 0.82 0.84 0.84 0.85 0.87 ## q[525] 0.77 0.00 0.02 0.72 0.75 0.77 0.78 0.81 ## q[526] 0.77 0.00 0.02 0.72 0.75 0.77 0.78 0.81 ## q[527] 0.79 0.00 0.02 0.75 0.78 0.79 0.80 0.82 ## q[528] 0.79 0.00 0.02 0.75 0.78 0.79 0.80 0.82 ## q[529] 0.84 0.00 0.01 0.82 0.84 0.84 0.85 0.87 ## q[530] 0.84 0.00 0.01 0.82 0.84 0.84 0.85 0.87 ## q[531] 0.79 0.00 0.02 0.75 0.78 0.79 0.80 0.82 ## q[532] 0.79 0.00 0.02 0.75 0.78 0.79 0.80 0.82 ## q[533] 0.84 0.00 0.01 0.82 0.84 0.84 0.85 0.87 ## q[534] 0.84 0.00 0.01 0.82 0.84 0.84 0.85 0.87 ## q[535] 0.84 0.00 0.01 0.82 0.84 0.84 0.85 0.87 ## q[536] 0.77 0.00 0.02 0.72 0.75 0.77 0.78 0.81 ## q[537] 0.77 0.00 0.02 0.72 0.75 0.77 0.78 0.81 ## q[538] 0.74 0.00 0.02 0.70 0.72 0.74 0.75 0.77 ## q[539] 0.74 0.00 0.02 0.70 0.72 0.74 0.75 0.77 ## q[540] 0.74 0.00 0.02 0.70 0.72 0.74 0.75 0.77 ## q[541] 0.66 0.00 0.02 0.61 0.64 0.66 0.67 0.70 ## q[542] 0.66 0.00 0.02 0.61 0.64 0.66 0.67 0.70 ## q[543] 0.74 0.00 0.02 0.70 0.72 0.74 0.75 0.77 ## q[544] 0.74 0.00 0.02 0.70 0.72 0.74 0.75 0.77 ## q[545] 0.63 0.00 0.03 0.57 0.61 0.63 0.65 0.68 ## q[546] 0.74 0.00 0.02 0.70 0.72 0.74 0.75 0.77 ## q[547] 0.74 0.00 0.02 0.70 0.72 0.74 0.75 0.77 ## q[548] 0.74 0.00 0.02 0.70 0.72 0.74 0.75 0.77 ## q[549] 0.74 0.00 0.02 0.70 0.72 0.74 0.75 0.77 ## q[550] 0.74 0.00 0.02 0.70 0.72 0.74 0.75 0.77 ## q[551] 0.66 0.00 0.02 0.61 0.64 0.66 0.67 0.70 ## q[552] 0.63 0.00 0.03 0.57 0.61 0.63 0.65 0.68 ## q[553] 0.66 0.00 0.02 0.61 0.64 0.66 0.67 0.70 ## q[554] 0.66 0.00 0.02 0.61 0.64 0.66 0.67 0.70 ## q[555] 0.63 0.00 0.03 0.57 0.61 0.63 0.65 0.68 ## q[556] 0.63 0.00 0.03 0.57 0.61 0.63 0.65 0.68 ## q[557] 0.74 0.00 0.02 0.70 0.72 0.74 0.75 0.77 ## q[558] 0.74 0.00 0.02 0.70 0.72 0.74 0.75 0.77 ## q[559] 0.63 0.00 0.03 0.57 0.61 0.63 0.65 0.68 ## q[560] 0.66 0.00 0.02 0.61 0.64 0.66 0.67 0.70 ## q[561] 0.63 0.00 0.03 0.57 0.61 0.63 0.65 0.68 ## q[562] 0.66 0.00 0.02 0.61 0.64 0.66 0.67 0.70 ## q[563] 0.74 0.00 0.02 0.70 0.72 0.74 0.75 0.77 ## q[564] 0.66 0.00 0.02 0.61 0.64 0.66 0.67 0.70 ## q[565] 0.74 0.00 0.02 0.70 0.72 0.74 0.75 0.77 ## q[566] 0.63 0.00 0.03 0.57 0.61 0.63 0.65 0.68 ## q[567] 0.74 0.00 0.02 0.70 0.72 0.74 0.75 0.77 ## q[568] 0.66 0.00 0.02 0.61 0.64 0.66 0.67 0.70 ## q[569] 0.74 0.00 0.02 0.70 0.72 0.74 0.75 0.77 ## q[570] 0.74 0.00 0.02 0.70 0.72 0.74 0.75 0.77 ## q[571] 0.74 0.00 0.02 0.70 0.72 0.74 0.75 0.77 ## q[572] 0.83 0.00 0.01 0.81 0.82 0.83 0.84 0.86 ## q[573] 0.83 0.00 0.01 0.81 0.82 0.83 0.84 0.86 ## q[574] 0.77 0.00 0.02 0.74 0.76 0.77 0.79 0.81 ## q[575] 0.77 0.00 0.02 0.74 0.76 0.77 0.79 0.81 ## q[576] 0.75 0.00 0.02 0.71 0.74 0.75 0.77 0.79 ## q[577] 0.77 0.00 0.02 0.74 0.76 0.77 0.79 0.81 ## q[578] 0.83 0.00 0.01 0.81 0.82 0.83 0.84 0.86 ## q[579] 0.83 0.00 0.01 0.81 0.82 0.83 0.84 0.86 ## q[580] 0.77 0.00 0.02 0.74 0.76 0.77 0.79 0.81 ## q[581] 0.75 0.00 0.02 0.71 0.74 0.75 0.77 0.79 ## q[582] 0.83 0.00 0.01 0.81 0.82 0.83 0.84 0.86 ## q[583] 0.83 0.00 0.01 0.81 0.82 0.83 0.84 0.86 ## q[584] 0.77 0.00 0.02 0.74 0.76 0.77 0.79 0.81 ## q[585] 0.83 0.00 0.01 0.81 0.82 0.83 0.84 0.86 ## q[586] 0.77 0.00 0.02 0.74 0.76 0.77 0.79 0.81 ## q[587] 0.83 0.00 0.01 0.81 0.82 0.83 0.84 0.86 ## q[588] 0.77 0.00 0.02 0.74 0.76 0.77 0.79 0.81 ## q[589] 0.83 0.00 0.01 0.81 0.82 0.83 0.84 0.86 ## q[590] 0.83 0.00 0.01 0.81 0.82 0.83 0.84 0.86 ## q[591] 0.77 0.00 0.02 0.74 0.76 0.77 0.79 0.81 ## q[592] 0.83 0.00 0.01 0.81 0.82 0.83 0.84 0.86 ## q[593] 0.83 0.00 0.01 0.81 0.82 0.83 0.84 0.86 ## q[594] 0.75 0.00 0.02 0.71 0.74 0.75 0.77 0.79 ## q[595] 0.75 0.00 0.02 0.71 0.74 0.75 0.77 0.79 ## q[596] 0.77 0.00 0.02 0.74 0.76 0.77 0.79 0.81 ## q[597] 0.77 0.00 0.02 0.74 0.76 0.77 0.79 0.81 ## q[598] 0.83 0.00 0.01 0.81 0.82 0.83 0.84 0.86 ## q[599] 0.75 0.00 0.02 0.71 0.74 0.75 0.77 0.79 ## q[600] 0.83 0.00 0.01 0.81 0.82 0.83 0.84 0.86 ## q[601] 0.83 0.00 0.01 0.81 0.82 0.83 0.84 0.86 ## q[602] 0.83 0.00 0.01 0.81 0.82 0.83 0.84 0.86 ## q[603] 0.77 0.00 0.02 0.74 0.76 0.77 0.79 0.81 ## q[604] 0.75 0.00 0.02 0.71 0.74 0.75 0.77 0.79 ## q[605] 0.77 0.00 0.02 0.74 0.76 0.77 0.79 0.81 ## q[606] 0.77 0.00 0.02 0.74 0.76 0.77 0.79 0.81 ## q[607] 0.83 0.00 0.01 0.81 0.82 0.83 0.84 0.86 ## q[608] 0.83 0.00 0.01 0.81 0.82 0.83 0.84 0.86 ## q[609] 0.77 0.00 0.02 0.74 0.76 0.77 0.79 0.81 ## q[610] 0.75 0.00 0.02 0.71 0.74 0.75 0.77 0.79 ## q[611] 0.77 0.00 0.02 0.74 0.76 0.77 0.79 0.81 ## q[612] 0.83 0.00 0.01 0.81 0.82 0.83 0.84 0.86 ## q[613] 0.83 0.00 0.01 0.81 0.82 0.83 0.84 0.86 ## q[614] 0.75 0.00 0.02 0.71 0.74 0.75 0.77 0.79 ## q[615] 0.75 0.00 0.02 0.71 0.74 0.75 0.77 0.79 ## q[616] 0.60 0.00 0.02 0.56 0.59 0.60 0.62 0.65 ## q[617] 0.69 0.00 0.02 0.66 0.68 0.69 0.70 0.72 ## q[618] 0.69 0.00 0.02 0.66 0.68 0.69 0.70 0.72 ## q[619] 0.60 0.00 0.02 0.56 0.59 0.60 0.62 0.65 ## q[620] 0.60 0.00 0.02 0.56 0.59 0.60 0.62 0.65 ## q[621] 0.57 0.00 0.03 0.52 0.56 0.57 0.59 0.63 ## q[622] 0.60 0.00 0.02 0.56 0.59 0.60 0.62 0.65 ## q[623] 0.57 0.00 0.03 0.52 0.56 0.57 0.59 0.63 ## q[624] 0.69 0.00 0.02 0.66 0.68 0.69 0.70 0.72 ## q[625] 0.69 0.00 0.02 0.66 0.68 0.69 0.70 0.72 ## q[626] 0.69 0.00 0.02 0.66 0.68 0.69 0.70 0.72 ## q[627] 0.69 0.00 0.02 0.66 0.68 0.69 0.70 0.72 ## q[628] 0.57 0.00 0.03 0.52 0.56 0.57 0.59 0.63 ## q[629] 0.69 0.00 0.02 0.66 0.68 0.69 0.70 0.72 ## q[630] 0.57 0.00 0.03 0.52 0.56 0.57 0.59 0.63 ## q[631] 0.69 0.00 0.02 0.66 0.68 0.69 0.70 0.72 ## q[632] 0.69 0.00 0.02 0.66 0.68 0.69 0.70 0.72 ## q[633] 0.69 0.00 0.02 0.66 0.68 0.69 0.70 0.72 ## q[634] 0.69 0.00 0.02 0.66 0.68 0.69 0.70 0.72 ## q[635] 0.69 0.00 0.02 0.66 0.68 0.69 0.70 0.72 ## q[636] 0.69 0.00 0.02 0.66 0.68 0.69 0.70 0.72 ## q[637] 0.60 0.00 0.02 0.56 0.59 0.60 0.62 0.65 ## q[638] 0.60 0.00 0.02 0.56 0.59 0.60 0.62 0.65 ## q[639] 0.69 0.00 0.02 0.66 0.68 0.69 0.70 0.72 ## q[640] 0.69 0.00 0.02 0.66 0.68 0.69 0.70 0.72 ## q[641] 0.69 0.00 0.02 0.66 0.68 0.69 0.70 0.72 ## q[642] 0.60 0.00 0.02 0.56 0.59 0.60 0.62 0.65 ## q[643] 0.57 0.00 0.03 0.52 0.56 0.57 0.59 0.63 ## q[644] 0.69 0.00 0.02 0.66 0.68 0.69 0.70 0.72 ## q[645] 0.69 0.00 0.02 0.66 0.68 0.69 0.70 0.72 ## q[646] 0.57 0.00 0.03 0.52 0.56 0.57 0.59 0.63 ## q[647] 0.69 0.00 0.02 0.66 0.68 0.69 0.70 0.72 ## q[648] 0.57 0.00 0.03 0.52 0.56 0.57 0.59 0.63 ## q[649] 0.69 0.00 0.02 0.66 0.68 0.69 0.70 0.72 ## q[650] 0.60 0.00 0.02 0.56 0.59 0.60 0.62 0.65 ## q[651] 0.69 0.00 0.02 0.66 0.68 0.69 0.70 0.72 ## q[652] 0.57 0.00 0.03 0.52 0.56 0.57 0.59 0.63 ## q[653] 0.60 0.00 0.02 0.56 0.59 0.60 0.62 0.65 ## q[654] 0.57 0.00 0.03 0.52 0.56 0.57 0.59 0.63 ## q[655] 0.69 0.00 0.02 0.66 0.68 0.69 0.70 0.72 ## q[656] 0.69 0.00 0.02 0.66 0.68 0.69 0.70 0.72 ## q[657] 0.60 0.00 0.02 0.56 0.59 0.60 0.62 0.65 ## q[658] 0.57 0.00 0.03 0.52 0.56 0.57 0.59 0.63 ## q[659] 0.60 0.00 0.02 0.56 0.59 0.60 0.62 0.65 ## q[660] 0.60 0.00 0.02 0.56 0.59 0.60 0.62 0.65 ## q[661] 0.69 0.00 0.02 0.66 0.68 0.69 0.70 0.72 ## q[662] 0.69 0.00 0.02 0.66 0.68 0.69 0.70 0.72 ## q[663] 0.60 0.00 0.02 0.56 0.59 0.60 0.62 0.65 ## q[664] 0.60 0.00 0.02 0.56 0.59 0.60 0.62 0.65 ## q[665] 0.60 0.00 0.02 0.56 0.59 0.60 0.62 0.65 ## q[666] 0.69 0.00 0.02 0.66 0.68 0.69 0.70 0.72 ## q[667] 0.57 0.00 0.03 0.52 0.56 0.57 0.59 0.63 ## q[668] 0.57 0.00 0.03 0.52 0.56 0.57 0.59 0.63 ## q[669] 0.73 0.00 0.02 0.69 0.71 0.73 0.74 0.76 ## q[670] 0.80 0.00 0.01 0.77 0.79 0.80 0.80 0.82 ## q[671] 0.73 0.00 0.02 0.69 0.71 0.73 0.74 0.76 ## q[672] 0.70 0.00 0.02 0.65 0.69 0.70 0.72 0.75 ## q[673] 0.70 0.00 0.02 0.65 0.69 0.70 0.72 0.75 ## q[674] 0.70 0.00 0.02 0.65 0.69 0.70 0.72 0.75 ## q[675] 0.80 0.00 0.01 0.77 0.79 0.80 0.80 0.82 ## q[676] 0.80 0.00 0.01 0.77 0.79 0.80 0.80 0.82 ## q[677] 0.80 0.00 0.01 0.77 0.79 0.80 0.80 0.82 ## q[678] 0.80 0.00 0.01 0.77 0.79 0.80 0.80 0.82 ## q[679] 0.80 0.00 0.01 0.77 0.79 0.80 0.80 0.82 ## q[680] 0.80 0.00 0.01 0.77 0.79 0.80 0.80 0.82 ## q[681] 0.73 0.00 0.02 0.69 0.71 0.73 0.74 0.76 ## q[682] 0.80 0.00 0.01 0.77 0.79 0.80 0.80 0.82 ## q[683] 0.73 0.00 0.02 0.69 0.71 0.73 0.74 0.76 ## q[684] 0.80 0.00 0.01 0.77 0.79 0.80 0.80 0.82 ## q[685] 0.80 0.00 0.01 0.77 0.79 0.80 0.80 0.82 ## q[686] 0.80 0.00 0.01 0.77 0.79 0.80 0.80 0.82 ## q[687] 0.73 0.00 0.02 0.69 0.71 0.73 0.74 0.76 ## q[688] 0.73 0.00 0.02 0.69 0.71 0.73 0.74 0.76 ## q[689] 0.73 0.00 0.02 0.69 0.71 0.73 0.74 0.76 ## q[690] 0.80 0.00 0.01 0.77 0.79 0.80 0.80 0.82 ## q[691] 0.80 0.00 0.01 0.77 0.79 0.80 0.80 0.82 ## q[692] 0.70 0.00 0.02 0.65 0.69 0.70 0.72 0.75 ## q[693] 0.70 0.00 0.02 0.65 0.69 0.70 0.72 0.75 ## q[694] 0.80 0.00 0.01 0.77 0.79 0.80 0.80 0.82 ## q[695] 0.80 0.00 0.01 0.77 0.79 0.80 0.80 0.82 ## q[696] 0.80 0.00 0.01 0.77 0.79 0.80 0.80 0.82 ## q[697] 0.80 0.00 0.01 0.77 0.79 0.80 0.80 0.82 ## q[698] 0.73 0.00 0.02 0.69 0.71 0.73 0.74 0.76 ## q[699] 0.70 0.00 0.02 0.65 0.69 0.70 0.72 0.75 ## q[700] 0.80 0.00 0.01 0.77 0.79 0.80 0.80 0.82 ## q[701] 0.80 0.00 0.01 0.77 0.79 0.80 0.80 0.82 ## q[702] 0.70 0.00 0.02 0.65 0.69 0.70 0.72 0.75 ## q[703] 0.70 0.00 0.02 0.65 0.69 0.70 0.72 0.75 ## q[704] 0.73 0.00 0.02 0.69 0.71 0.73 0.74 0.76 ## q[705] 0.80 0.00 0.01 0.77 0.79 0.80 0.80 0.82 ## q[706] 0.73 0.00 0.02 0.69 0.71 0.73 0.74 0.76 ## q[707] 0.80 0.00 0.01 0.77 0.79 0.80 0.80 0.82 ## q[708] 0.70 0.00 0.02 0.65 0.69 0.70 0.72 0.75 ## q[709] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.82 ## q[710] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.82 ## q[711] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.82 ## q[712] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.82 ## q[713] 0.72 0.00 0.02 0.69 0.71 0.72 0.73 0.76 ## q[714] 0.72 0.00 0.02 0.69 0.71 0.72 0.73 0.76 ## q[715] 0.70 0.00 0.02 0.65 0.68 0.70 0.72 0.74 ## q[716] 0.72 0.00 0.02 0.69 0.71 0.72 0.73 0.76 ## q[717] 0.72 0.00 0.02 0.69 0.71 0.72 0.73 0.76 ## q[718] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.82 ## q[719] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.82 ## q[720] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.82 ## q[721] 0.72 0.00 0.02 0.69 0.71 0.72 0.73 0.76 ## q[722] 0.70 0.00 0.02 0.65 0.68 0.70 0.72 0.74 ## q[723] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.82 ## q[724] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.82 ## q[725] 0.72 0.00 0.02 0.69 0.71 0.72 0.73 0.76 ## q[726] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.82 ## q[727] 0.72 0.00 0.02 0.69 0.71 0.72 0.73 0.76 ## q[728] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.82 ## q[729] 0.72 0.00 0.02 0.69 0.71 0.72 0.73 0.76 ## q[730] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.82 ## q[731] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.82 ## q[732] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.82 ## q[733] 0.72 0.00 0.02 0.69 0.71 0.72 0.73 0.76 ## q[734] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.82 ## q[735] 0.70 0.00 0.02 0.65 0.68 0.70 0.72 0.74 ## q[736] 0.72 0.00 0.02 0.69 0.71 0.72 0.73 0.76 ## q[737] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.82 ## q[738] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.82 ## q[739] 0.72 0.00 0.02 0.69 0.71 0.72 0.73 0.76 ## q[740] 0.70 0.00 0.02 0.65 0.68 0.70 0.72 0.74 ## q[741] 0.70 0.00 0.02 0.65 0.68 0.70 0.72 0.74 ## q[742] 0.72 0.00 0.02 0.69 0.71 0.72 0.73 0.76 ## q[743] 0.72 0.00 0.02 0.69 0.71 0.72 0.73 0.76 ## q[744] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.82 ## q[745] 0.70 0.00 0.02 0.65 0.68 0.70 0.72 0.74 ## q[746] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.82 ## q[747] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.82 ## q[748] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.82 ## q[749] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.82 ## q[750] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.82 ## q[751] 0.72 0.00 0.02 0.69 0.71 0.72 0.73 0.76 ## q[752] 0.70 0.00 0.02 0.65 0.68 0.70 0.72 0.74 ## q[753] 0.72 0.00 0.02 0.69 0.71 0.72 0.73 0.76 ## q[754] 0.72 0.00 0.02 0.69 0.71 0.72 0.73 0.76 ## q[755] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.82 ## q[756] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.82 ## q[757] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.82 ## q[758] 0.72 0.00 0.02 0.69 0.71 0.72 0.73 0.76 ## q[759] 0.70 0.00 0.02 0.65 0.68 0.70 0.72 0.74 ## q[760] 0.72 0.00 0.02 0.69 0.71 0.72 0.73 0.76 ## q[761] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.82 ## q[762] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.82 ## q[763] 0.70 0.00 0.02 0.65 0.68 0.70 0.72 0.74 ## q[764] 0.70 0.00 0.02 0.65 0.68 0.70 0.72 0.74 ## q[765] 0.56 0.00 0.03 0.51 0.54 0.56 0.57 0.61 ## q[766] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[767] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[768] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[769] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[770] 0.56 0.00 0.03 0.51 0.54 0.56 0.57 0.61 ## q[771] 0.56 0.00 0.03 0.51 0.54 0.56 0.57 0.61 ## q[772] 0.56 0.00 0.03 0.51 0.54 0.56 0.57 0.61 ## q[773] 0.56 0.00 0.03 0.51 0.54 0.56 0.57 0.61 ## q[774] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.59 ## q[775] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[776] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[777] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[778] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.59 ## q[779] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[780] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[781] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[782] 0.56 0.00 0.03 0.51 0.54 0.56 0.57 0.61 ## q[783] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[784] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[785] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[786] 0.56 0.00 0.03 0.51 0.54 0.56 0.57 0.61 ## q[787] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[788] 0.56 0.00 0.03 0.51 0.54 0.56 0.57 0.61 ## q[789] 0.56 0.00 0.03 0.51 0.54 0.56 0.57 0.61 ## q[790] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[791] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[792] 0.56 0.00 0.03 0.51 0.54 0.56 0.57 0.61 ## q[793] 0.56 0.00 0.03 0.51 0.54 0.56 0.57 0.61 ## q[794] 0.56 0.00 0.03 0.51 0.54 0.56 0.57 0.61 ## q[795] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[796] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.59 ## q[797] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.59 ## q[798] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[799] 0.56 0.00 0.03 0.51 0.54 0.56 0.57 0.61 ## q[800] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[801] 0.56 0.00 0.03 0.51 0.54 0.56 0.57 0.61 ## q[802] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[803] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[804] 0.56 0.00 0.03 0.51 0.54 0.56 0.57 0.61 ## q[805] 0.56 0.00 0.03 0.51 0.54 0.56 0.57 0.61 ## q[806] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[807] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.59 ## q[808] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[809] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[810] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[811] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[812] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[813] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[814] 0.56 0.00 0.03 0.51 0.54 0.56 0.57 0.61 ## q[815] 0.56 0.00 0.03 0.51 0.54 0.56 0.57 0.61 ## q[816] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.59 ## q[817] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[818] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[819] 0.56 0.00 0.03 0.51 0.54 0.56 0.57 0.61 ## q[820] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.59 ## q[821] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[822] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[823] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[824] 0.80 0.00 0.01 0.77 0.79 0.80 0.81 0.83 ## q[825] 0.80 0.00 0.01 0.77 0.79 0.80 0.81 0.83 ## q[826] 0.80 0.00 0.01 0.77 0.79 0.80 0.81 0.83 ## q[827] 0.73 0.00 0.02 0.70 0.72 0.73 0.74 0.77 ## q[828] 0.73 0.00 0.02 0.70 0.72 0.73 0.74 0.77 ## q[829] 0.73 0.00 0.02 0.70 0.72 0.73 0.74 0.77 ## q[830] 0.80 0.00 0.01 0.77 0.79 0.80 0.81 0.83 ## q[831] 0.80 0.00 0.01 0.77 0.79 0.80 0.81 0.83 ## q[832] 0.80 0.00 0.01 0.77 0.79 0.80 0.81 0.83 ## q[833] 0.73 0.00 0.02 0.70 0.72 0.73 0.74 0.77 ## q[834] 0.71 0.00 0.02 0.66 0.69 0.71 0.72 0.75 ## q[835] 0.80 0.00 0.01 0.77 0.79 0.80 0.81 0.83 ## q[836] 0.73 0.00 0.02 0.70 0.72 0.73 0.74 0.77 ## q[837] 0.80 0.00 0.01 0.77 0.79 0.80 0.81 0.83 ## q[838] 0.73 0.00 0.02 0.70 0.72 0.73 0.74 0.77 ## q[839] 0.80 0.00 0.01 0.77 0.79 0.80 0.81 0.83 ## q[840] 0.73 0.00 0.02 0.70 0.72 0.73 0.74 0.77 ## q[841] 0.80 0.00 0.01 0.77 0.79 0.80 0.81 0.83 ## q[842] 0.71 0.00 0.02 0.66 0.69 0.71 0.72 0.75 ## q[843] 0.73 0.00 0.02 0.70 0.72 0.73 0.74 0.77 ## q[844] 0.80 0.00 0.01 0.77 0.79 0.80 0.81 0.83 ## q[845] 0.73 0.00 0.02 0.70 0.72 0.73 0.74 0.77 ## q[846] 0.73 0.00 0.02 0.70 0.72 0.73 0.74 0.77 ## q[847] 0.80 0.00 0.01 0.77 0.79 0.80 0.81 0.83 ## q[848] 0.80 0.00 0.01 0.77 0.79 0.80 0.81 0.83 ## q[849] 0.80 0.00 0.01 0.77 0.79 0.80 0.81 0.83 ## q[850] 0.80 0.00 0.01 0.77 0.79 0.80 0.81 0.83 ## q[851] 0.80 0.00 0.01 0.77 0.79 0.80 0.81 0.83 ## q[852] 0.80 0.00 0.01 0.77 0.79 0.80 0.81 0.83 ## q[853] 0.73 0.00 0.02 0.70 0.72 0.73 0.74 0.77 ## q[854] 0.73 0.00 0.02 0.70 0.72 0.73 0.74 0.77 ## q[855] 0.80 0.00 0.01 0.77 0.79 0.80 0.81 0.83 ## q[856] 0.80 0.00 0.01 0.77 0.79 0.80 0.81 0.83 ## q[857] 0.71 0.00 0.02 0.66 0.69 0.71 0.72 0.75 ## q[858] 0.73 0.00 0.02 0.70 0.72 0.73 0.74 0.77 ## q[859] 0.80 0.00 0.01 0.77 0.79 0.80 0.81 0.83 ## q[860] 0.80 0.00 0.01 0.77 0.79 0.80 0.81 0.83 ## q[861] 0.71 0.00 0.02 0.66 0.69 0.71 0.72 0.75 ## q[862] 0.67 0.00 0.02 0.62 0.65 0.67 0.68 0.72 ## q[863] 0.74 0.00 0.02 0.70 0.73 0.74 0.76 0.78 ## q[864] 0.74 0.00 0.02 0.70 0.73 0.74 0.76 0.78 ## q[865] 0.74 0.00 0.02 0.70 0.73 0.74 0.76 0.78 ## q[866] 0.74 0.00 0.02 0.70 0.73 0.74 0.76 0.78 ## q[867] 0.74 0.00 0.02 0.70 0.73 0.74 0.76 0.78 ## q[868] 0.67 0.00 0.02 0.62 0.65 0.67 0.68 0.72 ## q[869] 0.67 0.00 0.02 0.62 0.65 0.67 0.68 0.72 ## q[870] 0.67 0.00 0.02 0.62 0.65 0.67 0.68 0.72 ## q[871] 0.64 0.00 0.03 0.58 0.62 0.64 0.66 0.70 ## q[872] 0.74 0.00 0.02 0.70 0.73 0.74 0.76 0.78 ## q[873] 0.74 0.00 0.02 0.70 0.73 0.74 0.76 0.78 ## q[874] 0.74 0.00 0.02 0.70 0.73 0.74 0.76 0.78 ## q[875] 0.74 0.00 0.02 0.70 0.73 0.74 0.76 0.78 ## q[876] 0.74 0.00 0.02 0.70 0.73 0.74 0.76 0.78 ## q[877] 0.74 0.00 0.02 0.70 0.73 0.74 0.76 0.78 ## q[878] 0.64 0.00 0.03 0.58 0.62 0.64 0.66 0.70 ## q[879] 0.74 0.00 0.02 0.70 0.73 0.74 0.76 0.78 ## q[880] 0.74 0.00 0.02 0.70 0.73 0.74 0.76 0.78 ## q[881] 0.74 0.00 0.02 0.70 0.73 0.74 0.76 0.78 ## q[882] 0.67 0.00 0.02 0.62 0.65 0.67 0.68 0.72 ## q[883] 0.74 0.00 0.02 0.70 0.73 0.74 0.76 0.78 ## q[884] 0.74 0.00 0.02 0.70 0.73 0.74 0.76 0.78 ## q[885] 0.67 0.00 0.02 0.62 0.65 0.67 0.68 0.72 ## q[886] 0.67 0.00 0.02 0.62 0.65 0.67 0.68 0.72 ## q[887] 0.64 0.00 0.03 0.58 0.62 0.64 0.66 0.70 ## q[888] 0.74 0.00 0.02 0.70 0.73 0.74 0.76 0.78 ## q[889] 0.67 0.00 0.02 0.62 0.65 0.67 0.68 0.72 ## q[890] 0.74 0.00 0.02 0.70 0.73 0.74 0.76 0.78 ## q[891] 0.67 0.00 0.02 0.62 0.65 0.67 0.68 0.72 ## q[892] 0.74 0.00 0.02 0.70 0.73 0.74 0.76 0.78 ## q[893] 0.74 0.00 0.02 0.70 0.73 0.74 0.76 0.78 ## q[894] 0.74 0.00 0.02 0.70 0.73 0.74 0.76 0.78 ## q[895] 0.74 0.00 0.02 0.70 0.73 0.74 0.76 0.78 ## q[896] 0.74 0.00 0.02 0.70 0.73 0.74 0.76 0.78 ## q[897] 0.64 0.00 0.03 0.58 0.62 0.64 0.66 0.70 ## q[898] 0.74 0.00 0.02 0.70 0.73 0.74 0.76 0.78 ## q[899] 0.67 0.00 0.02 0.62 0.65 0.67 0.68 0.72 ## q[900] 0.74 0.00 0.02 0.70 0.73 0.74 0.76 0.78 ## q[901] 0.74 0.00 0.02 0.70 0.73 0.74 0.76 0.78 ## q[902] 0.67 0.00 0.02 0.62 0.65 0.67 0.68 0.72 ## q[903] 0.74 0.00 0.02 0.70 0.73 0.74 0.76 0.78 ## q[904] 0.74 0.00 0.02 0.70 0.73 0.74 0.76 0.78 ## q[905] 0.64 0.00 0.03 0.58 0.62 0.64 0.66 0.70 ## q[906] 0.74 0.00 0.02 0.70 0.73 0.74 0.76 0.78 ## q[907] 0.89 0.00 0.01 0.86 0.88 0.89 0.90 0.92 ## q[908] 0.83 0.00 0.02 0.78 0.81 0.83 0.85 0.87 ## q[909] 0.85 0.00 0.02 0.80 0.83 0.85 0.86 0.88 ## q[910] 0.85 0.00 0.02 0.80 0.83 0.85 0.86 0.88 ## q[911] 0.83 0.00 0.02 0.78 0.81 0.83 0.85 0.87 ## q[912] 0.89 0.00 0.01 0.86 0.88 0.89 0.90 0.92 ## q[913] 0.85 0.00 0.02 0.80 0.83 0.85 0.86 0.88 ## q[914] 0.89 0.00 0.01 0.86 0.88 0.89 0.90 0.92 ## q[915] 0.89 0.00 0.01 0.86 0.88 0.89 0.90 0.92 ## q[916] 0.85 0.00 0.02 0.80 0.83 0.85 0.86 0.88 ## q[917] 0.89 0.00 0.01 0.86 0.88 0.89 0.90 0.92 ## q[918] 0.85 0.00 0.02 0.80 0.83 0.85 0.86 0.88 ## q[919] 0.89 0.00 0.01 0.86 0.88 0.89 0.90 0.92 ## q[920] 0.89 0.00 0.01 0.86 0.88 0.89 0.90 0.92 ## q[921] 0.85 0.00 0.02 0.80 0.83 0.85 0.86 0.88 ## q[922] 0.85 0.00 0.02 0.80 0.83 0.85 0.86 0.88 ## q[923] 0.83 0.00 0.02 0.78 0.81 0.83 0.85 0.87 ## q[924] 0.85 0.00 0.02 0.80 0.83 0.85 0.86 0.88 ## q[925] 0.89 0.00 0.01 0.86 0.88 0.89 0.90 0.92 ## q[926] 0.85 0.00 0.02 0.80 0.83 0.85 0.86 0.88 ## q[927] 0.85 0.00 0.02 0.80 0.83 0.85 0.86 0.88 ## q[928] 0.89 0.00 0.01 0.86 0.88 0.89 0.90 0.92 ## q[929] 0.85 0.00 0.02 0.80 0.83 0.85 0.86 0.88 ## q[930] 0.83 0.00 0.02 0.78 0.81 0.83 0.85 0.87 ## q[931] 0.89 0.00 0.01 0.86 0.88 0.89 0.90 0.92 ## q[932] 0.83 0.00 0.02 0.78 0.81 0.83 0.85 0.87 ## q[933] 0.85 0.00 0.02 0.80 0.83 0.85 0.86 0.88 ## q[934] 0.89 0.00 0.01 0.86 0.88 0.89 0.90 0.92 ## q[935] 0.89 0.00 0.01 0.86 0.88 0.89 0.90 0.92 ## q[936] 0.85 0.00 0.02 0.80 0.83 0.85 0.86 0.88 ## q[937] 0.85 0.00 0.02 0.80 0.83 0.85 0.86 0.88 ## q[938] 0.89 0.00 0.01 0.86 0.88 0.89 0.90 0.92 ## q[939] 0.89 0.00 0.01 0.86 0.88 0.89 0.90 0.92 ## q[940] 0.89 0.00 0.01 0.86 0.88 0.89 0.90 0.92 ## q[941] 0.89 0.00 0.01 0.86 0.88 0.89 0.90 0.92 ## q[942] 0.89 0.00 0.01 0.86 0.88 0.89 0.90 0.92 ## q[943] 0.85 0.00 0.02 0.80 0.83 0.85 0.86 0.88 ## q[944] 0.89 0.00 0.01 0.86 0.88 0.89 0.90 0.92 ## q[945] 0.89 0.00 0.01 0.86 0.88 0.89 0.90 0.92 ## q[946] 0.83 0.00 0.02 0.78 0.81 0.83 0.85 0.87 ## q[947] 0.83 0.00 0.02 0.78 0.81 0.83 0.85 0.87 ## q[948] 0.89 0.00 0.01 0.86 0.88 0.89 0.90 0.92 ## q[949] 0.85 0.00 0.02 0.80 0.83 0.85 0.86 0.88 ## q[950] 0.89 0.00 0.01 0.86 0.88 0.89 0.90 0.92 ## q[951] 0.89 0.00 0.01 0.86 0.88 0.89 0.90 0.92 ## q[952] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[953] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[954] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[955] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[956] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[957] 0.69 0.00 0.02 0.64 0.67 0.69 0.71 0.73 ## q[958] 0.69 0.00 0.02 0.64 0.67 0.69 0.71 0.73 ## q[959] 0.66 0.00 0.03 0.60 0.64 0.66 0.68 0.72 ## q[960] 0.69 0.00 0.02 0.64 0.67 0.69 0.71 0.73 ## q[961] 0.69 0.00 0.02 0.64 0.67 0.69 0.71 0.73 ## q[962] 0.66 0.00 0.03 0.60 0.64 0.66 0.68 0.72 ## q[963] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[964] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[965] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[966] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[967] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[968] 0.66 0.00 0.03 0.60 0.64 0.66 0.68 0.72 ## q[969] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[970] 0.66 0.00 0.03 0.60 0.64 0.66 0.68 0.72 ## q[971] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[972] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[973] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[974] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[975] 0.69 0.00 0.02 0.64 0.67 0.69 0.71 0.73 ## q[976] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[977] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[978] 0.69 0.00 0.02 0.64 0.67 0.69 0.71 0.73 ## q[979] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[980] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[981] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[982] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[983] 0.66 0.00 0.03 0.60 0.64 0.66 0.68 0.72 ## q[984] 0.69 0.00 0.02 0.64 0.67 0.69 0.71 0.73 ## q[985] 0.66 0.00 0.03 0.60 0.64 0.66 0.68 0.72 ## q[986] 0.69 0.00 0.02 0.64 0.67 0.69 0.71 0.73 ## q[987] 0.66 0.00 0.03 0.60 0.64 0.66 0.68 0.72 ## q[988] 0.69 0.00 0.02 0.64 0.67 0.69 0.71 0.73 ## q[989] 0.69 0.00 0.02 0.64 0.67 0.69 0.71 0.73 ## q[990] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[991] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[992] 0.69 0.00 0.02 0.64 0.67 0.69 0.71 0.73 ## q[993] 0.66 0.00 0.03 0.60 0.64 0.66 0.68 0.72 ## q[994] 0.66 0.00 0.03 0.60 0.64 0.66 0.68 0.72 ## q[995] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[996] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[997] 0.66 0.00 0.03 0.60 0.64 0.66 0.68 0.72 ## q[998] 0.66 0.00 0.03 0.60 0.64 0.66 0.68 0.72 ## q[999] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[1000] 0.69 0.00 0.02 0.64 0.67 0.69 0.71 0.73 ## q[1001] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[1002] 0.66 0.00 0.03 0.60 0.64 0.66 0.68 0.72 ## q[1003] 0.69 0.00 0.02 0.64 0.67 0.69 0.71 0.73 ## q[1004] 0.69 0.00 0.02 0.64 0.67 0.69 0.71 0.73 ## q[1005] 0.66 0.00 0.03 0.60 0.64 0.66 0.68 0.72 ## q[1006] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[1007] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[1008] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[1009] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[1010] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[1011] 0.69 0.00 0.02 0.64 0.67 0.69 0.71 0.73 ## q[1012] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[1013] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[1014] 0.66 0.00 0.03 0.60 0.64 0.66 0.68 0.72 ## q[1015] 0.66 0.00 0.03 0.60 0.64 0.66 0.68 0.72 ## q[1016] 0.69 0.00 0.02 0.64 0.67 0.69 0.71 0.73 ## q[1017] 0.69 0.00 0.02 0.64 0.67 0.69 0.71 0.73 ## q[1018] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[1019] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[1020] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[1021] 0.69 0.00 0.02 0.64 0.67 0.69 0.71 0.73 ## q[1022] 0.69 0.00 0.02 0.64 0.67 0.69 0.71 0.73 ## q[1023] 0.69 0.00 0.02 0.64 0.67 0.69 0.71 0.73 ## q[1024] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[1025] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[1026] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[1027] 0.66 0.00 0.03 0.60 0.64 0.66 0.68 0.72 ## q[1028] 0.66 0.00 0.03 0.60 0.64 0.66 0.68 0.72 ## q[1029] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.58 ## q[1030] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[1031] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[1032] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[1033] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.58 ## q[1034] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.58 ## q[1035] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.58 ## q[1036] 0.50 0.00 0.03 0.43 0.47 0.50 0.52 0.56 ## q[1037] 0.50 0.00 0.03 0.43 0.47 0.50 0.52 0.56 ## q[1038] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[1039] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[1040] 0.50 0.00 0.03 0.43 0.47 0.50 0.52 0.56 ## q[1041] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[1042] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[1043] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[1044] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[1045] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.58 ## q[1046] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[1047] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[1048] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.58 ## q[1049] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[1050] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[1051] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[1052] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.58 ## q[1053] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[1054] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[1055] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.58 ## q[1056] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.58 ## q[1057] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[1058] 0.50 0.00 0.03 0.43 0.47 0.50 0.52 0.56 ## q[1059] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.58 ## q[1060] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.58 ## q[1061] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.58 ## q[1062] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[1063] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.58 ## q[1064] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.58 ## q[1065] 0.50 0.00 0.03 0.43 0.47 0.50 0.52 0.56 ## q[1066] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.58 ## q[1067] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[1068] 0.50 0.00 0.03 0.43 0.47 0.50 0.52 0.56 ## q[1069] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[1070] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[1071] 0.50 0.00 0.03 0.43 0.47 0.50 0.52 0.56 ## q[1072] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[1073] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[1074] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.58 ## q[1075] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[1076] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.58 ## q[1077] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[1078] 0.50 0.00 0.03 0.43 0.47 0.50 0.52 0.56 ## q[1079] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.58 ## q[1080] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[1081] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[1082] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[1083] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[1084] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[1085] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.58 ## q[1086] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[1087] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.58 ## q[1088] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[1089] 0.50 0.00 0.03 0.43 0.47 0.50 0.52 0.56 ## q[1090] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[1091] 0.50 0.00 0.03 0.43 0.47 0.50 0.52 0.56 ## q[1092] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.58 ## q[1093] 0.50 0.00 0.03 0.43 0.47 0.50 0.52 0.56 ## q[1094] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[1095] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[1096] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[1097] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[1098] 0.62 0.00 0.02 0.57 0.60 0.62 0.64 0.67 ## q[1099] 0.67 0.00 0.02 0.63 0.66 0.67 0.68 0.70 ## q[1100] 0.67 0.00 0.02 0.63 0.66 0.67 0.68 0.70 ## q[1101] 0.67 0.00 0.02 0.63 0.66 0.67 0.68 0.70 ## q[1102] 0.67 0.00 0.02 0.63 0.66 0.67 0.68 0.70 ## q[1103] 0.67 0.00 0.02 0.63 0.66 0.67 0.68 0.70 ## q[1104] 0.58 0.00 0.02 0.53 0.56 0.58 0.60 0.63 ## q[1105] 0.55 0.00 0.03 0.49 0.53 0.55 0.57 0.61 ## q[1106] 0.58 0.00 0.02 0.53 0.56 0.58 0.60 0.63 ## q[1107] 0.58 0.00 0.02 0.53 0.56 0.58 0.60 0.63 ## q[1108] 0.58 0.00 0.02 0.53 0.56 0.58 0.60 0.63 ## q[1109] 0.58 0.00 0.02 0.53 0.56 0.58 0.60 0.63 ## q[1110] 0.55 0.00 0.03 0.49 0.53 0.55 0.57 0.61 ## q[1111] 0.67 0.00 0.02 0.63 0.66 0.67 0.68 0.70 ## q[1112] 0.67 0.00 0.02 0.63 0.66 0.67 0.68 0.70 ## q[1113] 0.67 0.00 0.02 0.63 0.66 0.67 0.68 0.70 ## q[1114] 0.55 0.00 0.03 0.49 0.53 0.55 0.57 0.61 ## q[1115] 0.67 0.00 0.02 0.63 0.66 0.67 0.68 0.70 ## q[1116] 0.67 0.00 0.02 0.63 0.66 0.67 0.68 0.70 ## q[1117] 0.58 0.00 0.02 0.53 0.56 0.58 0.60 0.63 ## q[1118] 0.67 0.00 0.02 0.63 0.66 0.67 0.68 0.70 ## q[1119] 0.67 0.00 0.02 0.63 0.66 0.67 0.68 0.70 ## q[1120] 0.67 0.00 0.02 0.63 0.66 0.67 0.68 0.70 ## q[1121] 0.58 0.00 0.02 0.53 0.56 0.58 0.60 0.63 ## q[1122] 0.67 0.00 0.02 0.63 0.66 0.67 0.68 0.70 ## q[1123] 0.58 0.00 0.02 0.53 0.56 0.58 0.60 0.63 ## q[1124] 0.67 0.00 0.02 0.63 0.66 0.67 0.68 0.70 ## q[1125] 0.67 0.00 0.02 0.63 0.66 0.67 0.68 0.70 ## q[1126] 0.58 0.00 0.02 0.53 0.56 0.58 0.60 0.63 ## q[1127] 0.67 0.00 0.02 0.63 0.66 0.67 0.68 0.70 ## q[1128] 0.67 0.00 0.02 0.63 0.66 0.67 0.68 0.70 ## q[1129] 0.67 0.00 0.02 0.63 0.66 0.67 0.68 0.70 ## q[1130] 0.58 0.00 0.02 0.53 0.56 0.58 0.60 0.63 ## q[1131] 0.58 0.00 0.02 0.53 0.56 0.58 0.60 0.63 ## q[1132] 0.67 0.00 0.02 0.63 0.66 0.67 0.68 0.70 ## q[1133] 0.55 0.00 0.03 0.49 0.53 0.55 0.57 0.61 ## q[1134] 0.58 0.00 0.02 0.53 0.56 0.58 0.60 0.63 ## q[1135] 0.55 0.00 0.03 0.49 0.53 0.55 0.57 0.61 ## q[1136] 0.58 0.00 0.02 0.53 0.56 0.58 0.60 0.63 ## q[1137] 0.58 0.00 0.02 0.53 0.56 0.58 0.60 0.63 ## q[1138] 0.67 0.00 0.02 0.63 0.66 0.67 0.68 0.70 ## q[1139] 0.67 0.00 0.02 0.63 0.66 0.67 0.68 0.70 ## q[1140] 0.58 0.00 0.02 0.53 0.56 0.58 0.60 0.63 ## q[1141] 0.55 0.00 0.03 0.49 0.53 0.55 0.57 0.61 ## q[1142] 0.67 0.00 0.02 0.63 0.66 0.67 0.68 0.70 ## q[1143] 0.55 0.00 0.03 0.49 0.53 0.55 0.57 0.61 ## q[1144] 0.67 0.00 0.02 0.63 0.66 0.67 0.68 0.70 ## q[1145] 0.58 0.00 0.02 0.53 0.56 0.58 0.60 0.63 ## q[1146] 0.67 0.00 0.02 0.63 0.66 0.67 0.68 0.70 ## q[1147] 0.58 0.00 0.02 0.53 0.56 0.58 0.60 0.63 ## q[1148] 0.55 0.00 0.03 0.49 0.53 0.55 0.57 0.61 ## q[1149] 0.58 0.00 0.02 0.53 0.56 0.58 0.60 0.63 ## q[1150] 0.67 0.00 0.02 0.63 0.66 0.67 0.68 0.70 ## q[1151] 0.55 0.00 0.03 0.49 0.53 0.55 0.57 0.61 ## q[1152] 0.67 0.00 0.02 0.63 0.66 0.67 0.68 0.70 ## q[1153] 0.67 0.00 0.02 0.63 0.66 0.67 0.68 0.70 ## q[1154] 0.67 0.00 0.02 0.63 0.66 0.67 0.68 0.70 ## q[1155] 0.67 0.00 0.02 0.63 0.66 0.67 0.68 0.70 ## q[1156] 0.67 0.00 0.02 0.63 0.66 0.67 0.68 0.70 ## q[1157] 0.67 0.00 0.02 0.63 0.66 0.67 0.68 0.70 ## q[1158] 0.58 0.00 0.02 0.53 0.56 0.58 0.60 0.63 ## q[1159] 0.67 0.00 0.02 0.63 0.66 0.67 0.68 0.70 ## q[1160] 0.58 0.00 0.02 0.53 0.56 0.58 0.60 0.63 ## q[1161] 0.55 0.00 0.03 0.49 0.53 0.55 0.57 0.61 ## q[1162] 0.55 0.00 0.03 0.49 0.53 0.55 0.57 0.61 ## q[1163] 0.67 0.00 0.02 0.63 0.66 0.67 0.68 0.70 ## q[1164] 0.55 0.00 0.03 0.49 0.53 0.55 0.57 0.61 ## q[1165] 0.58 0.00 0.02 0.53 0.56 0.58 0.60 0.63 ## q[1166] 0.67 0.00 0.02 0.63 0.66 0.67 0.68 0.70 ## q[1167] 0.67 0.00 0.02 0.63 0.66 0.67 0.68 0.70 ## q[1168] 0.58 0.00 0.02 0.53 0.56 0.58 0.60 0.63 ## q[1169] 0.58 0.00 0.02 0.53 0.56 0.58 0.60 0.63 ## q[1170] 0.55 0.00 0.03 0.49 0.53 0.55 0.57 0.61 ## q[1171] 0.67 0.00 0.02 0.63 0.66 0.67 0.68 0.70 ## q[1172] 0.67 0.00 0.02 0.63 0.66 0.67 0.68 0.70 ## q[1173] 0.67 0.00 0.02 0.63 0.66 0.67 0.68 0.70 ## q[1174] 0.67 0.00 0.02 0.63 0.66 0.67 0.68 0.70 ## q[1175] 0.55 0.00 0.03 0.49 0.53 0.55 0.57 0.61 ## q[1176] 0.57 0.00 0.02 0.53 0.56 0.57 0.59 0.62 ## q[1177] 0.66 0.00 0.02 0.63 0.65 0.66 0.68 0.70 ## q[1178] 0.66 0.00 0.02 0.63 0.65 0.66 0.68 0.70 ## q[1179] 0.55 0.00 0.03 0.49 0.53 0.55 0.57 0.60 ## q[1180] 0.57 0.00 0.02 0.53 0.56 0.57 0.59 0.62 ## q[1181] 0.55 0.00 0.03 0.49 0.53 0.55 0.57 0.60 ## q[1182] 0.57 0.00 0.02 0.53 0.56 0.57 0.59 0.62 ## q[1183] 0.55 0.00 0.03 0.49 0.53 0.55 0.57 0.60 ## q[1184] 0.57 0.00 0.02 0.53 0.56 0.57 0.59 0.62 ## q[1185] 0.66 0.00 0.02 0.63 0.65 0.66 0.68 0.70 ## q[1186] 0.66 0.00 0.02 0.63 0.65 0.66 0.68 0.70 ## q[1187] 0.57 0.00 0.02 0.53 0.56 0.57 0.59 0.62 ## q[1188] 0.66 0.00 0.02 0.63 0.65 0.66 0.68 0.70 ## q[1189] 0.66 0.00 0.02 0.63 0.65 0.66 0.68 0.70 ## q[1190] 0.66 0.00 0.02 0.63 0.65 0.66 0.68 0.70 ## q[1191] 0.57 0.00 0.02 0.53 0.56 0.57 0.59 0.62 ## q[1192] 0.66 0.00 0.02 0.63 0.65 0.66 0.68 0.70 ## q[1193] 0.66 0.00 0.02 0.63 0.65 0.66 0.68 0.70 ## q[1194] 0.57 0.00 0.02 0.53 0.56 0.57 0.59 0.62 ## q[1195] 0.66 0.00 0.02 0.63 0.65 0.66 0.68 0.70 ## q[1196] 0.57 0.00 0.02 0.53 0.56 0.57 0.59 0.62 ## q[1197] 0.66 0.00 0.02 0.63 0.65 0.66 0.68 0.70 ## q[1198] 0.57 0.00 0.02 0.53 0.56 0.57 0.59 0.62 ## q[1199] 0.57 0.00 0.02 0.53 0.56 0.57 0.59 0.62 ## q[1200] 0.66 0.00 0.02 0.63 0.65 0.66 0.68 0.70 ## q[1201] 0.66 0.00 0.02 0.63 0.65 0.66 0.68 0.70 ## q[1202] 0.57 0.00 0.02 0.53 0.56 0.57 0.59 0.62 ## q[1203] 0.66 0.00 0.02 0.63 0.65 0.66 0.68 0.70 ## q[1204] 0.55 0.00 0.03 0.49 0.53 0.55 0.57 0.60 ## q[1205] 0.57 0.00 0.02 0.53 0.56 0.57 0.59 0.62 ## q[1206] 0.57 0.00 0.02 0.53 0.56 0.57 0.59 0.62 ## q[1207] 0.66 0.00 0.02 0.63 0.65 0.66 0.68 0.70 ## q[1208] 0.57 0.00 0.02 0.53 0.56 0.57 0.59 0.62 ## q[1209] 0.66 0.00 0.02 0.63 0.65 0.66 0.68 0.70 ## q[1210] 0.66 0.00 0.02 0.63 0.65 0.66 0.68 0.70 ## q[1211] 0.66 0.00 0.02 0.63 0.65 0.66 0.68 0.70 ## q[1212] 0.55 0.00 0.03 0.49 0.53 0.55 0.57 0.60 ## q[1213] 0.55 0.00 0.03 0.49 0.53 0.55 0.57 0.60 ## q[1214] 0.66 0.00 0.02 0.63 0.65 0.66 0.68 0.70 ## q[1215] 0.55 0.00 0.03 0.49 0.53 0.55 0.57 0.60 ## q[1216] 0.57 0.00 0.02 0.53 0.56 0.57 0.59 0.62 ## q[1217] 0.55 0.00 0.03 0.49 0.53 0.55 0.57 0.60 ## q[1218] 0.66 0.00 0.02 0.63 0.65 0.66 0.68 0.70 ## q[1219] 0.57 0.00 0.02 0.53 0.56 0.57 0.59 0.62 ## q[1220] 0.66 0.00 0.02 0.63 0.65 0.66 0.68 0.70 ## q[1221] 0.57 0.00 0.02 0.53 0.56 0.57 0.59 0.62 ## q[1222] 0.57 0.00 0.02 0.53 0.56 0.57 0.59 0.62 ## q[1223] 0.66 0.00 0.02 0.63 0.65 0.66 0.68 0.70 ## q[1224] 0.55 0.00 0.03 0.49 0.53 0.55 0.57 0.60 ## q[1225] 0.66 0.00 0.02 0.63 0.65 0.66 0.68 0.70 ## q[1226] 0.66 0.00 0.02 0.63 0.65 0.66 0.68 0.70 ## q[1227] 0.57 0.00 0.02 0.53 0.56 0.57 0.59 0.62 ## q[1228] 0.66 0.00 0.02 0.63 0.65 0.66 0.68 0.70 ## q[1229] 0.66 0.00 0.02 0.63 0.65 0.66 0.68 0.70 ## q[1230] 0.55 0.00 0.03 0.49 0.53 0.55 0.57 0.60 ## q[1231] 0.57 0.00 0.02 0.53 0.56 0.57 0.59 0.62 ## q[1232] 0.66 0.00 0.02 0.63 0.65 0.66 0.68 0.70 ## q[1233] 0.57 0.00 0.02 0.53 0.56 0.57 0.59 0.62 ## q[1234] 0.55 0.00 0.03 0.49 0.53 0.55 0.57 0.60 ## q[1235] 0.57 0.00 0.02 0.53 0.56 0.57 0.59 0.62 ## q[1236] 0.66 0.00 0.02 0.63 0.65 0.66 0.68 0.70 ## q[1237] 0.66 0.00 0.02 0.63 0.65 0.66 0.68 0.70 ## q[1238] 0.55 0.00 0.03 0.49 0.53 0.55 0.57 0.60 ## q[1239] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.77 ## q[1240] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.77 ## q[1241] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.77 ## q[1242] 0.63 0.00 0.03 0.58 0.61 0.63 0.65 0.69 ## q[1243] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.77 ## q[1244] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.77 ## q[1245] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.77 ## q[1246] 0.66 0.00 0.02 0.62 0.65 0.66 0.68 0.70 ## q[1247] 0.66 0.00 0.02 0.62 0.65 0.66 0.68 0.70 ## q[1248] 0.66 0.00 0.02 0.62 0.65 0.66 0.68 0.70 ## q[1249] 0.66 0.00 0.02 0.62 0.65 0.66 0.68 0.70 ## q[1250] 0.66 0.00 0.02 0.62 0.65 0.66 0.68 0.70 ## q[1251] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.77 ## q[1252] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.77 ## q[1253] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.77 ## q[1254] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.77 ## q[1255] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.77 ## q[1256] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.77 ## q[1257] 0.66 0.00 0.02 0.62 0.65 0.66 0.68 0.70 ## q[1258] 0.63 0.00 0.03 0.58 0.61 0.63 0.65 0.69 ## q[1259] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.77 ## q[1260] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.77 ## q[1261] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.77 ## q[1262] 0.66 0.00 0.02 0.62 0.65 0.66 0.68 0.70 ## q[1263] 0.66 0.00 0.02 0.62 0.65 0.66 0.68 0.70 ## q[1264] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.77 ## q[1265] 0.66 0.00 0.02 0.62 0.65 0.66 0.68 0.70 ## q[1266] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.77 ## q[1267] 0.66 0.00 0.02 0.62 0.65 0.66 0.68 0.70 ## q[1268] 0.63 0.00 0.03 0.58 0.61 0.63 0.65 0.69 ## q[1269] 0.66 0.00 0.02 0.62 0.65 0.66 0.68 0.70 ## q[1270] 0.66 0.00 0.02 0.62 0.65 0.66 0.68 0.70 ## q[1271] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.77 ## q[1272] 0.66 0.00 0.02 0.62 0.65 0.66 0.68 0.70 ## q[1273] 0.63 0.00 0.03 0.58 0.61 0.63 0.65 0.69 ## q[1274] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.77 ## q[1275] 0.66 0.00 0.02 0.62 0.65 0.66 0.68 0.70 ## q[1276] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.77 ## q[1277] 0.66 0.00 0.02 0.62 0.65 0.66 0.68 0.70 ## q[1278] 0.63 0.00 0.03 0.58 0.61 0.63 0.65 0.69 ## q[1279] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.77 ## q[1280] 0.63 0.00 0.03 0.58 0.61 0.63 0.65 0.69 ## q[1281] 0.66 0.00 0.02 0.62 0.65 0.66 0.68 0.70 ## q[1282] 0.66 0.00 0.02 0.62 0.65 0.66 0.68 0.70 ## q[1283] 0.66 0.00 0.02 0.62 0.65 0.66 0.68 0.70 ## q[1284] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.77 ## q[1285] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.77 ## q[1286] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.77 ## q[1287] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.77 ## q[1288] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.77 ## q[1289] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.77 ## q[1290] 0.66 0.00 0.02 0.62 0.65 0.66 0.68 0.70 ## q[1291] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.77 ## q[1292] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.77 ## q[1293] 0.63 0.00 0.03 0.58 0.61 0.63 0.65 0.69 ## q[1294] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.77 ## q[1295] 0.66 0.00 0.02 0.62 0.65 0.66 0.68 0.70 ## q[1296] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.77 ## q[1297] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.77 ## q[1298] 0.63 0.00 0.03 0.58 0.61 0.63 0.65 0.69 ## q[1299] 0.66 0.00 0.02 0.62 0.65 0.66 0.68 0.70 ## q[1300] 0.66 0.00 0.02 0.62 0.65 0.66 0.68 0.70 ## q[1301] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.77 ## q[1302] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.77 ## q[1303] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.77 ## q[1304] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.77 ## q[1305] 0.63 0.00 0.03 0.58 0.61 0.63 0.65 0.69 ## q[1306] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.77 ## q[1307] 0.69 0.00 0.02 0.65 0.68 0.69 0.70 0.72 ## q[1308] 0.69 0.00 0.02 0.65 0.68 0.69 0.70 0.72 ## q[1309] 0.69 0.00 0.02 0.65 0.68 0.69 0.70 0.72 ## q[1310] 0.60 0.00 0.02 0.56 0.59 0.60 0.62 0.64 ## q[1311] 0.60 0.00 0.02 0.56 0.59 0.60 0.62 0.64 ## q[1312] 0.60 0.00 0.02 0.56 0.59 0.60 0.62 0.64 ## q[1313] 0.60 0.00 0.02 0.56 0.59 0.60 0.62 0.64 ## q[1314] 0.69 0.00 0.02 0.65 0.68 0.69 0.70 0.72 ## q[1315] 0.69 0.00 0.02 0.65 0.68 0.69 0.70 0.72 ## q[1316] 0.69 0.00 0.02 0.65 0.68 0.69 0.70 0.72 ## q[1317] 0.69 0.00 0.02 0.65 0.68 0.69 0.70 0.72 ## q[1318] 0.69 0.00 0.02 0.65 0.68 0.69 0.70 0.72 ## q[1319] 0.57 0.00 0.03 0.51 0.55 0.57 0.59 0.63 ## q[1320] 0.60 0.00 0.02 0.56 0.59 0.60 0.62 0.64 ## q[1321] 0.57 0.00 0.03 0.51 0.55 0.57 0.59 0.63 ## q[1322] 0.69 0.00 0.02 0.65 0.68 0.69 0.70 0.72 ## q[1323] 0.69 0.00 0.02 0.65 0.68 0.69 0.70 0.72 ## q[1324] 0.60 0.00 0.02 0.56 0.59 0.60 0.62 0.64 ## q[1325] 0.69 0.00 0.02 0.65 0.68 0.69 0.70 0.72 ## q[1326] 0.60 0.00 0.02 0.56 0.59 0.60 0.62 0.64 ## q[1327] 0.69 0.00 0.02 0.65 0.68 0.69 0.70 0.72 ## q[1328] 0.60 0.00 0.02 0.56 0.59 0.60 0.62 0.64 ## q[1329] 0.69 0.00 0.02 0.65 0.68 0.69 0.70 0.72 ## q[1330] 0.57 0.00 0.03 0.51 0.55 0.57 0.59 0.63 ## q[1331] 0.57 0.00 0.03 0.51 0.55 0.57 0.59 0.63 ## q[1332] 0.60 0.00 0.02 0.56 0.59 0.60 0.62 0.64 ## q[1333] 0.69 0.00 0.02 0.65 0.68 0.69 0.70 0.72 ## q[1334] 0.60 0.00 0.02 0.56 0.59 0.60 0.62 0.64 ## q[1335] 0.69 0.00 0.02 0.65 0.68 0.69 0.70 0.72 ## q[1336] 0.60 0.00 0.02 0.56 0.59 0.60 0.62 0.64 ## q[1337] 0.60 0.00 0.02 0.56 0.59 0.60 0.62 0.64 ## q[1338] 0.57 0.00 0.03 0.51 0.55 0.57 0.59 0.63 ## q[1339] 0.69 0.00 0.02 0.65 0.68 0.69 0.70 0.72 ## q[1340] 0.69 0.00 0.02 0.65 0.68 0.69 0.70 0.72 ## q[1341] 0.69 0.00 0.02 0.65 0.68 0.69 0.70 0.72 ## q[1342] 0.69 0.00 0.02 0.65 0.68 0.69 0.70 0.72 ## q[1343] 0.69 0.00 0.02 0.65 0.68 0.69 0.70 0.72 ## q[1344] 0.69 0.00 0.02 0.65 0.68 0.69 0.70 0.72 ## q[1345] 0.69 0.00 0.02 0.65 0.68 0.69 0.70 0.72 ## q[1346] 0.60 0.00 0.02 0.56 0.59 0.60 0.62 0.64 ## q[1347] 0.60 0.00 0.02 0.56 0.59 0.60 0.62 0.64 ## q[1348] 0.60 0.00 0.02 0.56 0.59 0.60 0.62 0.64 ## q[1349] 0.69 0.00 0.02 0.65 0.68 0.69 0.70 0.72 ## q[1350] 0.69 0.00 0.02 0.65 0.68 0.69 0.70 0.72 ## q[1351] 0.60 0.00 0.02 0.56 0.59 0.60 0.62 0.64 ## q[1352] 0.57 0.00 0.03 0.51 0.55 0.57 0.59 0.63 ## q[1353] 0.60 0.00 0.02 0.56 0.59 0.60 0.62 0.64 ## q[1354] 0.69 0.00 0.02 0.65 0.68 0.69 0.70 0.72 ## q[1355] 0.69 0.00 0.02 0.65 0.68 0.69 0.70 0.72 ## q[1356] 0.57 0.00 0.03 0.51 0.55 0.57 0.59 0.63 ## q[1357] 0.71 0.00 0.02 0.68 0.70 0.71 0.72 0.74 ## q[1358] 0.71 0.00 0.02 0.68 0.70 0.71 0.72 0.74 ## q[1359] 0.71 0.00 0.02 0.68 0.70 0.71 0.72 0.74 ## q[1360] 0.71 0.00 0.02 0.68 0.70 0.71 0.72 0.74 ## q[1361] 0.71 0.00 0.02 0.68 0.70 0.71 0.72 0.74 ## q[1362] 0.63 0.00 0.02 0.58 0.61 0.63 0.64 0.67 ## q[1363] 0.60 0.00 0.03 0.54 0.58 0.60 0.62 0.65 ## q[1364] 0.63 0.00 0.02 0.58 0.61 0.63 0.64 0.67 ## q[1365] 0.63 0.00 0.02 0.58 0.61 0.63 0.64 0.67 ## q[1366] 0.60 0.00 0.03 0.54 0.58 0.60 0.62 0.65 ## q[1367] 0.71 0.00 0.02 0.68 0.70 0.71 0.72 0.74 ## q[1368] 0.71 0.00 0.02 0.68 0.70 0.71 0.72 0.74 ## q[1369] 0.71 0.00 0.02 0.68 0.70 0.71 0.72 0.74 ## q[1370] 0.71 0.00 0.02 0.68 0.70 0.71 0.72 0.74 ## q[1371] 0.71 0.00 0.02 0.68 0.70 0.71 0.72 0.74 ## q[1372] 0.71 0.00 0.02 0.68 0.70 0.71 0.72 0.74 ## q[1373] 0.63 0.00 0.02 0.58 0.61 0.63 0.64 0.67 ## q[1374] 0.71 0.00 0.02 0.68 0.70 0.71 0.72 0.74 ## q[1375] 0.71 0.00 0.02 0.68 0.70 0.71 0.72 0.74 ## q[1376] 0.71 0.00 0.02 0.68 0.70 0.71 0.72 0.74 ## q[1377] 0.71 0.00 0.02 0.68 0.70 0.71 0.72 0.74 ## q[1378] 0.63 0.00 0.02 0.58 0.61 0.63 0.64 0.67 ## q[1379] 0.60 0.00 0.03 0.54 0.58 0.60 0.62 0.65 ## q[1380] 0.63 0.00 0.02 0.58 0.61 0.63 0.64 0.67 ## q[1381] 0.63 0.00 0.02 0.58 0.61 0.63 0.64 0.67 ## q[1382] 0.63 0.00 0.02 0.58 0.61 0.63 0.64 0.67 ## q[1383] 0.71 0.00 0.02 0.68 0.70 0.71 0.72 0.74 ## q[1384] 0.71 0.00 0.02 0.68 0.70 0.71 0.72 0.74 ## q[1385] 0.63 0.00 0.02 0.58 0.61 0.63 0.64 0.67 ## q[1386] 0.60 0.00 0.03 0.54 0.58 0.60 0.62 0.65 ## q[1387] 0.60 0.00 0.03 0.54 0.58 0.60 0.62 0.65 ## q[1388] 0.71 0.00 0.02 0.68 0.70 0.71 0.72 0.74 ## q[1389] 0.71 0.00 0.02 0.68 0.70 0.71 0.72 0.74 ## q[1390] 0.63 0.00 0.02 0.58 0.61 0.63 0.64 0.67 ## q[1391] 0.60 0.00 0.03 0.54 0.58 0.60 0.62 0.65 ## q[1392] 0.71 0.00 0.02 0.68 0.70 0.71 0.72 0.74 ## q[1393] 0.71 0.00 0.02 0.68 0.70 0.71 0.72 0.74 ## q[1394] 0.71 0.00 0.02 0.68 0.70 0.71 0.72 0.74 ## q[1395] 0.71 0.00 0.02 0.68 0.70 0.71 0.72 0.74 ## q[1396] 0.60 0.00 0.03 0.54 0.58 0.60 0.62 0.65 ## q[1397] 0.60 0.00 0.03 0.54 0.58 0.60 0.62 0.65 ## q[1398] 0.71 0.00 0.02 0.68 0.70 0.71 0.72 0.74 ## q[1399] 0.60 0.00 0.03 0.54 0.58 0.60 0.62 0.65 ## q[1400] 0.63 0.00 0.02 0.58 0.61 0.63 0.64 0.67 ## q[1401] 0.71 0.00 0.02 0.68 0.70 0.71 0.72 0.74 ## q[1402] 0.71 0.00 0.02 0.68 0.70 0.71 0.72 0.74 ## q[1403] 0.63 0.00 0.02 0.58 0.61 0.63 0.64 0.67 ## q[1404] 0.71 0.00 0.02 0.68 0.70 0.71 0.72 0.74 ## q[1405] 0.71 0.00 0.02 0.68 0.70 0.71 0.72 0.74 ## q[1406] 0.71 0.00 0.02 0.68 0.70 0.71 0.72 0.74 ## q[1407] 0.60 0.00 0.03 0.54 0.58 0.60 0.62 0.65 ## q[1408] 0.72 0.00 0.02 0.67 0.70 0.72 0.73 0.76 ## q[1409] 0.81 0.00 0.01 0.78 0.80 0.81 0.82 0.83 ## q[1410] 0.81 0.00 0.01 0.78 0.80 0.81 0.82 0.83 ## q[1411] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.78 ## q[1412] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.78 ## q[1413] 0.72 0.00 0.02 0.67 0.70 0.72 0.73 0.76 ## q[1414] 0.81 0.00 0.01 0.78 0.80 0.81 0.82 0.83 ## q[1415] 0.81 0.00 0.01 0.78 0.80 0.81 0.82 0.83 ## q[1416] 0.81 0.00 0.01 0.78 0.80 0.81 0.82 0.83 ## q[1417] 0.81 0.00 0.01 0.78 0.80 0.81 0.82 0.83 ## q[1418] 0.81 0.00 0.01 0.78 0.80 0.81 0.82 0.83 ## q[1419] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.78 ## q[1420] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.78 ## q[1421] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.78 ## q[1422] 0.81 0.00 0.01 0.78 0.80 0.81 0.82 0.83 ## q[1423] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.78 ## q[1424] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.78 ## q[1425] 0.81 0.00 0.01 0.78 0.80 0.81 0.82 0.83 ## q[1426] 0.81 0.00 0.01 0.78 0.80 0.81 0.82 0.83 ## q[1427] 0.81 0.00 0.01 0.78 0.80 0.81 0.82 0.83 ## q[1428] 0.81 0.00 0.01 0.78 0.80 0.81 0.82 0.83 ## q[1429] 0.81 0.00 0.01 0.78 0.80 0.81 0.82 0.83 ## q[1430] 0.72 0.00 0.02 0.67 0.70 0.72 0.73 0.76 ## q[1431] 0.81 0.00 0.01 0.78 0.80 0.81 0.82 0.83 ## q[1432] 0.72 0.00 0.02 0.67 0.70 0.72 0.73 0.76 ## q[1433] 0.81 0.00 0.01 0.78 0.80 0.81 0.82 0.83 ## q[1434] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.78 ## q[1435] 0.81 0.00 0.01 0.78 0.80 0.81 0.82 0.83 ## q[1436] 0.81 0.00 0.01 0.78 0.80 0.81 0.82 0.83 ## q[1437] 0.81 0.00 0.01 0.78 0.80 0.81 0.82 0.83 ## q[1438] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.78 ## q[1439] 0.72 0.00 0.02 0.67 0.70 0.72 0.73 0.76 ## q[1440] 0.81 0.00 0.01 0.78 0.80 0.81 0.82 0.83 ## q[1441] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.78 ## q[1442] 0.81 0.00 0.01 0.78 0.80 0.81 0.82 0.83 ## q[1443] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.78 ## q[1444] 0.81 0.00 0.01 0.78 0.80 0.81 0.82 0.83 ## q[1445] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.78 ## q[1446] 0.81 0.00 0.01 0.78 0.80 0.81 0.82 0.83 ## q[1447] 0.81 0.00 0.01 0.78 0.80 0.81 0.82 0.83 ## q[1448] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.78 ## q[1449] 0.72 0.00 0.02 0.67 0.70 0.72 0.73 0.76 ## q[1450] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.78 ## q[1451] 0.81 0.00 0.01 0.78 0.80 0.81 0.82 0.83 ## q[1452] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.78 ## q[1453] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.78 ## q[1454] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.78 ## q[1455] 0.72 0.00 0.02 0.67 0.70 0.72 0.73 0.76 ## q[1456] 0.81 0.00 0.01 0.78 0.80 0.81 0.82 0.83 ## q[1457] 0.72 0.00 0.02 0.67 0.70 0.72 0.73 0.76 ## q[1458] 0.81 0.00 0.01 0.78 0.80 0.81 0.82 0.83 ## q[1459] 0.81 0.00 0.01 0.78 0.80 0.81 0.82 0.83 ## q[1460] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.78 ## q[1461] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.78 ## q[1462] 0.81 0.00 0.01 0.78 0.80 0.81 0.82 0.83 ## q[1463] 0.81 0.00 0.01 0.78 0.80 0.81 0.82 0.83 ## q[1464] 0.81 0.00 0.01 0.78 0.80 0.81 0.82 0.83 ## q[1465] 0.81 0.00 0.01 0.78 0.80 0.81 0.82 0.83 ## q[1466] 0.81 0.00 0.01 0.78 0.80 0.81 0.82 0.83 ## q[1467] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.78 ## q[1468] 0.81 0.00 0.01 0.78 0.80 0.81 0.82 0.83 ## q[1469] 0.72 0.00 0.02 0.67 0.70 0.72 0.73 0.76 ## q[1470] 0.72 0.00 0.02 0.67 0.70 0.72 0.73 0.76 ## q[1471] 0.81 0.00 0.01 0.78 0.80 0.81 0.82 0.83 ## q[1472] 0.81 0.00 0.01 0.78 0.80 0.81 0.82 0.83 ## q[1473] 0.81 0.00 0.01 0.78 0.80 0.81 0.82 0.83 ## q[1474] 0.86 0.00 0.01 0.84 0.85 0.86 0.87 0.89 ## q[1475] 0.81 0.00 0.02 0.77 0.80 0.81 0.83 0.85 ## q[1476] 0.79 0.00 0.02 0.75 0.78 0.79 0.81 0.83 ## q[1477] 0.86 0.00 0.01 0.84 0.85 0.86 0.87 0.89 ## q[1478] 0.86 0.00 0.01 0.84 0.85 0.86 0.87 0.89 ## q[1479] 0.81 0.00 0.02 0.77 0.80 0.81 0.83 0.85 ## q[1480] 0.86 0.00 0.01 0.84 0.85 0.86 0.87 0.89 ## q[1481] 0.86 0.00 0.01 0.84 0.85 0.86 0.87 0.89 ## q[1482] 0.86 0.00 0.01 0.84 0.85 0.86 0.87 0.89 ## q[1483] 0.79 0.00 0.02 0.75 0.78 0.79 0.81 0.83 ## q[1484] 0.79 0.00 0.02 0.75 0.78 0.79 0.81 0.83 ## q[1485] 0.81 0.00 0.02 0.77 0.80 0.81 0.83 0.85 ## q[1486] 0.79 0.00 0.02 0.75 0.78 0.79 0.81 0.83 ## q[1487] 0.79 0.00 0.02 0.75 0.78 0.79 0.81 0.83 ## q[1488] 0.81 0.00 0.02 0.77 0.80 0.81 0.83 0.85 ## q[1489] 0.86 0.00 0.01 0.84 0.85 0.86 0.87 0.89 ## q[1490] 0.81 0.00 0.02 0.77 0.80 0.81 0.83 0.85 ## q[1491] 0.79 0.00 0.02 0.75 0.78 0.79 0.81 0.83 ## q[1492] 0.73 0.00 0.02 0.69 0.72 0.73 0.74 0.77 ## q[1493] 0.80 0.00 0.01 0.77 0.79 0.80 0.81 0.82 ## q[1494] 0.80 0.00 0.01 0.77 0.79 0.80 0.81 0.82 ## q[1495] 0.73 0.00 0.02 0.69 0.72 0.73 0.74 0.77 ## q[1496] 0.73 0.00 0.02 0.69 0.72 0.73 0.74 0.77 ## q[1497] 0.71 0.00 0.02 0.66 0.69 0.71 0.72 0.75 ## q[1498] 0.80 0.00 0.01 0.77 0.79 0.80 0.81 0.82 ## q[1499] 0.80 0.00 0.01 0.77 0.79 0.80 0.81 0.82 ## q[1500] 0.80 0.00 0.01 0.77 0.79 0.80 0.81 0.82 ## q[1501] 0.80 0.00 0.01 0.77 0.79 0.80 0.81 0.82 ## q[1502] 0.73 0.00 0.02 0.69 0.72 0.73 0.74 0.77 ## q[1503] 0.71 0.00 0.02 0.66 0.69 0.71 0.72 0.75 ## q[1504] 0.80 0.00 0.01 0.77 0.79 0.80 0.81 0.82 ## q[1505] 0.80 0.00 0.01 0.77 0.79 0.80 0.81 0.82 ## q[1506] 0.73 0.00 0.02 0.69 0.72 0.73 0.74 0.77 ## q[1507] 0.80 0.00 0.01 0.77 0.79 0.80 0.81 0.82 ## q[1508] 0.80 0.00 0.01 0.77 0.79 0.80 0.81 0.82 ## q[1509] 0.73 0.00 0.02 0.69 0.72 0.73 0.74 0.77 ## q[1510] 0.80 0.00 0.01 0.77 0.79 0.80 0.81 0.82 ## q[1511] 0.73 0.00 0.02 0.69 0.72 0.73 0.74 0.77 ## q[1512] 0.73 0.00 0.02 0.69 0.72 0.73 0.74 0.77 ## q[1513] 0.80 0.00 0.01 0.77 0.79 0.80 0.81 0.82 ## q[1514] 0.71 0.00 0.02 0.66 0.69 0.71 0.72 0.75 ## q[1515] 0.73 0.00 0.02 0.69 0.72 0.73 0.74 0.77 ## q[1516] 0.73 0.00 0.02 0.69 0.72 0.73 0.74 0.77 ## q[1517] 0.80 0.00 0.01 0.77 0.79 0.80 0.81 0.82 ## q[1518] 0.73 0.00 0.02 0.69 0.72 0.73 0.74 0.77 ## q[1519] 0.80 0.00 0.01 0.77 0.79 0.80 0.81 0.82 ## q[1520] 0.71 0.00 0.02 0.66 0.69 0.71 0.72 0.75 ## q[1521] 0.80 0.00 0.01 0.77 0.79 0.80 0.81 0.82 ## q[1522] 0.71 0.00 0.02 0.66 0.69 0.71 0.72 0.75 ## q[1523] 0.80 0.00 0.01 0.77 0.79 0.80 0.81 0.82 ## q[1524] 0.73 0.00 0.02 0.69 0.72 0.73 0.74 0.77 ## q[1525] 0.80 0.00 0.01 0.77 0.79 0.80 0.81 0.82 ## q[1526] 0.73 0.00 0.02 0.69 0.72 0.73 0.74 0.77 ## q[1527] 0.80 0.00 0.01 0.77 0.79 0.80 0.81 0.82 ## q[1528] 0.80 0.00 0.01 0.77 0.79 0.80 0.81 0.82 ## q[1529] 0.80 0.00 0.01 0.77 0.79 0.80 0.81 0.82 ## q[1530] 0.80 0.00 0.01 0.77 0.79 0.80 0.81 0.82 ## q[1531] 0.73 0.00 0.02 0.69 0.72 0.73 0.74 0.77 ## q[1532] 0.80 0.00 0.01 0.77 0.79 0.80 0.81 0.82 ## q[1533] 0.73 0.00 0.02 0.69 0.72 0.73 0.74 0.77 ## q[1534] 0.80 0.00 0.01 0.77 0.79 0.80 0.81 0.82 ## q[1535] 0.71 0.00 0.02 0.66 0.69 0.71 0.72 0.75 ## q[1536] 0.73 0.00 0.02 0.69 0.72 0.73 0.74 0.77 ## q[1537] 0.80 0.00 0.01 0.77 0.79 0.80 0.81 0.82 ## q[1538] 0.80 0.00 0.01 0.77 0.79 0.80 0.81 0.82 ## q[1539] 0.80 0.00 0.01 0.77 0.79 0.80 0.81 0.82 ## q[1540] 0.71 0.00 0.02 0.66 0.69 0.71 0.72 0.75 ## q[1541] 0.78 0.00 0.02 0.74 0.77 0.78 0.79 0.81 ## q[1542] 0.78 0.00 0.02 0.74 0.77 0.78 0.79 0.81 ## q[1543] 0.78 0.00 0.02 0.74 0.77 0.78 0.79 0.81 ## q[1544] 0.68 0.00 0.03 0.62 0.66 0.68 0.70 0.74 ## q[1545] 0.71 0.00 0.03 0.66 0.69 0.71 0.72 0.75 ## q[1546] 0.71 0.00 0.03 0.66 0.69 0.71 0.72 0.75 ## q[1547] 0.68 0.00 0.03 0.62 0.66 0.68 0.70 0.74 ## q[1548] 0.71 0.00 0.03 0.66 0.69 0.71 0.72 0.75 ## q[1549] 0.71 0.00 0.03 0.66 0.69 0.71 0.72 0.75 ## q[1550] 0.68 0.00 0.03 0.62 0.66 0.68 0.70 0.74 ## q[1551] 0.78 0.00 0.02 0.74 0.77 0.78 0.79 0.81 ## q[1552] 0.78 0.00 0.02 0.74 0.77 0.78 0.79 0.81 ## q[1553] 0.78 0.00 0.02 0.74 0.77 0.78 0.79 0.81 ## q[1554] 0.71 0.00 0.03 0.66 0.69 0.71 0.72 0.75 ## q[1555] 0.68 0.00 0.03 0.62 0.66 0.68 0.70 0.74 ## q[1556] 0.78 0.00 0.02 0.74 0.77 0.78 0.79 0.81 ## q[1557] 0.78 0.00 0.02 0.74 0.77 0.78 0.79 0.81 ## q[1558] 0.78 0.00 0.02 0.74 0.77 0.78 0.79 0.81 ## q[1559] 0.78 0.00 0.02 0.74 0.77 0.78 0.79 0.81 ## q[1560] 0.71 0.00 0.03 0.66 0.69 0.71 0.72 0.75 ## q[1561] 0.78 0.00 0.02 0.74 0.77 0.78 0.79 0.81 ## q[1562] 0.71 0.00 0.03 0.66 0.69 0.71 0.72 0.75 ## q[1563] 0.78 0.00 0.02 0.74 0.77 0.78 0.79 0.81 ## q[1564] 0.71 0.00 0.03 0.66 0.69 0.71 0.72 0.75 ## q[1565] 0.78 0.00 0.02 0.74 0.77 0.78 0.79 0.81 ## q[1566] 0.78 0.00 0.02 0.74 0.77 0.78 0.79 0.81 ## q[1567] 0.71 0.00 0.03 0.66 0.69 0.71 0.72 0.75 ## q[1568] 0.78 0.00 0.02 0.74 0.77 0.78 0.79 0.81 ## q[1569] 0.71 0.00 0.03 0.66 0.69 0.71 0.72 0.75 ## q[1570] 0.78 0.00 0.02 0.74 0.77 0.78 0.79 0.81 ## q[1571] 0.68 0.00 0.03 0.62 0.66 0.68 0.70 0.74 ## q[1572] 0.71 0.00 0.03 0.66 0.69 0.71 0.72 0.75 ## q[1573] 0.78 0.00 0.02 0.74 0.77 0.78 0.79 0.81 ## q[1574] 0.71 0.00 0.03 0.66 0.69 0.71 0.72 0.75 ## q[1575] 0.71 0.00 0.03 0.66 0.69 0.71 0.72 0.75 ## q[1576] 0.78 0.00 0.02 0.74 0.77 0.78 0.79 0.81 ## q[1577] 0.71 0.00 0.03 0.66 0.69 0.71 0.72 0.75 ## q[1578] 0.78 0.00 0.02 0.74 0.77 0.78 0.79 0.81 ## q[1579] 0.68 0.00 0.03 0.62 0.66 0.68 0.70 0.74 ## q[1580] 0.68 0.00 0.03 0.62 0.66 0.68 0.70 0.74 ## q[1581] 0.78 0.00 0.02 0.74 0.77 0.78 0.79 0.81 ## q[1582] 0.68 0.00 0.03 0.62 0.66 0.68 0.70 0.74 ## q[1583] 0.71 0.00 0.03 0.66 0.69 0.71 0.72 0.75 ## q[1584] 0.68 0.00 0.03 0.62 0.66 0.68 0.70 0.74 ## q[1585] 0.78 0.00 0.02 0.74 0.77 0.78 0.79 0.81 ## q[1586] 0.78 0.00 0.02 0.74 0.77 0.78 0.79 0.81 ## q[1587] 0.71 0.00 0.03 0.66 0.69 0.71 0.72 0.75 ## q[1588] 0.71 0.00 0.03 0.66 0.69 0.71 0.72 0.75 ## q[1589] 0.71 0.00 0.03 0.66 0.69 0.71 0.72 0.75 ## q[1590] 0.78 0.00 0.02 0.74 0.77 0.78 0.79 0.81 ## q[1591] 0.68 0.00 0.03 0.62 0.66 0.68 0.70 0.74 ## q[1592] 0.78 0.00 0.02 0.74 0.77 0.78 0.79 0.81 ## q[1593] 0.78 0.00 0.02 0.74 0.77 0.78 0.79 0.81 ## q[1594] 0.78 0.00 0.02 0.74 0.77 0.78 0.79 0.81 ## q[1595] 0.78 0.00 0.02 0.74 0.77 0.78 0.79 0.81 ## q[1596] 0.78 0.00 0.02 0.74 0.77 0.78 0.79 0.81 ## q[1597] 0.71 0.00 0.03 0.66 0.69 0.71 0.72 0.75 ## q[1598] 0.78 0.00 0.02 0.74 0.77 0.78 0.79 0.81 ## q[1599] 0.78 0.00 0.02 0.74 0.77 0.78 0.79 0.81 ## q[1600] 0.68 0.00 0.03 0.62 0.66 0.68 0.70 0.74 ## q[1601] 0.68 0.00 0.03 0.62 0.66 0.68 0.70 0.74 ## q[1602] 0.71 0.00 0.03 0.66 0.69 0.71 0.72 0.75 ## q[1603] 0.71 0.00 0.03 0.66 0.69 0.71 0.72 0.75 ## q[1604] 0.78 0.00 0.02 0.74 0.77 0.78 0.79 0.81 ## q[1605] 0.78 0.00 0.02 0.74 0.77 0.78 0.79 0.81 ## q[1606] 0.71 0.00 0.03 0.66 0.69 0.71 0.72 0.75 ## q[1607] 0.68 0.00 0.03 0.62 0.66 0.68 0.70 0.74 ## q[1608] 0.71 0.00 0.03 0.66 0.69 0.71 0.72 0.75 ## q[1609] 0.78 0.00 0.02 0.74 0.77 0.78 0.79 0.81 ## q[1610] 0.71 0.00 0.03 0.66 0.69 0.71 0.72 0.75 ## q[1611] 0.78 0.00 0.02 0.74 0.77 0.78 0.79 0.81 ## q[1612] 0.78 0.00 0.02 0.74 0.77 0.78 0.79 0.81 ## q[1613] 0.78 0.00 0.02 0.74 0.77 0.78 0.79 0.81 ## q[1614] 0.68 0.00 0.03 0.62 0.66 0.68 0.70 0.74 ## q[1615] 0.68 0.00 0.03 0.62 0.66 0.68 0.70 0.74 ## q[1616] 0.59 0.00 0.03 0.53 0.57 0.59 0.61 0.65 ## q[1617] 0.59 0.00 0.03 0.53 0.57 0.59 0.61 0.65 ## q[1618] 0.50 0.00 0.03 0.44 0.47 0.50 0.52 0.56 ## q[1619] 0.59 0.00 0.03 0.53 0.57 0.59 0.61 0.65 ## q[1620] 0.59 0.00 0.03 0.53 0.57 0.59 0.61 0.65 ## q[1621] 0.47 0.00 0.04 0.40 0.44 0.47 0.49 0.54 ## q[1622] 0.50 0.00 0.03 0.44 0.47 0.50 0.52 0.56 ## q[1623] 0.59 0.00 0.03 0.53 0.57 0.59 0.61 0.65 ## q[1624] 0.50 0.00 0.03 0.44 0.47 0.50 0.52 0.56 ## q[1625] 0.59 0.00 0.03 0.53 0.57 0.59 0.61 0.65 ## q[1626] 0.59 0.00 0.03 0.53 0.57 0.59 0.61 0.65 ## q[1627] 0.59 0.00 0.03 0.53 0.57 0.59 0.61 0.65 ## q[1628] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[1629] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[1630] 0.62 0.00 0.03 0.57 0.60 0.62 0.64 0.67 ## q[1631] 0.73 0.00 0.02 0.70 0.72 0.73 0.74 0.76 ## q[1632] 0.73 0.00 0.02 0.70 0.72 0.73 0.74 0.76 ## q[1633] 0.62 0.00 0.03 0.57 0.60 0.62 0.64 0.67 ## q[1634] 0.73 0.00 0.02 0.70 0.72 0.73 0.74 0.76 ## q[1635] 0.73 0.00 0.02 0.70 0.72 0.73 0.74 0.76 ## q[1636] 0.73 0.00 0.02 0.70 0.72 0.73 0.74 0.76 ## q[1637] 0.73 0.00 0.02 0.70 0.72 0.73 0.74 0.76 ## q[1638] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[1639] 0.73 0.00 0.02 0.70 0.72 0.73 0.74 0.76 ## q[1640] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[1641] 0.62 0.00 0.03 0.57 0.60 0.62 0.64 0.67 ## q[1642] 0.73 0.00 0.02 0.70 0.72 0.73 0.74 0.76 ## q[1643] 0.73 0.00 0.02 0.70 0.72 0.73 0.74 0.76 ## q[1644] 0.73 0.00 0.02 0.70 0.72 0.73 0.74 0.76 ## q[1645] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[1646] 0.73 0.00 0.02 0.70 0.72 0.73 0.74 0.76 ## q[1647] 0.62 0.00 0.03 0.57 0.60 0.62 0.64 0.67 ## q[1648] 0.73 0.00 0.02 0.70 0.72 0.73 0.74 0.76 ## q[1649] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[1650] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[1651] 0.62 0.00 0.02 0.57 0.60 0.62 0.63 0.66 ## q[1652] 0.62 0.00 0.02 0.57 0.60 0.62 0.63 0.66 ## q[1653] 0.59 0.00 0.03 0.53 0.57 0.59 0.61 0.64 ## q[1654] 0.70 0.00 0.02 0.67 0.69 0.70 0.71 0.73 ## q[1655] 0.70 0.00 0.02 0.67 0.69 0.70 0.71 0.73 ## q[1656] 0.59 0.00 0.03 0.53 0.57 0.59 0.61 0.64 ## q[1657] 0.62 0.00 0.02 0.57 0.60 0.62 0.63 0.66 ## q[1658] 0.70 0.00 0.02 0.67 0.69 0.70 0.71 0.73 ## q[1659] 0.70 0.00 0.02 0.67 0.69 0.70 0.71 0.73 ## q[1660] 0.62 0.00 0.02 0.57 0.60 0.62 0.63 0.66 ## q[1661] 0.70 0.00 0.02 0.67 0.69 0.70 0.71 0.73 ## q[1662] 0.70 0.00 0.02 0.67 0.69 0.70 0.71 0.73 ## q[1663] 0.62 0.00 0.02 0.57 0.60 0.62 0.63 0.66 ## q[1664] 0.62 0.00 0.02 0.57 0.60 0.62 0.63 0.66 ## q[1665] 0.70 0.00 0.02 0.67 0.69 0.70 0.71 0.73 ## q[1666] 0.62 0.00 0.02 0.57 0.60 0.62 0.63 0.66 ## q[1667] 0.59 0.00 0.03 0.53 0.57 0.59 0.61 0.64 ## q[1668] 0.62 0.00 0.02 0.57 0.60 0.62 0.63 0.66 ## q[1669] 0.70 0.00 0.02 0.67 0.69 0.70 0.71 0.73 ## q[1670] 0.70 0.00 0.02 0.67 0.69 0.70 0.71 0.73 ## q[1671] 0.62 0.00 0.02 0.57 0.60 0.62 0.63 0.66 ## q[1672] 0.70 0.00 0.02 0.67 0.69 0.70 0.71 0.73 ## q[1673] 0.62 0.00 0.02 0.57 0.60 0.62 0.63 0.66 ## q[1674] 0.59 0.00 0.03 0.53 0.57 0.59 0.61 0.64 ## q[1675] 0.70 0.00 0.02 0.67 0.69 0.70 0.71 0.73 ## q[1676] 0.70 0.00 0.02 0.67 0.69 0.70 0.71 0.73 ## q[1677] 0.70 0.00 0.02 0.67 0.69 0.70 0.71 0.73 ## q[1678] 0.70 0.00 0.02 0.67 0.69 0.70 0.71 0.73 ## q[1679] 0.70 0.00 0.02 0.67 0.69 0.70 0.71 0.73 ## q[1680] 0.62 0.00 0.02 0.57 0.60 0.62 0.63 0.66 ## q[1681] 0.70 0.00 0.02 0.67 0.69 0.70 0.71 0.73 ## q[1682] 0.62 0.00 0.02 0.57 0.60 0.62 0.63 0.66 ## q[1683] 0.59 0.00 0.03 0.53 0.57 0.59 0.61 0.64 ## q[1684] 0.62 0.00 0.02 0.57 0.60 0.62 0.63 0.66 ## q[1685] 0.59 0.00 0.03 0.53 0.57 0.59 0.61 0.64 ## q[1686] 0.70 0.00 0.02 0.67 0.69 0.70 0.71 0.73 ## q[1687] 0.70 0.00 0.02 0.67 0.69 0.70 0.71 0.73 ## q[1688] 0.75 0.00 0.02 0.72 0.74 0.75 0.77 0.79 ## q[1689] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.84 ## q[1690] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.84 ## q[1691] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.84 ## q[1692] 0.75 0.00 0.02 0.72 0.74 0.75 0.77 0.79 ## q[1693] 0.75 0.00 0.02 0.72 0.74 0.75 0.77 0.79 ## q[1694] 0.75 0.00 0.02 0.72 0.74 0.75 0.77 0.79 ## q[1695] 0.75 0.00 0.02 0.72 0.74 0.75 0.77 0.79 ## q[1696] 0.73 0.00 0.02 0.69 0.72 0.73 0.75 0.77 ## q[1697] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.84 ## q[1698] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.84 ## q[1699] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.84 ## q[1700] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.84 ## q[1701] 0.73 0.00 0.02 0.69 0.72 0.73 0.75 0.77 ## q[1702] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.84 ## q[1703] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.84 ## q[1704] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.84 ## q[1705] 0.75 0.00 0.02 0.72 0.74 0.75 0.77 0.79 ## q[1706] 0.73 0.00 0.02 0.69 0.72 0.73 0.75 0.77 ## q[1707] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.84 ## q[1708] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.84 ## q[1709] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.84 ## q[1710] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.84 ## q[1711] 0.75 0.00 0.02 0.72 0.74 0.75 0.77 0.79 ## q[1712] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.84 ## q[1713] 0.75 0.00 0.02 0.72 0.74 0.75 0.77 0.79 ## q[1714] 0.75 0.00 0.02 0.72 0.74 0.75 0.77 0.79 ## q[1715] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.84 ## q[1716] 0.75 0.00 0.02 0.72 0.74 0.75 0.77 0.79 ## q[1717] 0.75 0.00 0.02 0.72 0.74 0.75 0.77 0.79 ## q[1718] 0.75 0.00 0.02 0.72 0.74 0.75 0.77 0.79 ## q[1719] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.84 ## q[1720] 0.73 0.00 0.02 0.69 0.72 0.73 0.75 0.77 ## q[1721] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.84 ## q[1722] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.84 ## q[1723] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.84 ## q[1724] 0.75 0.00 0.02 0.72 0.74 0.75 0.77 0.79 ## q[1725] 0.75 0.00 0.02 0.72 0.74 0.75 0.77 0.79 ## q[1726] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.84 ## q[1727] 0.73 0.00 0.02 0.69 0.72 0.73 0.75 0.77 ## q[1728] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.84 ## q[1729] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.84 ## q[1730] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.84 ## q[1731] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.84 ## q[1732] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.84 ## q[1733] 0.75 0.00 0.02 0.72 0.74 0.75 0.77 0.79 ## q[1734] 0.75 0.00 0.02 0.72 0.74 0.75 0.77 0.79 ## q[1735] 0.73 0.00 0.02 0.69 0.72 0.73 0.75 0.77 ## q[1736] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.84 ## q[1737] 0.75 0.00 0.02 0.72 0.74 0.75 0.77 0.79 ## q[1738] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.84 ## q[1739] 0.75 0.00 0.02 0.72 0.74 0.75 0.77 0.79 ## q[1740] 0.73 0.00 0.02 0.69 0.72 0.73 0.75 0.77 ## q[1741] 0.75 0.00 0.02 0.72 0.74 0.75 0.77 0.79 ## q[1742] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.84 ## q[1743] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.84 ## q[1744] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.84 ## q[1745] 0.73 0.00 0.02 0.69 0.72 0.73 0.75 0.77 ## q[1746] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.84 ## q[1747] 0.83 0.00 0.01 0.80 0.82 0.83 0.83 0.85 ## q[1748] 0.83 0.00 0.01 0.80 0.82 0.83 0.83 0.85 ## q[1749] 0.74 0.00 0.02 0.70 0.73 0.74 0.76 0.78 ## q[1750] 0.83 0.00 0.01 0.80 0.82 0.83 0.83 0.85 ## q[1751] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[1752] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[1753] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[1754] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[1755] 0.74 0.00 0.02 0.70 0.73 0.74 0.76 0.78 ## q[1756] 0.83 0.00 0.01 0.80 0.82 0.83 0.83 0.85 ## q[1757] 0.83 0.00 0.01 0.80 0.82 0.83 0.83 0.85 ## q[1758] 0.83 0.00 0.01 0.80 0.82 0.83 0.83 0.85 ## q[1759] 0.83 0.00 0.01 0.80 0.82 0.83 0.83 0.85 ## q[1760] 0.83 0.00 0.01 0.80 0.82 0.83 0.83 0.85 ## q[1761] 0.74 0.00 0.02 0.70 0.73 0.74 0.76 0.78 ## q[1762] 0.74 0.00 0.02 0.70 0.73 0.74 0.76 0.78 ## q[1763] 0.83 0.00 0.01 0.80 0.82 0.83 0.83 0.85 ## q[1764] 0.83 0.00 0.01 0.80 0.82 0.83 0.83 0.85 ## q[1765] 0.83 0.00 0.01 0.80 0.82 0.83 0.83 0.85 ## q[1766] 0.83 0.00 0.01 0.80 0.82 0.83 0.83 0.85 ## q[1767] 0.83 0.00 0.01 0.80 0.82 0.83 0.83 0.85 ## q[1768] 0.83 0.00 0.01 0.80 0.82 0.83 0.83 0.85 ## q[1769] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[1770] 0.74 0.00 0.02 0.70 0.73 0.74 0.76 0.78 ## q[1771] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[1772] 0.74 0.00 0.02 0.70 0.73 0.74 0.76 0.78 ## q[1773] 0.83 0.00 0.01 0.80 0.82 0.83 0.83 0.85 ## q[1774] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[1775] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[1776] 0.83 0.00 0.01 0.80 0.82 0.83 0.83 0.85 ## q[1777] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[1778] 0.83 0.00 0.01 0.80 0.82 0.83 0.83 0.85 ## q[1779] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[1780] 0.83 0.00 0.01 0.80 0.82 0.83 0.83 0.85 ## q[1781] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[1782] 0.83 0.00 0.01 0.80 0.82 0.83 0.83 0.85 ## q[1783] 0.74 0.00 0.02 0.70 0.73 0.74 0.76 0.78 ## q[1784] 0.83 0.00 0.01 0.80 0.82 0.83 0.83 0.85 ## q[1785] 0.83 0.00 0.01 0.80 0.82 0.83 0.83 0.85 ## q[1786] 0.83 0.00 0.01 0.80 0.82 0.83 0.83 0.85 ## q[1787] 0.83 0.00 0.01 0.80 0.82 0.83 0.83 0.85 ## q[1788] 0.83 0.00 0.01 0.80 0.82 0.83 0.83 0.85 ## q[1789] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[1790] 0.83 0.00 0.01 0.80 0.82 0.83 0.83 0.85 ## q[1791] 0.83 0.00 0.01 0.80 0.82 0.83 0.83 0.85 ## q[1792] 0.74 0.00 0.02 0.70 0.73 0.74 0.76 0.78 ## q[1793] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[1794] 0.83 0.00 0.01 0.80 0.82 0.83 0.83 0.85 ## q[1795] 0.83 0.00 0.01 0.80 0.82 0.83 0.83 0.85 ## q[1796] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[1797] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[1798] 0.83 0.00 0.01 0.80 0.82 0.83 0.83 0.85 ## q[1799] 0.76 0.00 0.02 0.73 0.75 0.76 0.78 0.80 ## q[1800] 0.83 0.00 0.01 0.80 0.82 0.83 0.83 0.85 ## q[1801] 0.74 0.00 0.02 0.70 0.73 0.74 0.76 0.78 ## q[1802] 0.57 0.00 0.02 0.52 0.56 0.57 0.59 0.62 ## q[1803] 0.66 0.00 0.02 0.62 0.65 0.66 0.67 0.70 ## q[1804] 0.66 0.00 0.02 0.62 0.65 0.66 0.67 0.70 ## q[1805] 0.66 0.00 0.02 0.62 0.65 0.66 0.67 0.70 ## q[1806] 0.66 0.00 0.02 0.62 0.65 0.66 0.67 0.70 ## q[1807] 0.66 0.00 0.02 0.62 0.65 0.66 0.67 0.70 ## q[1808] 0.66 0.00 0.02 0.62 0.65 0.66 0.67 0.70 ## q[1809] 0.57 0.00 0.02 0.52 0.56 0.57 0.59 0.62 ## q[1810] 0.54 0.00 0.03 0.48 0.52 0.54 0.56 0.60 ## q[1811] 0.57 0.00 0.02 0.52 0.56 0.57 0.59 0.62 ## q[1812] 0.57 0.00 0.02 0.52 0.56 0.57 0.59 0.62 ## q[1813] 0.57 0.00 0.02 0.52 0.56 0.57 0.59 0.62 ## q[1814] 0.54 0.00 0.03 0.48 0.52 0.54 0.56 0.60 ## q[1815] 0.54 0.00 0.03 0.48 0.52 0.54 0.56 0.60 ## q[1816] 0.66 0.00 0.02 0.62 0.65 0.66 0.67 0.70 ## q[1817] 0.66 0.00 0.02 0.62 0.65 0.66 0.67 0.70 ## q[1818] 0.66 0.00 0.02 0.62 0.65 0.66 0.67 0.70 ## q[1819] 0.66 0.00 0.02 0.62 0.65 0.66 0.67 0.70 ## q[1820] 0.66 0.00 0.02 0.62 0.65 0.66 0.67 0.70 ## q[1821] 0.57 0.00 0.02 0.52 0.56 0.57 0.59 0.62 ## q[1822] 0.66 0.00 0.02 0.62 0.65 0.66 0.67 0.70 ## q[1823] 0.66 0.00 0.02 0.62 0.65 0.66 0.67 0.70 ## q[1824] 0.57 0.00 0.02 0.52 0.56 0.57 0.59 0.62 ## q[1825] 0.66 0.00 0.02 0.62 0.65 0.66 0.67 0.70 ## q[1826] 0.66 0.00 0.02 0.62 0.65 0.66 0.67 0.70 ## q[1827] 0.66 0.00 0.02 0.62 0.65 0.66 0.67 0.70 ## q[1828] 0.57 0.00 0.02 0.52 0.56 0.57 0.59 0.62 ## q[1829] 0.66 0.00 0.02 0.62 0.65 0.66 0.67 0.70 ## q[1830] 0.57 0.00 0.02 0.52 0.56 0.57 0.59 0.62 ## q[1831] 0.66 0.00 0.02 0.62 0.65 0.66 0.67 0.70 ## q[1832] 0.66 0.00 0.02 0.62 0.65 0.66 0.67 0.70 ## q[1833] 0.57 0.00 0.02 0.52 0.56 0.57 0.59 0.62 ## q[1834] 0.57 0.00 0.02 0.52 0.56 0.57 0.59 0.62 ## q[1835] 0.66 0.00 0.02 0.62 0.65 0.66 0.67 0.70 ## q[1836] 0.66 0.00 0.02 0.62 0.65 0.66 0.67 0.70 ## q[1837] 0.66 0.00 0.02 0.62 0.65 0.66 0.67 0.70 ## q[1838] 0.66 0.00 0.02 0.62 0.65 0.66 0.67 0.70 ## q[1839] 0.54 0.00 0.03 0.48 0.52 0.54 0.56 0.60 ## q[1840] 0.57 0.00 0.02 0.52 0.56 0.57 0.59 0.62 ## q[1841] 0.57 0.00 0.02 0.52 0.56 0.57 0.59 0.62 ## q[1842] 0.57 0.00 0.02 0.52 0.56 0.57 0.59 0.62 ## q[1843] 0.66 0.00 0.02 0.62 0.65 0.66 0.67 0.70 ## q[1844] 0.57 0.00 0.02 0.52 0.56 0.57 0.59 0.62 ## q[1845] 0.54 0.00 0.03 0.48 0.52 0.54 0.56 0.60 ## q[1846] 0.57 0.00 0.02 0.52 0.56 0.57 0.59 0.62 ## q[1847] 0.57 0.00 0.02 0.52 0.56 0.57 0.59 0.62 ## q[1848] 0.66 0.00 0.02 0.62 0.65 0.66 0.67 0.70 ## q[1849] 0.57 0.00 0.02 0.52 0.56 0.57 0.59 0.62 ## q[1850] 0.66 0.00 0.02 0.62 0.65 0.66 0.67 0.70 ## q[1851] 0.66 0.00 0.02 0.62 0.65 0.66 0.67 0.70 ## q[1852] 0.57 0.00 0.02 0.52 0.56 0.57 0.59 0.62 ## q[1853] 0.54 0.00 0.03 0.48 0.52 0.54 0.56 0.60 ## q[1854] 0.54 0.00 0.03 0.48 0.52 0.54 0.56 0.60 ## q[1855] 0.66 0.00 0.02 0.62 0.65 0.66 0.67 0.70 ## q[1856] 0.54 0.00 0.03 0.48 0.52 0.54 0.56 0.60 ## q[1857] 0.54 0.00 0.03 0.48 0.52 0.54 0.56 0.60 ## q[1858] 0.66 0.00 0.02 0.62 0.65 0.66 0.67 0.70 ## q[1859] 0.66 0.00 0.02 0.62 0.65 0.66 0.67 0.70 ## q[1860] 0.66 0.00 0.02 0.62 0.65 0.66 0.67 0.70 ## q[1861] 0.57 0.00 0.02 0.52 0.56 0.57 0.59 0.62 ## q[1862] 0.66 0.00 0.02 0.62 0.65 0.66 0.67 0.70 ## q[1863] 0.57 0.00 0.02 0.52 0.56 0.57 0.59 0.62 ## q[1864] 0.57 0.00 0.02 0.52 0.56 0.57 0.59 0.62 ## q[1865] 0.66 0.00 0.02 0.62 0.65 0.66 0.67 0.70 ## q[1866] 0.54 0.00 0.03 0.48 0.52 0.54 0.56 0.60 ## q[1867] 0.66 0.00 0.02 0.62 0.65 0.66 0.67 0.70 ## q[1868] 0.66 0.00 0.02 0.62 0.65 0.66 0.67 0.70 ## q[1869] 0.66 0.00 0.02 0.62 0.65 0.66 0.67 0.70 ## q[1870] 0.66 0.00 0.02 0.62 0.65 0.66 0.67 0.70 ## q[1871] 0.66 0.00 0.02 0.62 0.65 0.66 0.67 0.70 ## q[1872] 0.66 0.00 0.02 0.62 0.65 0.66 0.67 0.70 ## q[1873] 0.57 0.00 0.02 0.52 0.56 0.57 0.59 0.62 ## q[1874] 0.66 0.00 0.02 0.62 0.65 0.66 0.67 0.70 ## q[1875] 0.54 0.00 0.03 0.48 0.52 0.54 0.56 0.60 ## q[1876] 0.54 0.00 0.03 0.48 0.52 0.54 0.56 0.60 ## q[1877] 0.66 0.00 0.02 0.62 0.65 0.66 0.67 0.70 ## q[1878] 0.54 0.00 0.03 0.48 0.52 0.54 0.56 0.60 ## q[1879] 0.57 0.00 0.02 0.52 0.56 0.57 0.59 0.62 ## q[1880] 0.66 0.00 0.02 0.62 0.65 0.66 0.67 0.70 ## q[1881] 0.66 0.00 0.02 0.62 0.65 0.66 0.67 0.70 ## q[1882] 0.57 0.00 0.02 0.52 0.56 0.57 0.59 0.62 ## q[1883] 0.54 0.00 0.03 0.48 0.52 0.54 0.56 0.60 ## q[1884] 0.66 0.00 0.02 0.62 0.65 0.66 0.67 0.70 ## q[1885] 0.66 0.00 0.02 0.62 0.65 0.66 0.67 0.70 ## q[1886] 0.66 0.00 0.02 0.62 0.65 0.66 0.67 0.70 ## q[1887] 0.66 0.00 0.02 0.62 0.65 0.66 0.67 0.70 ## q[1888] 0.66 0.00 0.02 0.62 0.65 0.66 0.67 0.70 ## q[1889] 0.54 0.00 0.03 0.48 0.52 0.54 0.56 0.60 ## q[1890] 0.79 0.00 0.01 0.77 0.78 0.79 0.80 0.82 ## q[1891] 0.79 0.00 0.01 0.77 0.78 0.79 0.80 0.82 ## q[1892] 0.79 0.00 0.01 0.77 0.78 0.79 0.80 0.82 ## q[1893] 0.79 0.00 0.01 0.77 0.78 0.79 0.80 0.82 ## q[1894] 0.72 0.00 0.02 0.69 0.71 0.72 0.74 0.76 ## q[1895] 0.72 0.00 0.02 0.69 0.71 0.72 0.74 0.76 ## q[1896] 0.72 0.00 0.02 0.69 0.71 0.72 0.74 0.76 ## q[1897] 0.70 0.00 0.02 0.65 0.68 0.70 0.72 0.75 ## q[1898] 0.79 0.00 0.01 0.77 0.78 0.79 0.80 0.82 ## q[1899] 0.79 0.00 0.01 0.77 0.78 0.79 0.80 0.82 ## q[1900] 0.79 0.00 0.01 0.77 0.78 0.79 0.80 0.82 ## q[1901] 0.72 0.00 0.02 0.69 0.71 0.72 0.74 0.76 ## q[1902] 0.79 0.00 0.01 0.77 0.78 0.79 0.80 0.82 ## q[1903] 0.72 0.00 0.02 0.69 0.71 0.72 0.74 0.76 ## q[1904] 0.79 0.00 0.01 0.77 0.78 0.79 0.80 0.82 ## q[1905] 0.79 0.00 0.01 0.77 0.78 0.79 0.80 0.82 ## q[1906] 0.72 0.00 0.02 0.69 0.71 0.72 0.74 0.76 ## q[1907] 0.79 0.00 0.01 0.77 0.78 0.79 0.80 0.82 ## q[1908] 0.72 0.00 0.02 0.69 0.71 0.72 0.74 0.76 ## q[1909] 0.72 0.00 0.02 0.69 0.71 0.72 0.74 0.76 ## q[1910] 0.79 0.00 0.01 0.77 0.78 0.79 0.80 0.82 ## q[1911] 0.70 0.00 0.02 0.65 0.68 0.70 0.72 0.75 ## q[1912] 0.72 0.00 0.02 0.69 0.71 0.72 0.74 0.76 ## q[1913] 0.72 0.00 0.02 0.69 0.71 0.72 0.74 0.76 ## q[1914] 0.72 0.00 0.02 0.69 0.71 0.72 0.74 0.76 ## q[1915] 0.79 0.00 0.01 0.77 0.78 0.79 0.80 0.82 ## q[1916] 0.72 0.00 0.02 0.69 0.71 0.72 0.74 0.76 ## q[1917] 0.79 0.00 0.01 0.77 0.78 0.79 0.80 0.82 ## q[1918] 0.79 0.00 0.01 0.77 0.78 0.79 0.80 0.82 ## q[1919] 0.72 0.00 0.02 0.69 0.71 0.72 0.74 0.76 ## q[1920] 0.79 0.00 0.01 0.77 0.78 0.79 0.80 0.82 ## q[1921] 0.79 0.00 0.01 0.77 0.78 0.79 0.80 0.82 ## q[1922] 0.79 0.00 0.01 0.77 0.78 0.79 0.80 0.82 ## q[1923] 0.79 0.00 0.01 0.77 0.78 0.79 0.80 0.82 ## q[1924] 0.79 0.00 0.01 0.77 0.78 0.79 0.80 0.82 ## q[1925] 0.79 0.00 0.01 0.77 0.78 0.79 0.80 0.82 ## q[1926] 0.79 0.00 0.01 0.77 0.78 0.79 0.80 0.82 ## q[1927] 0.72 0.00 0.02 0.69 0.71 0.72 0.74 0.76 ## q[1928] 0.70 0.00 0.02 0.65 0.68 0.70 0.72 0.75 ## q[1929] 0.79 0.00 0.01 0.77 0.78 0.79 0.80 0.82 ## q[1930] 0.70 0.00 0.02 0.65 0.68 0.70 0.72 0.75 ## q[1931] 0.79 0.00 0.01 0.77 0.78 0.79 0.80 0.82 ## q[1932] 0.70 0.00 0.02 0.65 0.68 0.70 0.72 0.75 ## q[1933] 0.79 0.00 0.01 0.77 0.78 0.79 0.80 0.82 ## q[1934] 0.79 0.00 0.01 0.77 0.78 0.79 0.80 0.82 ## q[1935] 0.79 0.00 0.01 0.77 0.78 0.79 0.80 0.82 ## q[1936] 0.79 0.00 0.01 0.77 0.78 0.79 0.80 0.82 ## q[1937] 0.68 0.00 0.02 0.64 0.67 0.68 0.70 0.73 ## q[1938] 0.65 0.00 0.03 0.60 0.64 0.65 0.68 0.71 ## q[1939] 0.65 0.00 0.03 0.60 0.64 0.65 0.68 0.71 ## q[1940] 0.76 0.00 0.02 0.72 0.75 0.76 0.77 0.79 ## q[1941] 0.76 0.00 0.02 0.72 0.75 0.76 0.77 0.79 ## q[1942] 0.76 0.00 0.02 0.72 0.75 0.76 0.77 0.79 ## q[1943] 0.76 0.00 0.02 0.72 0.75 0.76 0.77 0.79 ## q[1944] 0.76 0.00 0.02 0.72 0.75 0.76 0.77 0.79 ## q[1945] 0.68 0.00 0.02 0.64 0.67 0.68 0.70 0.73 ## q[1946] 0.76 0.00 0.02 0.72 0.75 0.76 0.77 0.79 ## q[1947] 0.68 0.00 0.02 0.64 0.67 0.68 0.70 0.73 ## q[1948] 0.68 0.00 0.02 0.64 0.67 0.68 0.70 0.73 ## q[1949] 0.68 0.00 0.02 0.64 0.67 0.68 0.70 0.73 ## q[1950] 0.76 0.00 0.02 0.72 0.75 0.76 0.77 0.79 ## q[1951] 0.76 0.00 0.02 0.72 0.75 0.76 0.77 0.79 ## q[1952] 0.76 0.00 0.02 0.72 0.75 0.76 0.77 0.79 ## q[1953] 0.76 0.00 0.02 0.72 0.75 0.76 0.77 0.79 ## q[1954] 0.76 0.00 0.02 0.72 0.75 0.76 0.77 0.79 ## q[1955] 0.76 0.00 0.02 0.72 0.75 0.76 0.77 0.79 ## q[1956] 0.76 0.00 0.02 0.72 0.75 0.76 0.77 0.79 ## q[1957] 0.65 0.00 0.03 0.60 0.64 0.65 0.68 0.71 ## q[1958] 0.76 0.00 0.02 0.72 0.75 0.76 0.77 0.79 ## q[1959] 0.68 0.00 0.02 0.63 0.66 0.68 0.70 0.72 ## q[1960] 0.76 0.00 0.02 0.72 0.74 0.76 0.77 0.79 ## q[1961] 0.65 0.00 0.03 0.59 0.63 0.65 0.67 0.71 ## q[1962] 0.76 0.00 0.02 0.72 0.74 0.76 0.77 0.79 ## q[1963] 0.76 0.00 0.02 0.72 0.74 0.76 0.77 0.79 ## q[1964] 0.68 0.00 0.02 0.63 0.66 0.68 0.70 0.72 ## q[1965] 0.65 0.00 0.03 0.59 0.63 0.65 0.67 0.71 ## q[1966] 0.68 0.00 0.02 0.63 0.66 0.68 0.70 0.72 ## q[1967] 0.76 0.00 0.02 0.72 0.74 0.76 0.77 0.79 ## q[1968] 0.76 0.00 0.02 0.72 0.74 0.76 0.77 0.79 ## q[1969] 0.76 0.00 0.02 0.72 0.74 0.76 0.77 0.79 ## q[1970] 0.76 0.00 0.02 0.72 0.74 0.76 0.77 0.79 ## q[1971] 0.76 0.00 0.02 0.72 0.74 0.76 0.77 0.79 ## q[1972] 0.76 0.00 0.02 0.72 0.74 0.76 0.77 0.79 ## q[1973] 0.68 0.00 0.02 0.63 0.66 0.68 0.70 0.72 ## q[1974] 0.76 0.00 0.02 0.72 0.74 0.76 0.77 0.79 ## q[1975] 0.76 0.00 0.02 0.72 0.74 0.76 0.77 0.79 ## q[1976] 0.68 0.00 0.02 0.63 0.66 0.68 0.70 0.72 ## q[1977] 0.76 0.00 0.02 0.72 0.74 0.76 0.77 0.79 ## q[1978] 0.68 0.00 0.02 0.63 0.66 0.68 0.70 0.72 ## q[1979] 0.65 0.00 0.03 0.59 0.63 0.65 0.67 0.71 ## q[1980] 0.68 0.00 0.02 0.63 0.66 0.68 0.70 0.72 ## q[1981] 0.68 0.00 0.02 0.63 0.66 0.68 0.70 0.72 ## q[1982] 0.68 0.00 0.02 0.63 0.66 0.68 0.70 0.72 ## q[1983] 0.76 0.00 0.02 0.72 0.74 0.76 0.77 0.79 ## q[1984] 0.76 0.00 0.02 0.72 0.74 0.76 0.77 0.79 ## q[1985] 0.65 0.00 0.03 0.59 0.63 0.65 0.67 0.71 ## q[1986] 0.76 0.00 0.02 0.72 0.74 0.76 0.77 0.79 ## q[1987] 0.65 0.00 0.03 0.59 0.63 0.65 0.67 0.71 ## q[1988] 0.68 0.00 0.02 0.63 0.66 0.68 0.70 0.72 ## q[1989] 0.76 0.00 0.02 0.72 0.74 0.76 0.77 0.79 ## q[1990] 0.76 0.00 0.02 0.72 0.74 0.76 0.77 0.79 ## q[1991] 0.68 0.00 0.02 0.63 0.66 0.68 0.70 0.72 ## q[1992] 0.76 0.00 0.02 0.72 0.74 0.76 0.77 0.79 ## q[1993] 0.76 0.00 0.02 0.72 0.74 0.76 0.77 0.79 ## q[1994] 0.65 0.00 0.03 0.59 0.63 0.65 0.67 0.71 ## q[1995] 0.76 0.00 0.02 0.72 0.74 0.76 0.77 0.79 ## q[1996] 0.68 0.00 0.02 0.63 0.66 0.68 0.70 0.72 ## q[1997] 0.76 0.00 0.02 0.72 0.74 0.76 0.77 0.79 ## q[1998] 0.76 0.00 0.02 0.72 0.74 0.76 0.77 0.79 ## q[1999] 0.76 0.00 0.02 0.72 0.74 0.76 0.77 0.79 ## q[2000] 0.76 0.00 0.02 0.73 0.75 0.76 0.77 0.79 ## q[2001] 0.82 0.00 0.01 0.80 0.81 0.82 0.83 0.85 ## q[2002] 0.74 0.00 0.02 0.69 0.72 0.74 0.75 0.78 ## q[2003] 0.76 0.00 0.02 0.73 0.75 0.76 0.77 0.79 ## q[2004] 0.76 0.00 0.02 0.73 0.75 0.76 0.77 0.79 ## q[2005] 0.74 0.00 0.02 0.69 0.72 0.74 0.75 0.78 ## q[2006] 0.76 0.00 0.02 0.73 0.75 0.76 0.77 0.79 ## q[2007] 0.82 0.00 0.01 0.80 0.81 0.82 0.83 0.85 ## q[2008] 0.82 0.00 0.01 0.80 0.81 0.82 0.83 0.85 ## q[2009] 0.74 0.00 0.02 0.69 0.72 0.74 0.75 0.78 ## q[2010] 0.82 0.00 0.01 0.80 0.81 0.82 0.83 0.85 ## q[2011] 0.82 0.00 0.01 0.80 0.81 0.82 0.83 0.85 ## q[2012] 0.76 0.00 0.02 0.73 0.75 0.76 0.77 0.79 ## q[2013] 0.82 0.00 0.01 0.80 0.81 0.82 0.83 0.85 ## q[2014] 0.82 0.00 0.01 0.80 0.81 0.82 0.83 0.85 ## q[2015] 0.82 0.00 0.01 0.80 0.81 0.82 0.83 0.85 ## q[2016] 0.76 0.00 0.02 0.73 0.75 0.76 0.77 0.79 ## q[2017] 0.82 0.00 0.01 0.80 0.81 0.82 0.83 0.85 ## q[2018] 0.82 0.00 0.01 0.80 0.81 0.82 0.83 0.85 ## q[2019] 0.76 0.00 0.02 0.73 0.75 0.76 0.77 0.79 ## q[2020] 0.82 0.00 0.01 0.80 0.81 0.82 0.83 0.85 ## q[2021] 0.76 0.00 0.02 0.73 0.75 0.76 0.77 0.79 ## q[2022] 0.76 0.00 0.02 0.73 0.75 0.76 0.77 0.79 ## q[2023] 0.82 0.00 0.01 0.80 0.81 0.82 0.83 0.85 ## q[2024] 0.76 0.00 0.02 0.73 0.75 0.76 0.77 0.79 ## q[2025] 0.74 0.00 0.02 0.69 0.72 0.74 0.75 0.78 ## q[2026] 0.76 0.00 0.02 0.73 0.75 0.76 0.77 0.79 ## q[2027] 0.76 0.00 0.02 0.73 0.75 0.76 0.77 0.79 ## q[2028] 0.82 0.00 0.01 0.80 0.81 0.82 0.83 0.85 ## q[2029] 0.76 0.00 0.02 0.73 0.75 0.76 0.77 0.79 ## q[2030] 0.74 0.00 0.02 0.69 0.72 0.74 0.75 0.78 ## q[2031] 0.82 0.00 0.01 0.80 0.81 0.82 0.83 0.85 ## q[2032] 0.82 0.00 0.01 0.80 0.81 0.82 0.83 0.85 ## q[2033] 0.74 0.00 0.02 0.69 0.72 0.74 0.75 0.78 ## q[2034] 0.82 0.00 0.01 0.80 0.81 0.82 0.83 0.85 ## q[2035] 0.82 0.00 0.01 0.80 0.81 0.82 0.83 0.85 ## q[2036] 0.74 0.00 0.02 0.69 0.72 0.74 0.75 0.78 ## q[2037] 0.76 0.00 0.02 0.73 0.75 0.76 0.77 0.79 ## q[2038] 0.82 0.00 0.01 0.80 0.81 0.82 0.83 0.85 ## q[2039] 0.76 0.00 0.02 0.73 0.75 0.76 0.77 0.79 ## q[2040] 0.76 0.00 0.02 0.73 0.75 0.76 0.77 0.79 ## q[2041] 0.82 0.00 0.01 0.80 0.81 0.82 0.83 0.85 ## q[2042] 0.74 0.00 0.02 0.69 0.72 0.74 0.75 0.78 ## q[2043] 0.76 0.00 0.02 0.73 0.75 0.76 0.77 0.79 ## q[2044] 0.82 0.00 0.01 0.80 0.81 0.82 0.83 0.85 ## q[2045] 0.82 0.00 0.01 0.80 0.81 0.82 0.83 0.85 ## q[2046] 0.82 0.00 0.01 0.80 0.81 0.82 0.83 0.85 ## q[2047] 0.82 0.00 0.01 0.80 0.81 0.82 0.83 0.85 ## q[2048] 0.76 0.00 0.02 0.73 0.75 0.76 0.77 0.79 ## q[2049] 0.76 0.00 0.02 0.73 0.75 0.76 0.77 0.79 ## q[2050] 0.82 0.00 0.01 0.80 0.81 0.82 0.83 0.85 ## q[2051] 0.82 0.00 0.01 0.80 0.81 0.82 0.83 0.85 ## q[2052] 0.76 0.00 0.02 0.73 0.75 0.76 0.77 0.79 ## q[2053] 0.74 0.00 0.02 0.69 0.72 0.74 0.75 0.78 ## q[2054] 0.76 0.00 0.02 0.73 0.75 0.76 0.77 0.79 ## q[2055] 0.82 0.00 0.01 0.80 0.81 0.82 0.83 0.85 ## q[2056] 0.82 0.00 0.01 0.80 0.81 0.82 0.83 0.85 ## q[2057] 0.83 0.00 0.01 0.80 0.82 0.83 0.84 0.85 ## q[2058] 0.77 0.00 0.02 0.73 0.76 0.77 0.78 0.80 ## q[2059] 0.75 0.00 0.02 0.70 0.73 0.75 0.76 0.79 ## q[2060] 0.77 0.00 0.02 0.73 0.76 0.77 0.78 0.80 ## q[2061] 0.77 0.00 0.02 0.73 0.76 0.77 0.78 0.80 ## q[2062] 0.83 0.00 0.01 0.80 0.82 0.83 0.84 0.85 ## q[2063] 0.77 0.00 0.02 0.73 0.76 0.77 0.78 0.80 ## q[2064] 0.83 0.00 0.01 0.80 0.82 0.83 0.84 0.85 ## q[2065] 0.77 0.00 0.02 0.73 0.76 0.77 0.78 0.80 ## q[2066] 0.83 0.00 0.01 0.80 0.82 0.83 0.84 0.85 ## q[2067] 0.77 0.00 0.02 0.73 0.76 0.77 0.78 0.80 ## q[2068] 0.83 0.00 0.01 0.80 0.82 0.83 0.84 0.85 ## q[2069] 0.83 0.00 0.01 0.80 0.82 0.83 0.84 0.85 ## q[2070] 0.77 0.00 0.02 0.73 0.76 0.77 0.78 0.80 ## q[2071] 0.83 0.00 0.01 0.80 0.82 0.83 0.84 0.85 ## q[2072] 0.83 0.00 0.01 0.80 0.82 0.83 0.84 0.85 ## q[2073] 0.75 0.00 0.02 0.70 0.73 0.75 0.76 0.79 ## q[2074] 0.75 0.00 0.02 0.70 0.73 0.75 0.76 0.79 ## q[2075] 0.77 0.00 0.02 0.73 0.76 0.77 0.78 0.80 ## q[2076] 0.77 0.00 0.02 0.73 0.76 0.77 0.78 0.80 ## q[2077] 0.83 0.00 0.01 0.80 0.82 0.83 0.84 0.85 ## q[2078] 0.75 0.00 0.02 0.70 0.73 0.75 0.76 0.79 ## q[2079] 0.83 0.00 0.01 0.80 0.82 0.83 0.84 0.85 ## q[2080] 0.83 0.00 0.01 0.80 0.82 0.83 0.84 0.85 ## q[2081] 0.77 0.00 0.02 0.73 0.76 0.77 0.78 0.80 ## q[2082] 0.75 0.00 0.02 0.70 0.73 0.75 0.76 0.79 ## q[2083] 0.77 0.00 0.02 0.73 0.76 0.77 0.78 0.80 ## q[2084] 0.83 0.00 0.01 0.80 0.82 0.83 0.84 0.85 ## q[2085] 0.77 0.00 0.02 0.73 0.76 0.77 0.78 0.80 ## q[2086] 0.75 0.00 0.02 0.70 0.73 0.75 0.76 0.79 ## q[2087] 0.83 0.00 0.01 0.80 0.82 0.83 0.84 0.85 ## q[2088] 0.75 0.00 0.02 0.70 0.73 0.75 0.76 0.79 ## q[2089] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[2090] 0.81 0.00 0.01 0.79 0.80 0.81 0.82 0.84 ## q[2091] 0.81 0.00 0.01 0.79 0.80 0.81 0.82 0.84 ## q[2092] 0.81 0.00 0.01 0.79 0.80 0.81 0.82 0.84 ## q[2093] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[2094] 0.72 0.00 0.02 0.68 0.71 0.72 0.74 0.77 ## q[2095] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[2096] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[2097] 0.72 0.00 0.02 0.68 0.71 0.72 0.74 0.77 ## q[2098] 0.81 0.00 0.01 0.79 0.80 0.81 0.82 0.84 ## q[2099] 0.81 0.00 0.01 0.79 0.80 0.81 0.82 0.84 ## q[2100] 0.81 0.00 0.01 0.79 0.80 0.81 0.82 0.84 ## q[2101] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[2102] 0.81 0.00 0.01 0.79 0.80 0.81 0.82 0.84 ## q[2103] 0.81 0.00 0.01 0.79 0.80 0.81 0.82 0.84 ## q[2104] 0.81 0.00 0.01 0.79 0.80 0.81 0.82 0.84 ## q[2105] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[2106] 0.81 0.00 0.01 0.79 0.80 0.81 0.82 0.84 ## q[2107] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[2108] 0.81 0.00 0.01 0.79 0.80 0.81 0.82 0.84 ## q[2109] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[2110] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[2111] 0.81 0.00 0.01 0.79 0.80 0.81 0.82 0.84 ## q[2112] 0.81 0.00 0.01 0.79 0.80 0.81 0.82 0.84 ## q[2113] 0.81 0.00 0.01 0.79 0.80 0.81 0.82 0.84 ## q[2114] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[2115] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[2116] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[2117] 0.81 0.00 0.01 0.79 0.80 0.81 0.82 0.84 ## q[2118] 0.81 0.00 0.01 0.79 0.80 0.81 0.82 0.84 ## q[2119] 0.81 0.00 0.01 0.79 0.80 0.81 0.82 0.84 ## q[2120] 0.72 0.00 0.02 0.68 0.71 0.72 0.74 0.77 ## q[2121] 0.72 0.00 0.02 0.68 0.71 0.72 0.74 0.77 ## q[2122] 0.81 0.00 0.01 0.79 0.80 0.81 0.82 0.84 ## q[2123] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[2124] 0.81 0.00 0.01 0.79 0.80 0.81 0.82 0.84 ## q[2125] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[2126] 0.81 0.00 0.01 0.79 0.80 0.81 0.82 0.84 ## q[2127] 0.72 0.00 0.02 0.68 0.71 0.72 0.74 0.77 ## q[2128] 0.81 0.00 0.01 0.79 0.80 0.81 0.82 0.84 ## q[2129] 0.81 0.00 0.01 0.79 0.80 0.81 0.82 0.84 ## q[2130] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[2131] 0.72 0.00 0.02 0.68 0.71 0.72 0.74 0.77 ## q[2132] 0.72 0.00 0.02 0.68 0.71 0.72 0.74 0.77 ## q[2133] 0.81 0.00 0.01 0.79 0.80 0.81 0.82 0.84 ## q[2134] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[2135] 0.81 0.00 0.01 0.79 0.80 0.81 0.82 0.84 ## q[2136] 0.75 0.00 0.02 0.71 0.74 0.75 0.76 0.78 ## q[2137] 0.72 0.00 0.02 0.68 0.71 0.72 0.74 0.77 ## q[2138] 0.81 0.00 0.01 0.79 0.80 0.81 0.82 0.84 ## q[2139] 0.81 0.00 0.01 0.79 0.80 0.81 0.82 0.84 ## q[2140] 0.81 0.00 0.01 0.79 0.80 0.81 0.82 0.84 ## q[2141] 0.72 0.00 0.02 0.68 0.71 0.72 0.74 0.77 ## q[2142] 0.79 0.00 0.02 0.76 0.78 0.79 0.80 0.83 ## q[2143] 0.85 0.00 0.01 0.82 0.84 0.85 0.86 0.87 ## q[2144] 0.77 0.00 0.02 0.73 0.76 0.77 0.79 0.81 ## q[2145] 0.77 0.00 0.02 0.73 0.76 0.77 0.79 0.81 ## q[2146] 0.79 0.00 0.02 0.76 0.78 0.79 0.80 0.83 ## q[2147] 0.85 0.00 0.01 0.82 0.84 0.85 0.86 0.87 ## q[2148] 0.85 0.00 0.01 0.82 0.84 0.85 0.86 0.87 ## q[2149] 0.85 0.00 0.01 0.82 0.84 0.85 0.86 0.87 ## q[2150] 0.85 0.00 0.01 0.82 0.84 0.85 0.86 0.87 ## q[2151] 0.79 0.00 0.02 0.76 0.78 0.79 0.80 0.83 ## q[2152] 0.85 0.00 0.01 0.82 0.84 0.85 0.86 0.87 ## q[2153] 0.85 0.00 0.01 0.82 0.84 0.85 0.86 0.87 ## q[2154] 0.79 0.00 0.02 0.76 0.78 0.79 0.80 0.83 ## q[2155] 0.85 0.00 0.01 0.82 0.84 0.85 0.86 0.87 ## q[2156] 0.79 0.00 0.02 0.76 0.78 0.79 0.80 0.83 ## q[2157] 0.77 0.00 0.02 0.73 0.76 0.77 0.79 0.81 ## q[2158] 0.79 0.00 0.02 0.76 0.78 0.79 0.80 0.83 ## q[2159] 0.79 0.00 0.02 0.76 0.78 0.79 0.80 0.83 ## q[2160] 0.85 0.00 0.01 0.82 0.84 0.85 0.86 0.87 ## q[2161] 0.85 0.00 0.01 0.82 0.84 0.85 0.86 0.87 ## q[2162] 0.77 0.00 0.02 0.73 0.76 0.77 0.79 0.81 ## q[2163] 0.85 0.00 0.01 0.82 0.84 0.85 0.86 0.87 ## q[2164] 0.77 0.00 0.02 0.73 0.76 0.77 0.79 0.81 ## q[2165] 0.79 0.00 0.02 0.76 0.78 0.79 0.80 0.83 ## q[2166] 0.85 0.00 0.01 0.82 0.84 0.85 0.86 0.87 ## q[2167] 0.85 0.00 0.01 0.82 0.84 0.85 0.86 0.87 ## q[2168] 0.79 0.00 0.02 0.76 0.78 0.79 0.80 0.83 ## q[2169] 0.85 0.00 0.01 0.82 0.84 0.85 0.86 0.87 ## q[2170] 0.85 0.00 0.01 0.82 0.84 0.85 0.86 0.87 ## q[2171] 0.79 0.00 0.02 0.76 0.78 0.79 0.80 0.83 ## q[2172] 0.85 0.00 0.01 0.82 0.84 0.85 0.86 0.87 ## q[2173] 0.89 0.00 0.01 0.85 0.88 0.89 0.90 0.91 ## q[2174] 0.89 0.00 0.01 0.85 0.88 0.89 0.90 0.91 ## q[2175] 0.84 0.00 0.02 0.80 0.83 0.84 0.86 0.88 ## q[2176] 0.82 0.00 0.02 0.77 0.81 0.83 0.84 0.87 ## q[2177] 0.89 0.00 0.01 0.85 0.88 0.89 0.90 0.91 ## q[2178] 0.89 0.00 0.01 0.85 0.88 0.89 0.90 0.91 ## q[2179] 0.89 0.00 0.01 0.85 0.88 0.89 0.90 0.91 ## q[2180] 0.89 0.00 0.01 0.85 0.88 0.89 0.90 0.91 ## q[2181] 0.82 0.00 0.02 0.77 0.81 0.83 0.84 0.87 ## q[2182] 0.84 0.00 0.02 0.80 0.83 0.84 0.86 0.88 ## q[2183] 0.84 0.00 0.02 0.80 0.83 0.84 0.86 0.88 ## q[2184] 0.84 0.00 0.02 0.80 0.83 0.84 0.86 0.88 ## q[2185] 0.89 0.00 0.01 0.85 0.88 0.89 0.90 0.91 ## q[2186] 0.84 0.00 0.02 0.80 0.83 0.84 0.86 0.88 ## q[2187] 0.89 0.00 0.01 0.85 0.88 0.89 0.90 0.91 ## q[2188] 0.89 0.00 0.01 0.85 0.88 0.89 0.90 0.91 ## q[2189] 0.89 0.00 0.01 0.85 0.88 0.89 0.90 0.91 ## q[2190] 0.89 0.00 0.01 0.85 0.88 0.89 0.90 0.91 ## q[2191] 0.89 0.00 0.01 0.85 0.88 0.89 0.90 0.91 ## q[2192] 0.89 0.00 0.01 0.85 0.88 0.89 0.90 0.91 ## q[2193] 0.82 0.00 0.02 0.77 0.81 0.83 0.84 0.87 ## q[2194] 0.89 0.00 0.01 0.85 0.88 0.89 0.90 0.91 ## q[2195] 0.89 0.00 0.01 0.85 0.88 0.89 0.90 0.91 ## q[2196] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.81 ## q[2197] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.81 ## q[2198] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.81 ## q[2199] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.81 ## q[2200] 0.71 0.00 0.02 0.68 0.70 0.71 0.73 0.75 ## q[2201] 0.69 0.00 0.03 0.64 0.67 0.69 0.71 0.74 ## q[2202] 0.71 0.00 0.02 0.68 0.70 0.71 0.73 0.75 ## q[2203] 0.69 0.00 0.03 0.64 0.67 0.69 0.71 0.74 ## q[2204] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.81 ## q[2205] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.81 ## q[2206] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.81 ## q[2207] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.81 ## q[2208] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.81 ## q[2209] 0.71 0.00 0.02 0.68 0.70 0.71 0.73 0.75 ## q[2210] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.81 ## q[2211] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.81 ## q[2212] 0.71 0.00 0.02 0.68 0.70 0.71 0.73 0.75 ## q[2213] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.81 ## q[2214] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.81 ## q[2215] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.81 ## q[2216] 0.69 0.00 0.03 0.64 0.67 0.69 0.71 0.74 ## q[2217] 0.71 0.00 0.02 0.68 0.70 0.71 0.73 0.75 ## q[2218] 0.71 0.00 0.02 0.68 0.70 0.71 0.73 0.75 ## q[2219] 0.71 0.00 0.02 0.68 0.70 0.71 0.73 0.75 ## q[2220] 0.71 0.00 0.02 0.68 0.70 0.71 0.73 0.75 ## q[2221] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.81 ## q[2222] 0.69 0.00 0.03 0.64 0.67 0.69 0.71 0.74 ## q[2223] 0.69 0.00 0.03 0.64 0.67 0.69 0.71 0.74 ## q[2224] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.81 ## q[2225] 0.69 0.00 0.03 0.64 0.67 0.69 0.71 0.74 ## q[2226] 0.69 0.00 0.03 0.64 0.67 0.69 0.71 0.74 ## q[2227] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.81 ## q[2228] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.81 ## q[2229] 0.71 0.00 0.02 0.68 0.70 0.71 0.73 0.75 ## q[2230] 0.71 0.00 0.02 0.68 0.70 0.71 0.73 0.75 ## q[2231] 0.69 0.00 0.03 0.64 0.67 0.69 0.71 0.74 ## q[2232] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.81 ## q[2233] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.81 ## q[2234] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.81 ## q[2235] 0.69 0.00 0.03 0.64 0.67 0.69 0.71 0.74 ## q[2236] 0.69 0.00 0.03 0.64 0.67 0.69 0.71 0.74 ## q[2237] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.81 ## q[2238] 0.69 0.00 0.03 0.64 0.67 0.69 0.71 0.74 ## q[2239] 0.71 0.00 0.02 0.68 0.70 0.71 0.73 0.75 ## q[2240] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.81 ## q[2241] 0.71 0.00 0.02 0.68 0.70 0.71 0.73 0.75 ## q[2242] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.81 ## q[2243] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.81 ## q[2244] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.81 ## q[2245] 0.79 0.00 0.01 0.76 0.78 0.79 0.80 0.81 ## q[2246] 0.69 0.00 0.03 0.64 0.67 0.69 0.71 0.74 ## q[2247] 0.57 0.00 0.04 0.48 0.54 0.57 0.60 0.65 ## q[2248] 0.60 0.00 0.04 0.53 0.57 0.60 0.62 0.67 ## q[2249] 0.60 0.00 0.04 0.53 0.57 0.60 0.62 0.67 ## q[2250] 0.60 0.00 0.04 0.53 0.57 0.60 0.62 0.67 ## q[2251] 0.68 0.00 0.03 0.62 0.66 0.68 0.70 0.74 ## q[2252] 0.60 0.00 0.04 0.53 0.57 0.60 0.62 0.67 ## q[2253] 0.68 0.00 0.03 0.62 0.66 0.68 0.70 0.74 ## q[2254] 0.60 0.00 0.04 0.53 0.57 0.60 0.62 0.67 ## q[2255] 0.60 0.00 0.04 0.53 0.57 0.60 0.62 0.67 ## q[2256] 0.60 0.00 0.04 0.53 0.57 0.60 0.62 0.67 ## q[2257] 0.68 0.00 0.03 0.62 0.66 0.68 0.70 0.74 ## q[2258] 0.68 0.00 0.03 0.62 0.66 0.68 0.70 0.74 ## q[2259] 0.60 0.00 0.04 0.53 0.57 0.60 0.62 0.67 ## q[2260] 0.60 0.00 0.04 0.53 0.57 0.60 0.62 0.67 ## q[2261] 0.68 0.00 0.03 0.62 0.66 0.68 0.70 0.74 ## q[2262] 0.68 0.00 0.03 0.62 0.66 0.68 0.70 0.74 ## q[2263] 0.68 0.00 0.03 0.62 0.66 0.68 0.70 0.74 ## q[2264] 0.60 0.00 0.04 0.53 0.57 0.60 0.62 0.67 ## q[2265] 0.68 0.00 0.03 0.62 0.66 0.68 0.70 0.74 ## q[2266] 0.57 0.00 0.04 0.48 0.54 0.57 0.60 0.65 ## q[2267] 0.60 0.00 0.04 0.53 0.57 0.60 0.62 0.67 ## q[2268] 0.68 0.00 0.03 0.62 0.66 0.68 0.70 0.74 ## q[2269] 0.85 0.00 0.01 0.82 0.84 0.85 0.86 0.88 ## q[2270] 0.80 0.00 0.02 0.76 0.78 0.80 0.81 0.83 ## q[2271] 0.80 0.00 0.02 0.76 0.78 0.80 0.81 0.83 ## q[2272] 0.85 0.00 0.01 0.82 0.84 0.85 0.86 0.88 ## q[2273] 0.85 0.00 0.01 0.82 0.84 0.85 0.86 0.88 ## q[2274] 0.78 0.00 0.02 0.73 0.76 0.78 0.79 0.82 ## q[2275] 0.85 0.00 0.01 0.82 0.84 0.85 0.86 0.88 ## q[2276] 0.80 0.00 0.02 0.76 0.78 0.80 0.81 0.83 ## q[2277] 0.85 0.00 0.01 0.82 0.84 0.85 0.86 0.88 ## q[2278] 0.85 0.00 0.01 0.82 0.84 0.85 0.86 0.88 ## q[2279] 0.80 0.00 0.02 0.76 0.78 0.80 0.81 0.83 ## q[2280] 0.80 0.00 0.02 0.76 0.78 0.80 0.81 0.83 ## q[2281] 0.85 0.00 0.01 0.82 0.84 0.85 0.86 0.88 ## q[2282] 0.80 0.00 0.02 0.76 0.78 0.80 0.81 0.83 ## q[2283] 0.78 0.00 0.02 0.73 0.76 0.78 0.79 0.82 ## q[2284] 0.80 0.00 0.02 0.76 0.78 0.80 0.81 0.83 ## q[2285] 0.85 0.00 0.01 0.82 0.84 0.85 0.86 0.88 ## q[2286] 0.80 0.00 0.02 0.76 0.78 0.80 0.81 0.83 ## q[2287] 0.78 0.00 0.02 0.73 0.76 0.78 0.79 0.82 ## q[2288] 0.78 0.00 0.02 0.73 0.76 0.78 0.79 0.82 ## q[2289] 0.85 0.00 0.01 0.82 0.84 0.85 0.86 0.88 ## q[2290] 0.85 0.00 0.01 0.82 0.84 0.85 0.86 0.88 ## q[2291] 0.78 0.00 0.02 0.73 0.76 0.78 0.79 0.82 ## q[2292] 0.80 0.00 0.02 0.76 0.78 0.80 0.81 0.83 ## q[2293] 0.80 0.00 0.02 0.76 0.78 0.80 0.81 0.83 ## q[2294] 0.78 0.00 0.02 0.73 0.76 0.78 0.79 0.82 ## q[2295] 0.80 0.00 0.02 0.76 0.78 0.80 0.81 0.83 ## q[2296] 0.85 0.00 0.01 0.82 0.84 0.85 0.86 0.88 ## q[2297] 0.85 0.00 0.01 0.82 0.84 0.85 0.86 0.88 ## q[2298] 0.85 0.00 0.01 0.82 0.84 0.85 0.86 0.88 ## q[2299] 0.85 0.00 0.01 0.82 0.84 0.85 0.86 0.88 ## q[2300] 0.80 0.00 0.02 0.76 0.78 0.80 0.81 0.83 ## q[2301] 0.80 0.00 0.02 0.76 0.78 0.80 0.81 0.83 ## q[2302] 0.85 0.00 0.01 0.82 0.84 0.85 0.86 0.88 ## q[2303] 0.80 0.00 0.02 0.76 0.78 0.80 0.81 0.83 ## q[2304] 0.78 0.00 0.02 0.73 0.76 0.78 0.79 0.82 ## q[2305] 0.85 0.00 0.01 0.82 0.84 0.85 0.86 0.88 ## q[2306] 0.85 0.00 0.01 0.82 0.84 0.85 0.86 0.88 ## q[2307] 0.70 0.00 0.02 0.66 0.69 0.70 0.72 0.74 ## q[2308] 0.68 0.00 0.03 0.62 0.66 0.68 0.70 0.73 ## q[2309] 0.78 0.00 0.02 0.74 0.77 0.78 0.79 0.80 ## q[2310] 0.78 0.00 0.02 0.74 0.77 0.78 0.79 0.80 ## q[2311] 0.68 0.00 0.03 0.62 0.66 0.68 0.70 0.73 ## q[2312] 0.78 0.00 0.02 0.74 0.77 0.78 0.79 0.80 ## q[2313] 0.78 0.00 0.02 0.74 0.77 0.78 0.79 0.80 ## q[2314] 0.78 0.00 0.02 0.74 0.77 0.78 0.79 0.80 ## q[2315] 0.70 0.00 0.02 0.66 0.69 0.70 0.72 0.74 ## q[2316] 0.68 0.00 0.03 0.62 0.66 0.68 0.70 0.73 ## q[2317] 0.70 0.00 0.02 0.66 0.69 0.70 0.72 0.74 ## q[2318] 0.78 0.00 0.02 0.74 0.77 0.78 0.79 0.80 ## q[2319] 0.78 0.00 0.02 0.74 0.77 0.78 0.79 0.80 ## q[2320] 0.70 0.00 0.02 0.66 0.69 0.70 0.72 0.74 ## q[2321] 0.78 0.00 0.02 0.74 0.77 0.78 0.79 0.80 ## q[2322] 0.68 0.00 0.03 0.62 0.66 0.68 0.70 0.73 ## q[2323] 0.78 0.00 0.02 0.74 0.77 0.78 0.79 0.80 ## q[2324] 0.78 0.00 0.02 0.74 0.77 0.78 0.79 0.80 ## q[2325] 0.70 0.00 0.02 0.66 0.69 0.70 0.72 0.74 ## q[2326] 0.78 0.00 0.02 0.74 0.77 0.78 0.79 0.80 ## q[2327] 0.68 0.00 0.03 0.62 0.66 0.68 0.70 0.73 ## q[2328] 0.70 0.00 0.02 0.66 0.69 0.70 0.72 0.74 ## q[2329] 0.78 0.00 0.02 0.74 0.77 0.78 0.79 0.80 ## q[2330] 0.56 0.00 0.03 0.51 0.54 0.56 0.57 0.61 ## q[2331] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[2332] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[2333] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[2334] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[2335] 0.56 0.00 0.03 0.51 0.54 0.56 0.57 0.61 ## q[2336] 0.56 0.00 0.03 0.51 0.54 0.56 0.57 0.61 ## q[2337] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.59 ## q[2338] 0.56 0.00 0.03 0.51 0.54 0.56 0.57 0.61 ## q[2339] 0.56 0.00 0.03 0.51 0.54 0.56 0.57 0.61 ## q[2340] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.59 ## q[2341] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[2342] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[2343] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[2344] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[2345] 0.56 0.00 0.03 0.51 0.54 0.56 0.57 0.61 ## q[2346] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.59 ## q[2347] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[2348] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[2349] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[2350] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[2351] 0.56 0.00 0.03 0.51 0.54 0.56 0.57 0.61 ## q[2352] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[2353] 0.56 0.00 0.03 0.51 0.54 0.56 0.57 0.61 ## q[2354] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[2355] 0.56 0.00 0.03 0.51 0.54 0.56 0.57 0.61 ## q[2356] 0.56 0.00 0.03 0.51 0.54 0.56 0.57 0.61 ## q[2357] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[2358] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[2359] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[2360] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[2361] 0.56 0.00 0.03 0.51 0.54 0.56 0.57 0.61 ## q[2362] 0.56 0.00 0.03 0.51 0.54 0.56 0.57 0.61 ## q[2363] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[2364] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.59 ## q[2365] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[2366] 0.56 0.00 0.03 0.51 0.54 0.56 0.57 0.61 ## q[2367] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[2368] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[2369] 0.56 0.00 0.03 0.51 0.54 0.56 0.57 0.61 ## q[2370] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.59 ## q[2371] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.59 ## q[2372] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[2373] 0.56 0.00 0.03 0.51 0.54 0.56 0.57 0.61 ## q[2374] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[2375] 0.56 0.00 0.03 0.51 0.54 0.56 0.57 0.61 ## q[2376] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[2377] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.59 ## q[2378] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[2379] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[2380] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[2381] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[2382] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[2383] 0.56 0.00 0.03 0.51 0.54 0.56 0.57 0.61 ## q[2384] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.59 ## q[2385] 0.56 0.00 0.03 0.51 0.54 0.56 0.57 0.61 ## q[2386] 0.56 0.00 0.03 0.51 0.54 0.56 0.57 0.61 ## q[2387] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[2388] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[2389] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[2390] 0.56 0.00 0.03 0.51 0.54 0.56 0.57 0.61 ## q[2391] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.59 ## q[2392] 0.56 0.00 0.03 0.51 0.54 0.56 0.57 0.61 ## q[2393] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[2394] 0.65 0.00 0.02 0.61 0.63 0.65 0.66 0.69 ## q[2395] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.59 ## q[2396] 0.53 0.00 0.03 0.47 0.51 0.53 0.55 0.59 ## lp__ -1379.61 0.04 1.55 -1383.60 -1380.40 -1379.31 -1378.45 -1377.57 ## n_eff Rhat ## b[1] 1975 1 ## b[2] 3064 1 ## b[3] 2053 1 ## b[4] 2756 1 ## b[5] 2948 1 ## OR1 3081 1 ## OR2 2053 1 ## OR3 2713 1 ## OR4 2979 1 ## q[1] 2488 1 ## q[2] 2122 1 ## q[3] 2522 1 ## q[4] 2122 1 ## q[5] 2488 1 ## q[6] 2488 1 ## q[7] 2522 1 ## q[8] 2488 1 ## q[9] 2122 1 ## q[10] 2122 1 ## q[11] 2122 1 ## q[12] 2522 1 ## q[13] 2122 1 ## q[14] 2122 1 ## q[15] 2122 1 ## q[16] 2122 1 ## q[17] 2122 1 ## q[18] 2488 1 ## q[19] 2122 1 ## q[20] 2122 1 ## q[21] 2488 1 ## q[22] 2488 1 ## q[23] 2522 1 ## q[24] 2522 1 ## q[25] 2122 1 ## q[26] 2122 1 ## q[27] 2488 1 ## q[28] 2122 1 ## q[29] 2488 1 ## q[30] 2122 1 ## q[31] 2488 1 ## q[32] 2122 1 ## q[33] 2488 1 ## q[34] 2122 1 ## q[35] 2522 1 ## q[36] 2122 1 ## q[37] 2122 1 ## q[38] 2122 1 ## q[39] 2488 1 ## q[40] 2122 1 ## q[41] 2122 1 ## q[42] 2488 1 ## q[43] 2488 1 ## q[44] 2961 1 ## q[45] 2961 1 ## q[46] 2961 1 ## q[47] 3069 1 ## q[48] 2961 1 ## q[49] 2653 1 ## q[50] 2653 1 ## q[51] 2653 1 ## q[52] 2653 1 ## q[53] 2961 1 ## q[54] 2961 1 ## q[55] 2961 1 ## q[56] 2961 1 ## q[57] 2961 1 ## q[58] 3069 1 ## q[59] 2961 1 ## q[60] 3069 1 ## q[61] 2961 1 ## q[62] 2961 1 ## q[63] 2961 1 ## q[64] 2961 1 ## q[65] 2653 1 ## q[66] 2961 1 ## q[67] 2653 1 ## q[68] 3069 1 ## q[69] 2653 1 ## q[70] 3069 1 ## q[71] 3069 1 ## q[72] 2961 1 ## q[73] 2653 1 ## q[74] 2961 1 ## q[75] 2653 1 ## q[76] 3069 1 ## q[77] 2961 1 ## q[78] 2961 1 ## q[79] 3069 1 ## q[80] 2653 1 ## q[81] 2653 1 ## q[82] 3069 1 ## q[83] 2653 1 ## q[84] 2961 1 ## q[85] 2961 1 ## q[86] 2961 1 ## q[87] 2961 1 ## q[88] 2653 1 ## q[89] 2961 1 ## q[90] 2961 1 ## q[91] 2653 1 ## q[92] 2961 1 ## q[93] 2961 1 ## q[94] 2653 1 ## q[95] 2653 1 ## q[96] 2653 1 ## q[97] 2961 1 ## q[98] 2961 1 ## q[99] 3069 1 ## q[100] 3197 1 ## q[101] 3197 1 ## q[102] 3594 1 ## q[103] 3256 1 ## q[104] 3256 1 ## q[105] 3197 1 ## q[106] 3197 1 ## q[107] 3197 1 ## q[108] 3594 1 ## q[109] 3197 1 ## q[110] 3197 1 ## q[111] 3197 1 ## q[112] 3256 1 ## q[113] 3256 1 ## q[114] 3594 1 ## q[115] 3256 1 ## q[116] 3197 1 ## q[117] 3594 1 ## q[118] 3197 1 ## q[119] 3594 1 ## q[120] 3256 1 ## q[121] 3256 1 ## q[122] 3197 1 ## q[123] 3197 1 ## q[124] 3197 1 ## q[125] 3256 1 ## q[126] 3197 1 ## q[127] 3256 1 ## q[128] 3256 1 ## q[129] 3197 1 ## q[130] 3197 1 ## q[131] 3594 1 ## q[132] 2570 1 ## q[133] 2570 1 ## q[134] 2570 1 ## q[135] 2974 1 ## q[136] 2974 1 ## q[137] 2772 1 ## q[138] 2570 1 ## q[139] 2570 1 ## q[140] 2772 1 ## q[141] 2570 1 ## q[142] 2570 1 ## q[143] 2570 1 ## q[144] 2570 1 ## q[145] 2570 1 ## q[146] 2974 1 ## q[147] 2570 1 ## q[148] 2570 1 ## q[149] 2772 1 ## q[150] 2974 1 ## q[151] 2974 1 ## q[152] 2974 1 ## q[153] 2772 1 ## q[154] 2974 1 ## q[155] 2772 1 ## q[156] 2570 1 ## q[157] 2570 1 ## q[158] 2772 1 ## q[159] 2570 1 ## q[160] 2974 1 ## q[161] 2570 1 ## q[162] 2772 1 ## q[163] 2974 1 ## q[164] 2570 1 ## q[165] 2570 1 ## q[166] 2974 1 ## q[167] 2570 1 ## q[168] 2570 1 ## q[169] 2772 1 ## q[170] 2570 1 ## q[171] 2772 1 ## q[172] 2974 1 ## q[173] 2570 1 ## q[174] 2570 1 ## q[175] 2570 1 ## q[176] 2570 1 ## q[177] 2879 1 ## q[178] 2383 1 ## q[179] 2383 1 ## q[180] 2879 1 ## q[181] 2383 1 ## q[182] 2708 1 ## q[183] 2708 1 ## q[184] 2383 1 ## q[185] 2708 1 ## q[186] 2708 1 ## q[187] 2383 1 ## q[188] 2383 1 ## q[189] 2383 1 ## q[190] 2708 1 ## q[191] 2383 1 ## q[192] 2708 1 ## q[193] 2383 1 ## q[194] 2383 1 ## q[195] 2708 1 ## q[196] 2708 1 ## q[197] 2383 1 ## q[198] 2708 1 ## q[199] 2708 1 ## q[200] 2708 1 ## q[201] 2708 1 ## q[202] 2708 1 ## q[203] 2383 1 ## q[204] 2708 1 ## q[205] 2879 1 ## q[206] 2879 1 ## q[207] 2708 1 ## q[208] 2383 1 ## q[209] 2708 1 ## q[210] 3235 1 ## q[211] 3235 1 ## q[212] 3235 1 ## q[213] 3570 1 ## q[214] 3235 1 ## q[215] 3198 1 ## q[216] 3198 1 ## q[217] 3570 1 ## q[218] 3198 1 ## q[219] 3198 1 ## q[220] 3570 1 ## q[221] 3235 1 ## q[222] 3235 1 ## q[223] 3235 1 ## q[224] 3570 1 ## q[225] 3235 1 ## q[226] 3235 1 ## q[227] 3235 1 ## q[228] 3235 1 ## q[229] 3235 1 ## q[230] 3198 1 ## q[231] 3235 1 ## q[232] 3235 1 ## q[233] 3235 1 ## q[234] 3198 1 ## q[235] 3235 1 ## q[236] 3570 1 ## q[237] 3198 1 ## q[238] 3235 1 ## q[239] 3198 1 ## q[240] 3198 1 ## q[241] 3235 1 ## q[242] 3235 1 ## q[243] 3198 1 ## q[244] 3570 1 ## q[245] 3198 1 ## q[246] 3570 1 ## q[247] 3235 1 ## q[248] 3235 1 ## q[249] 3198 1 ## q[250] 3570 1 ## q[251] 3235 1 ## q[252] 3235 1 ## q[253] 3235 1 ## q[254] 3235 1 ## q[255] 3235 1 ## q[256] 3235 1 ## q[257] 3570 1 ## q[258] 3570 1 ## q[259] 3198 1 ## q[260] 3198 1 ## q[261] 3235 1 ## q[262] 3235 1 ## q[263] 3235 1 ## q[264] 3198 1 ## q[265] 3198 1 ## q[266] 3235 1 ## q[267] 3198 1 ## q[268] 3235 1 ## q[269] 3570 1 ## q[270] 3570 1 ## q[271] 2886 1 ## q[272] 3190 1 ## q[273] 3190 1 ## q[274] 3444 1 ## q[275] 2886 1 ## q[276] 2886 1 ## q[277] 3444 1 ## q[278] 2886 1 ## q[279] 3190 1 ## q[280] 2886 1 ## q[281] 2886 1 ## q[282] 3190 1 ## q[283] 2886 1 ## q[284] 3190 1 ## q[285] 2886 1 ## q[286] 2886 1 ## q[287] 3190 1 ## q[288] 3444 1 ## q[289] 3190 1 ## q[290] 2886 1 ## q[291] 3190 1 ## q[292] 3190 1 ## q[293] 3444 1 ## q[294] 3190 1 ## q[295] 3444 1 ## q[296] 2886 1 ## q[297] 2886 1 ## q[298] 3444 1 ## q[299] 2886 1 ## q[300] 3190 1 ## q[301] 2886 1 ## q[302] 3190 1 ## q[303] 3444 1 ## q[304] 3190 1 ## q[305] 2886 1 ## q[306] 2886 1 ## q[307] 2886 1 ## q[308] 2886 1 ## q[309] 2886 1 ## q[310] 3190 1 ## q[311] 2886 1 ## q[312] 3190 1 ## q[313] 2886 1 ## q[314] 3444 1 ## q[315] 3190 1 ## q[316] 3444 1 ## q[317] 2886 1 ## q[318] 2886 1 ## q[319] 2886 1 ## q[320] 2604 1 ## q[321] 2905 1 ## q[322] 2905 1 ## q[323] 3042 1 ## q[324] 2905 1 ## q[325] 2905 1 ## q[326] 2604 1 ## q[327] 2604 1 ## q[328] 2604 1 ## q[329] 3042 1 ## q[330] 2604 1 ## q[331] 3042 1 ## q[332] 2905 1 ## q[333] 2905 1 ## q[334] 2905 1 ## q[335] 2905 1 ## q[336] 3042 1 ## q[337] 2905 1 ## q[338] 2905 1 ## q[339] 2905 1 ## q[340] 2905 1 ## q[341] 3042 1 ## q[342] 2905 1 ## q[343] 2905 1 ## q[344] 2905 1 ## q[345] 2905 1 ## q[346] 2604 1 ## q[347] 2905 1 ## q[348] 2905 1 ## q[349] 2905 1 ## q[350] 2905 1 ## q[351] 2604 1 ## q[352] 2905 1 ## q[353] 2604 1 ## q[354] 3042 1 ## q[355] 2604 1 ## q[356] 2604 1 ## q[357] 2604 1 ## q[358] 2604 1 ## q[359] 3042 1 ## q[360] 2905 1 ## q[361] 2604 1 ## q[362] 2905 1 ## q[363] 3042 1 ## q[364] 2905 1 ## q[365] 2905 1 ## q[366] 3042 1 ## q[367] 2604 1 ## q[368] 2905 1 ## q[369] 2905 1 ## q[370] 2604 1 ## q[371] 2905 1 ## q[372] 2905 1 ## q[373] 3042 1 ## q[374] 2604 1 ## q[375] 2905 1 ## q[376] 2905 1 ## q[377] 2905 1 ## q[378] 2604 1 ## q[379] 2905 1 ## q[380] 2905 1 ## q[381] 2905 1 ## q[382] 3042 1 ## q[383] 2905 1 ## q[384] 3042 1 ## q[385] 2604 1 ## q[386] 2905 1 ## q[387] 2604 1 ## q[388] 2604 1 ## q[389] 2905 1 ## q[390] 2604 1 ## q[391] 2905 1 ## q[392] 2905 1 ## q[393] 2905 1 ## q[394] 3042 1 ## q[395] 2905 1 ## q[396] 2944 1 ## q[397] 3106 1 ## q[398] 3106 1 ## q[399] 3398 1 ## q[400] 3106 1 ## q[401] 3106 1 ## q[402] 3106 1 ## q[403] 2944 1 ## q[404] 2944 1 ## q[405] 2944 1 ## q[406] 2944 1 ## q[407] 3398 1 ## q[408] 2944 1 ## q[409] 3398 1 ## q[410] 3106 1 ## q[411] 3106 1 ## q[412] 3106 1 ## q[413] 3106 1 ## q[414] 3106 1 ## q[415] 3106 1 ## q[416] 2944 1 ## q[417] 3398 1 ## q[418] 3106 1 ## q[419] 3106 1 ## q[420] 3106 1 ## q[421] 3106 1 ## q[422] 3106 1 ## q[423] 2944 1 ## q[424] 3106 1 ## q[425] 3106 1 ## q[426] 2944 1 ## q[427] 2944 1 ## q[428] 3106 1 ## q[429] 3106 1 ## q[430] 2944 1 ## q[431] 2944 1 ## q[432] 2944 1 ## q[433] 2944 1 ## q[434] 3106 1 ## q[435] 3398 1 ## q[436] 2944 1 ## q[437] 3106 1 ## q[438] 2944 1 ## q[439] 3106 1 ## q[440] 2944 1 ## q[441] 3106 1 ## q[442] 2944 1 ## q[443] 2944 1 ## q[444] 3106 1 ## q[445] 3106 1 ## q[446] 3106 1 ## q[447] 2944 1 ## q[448] 3106 1 ## q[449] 3106 1 ## q[450] 3106 1 ## q[451] 3106 1 ## q[452] 3106 1 ## q[453] 3106 1 ## q[454] 3106 1 ## q[455] 3106 1 ## q[456] 3106 1 ## q[457] 2944 1 ## q[458] 3106 1 ## q[459] 3398 1 ## q[460] 3106 1 ## q[461] 3398 1 ## q[462] 2944 1 ## q[463] 3106 1 ## q[464] 3106 1 ## q[465] 3398 1 ## q[466] 2944 1 ## q[467] 3106 1 ## q[468] 2944 1 ## q[469] 3106 1 ## q[470] 3106 1 ## q[471] 3106 1 ## q[472] 3398 1 ## q[473] 3106 1 ## q[474] 2964 1 ## q[475] 3121 1 ## q[476] 3121 1 ## q[477] 3121 1 ## q[478] 2964 1 ## q[479] 2964 1 ## q[480] 3415 1 ## q[481] 3415 1 ## q[482] 3415 1 ## q[483] 3121 1 ## q[484] 3121 1 ## q[485] 3121 1 ## q[486] 3121 1 ## q[487] 3415 1 ## q[488] 3121 1 ## q[489] 3121 1 ## q[490] 3121 1 ## q[491] 2964 1 ## q[492] 3121 1 ## q[493] 3121 1 ## q[494] 3121 1 ## q[495] 3121 1 ## q[496] 2964 1 ## q[497] 3121 1 ## q[498] 2964 1 ## q[499] 3121 1 ## q[500] 3121 1 ## q[501] 3121 1 ## q[502] 3415 1 ## q[503] 2964 1 ## q[504] 2964 1 ## q[505] 2964 1 ## q[506] 2964 1 ## q[507] 3121 1 ## q[508] 3121 1 ## q[509] 3415 1 ## q[510] 3415 1 ## q[511] 3121 1 ## q[512] 3415 1 ## q[513] 3415 1 ## q[514] 3121 1 ## q[515] 3121 1 ## q[516] 3121 1 ## q[517] 3121 1 ## q[518] 2964 1 ## q[519] 2964 1 ## q[520] 3415 1 ## q[521] 3121 1 ## q[522] 3121 1 ## q[523] 3121 1 ## q[524] 3121 1 ## q[525] 3415 1 ## q[526] 3415 1 ## q[527] 2964 1 ## q[528] 2964 1 ## q[529] 3121 1 ## q[530] 3121 1 ## q[531] 2964 1 ## q[532] 2964 1 ## q[533] 3121 1 ## q[534] 3121 1 ## q[535] 3121 1 ## q[536] 3415 1 ## q[537] 3415 1 ## q[538] 3076 1 ## q[539] 3076 1 ## q[540] 3076 1 ## q[541] 2757 1 ## q[542] 2757 1 ## q[543] 3076 1 ## q[544] 3076 1 ## q[545] 3118 1 ## q[546] 3076 1 ## q[547] 3076 1 ## q[548] 3076 1 ## q[549] 3076 1 ## q[550] 3076 1 ## q[551] 2757 1 ## q[552] 3118 1 ## q[553] 2757 1 ## q[554] 2757 1 ## q[555] 3118 1 ## q[556] 3118 1 ## q[557] 3076 1 ## q[558] 3076 1 ## q[559] 3118 1 ## q[560] 2757 1 ## q[561] 3118 1 ## q[562] 2757 1 ## q[563] 3076 1 ## q[564] 2757 1 ## q[565] 3076 1 ## q[566] 3118 1 ## q[567] 3076 1 ## q[568] 2757 1 ## q[569] 3076 1 ## q[570] 3076 1 ## q[571] 3076 1 ## q[572] 3214 1 ## q[573] 3214 1 ## q[574] 3124 1 ## q[575] 3124 1 ## q[576] 3531 1 ## q[577] 3124 1 ## q[578] 3214 1 ## q[579] 3214 1 ## q[580] 3124 1 ## q[581] 3531 1 ## q[582] 3214 1 ## q[583] 3214 1 ## q[584] 3124 1 ## q[585] 3214 1 ## q[586] 3124 1 ## q[587] 3214 1 ## q[588] 3124 1 ## q[589] 3214 1 ## q[590] 3214 1 ## q[591] 3124 1 ## q[592] 3214 1 ## q[593] 3214 1 ## q[594] 3531 1 ## q[595] 3531 1 ## q[596] 3124 1 ## q[597] 3124 1 ## q[598] 3214 1 ## q[599] 3531 1 ## q[600] 3214 1 ## q[601] 3214 1 ## q[602] 3214 1 ## q[603] 3124 1 ## q[604] 3531 1 ## q[605] 3124 1 ## q[606] 3124 1 ## q[607] 3214 1 ## q[608] 3214 1 ## q[609] 3124 1 ## q[610] 3531 1 ## q[611] 3124 1 ## q[612] 3214 1 ## q[613] 3214 1 ## q[614] 3531 1 ## q[615] 3531 1 ## q[616] 3323 1 ## q[617] 3240 1 ## q[618] 3240 1 ## q[619] 3323 1 ## q[620] 3323 1 ## q[621] 3157 1 ## q[622] 3323 1 ## q[623] 3157 1 ## q[624] 3240 1 ## q[625] 3240 1 ## q[626] 3240 1 ## q[627] 3240 1 ## q[628] 3157 1 ## q[629] 3240 1 ## q[630] 3157 1 ## q[631] 3240 1 ## q[632] 3240 1 ## q[633] 3240 1 ## q[634] 3240 1 ## q[635] 3240 1 ## q[636] 3240 1 ## q[637] 3323 1 ## q[638] 3323 1 ## q[639] 3240 1 ## q[640] 3240 1 ## q[641] 3240 1 ## q[642] 3323 1 ## q[643] 3157 1 ## q[644] 3240 1 ## q[645] 3240 1 ## q[646] 3157 1 ## q[647] 3240 1 ## q[648] 3157 1 ## q[649] 3240 1 ## q[650] 3323 1 ## q[651] 3240 1 ## q[652] 3157 1 ## q[653] 3323 1 ## q[654] 3157 1 ## q[655] 3240 1 ## q[656] 3240 1 ## q[657] 3323 1 ## q[658] 3157 1 ## q[659] 3323 1 ## q[660] 3323 1 ## q[661] 3240 1 ## q[662] 3240 1 ## q[663] 3323 1 ## q[664] 3323 1 ## q[665] 3323 1 ## q[666] 3240 1 ## q[667] 3157 1 ## q[668] 3157 1 ## q[669] 3190 1 ## q[670] 2886 1 ## q[671] 3190 1 ## q[672] 3444 1 ## q[673] 3444 1 ## q[674] 3444 1 ## q[675] 2886 1 ## q[676] 2886 1 ## q[677] 2886 1 ## q[678] 2886 1 ## q[679] 2886 1 ## q[680] 2886 1 ## q[681] 3190 1 ## q[682] 2886 1 ## q[683] 3190 1 ## q[684] 2886 1 ## q[685] 2886 1 ## q[686] 2886 1 ## q[687] 3190 1 ## q[688] 3190 1 ## q[689] 3190 1 ## q[690] 2886 1 ## q[691] 2886 1 ## q[692] 3444 1 ## q[693] 3444 1 ## q[694] 2886 1 ## q[695] 2886 1 ## q[696] 2886 1 ## q[697] 2886 1 ## q[698] 3190 1 ## q[699] 3444 1 ## q[700] 2886 1 ## q[701] 2886 1 ## q[702] 3444 1 ## q[703] 3444 1 ## q[704] 3190 1 ## q[705] 2886 1 ## q[706] 3190 1 ## q[707] 2886 1 ## q[708] 3444 1 ## q[709] 2829 1 ## q[710] 2829 1 ## q[711] 2829 1 ## q[712] 2829 1 ## q[713] 3166 1 ## q[714] 3166 1 ## q[715] 3407 1 ## q[716] 3166 1 ## q[717] 3166 1 ## q[718] 2829 1 ## q[719] 2829 1 ## q[720] 2829 1 ## q[721] 3166 1 ## q[722] 3407 1 ## q[723] 2829 1 ## q[724] 2829 1 ## q[725] 3166 1 ## q[726] 2829 1 ## q[727] 3166 1 ## q[728] 2829 1 ## q[729] 3166 1 ## q[730] 2829 1 ## q[731] 2829 1 ## q[732] 2829 1 ## q[733] 3166 1 ## q[734] 2829 1 ## q[735] 3407 1 ## q[736] 3166 1 ## q[737] 2829 1 ## q[738] 2829 1 ## q[739] 3166 1 ## q[740] 3407 1 ## q[741] 3407 1 ## q[742] 3166 1 ## q[743] 3166 1 ## q[744] 2829 1 ## q[745] 3407 1 ## q[746] 2829 1 ## q[747] 2829 1 ## q[748] 2829 1 ## q[749] 2829 1 ## q[750] 2829 1 ## q[751] 3166 1 ## q[752] 3407 1 ## q[753] 3166 1 ## q[754] 3166 1 ## q[755] 2829 1 ## q[756] 2829 1 ## q[757] 2829 1 ## q[758] 3166 1 ## q[759] 3407 1 ## q[760] 3166 1 ## q[761] 2829 1 ## q[762] 2829 1 ## q[763] 3407 1 ## q[764] 3407 1 ## q[765] 3219 1 ## q[766] 2829 1 ## q[767] 2829 1 ## q[768] 2829 1 ## q[769] 2829 1 ## q[770] 3219 1 ## q[771] 3219 1 ## q[772] 3219 1 ## q[773] 3219 1 ## q[774] 2971 1 ## q[775] 2829 1 ## q[776] 2829 1 ## q[777] 2829 1 ## q[778] 2971 1 ## q[779] 2829 1 ## q[780] 2829 1 ## q[781] 2829 1 ## q[782] 3219 1 ## q[783] 2829 1 ## q[784] 2829 1 ## q[785] 2829 1 ## q[786] 3219 1 ## q[787] 2829 1 ## q[788] 3219 1 ## q[789] 3219 1 ## q[790] 2829 1 ## q[791] 2829 1 ## q[792] 3219 1 ## q[793] 3219 1 ## q[794] 3219 1 ## q[795] 2829 1 ## q[796] 2971 1 ## q[797] 2971 1 ## q[798] 2829 1 ## q[799] 3219 1 ## q[800] 2829 1 ## q[801] 3219 1 ## q[802] 2829 1 ## q[803] 2829 1 ## q[804] 3219 1 ## q[805] 3219 1 ## q[806] 2829 1 ## q[807] 2971 1 ## q[808] 2829 1 ## q[809] 2829 1 ## q[810] 2829 1 ## q[811] 2829 1 ## q[812] 2829 1 ## q[813] 2829 1 ## q[814] 3219 1 ## q[815] 3219 1 ## q[816] 2971 1 ## q[817] 2829 1 ## q[818] 2829 1 ## q[819] 3219 1 ## q[820] 2971 1 ## q[821] 2829 1 ## q[822] 2829 1 ## q[823] 2829 1 ## q[824] 2970 1 ## q[825] 2970 1 ## q[826] 2970 1 ## q[827] 3221 1 ## q[828] 3221 1 ## q[829] 3221 1 ## q[830] 2970 1 ## q[831] 2970 1 ## q[832] 2970 1 ## q[833] 3221 1 ## q[834] 3494 1 ## q[835] 2970 1 ## q[836] 3221 1 ## q[837] 2970 1 ## q[838] 3221 1 ## q[839] 2970 1 ## q[840] 3221 1 ## q[841] 2970 1 ## q[842] 3494 1 ## q[843] 3221 1 ## q[844] 2970 1 ## q[845] 3221 1 ## q[846] 3221 1 ## q[847] 2970 1 ## q[848] 2970 1 ## q[849] 2970 1 ## q[850] 2970 1 ## q[851] 2970 1 ## q[852] 2970 1 ## q[853] 3221 1 ## q[854] 3221 1 ## q[855] 2970 1 ## q[856] 2970 1 ## q[857] 3494 1 ## q[858] 3221 1 ## q[859] 2970 1 ## q[860] 2970 1 ## q[861] 3494 1 ## q[862] 2680 1 ## q[863] 2246 1 ## q[864] 2246 1 ## q[865] 2246 1 ## q[866] 2246 1 ## q[867] 2246 1 ## q[868] 2680 1 ## q[869] 2680 1 ## q[870] 2680 1 ## q[871] 2740 1 ## q[872] 2246 1 ## q[873] 2246 1 ## q[874] 2246 1 ## q[875] 2246 1 ## q[876] 2246 1 ## q[877] 2246 1 ## q[878] 2740 1 ## q[879] 2246 1 ## q[880] 2246 1 ## q[881] 2246 1 ## q[882] 2680 1 ## q[883] 2246 1 ## q[884] 2246 1 ## q[885] 2680 1 ## q[886] 2680 1 ## q[887] 2740 1 ## q[888] 2246 1 ## q[889] 2680 1 ## q[890] 2246 1 ## q[891] 2680 1 ## q[892] 2246 1 ## q[893] 2246 1 ## q[894] 2246 1 ## q[895] 2246 1 ## q[896] 2246 1 ## q[897] 2740 1 ## q[898] 2246 1 ## q[899] 2680 1 ## q[900] 2246 1 ## q[901] 2246 1 ## q[902] 2680 1 ## q[903] 2246 1 ## q[904] 2246 1 ## q[905] 2740 1 ## q[906] 2246 1 ## q[907] 2582 1 ## q[908] 2780 1 ## q[909] 2417 1 ## q[910] 2417 1 ## q[911] 2780 1 ## q[912] 2582 1 ## q[913] 2417 1 ## q[914] 2582 1 ## q[915] 2582 1 ## q[916] 2417 1 ## q[917] 2582 1 ## q[918] 2417 1 ## q[919] 2582 1 ## q[920] 2582 1 ## q[921] 2417 1 ## q[922] 2417 1 ## q[923] 2780 1 ## q[924] 2417 1 ## q[925] 2582 1 ## q[926] 2417 1 ## q[927] 2417 1 ## q[928] 2582 1 ## q[929] 2417 1 ## q[930] 2780 1 ## q[931] 2582 1 ## q[932] 2780 1 ## q[933] 2417 1 ## q[934] 2582 1 ## q[935] 2582 1 ## q[936] 2417 1 ## q[937] 2417 1 ## q[938] 2582 1 ## q[939] 2582 1 ## q[940] 2582 1 ## q[941] 2582 1 ## q[942] 2582 1 ## q[943] 2417 1 ## q[944] 2582 1 ## q[945] 2582 1 ## q[946] 2780 1 ## q[947] 2780 1 ## q[948] 2582 1 ## q[949] 2417 1 ## q[950] 2582 1 ## q[951] 2582 1 ## q[952] 2746 1 ## q[953] 2746 1 ## q[954] 2746 1 ## q[955] 2746 1 ## q[956] 2746 1 ## q[957] 2416 1 ## q[958] 2416 1 ## q[959] 2909 1 ## q[960] 2416 1 ## q[961] 2416 1 ## q[962] 2909 1 ## q[963] 2746 1 ## q[964] 2746 1 ## q[965] 2746 1 ## q[966] 2746 1 ## q[967] 2746 1 ## q[968] 2909 1 ## q[969] 2746 1 ## q[970] 2909 1 ## q[971] 2746 1 ## q[972] 2746 1 ## q[973] 2746 1 ## q[974] 2746 1 ## q[975] 2416 1 ## q[976] 2746 1 ## q[977] 2746 1 ## q[978] 2416 1 ## q[979] 2746 1 ## q[980] 2746 1 ## q[981] 2746 1 ## q[982] 2746 1 ## q[983] 2909 1 ## q[984] 2416 1 ## q[985] 2909 1 ## q[986] 2416 1 ## q[987] 2909 1 ## q[988] 2416 1 ## q[989] 2416 1 ## q[990] 2746 1 ## q[991] 2746 1 ## q[992] 2416 1 ## q[993] 2909 1 ## q[994] 2909 1 ## q[995] 2746 1 ## q[996] 2746 1 ## q[997] 2909 1 ## q[998] 2909 1 ## q[999] 2746 1 ## q[1000] 2416 1 ## q[1001] 2746 1 ## q[1002] 2909 1 ## q[1003] 2416 1 ## q[1004] 2416 1 ## q[1005] 2909 1 ## q[1006] 2746 1 ## q[1007] 2746 1 ## q[1008] 2746 1 ## q[1009] 2746 1 ## q[1010] 2746 1 ## q[1011] 2416 1 ## q[1012] 2746 1 ## q[1013] 2746 1 ## q[1014] 2909 1 ## q[1015] 2909 1 ## q[1016] 2416 1 ## q[1017] 2416 1 ## q[1018] 2746 1 ## q[1019] 2746 1 ## q[1020] 2746 1 ## q[1021] 2416 1 ## q[1022] 2416 1 ## q[1023] 2416 1 ## q[1024] 2746 1 ## q[1025] 2746 1 ## q[1026] 2746 1 ## q[1027] 2909 1 ## q[1028] 2909 1 ## q[1029] 2974 1 ## q[1030] 2570 1 ## q[1031] 2570 1 ## q[1032] 2570 1 ## q[1033] 2974 1 ## q[1034] 2974 1 ## q[1035] 2974 1 ## q[1036] 2772 1 ## q[1037] 2772 1 ## q[1038] 2570 1 ## q[1039] 2570 1 ## q[1040] 2772 1 ## q[1041] 2570 1 ## q[1042] 2570 1 ## q[1043] 2570 1 ## q[1044] 2570 1 ## q[1045] 2974 1 ## q[1046] 2570 1 ## q[1047] 2570 1 ## q[1048] 2974 1 ## q[1049] 2570 1 ## q[1050] 2570 1 ## q[1051] 2570 1 ## q[1052] 2974 1 ## q[1053] 2570 1 ## q[1054] 2570 1 ## q[1055] 2974 1 ## q[1056] 2974 1 ## q[1057] 2570 1 ## q[1058] 2772 1 ## q[1059] 2974 1 ## q[1060] 2974 1 ## q[1061] 2974 1 ## q[1062] 2570 1 ## q[1063] 2974 1 ## q[1064] 2974 1 ## q[1065] 2772 1 ## q[1066] 2974 1 ## q[1067] 2570 1 ## q[1068] 2772 1 ## q[1069] 2570 1 ## q[1070] 2570 1 ## q[1071] 2772 1 ## q[1072] 2570 1 ## q[1073] 2570 1 ## q[1074] 2974 1 ## q[1075] 2570 1 ## q[1076] 2974 1 ## q[1077] 2570 1 ## q[1078] 2772 1 ## q[1079] 2974 1 ## q[1080] 2570 1 ## q[1081] 2570 1 ## q[1082] 2570 1 ## q[1083] 2570 1 ## q[1084] 2570 1 ## q[1085] 2974 1 ## q[1086] 2570 1 ## q[1087] 2974 1 ## q[1088] 2570 1 ## q[1089] 2772 1 ## q[1090] 2570 1 ## q[1091] 2772 1 ## q[1092] 2974 1 ## q[1093] 2772 1 ## q[1094] 2570 1 ## q[1095] 2570 1 ## q[1096] 2570 1 ## q[1097] 2570 1 ## q[1098] 2570 1 ## q[1099] 3043 1 ## q[1100] 3043 1 ## q[1101] 3043 1 ## q[1102] 3043 1 ## q[1103] 3043 1 ## q[1104] 3352 1 ## q[1105] 3084 1 ## q[1106] 3352 1 ## q[1107] 3352 1 ## q[1108] 3352 1 ## q[1109] 3352 1 ## q[1110] 3084 1 ## q[1111] 3043 1 ## q[1112] 3043 1 ## q[1113] 3043 1 ## q[1114] 3084 1 ## q[1115] 3043 1 ## q[1116] 3043 1 ## q[1117] 3352 1 ## q[1118] 3043 1 ## q[1119] 3043 1 ## q[1120] 3043 1 ## q[1121] 3352 1 ## q[1122] 3043 1 ## q[1123] 3352 1 ## q[1124] 3043 1 ## q[1125] 3043 1 ## q[1126] 3352 1 ## q[1127] 3043 1 ## q[1128] 3043 1 ## q[1129] 3043 1 ## q[1130] 3352 1 ## q[1131] 3352 1 ## q[1132] 3043 1 ## q[1133] 3084 1 ## q[1134] 3352 1 ## q[1135] 3084 1 ## q[1136] 3352 1 ## q[1137] 3352 1 ## q[1138] 3043 1 ## q[1139] 3043 1 ## q[1140] 3352 1 ## q[1141] 3084 1 ## q[1142] 3043 1 ## q[1143] 3084 1 ## q[1144] 3043 1 ## q[1145] 3352 1 ## q[1146] 3043 1 ## q[1147] 3352 1 ## q[1148] 3084 1 ## q[1149] 3352 1 ## q[1150] 3043 1 ## q[1151] 3084 1 ## q[1152] 3043 1 ## q[1153] 3043 1 ## q[1154] 3043 1 ## q[1155] 3043 1 ## q[1156] 3043 1 ## q[1157] 3043 1 ## q[1158] 3352 1 ## q[1159] 3043 1 ## q[1160] 3352 1 ## q[1161] 3084 1 ## q[1162] 3084 1 ## q[1163] 3043 1 ## q[1164] 3084 1 ## q[1165] 3352 1 ## q[1166] 3043 1 ## q[1167] 3043 1 ## q[1168] 3352 1 ## q[1169] 3352 1 ## q[1170] 3084 1 ## q[1171] 3043 1 ## q[1172] 3043 1 ## q[1173] 3043 1 ## q[1174] 3043 1 ## q[1175] 3084 1 ## q[1176] 3334 1 ## q[1177] 2995 1 ## q[1178] 2995 1 ## q[1179] 3068 1 ## q[1180] 3334 1 ## q[1181] 3068 1 ## q[1182] 3334 1 ## q[1183] 3068 1 ## q[1184] 3334 1 ## q[1185] 2995 1 ## q[1186] 2995 1 ## q[1187] 3334 1 ## q[1188] 2995 1 ## q[1189] 2995 1 ## q[1190] 2995 1 ## q[1191] 3334 1 ## q[1192] 2995 1 ## q[1193] 2995 1 ## q[1194] 3334 1 ## q[1195] 2995 1 ## q[1196] 3334 1 ## q[1197] 2995 1 ## q[1198] 3334 1 ## q[1199] 3334 1 ## q[1200] 2995 1 ## q[1201] 2995 1 ## q[1202] 3334 1 ## q[1203] 2995 1 ## q[1204] 3068 1 ## q[1205] 3334 1 ## q[1206] 3334 1 ## q[1207] 2995 1 ## q[1208] 3334 1 ## q[1209] 2995 1 ## q[1210] 2995 1 ## q[1211] 2995 1 ## q[1212] 3068 1 ## q[1213] 3068 1 ## q[1214] 2995 1 ## q[1215] 3068 1 ## q[1216] 3334 1 ## q[1217] 3068 1 ## q[1218] 2995 1 ## q[1219] 3334 1 ## q[1220] 2995 1 ## q[1221] 3334 1 ## q[1222] 3334 1 ## q[1223] 2995 1 ## q[1224] 3068 1 ## q[1225] 2995 1 ## q[1226] 2995 1 ## q[1227] 3334 1 ## q[1228] 2995 1 ## q[1229] 2995 1 ## q[1230] 3068 1 ## q[1231] 3334 1 ## q[1232] 2995 1 ## q[1233] 3334 1 ## q[1234] 3068 1 ## q[1235] 3334 1 ## q[1236] 2995 1 ## q[1237] 2995 1 ## q[1238] 3068 1 ## q[1239] 3019 1 ## q[1240] 3019 1 ## q[1241] 3019 1 ## q[1242] 3094 1 ## q[1243] 3019 1 ## q[1244] 3019 1 ## q[1245] 3019 1 ## q[1246] 2704 1 ## q[1247] 2704 1 ## q[1248] 2704 1 ## q[1249] 2704 1 ## q[1250] 2704 1 ## q[1251] 3019 1 ## q[1252] 3019 1 ## q[1253] 3019 1 ## q[1254] 3019 1 ## q[1255] 3019 1 ## q[1256] 3019 1 ## q[1257] 2704 1 ## q[1258] 3094 1 ## q[1259] 3019 1 ## q[1260] 3019 1 ## q[1261] 3019 1 ## q[1262] 2704 1 ## q[1263] 2704 1 ## q[1264] 3019 1 ## q[1265] 2704 1 ## q[1266] 3019 1 ## q[1267] 2704 1 ## q[1268] 3094 1 ## q[1269] 2704 1 ## q[1270] 2704 1 ## q[1271] 3019 1 ## q[1272] 2704 1 ## q[1273] 3094 1 ## q[1274] 3019 1 ## q[1275] 2704 1 ## q[1276] 3019 1 ## q[1277] 2704 1 ## q[1278] 3094 1 ## q[1279] 3019 1 ## q[1280] 3094 1 ## q[1281] 2704 1 ## q[1282] 2704 1 ## q[1283] 2704 1 ## q[1284] 3019 1 ## q[1285] 3019 1 ## q[1286] 3019 1 ## q[1287] 3019 1 ## q[1288] 3019 1 ## q[1289] 3019 1 ## q[1290] 2704 1 ## q[1291] 3019 1 ## q[1292] 3019 1 ## q[1293] 3094 1 ## q[1294] 3019 1 ## q[1295] 2704 1 ## q[1296] 3019 1 ## q[1297] 3019 1 ## q[1298] 3094 1 ## q[1299] 2704 1 ## q[1300] 2704 1 ## q[1301] 3019 1 ## q[1302] 3019 1 ## q[1303] 3019 1 ## q[1304] 3019 1 ## q[1305] 3094 1 ## q[1306] 3019 1 ## q[1307] 3225 1 ## q[1308] 3225 1 ## q[1309] 3225 1 ## q[1310] 3335 1 ## q[1311] 3335 1 ## q[1312] 3335 1 ## q[1313] 3335 1 ## q[1314] 3225 1 ## q[1315] 3225 1 ## q[1316] 3225 1 ## q[1317] 3225 1 ## q[1318] 3225 1 ## q[1319] 3148 1 ## q[1320] 3335 1 ## q[1321] 3148 1 ## q[1322] 3225 1 ## q[1323] 3225 1 ## q[1324] 3335 1 ## q[1325] 3225 1 ## q[1326] 3335 1 ## q[1327] 3225 1 ## q[1328] 3335 1 ## q[1329] 3225 1 ## q[1330] 3148 1 ## q[1331] 3148 1 ## q[1332] 3335 1 ## q[1333] 3225 1 ## q[1334] 3335 1 ## q[1335] 3225 1 ## q[1336] 3335 1 ## q[1337] 3335 1 ## q[1338] 3148 1 ## q[1339] 3225 1 ## q[1340] 3225 1 ## q[1341] 3225 1 ## q[1342] 3225 1 ## q[1343] 3225 1 ## q[1344] 3225 1 ## q[1345] 3225 1 ## q[1346] 3335 1 ## q[1347] 3335 1 ## q[1348] 3335 1 ## q[1349] 3225 1 ## q[1350] 3225 1 ## q[1351] 3335 1 ## q[1352] 3148 1 ## q[1353] 3335 1 ## q[1354] 3225 1 ## q[1355] 3225 1 ## q[1356] 3148 1 ## q[1357] 3278 1 ## q[1358] 3278 1 ## q[1359] 3278 1 ## q[1360] 3278 1 ## q[1361] 3278 1 ## q[1362] 3121 1 ## q[1363] 3200 1 ## q[1364] 3121 1 ## q[1365] 3121 1 ## q[1366] 3200 1 ## q[1367] 3278 1 ## q[1368] 3278 1 ## q[1369] 3278 1 ## q[1370] 3278 1 ## q[1371] 3278 1 ## q[1372] 3278 1 ## q[1373] 3121 1 ## q[1374] 3278 1 ## q[1375] 3278 1 ## q[1376] 3278 1 ## q[1377] 3278 1 ## q[1378] 3121 1 ## q[1379] 3200 1 ## q[1380] 3121 1 ## q[1381] 3121 1 ## q[1382] 3121 1 ## q[1383] 3278 1 ## q[1384] 3278 1 ## q[1385] 3121 1 ## q[1386] 3200 1 ## q[1387] 3200 1 ## q[1388] 3278 1 ## q[1389] 3278 1 ## q[1390] 3121 1 ## q[1391] 3200 1 ## q[1392] 3278 1 ## q[1393] 3278 1 ## q[1394] 3278 1 ## q[1395] 3278 1 ## q[1396] 3200 1 ## q[1397] 3200 1 ## q[1398] 3278 1 ## q[1399] 3200 1 ## q[1400] 3121 1 ## q[1401] 3278 1 ## q[1402] 3278 1 ## q[1403] 3121 1 ## q[1404] 3278 1 ## q[1405] 3278 1 ## q[1406] 3278 1 ## q[1407] 3200 1 ## q[1408] 3558 1 ## q[1409] 3093 1 ## q[1410] 3093 1 ## q[1411] 3253 1 ## q[1412] 3253 1 ## q[1413] 3558 1 ## q[1414] 3093 1 ## q[1415] 3093 1 ## q[1416] 3093 1 ## q[1417] 3093 1 ## q[1418] 3093 1 ## q[1419] 3253 1 ## q[1420] 3253 1 ## q[1421] 3253 1 ## q[1422] 3093 1 ## q[1423] 3253 1 ## q[1424] 3253 1 ## q[1425] 3093 1 ## q[1426] 3093 1 ## q[1427] 3093 1 ## q[1428] 3093 1 ## q[1429] 3093 1 ## q[1430] 3558 1 ## q[1431] 3093 1 ## q[1432] 3558 1 ## q[1433] 3093 1 ## q[1434] 3253 1 ## q[1435] 3093 1 ## q[1436] 3093 1 ## q[1437] 3093 1 ## q[1438] 3253 1 ## q[1439] 3558 1 ## q[1440] 3093 1 ## q[1441] 3253 1 ## q[1442] 3093 1 ## q[1443] 3253 1 ## q[1444] 3093 1 ## q[1445] 3253 1 ## q[1446] 3093 1 ## q[1447] 3093 1 ## q[1448] 3253 1 ## q[1449] 3558 1 ## q[1450] 3253 1 ## q[1451] 3093 1 ## q[1452] 3253 1 ## q[1453] 3253 1 ## q[1454] 3253 1 ## q[1455] 3558 1 ## q[1456] 3093 1 ## q[1457] 3558 1 ## q[1458] 3093 1 ## q[1459] 3093 1 ## q[1460] 3253 1 ## q[1461] 3253 1 ## q[1462] 3093 1 ## q[1463] 3093 1 ## q[1464] 3093 1 ## q[1465] 3093 1 ## q[1466] 3093 1 ## q[1467] 3253 1 ## q[1468] 3093 1 ## q[1469] 3558 1 ## q[1470] 3558 1 ## q[1471] 3093 1 ## q[1472] 3093 1 ## q[1473] 3093 1 ## q[1474] 2870 1 ## q[1475] 2680 1 ## q[1476] 3130 1 ## q[1477] 2870 1 ## q[1478] 2870 1 ## q[1479] 2680 1 ## q[1480] 2870 1 ## q[1481] 2870 1 ## q[1482] 2870 1 ## q[1483] 3130 1 ## q[1484] 3130 1 ## q[1485] 2680 1 ## q[1486] 3130 1 ## q[1487] 3130 1 ## q[1488] 2680 1 ## q[1489] 2870 1 ## q[1490] 2680 1 ## q[1491] 3130 1 ## q[1492] 3211 1 ## q[1493] 2942 1 ## q[1494] 2942 1 ## q[1495] 3211 1 ## q[1496] 3211 1 ## q[1497] 3479 1 ## q[1498] 2942 1 ## q[1499] 2942 1 ## q[1500] 2942 1 ## q[1501] 2942 1 ## q[1502] 3211 1 ## q[1503] 3479 1 ## q[1504] 2942 1 ## q[1505] 2942 1 ## q[1506] 3211 1 ## q[1507] 2942 1 ## q[1508] 2942 1 ## q[1509] 3211 1 ## q[1510] 2942 1 ## q[1511] 3211 1 ## q[1512] 3211 1 ## q[1513] 2942 1 ## q[1514] 3479 1 ## q[1515] 3211 1 ## q[1516] 3211 1 ## q[1517] 2942 1 ## q[1518] 3211 1 ## q[1519] 2942 1 ## q[1520] 3479 1 ## q[1521] 2942 1 ## q[1522] 3479 1 ## q[1523] 2942 1 ## q[1524] 3211 1 ## q[1525] 2942 1 ## q[1526] 3211 1 ## q[1527] 2942 1 ## q[1528] 2942 1 ## q[1529] 2942 1 ## q[1530] 2942 1 ## q[1531] 3211 1 ## q[1532] 2942 1 ## q[1533] 3211 1 ## q[1534] 2942 1 ## q[1535] 3479 1 ## q[1536] 3211 1 ## q[1537] 2942 1 ## q[1538] 2942 1 ## q[1539] 2942 1 ## q[1540] 3479 1 ## q[1541] 2593 1 ## q[1542] 2593 1 ## q[1543] 2593 1 ## q[1544] 2775 1 ## q[1545] 2330 1 ## q[1546] 2330 1 ## q[1547] 2775 1 ## q[1548] 2330 1 ## q[1549] 2330 1 ## q[1550] 2775 1 ## q[1551] 2593 1 ## q[1552] 2593 1 ## q[1553] 2593 1 ## q[1554] 2330 1 ## q[1555] 2775 1 ## q[1556] 2593 1 ## q[1557] 2593 1 ## q[1558] 2593 1 ## q[1559] 2593 1 ## q[1560] 2330 1 ## q[1561] 2593 1 ## q[1562] 2330 1 ## q[1563] 2593 1 ## q[1564] 2330 1 ## q[1565] 2593 1 ## q[1566] 2593 1 ## q[1567] 2330 1 ## q[1568] 2593 1 ## q[1569] 2330 1 ## q[1570] 2593 1 ## q[1571] 2775 1 ## q[1572] 2330 1 ## q[1573] 2593 1 ## q[1574] 2330 1 ## q[1575] 2330 1 ## q[1576] 2593 1 ## q[1577] 2330 1 ## q[1578] 2593 1 ## q[1579] 2775 1 ## q[1580] 2775 1 ## q[1581] 2593 1 ## q[1582] 2775 1 ## q[1583] 2330 1 ## q[1584] 2775 1 ## q[1585] 2593 1 ## q[1586] 2593 1 ## q[1587] 2330 1 ## q[1588] 2330 1 ## q[1589] 2330 1 ## q[1590] 2593 1 ## q[1591] 2775 1 ## q[1592] 2593 1 ## q[1593] 2593 1 ## q[1594] 2593 1 ## q[1595] 2593 1 ## q[1596] 2593 1 ## q[1597] 2330 1 ## q[1598] 2593 1 ## q[1599] 2593 1 ## q[1600] 2775 1 ## q[1601] 2775 1 ## q[1602] 2330 1 ## q[1603] 2330 1 ## q[1604] 2593 1 ## q[1605] 2593 1 ## q[1606] 2330 1 ## q[1607] 2775 1 ## q[1608] 2330 1 ## q[1609] 2593 1 ## q[1610] 2330 1 ## q[1611] 2593 1 ## q[1612] 2593 1 ## q[1613] 2593 1 ## q[1614] 2775 1 ## q[1615] 2775 1 ## q[1616] 2404 1 ## q[1617] 2404 1 ## q[1618] 2772 1 ## q[1619] 2404 1 ## q[1620] 2404 1 ## q[1621] 2615 1 ## q[1622] 2772 1 ## q[1623] 2404 1 ## q[1624] 2772 1 ## q[1625] 2404 1 ## q[1626] 2404 1 ## q[1627] 2404 1 ## q[1628] 2840 1 ## q[1629] 2840 1 ## q[1630] 3149 1 ## q[1631] 3141 1 ## q[1632] 3141 1 ## q[1633] 3149 1 ## q[1634] 3141 1 ## q[1635] 3141 1 ## q[1636] 3141 1 ## q[1637] 3141 1 ## q[1638] 2840 1 ## q[1639] 3141 1 ## q[1640] 2840 1 ## q[1641] 3149 1 ## q[1642] 3141 1 ## q[1643] 3141 1 ## q[1644] 3141 1 ## q[1645] 2840 1 ## q[1646] 3141 1 ## q[1647] 3149 1 ## q[1648] 3141 1 ## q[1649] 2840 1 ## q[1650] 2840 1 ## q[1651] 3218 1 ## q[1652] 3218 1 ## q[1653] 3194 1 ## q[1654] 3287 1 ## q[1655] 3287 1 ## q[1656] 3194 1 ## q[1657] 3218 1 ## q[1658] 3287 1 ## q[1659] 3287 1 ## q[1660] 3218 1 ## q[1661] 3287 1 ## q[1662] 3287 1 ## q[1663] 3218 1 ## q[1664] 3218 1 ## q[1665] 3287 1 ## q[1666] 3218 1 ## q[1667] 3194 1 ## q[1668] 3218 1 ## q[1669] 3287 1 ## q[1670] 3287 1 ## q[1671] 3218 1 ## q[1672] 3287 1 ## q[1673] 3218 1 ## q[1674] 3194 1 ## q[1675] 3287 1 ## q[1676] 3287 1 ## q[1677] 3287 1 ## q[1678] 3287 1 ## q[1679] 3287 1 ## q[1680] 3218 1 ## q[1681] 3287 1 ## q[1682] 3218 1 ## q[1683] 3194 1 ## q[1684] 3218 1 ## q[1685] 3194 1 ## q[1686] 3287 1 ## q[1687] 3287 1 ## q[1688] 3253 1 ## q[1689] 3208 1 ## q[1690] 3208 1 ## q[1691] 3208 1 ## q[1692] 3253 1 ## q[1693] 3253 1 ## q[1694] 3253 1 ## q[1695] 3253 1 ## q[1696] 3595 1 ## q[1697] 3208 1 ## q[1698] 3208 1 ## q[1699] 3208 1 ## q[1700] 3208 1 ## q[1701] 3595 1 ## q[1702] 3208 1 ## q[1703] 3208 1 ## q[1704] 3208 1 ## q[1705] 3253 1 ## q[1706] 3595 1 ## q[1707] 3208 1 ## q[1708] 3208 1 ## q[1709] 3208 1 ## q[1710] 3208 1 ## q[1711] 3253 1 ## q[1712] 3208 1 ## q[1713] 3253 1 ## q[1714] 3253 1 ## q[1715] 3208 1 ## q[1716] 3253 1 ## q[1717] 3253 1 ## q[1718] 3253 1 ## q[1719] 3208 1 ## q[1720] 3595 1 ## q[1721] 3208 1 ## q[1722] 3208 1 ## q[1723] 3208 1 ## q[1724] 3253 1 ## q[1725] 3253 1 ## q[1726] 3208 1 ## q[1727] 3595 1 ## q[1728] 3208 1 ## q[1729] 3208 1 ## q[1730] 3208 1 ## q[1731] 3208 1 ## q[1732] 3208 1 ## q[1733] 3253 1 ## q[1734] 3253 1 ## q[1735] 3595 1 ## q[1736] 3208 1 ## q[1737] 3253 1 ## q[1738] 3208 1 ## q[1739] 3253 1 ## q[1740] 3595 1 ## q[1741] 3253 1 ## q[1742] 3208 1 ## q[1743] 3208 1 ## q[1744] 3208 1 ## q[1745] 3595 1 ## q[1746] 3208 1 ## q[1747] 3236 1 ## q[1748] 3236 1 ## q[1749] 3583 1 ## q[1750] 3236 1 ## q[1751] 3223 1 ## q[1752] 3223 1 ## q[1753] 3223 1 ## q[1754] 3223 1 ## q[1755] 3583 1 ## q[1756] 3236 1 ## q[1757] 3236 1 ## q[1758] 3236 1 ## q[1759] 3236 1 ## q[1760] 3236 1 ## q[1761] 3583 1 ## q[1762] 3583 1 ## q[1763] 3236 1 ## q[1764] 3236 1 ## q[1765] 3236 1 ## q[1766] 3236 1 ## q[1767] 3236 1 ## q[1768] 3236 1 ## q[1769] 3223 1 ## q[1770] 3583 1 ## q[1771] 3223 1 ## q[1772] 3583 1 ## q[1773] 3236 1 ## q[1774] 3223 1 ## q[1775] 3223 1 ## q[1776] 3236 1 ## q[1777] 3223 1 ## q[1778] 3236 1 ## q[1779] 3223 1 ## q[1780] 3236 1 ## q[1781] 3223 1 ## q[1782] 3236 1 ## q[1783] 3583 1 ## q[1784] 3236 1 ## q[1785] 3236 1 ## q[1786] 3236 1 ## q[1787] 3236 1 ## q[1788] 3236 1 ## q[1789] 3223 1 ## q[1790] 3236 1 ## q[1791] 3236 1 ## q[1792] 3583 1 ## q[1793] 3223 1 ## q[1794] 3236 1 ## q[1795] 3236 1 ## q[1796] 3223 1 ## q[1797] 3223 1 ## q[1798] 3236 1 ## q[1799] 3223 1 ## q[1800] 3236 1 ## q[1801] 3583 1 ## q[1802] 3320 1 ## q[1803] 2971 1 ## q[1804] 2971 1 ## q[1805] 2971 1 ## q[1806] 2971 1 ## q[1807] 2971 1 ## q[1808] 2971 1 ## q[1809] 3320 1 ## q[1810] 3059 1 ## q[1811] 3320 1 ## q[1812] 3320 1 ## q[1813] 3320 1 ## q[1814] 3059 1 ## q[1815] 3059 1 ## q[1816] 2971 1 ## q[1817] 2971 1 ## q[1818] 2971 1 ## q[1819] 2971 1 ## q[1820] 2971 1 ## q[1821] 3320 1 ## q[1822] 2971 1 ## q[1823] 2971 1 ## q[1824] 3320 1 ## q[1825] 2971 1 ## q[1826] 2971 1 ## q[1827] 2971 1 ## q[1828] 3320 1 ## q[1829] 2971 1 ## q[1830] 3320 1 ## q[1831] 2971 1 ## q[1832] 2971 1 ## q[1833] 3320 1 ## q[1834] 3320 1 ## q[1835] 2971 1 ## q[1836] 2971 1 ## q[1837] 2971 1 ## q[1838] 2971 1 ## q[1839] 3059 1 ## q[1840] 3320 1 ## q[1841] 3320 1 ## q[1842] 3320 1 ## q[1843] 2971 1 ## q[1844] 3320 1 ## q[1845] 3059 1 ## q[1846] 3320 1 ## q[1847] 3320 1 ## q[1848] 2971 1 ## q[1849] 3320 1 ## q[1850] 2971 1 ## q[1851] 2971 1 ## q[1852] 3320 1 ## q[1853] 3059 1 ## q[1854] 3059 1 ## q[1855] 2971 1 ## q[1856] 3059 1 ## q[1857] 3059 1 ## q[1858] 2971 1 ## q[1859] 2971 1 ## q[1860] 2971 1 ## q[1861] 3320 1 ## q[1862] 2971 1 ## q[1863] 3320 1 ## q[1864] 3320 1 ## q[1865] 2971 1 ## q[1866] 3059 1 ## q[1867] 2971 1 ## q[1868] 2971 1 ## q[1869] 2971 1 ## q[1870] 2971 1 ## q[1871] 2971 1 ## q[1872] 2971 1 ## q[1873] 3320 1 ## q[1874] 2971 1 ## q[1875] 3059 1 ## q[1876] 3059 1 ## q[1877] 2971 1 ## q[1878] 3059 1 ## q[1879] 3320 1 ## q[1880] 2971 1 ## q[1881] 2971 1 ## q[1882] 3320 1 ## q[1883] 3059 1 ## q[1884] 2971 1 ## q[1885] 2971 1 ## q[1886] 2971 1 ## q[1887] 2971 1 ## q[1888] 2971 1 ## q[1889] 3059 1 ## q[1890] 2858 1 ## q[1891] 2858 1 ## q[1892] 2858 1 ## q[1893] 2858 1 ## q[1894] 3178 1 ## q[1895] 3178 1 ## q[1896] 3178 1 ## q[1897] 3426 1 ## q[1898] 2858 1 ## q[1899] 2858 1 ## q[1900] 2858 1 ## q[1901] 3178 1 ## q[1902] 2858 1 ## q[1903] 3178 1 ## q[1904] 2858 1 ## q[1905] 2858 1 ## q[1906] 3178 1 ## q[1907] 2858 1 ## q[1908] 3178 1 ## q[1909] 3178 1 ## q[1910] 2858 1 ## q[1911] 3426 1 ## q[1912] 3178 1 ## q[1913] 3178 1 ## q[1914] 3178 1 ## q[1915] 2858 1 ## q[1916] 3178 1 ## q[1917] 2858 1 ## q[1918] 2858 1 ## q[1919] 3178 1 ## q[1920] 2858 1 ## q[1921] 2858 1 ## q[1922] 2858 1 ## q[1923] 2858 1 ## q[1924] 2858 1 ## q[1925] 2858 1 ## q[1926] 2858 1 ## q[1927] 3178 1 ## q[1928] 3426 1 ## q[1929] 2858 1 ## q[1930] 3426 1 ## q[1931] 2858 1 ## q[1932] 3426 1 ## q[1933] 2858 1 ## q[1934] 2858 1 ## q[1935] 2858 1 ## q[1936] 2858 1 ## q[1937] 2814 1 ## q[1938] 2892 1 ## q[1939] 2892 1 ## q[1940] 2350 1 ## q[1941] 2350 1 ## q[1942] 2350 1 ## q[1943] 2350 1 ## q[1944] 2350 1 ## q[1945] 2814 1 ## q[1946] 2350 1 ## q[1947] 2814 1 ## q[1948] 2814 1 ## q[1949] 2814 1 ## q[1950] 2350 1 ## q[1951] 2350 1 ## q[1952] 2350 1 ## q[1953] 2350 1 ## q[1954] 2350 1 ## q[1955] 2350 1 ## q[1956] 2350 1 ## q[1957] 2892 1 ## q[1958] 2350 1 ## q[1959] 2793 1 ## q[1960] 2333 1 ## q[1961] 2869 1 ## q[1962] 2333 1 ## q[1963] 2333 1 ## q[1964] 2793 1 ## q[1965] 2869 1 ## q[1966] 2793 1 ## q[1967] 2333 1 ## q[1968] 2333 1 ## q[1969] 2333 1 ## q[1970] 2333 1 ## q[1971] 2333 1 ## q[1972] 2333 1 ## q[1973] 2793 1 ## q[1974] 2333 1 ## q[1975] 2333 1 ## q[1976] 2793 1 ## q[1977] 2333 1 ## q[1978] 2793 1 ## q[1979] 2869 1 ## q[1980] 2793 1 ## q[1981] 2793 1 ## q[1982] 2793 1 ## q[1983] 2333 1 ## q[1984] 2333 1 ## q[1985] 2869 1 ## q[1986] 2333 1 ## q[1987] 2869 1 ## q[1988] 2793 1 ## q[1989] 2333 1 ## q[1990] 2333 1 ## q[1991] 2793 1 ## q[1992] 2333 1 ## q[1993] 2333 1 ## q[1994] 2869 1 ## q[1995] 2333 1 ## q[1996] 2793 1 ## q[1997] 2333 1 ## q[1998] 2333 1 ## q[1999] 2333 1 ## q[2000] 3238 1 ## q[2001] 3230 1 ## q[2002] 3592 1 ## q[2003] 3238 1 ## q[2004] 3238 1 ## q[2005] 3592 1 ## q[2006] 3238 1 ## q[2007] 3230 1 ## q[2008] 3230 1 ## q[2009] 3592 1 ## q[2010] 3230 1 ## q[2011] 3230 1 ## q[2012] 3238 1 ## q[2013] 3230 1 ## q[2014] 3230 1 ## q[2015] 3230 1 ## q[2016] 3238 1 ## q[2017] 3230 1 ## q[2018] 3230 1 ## q[2019] 3238 1 ## q[2020] 3230 1 ## q[2021] 3238 1 ## q[2022] 3238 1 ## q[2023] 3230 1 ## q[2024] 3238 1 ## q[2025] 3592 1 ## q[2026] 3238 1 ## q[2027] 3238 1 ## q[2028] 3230 1 ## q[2029] 3238 1 ## q[2030] 3592 1 ## q[2031] 3230 1 ## q[2032] 3230 1 ## q[2033] 3592 1 ## q[2034] 3230 1 ## q[2035] 3230 1 ## q[2036] 3592 1 ## q[2037] 3238 1 ## q[2038] 3230 1 ## q[2039] 3238 1 ## q[2040] 3238 1 ## q[2041] 3230 1 ## q[2042] 3592 1 ## q[2043] 3238 1 ## q[2044] 3230 1 ## q[2045] 3230 1 ## q[2046] 3230 1 ## q[2047] 3230 1 ## q[2048] 3238 1 ## q[2049] 3238 1 ## q[2050] 3230 1 ## q[2051] 3230 1 ## q[2052] 3238 1 ## q[2053] 3592 1 ## q[2054] 3238 1 ## q[2055] 3230 1 ## q[2056] 3230 1 ## q[2057] 3235 1 ## q[2058] 3198 1 ## q[2059] 3570 1 ## q[2060] 3198 1 ## q[2061] 3198 1 ## q[2062] 3235 1 ## q[2063] 3198 1 ## q[2064] 3235 1 ## q[2065] 3198 1 ## q[2066] 3235 1 ## q[2067] 3198 1 ## q[2068] 3235 1 ## q[2069] 3235 1 ## q[2070] 3198 1 ## q[2071] 3235 1 ## q[2072] 3235 1 ## q[2073] 3570 1 ## q[2074] 3570 1 ## q[2075] 3198 1 ## q[2076] 3198 1 ## q[2077] 3235 1 ## q[2078] 3570 1 ## q[2079] 3235 1 ## q[2080] 3235 1 ## q[2081] 3198 1 ## q[2082] 3570 1 ## q[2083] 3198 1 ## q[2084] 3235 1 ## q[2085] 3198 1 ## q[2086] 3570 1 ## q[2087] 3235 1 ## q[2088] 3570 1 ## q[2089] 3260 1 ## q[2090] 3152 1 ## q[2091] 3152 1 ## q[2092] 3152 1 ## q[2093] 3260 1 ## q[2094] 3582 1 ## q[2095] 3260 1 ## q[2096] 3260 1 ## q[2097] 3582 1 ## q[2098] 3152 1 ## q[2099] 3152 1 ## q[2100] 3152 1 ## q[2101] 3260 1 ## q[2102] 3152 1 ## q[2103] 3152 1 ## q[2104] 3152 1 ## q[2105] 3260 1 ## q[2106] 3152 1 ## q[2107] 3260 1 ## q[2108] 3152 1 ## q[2109] 3260 1 ## q[2110] 3260 1 ## q[2111] 3152 1 ## q[2112] 3152 1 ## q[2113] 3152 1 ## q[2114] 3260 1 ## q[2115] 3260 1 ## q[2116] 3260 1 ## q[2117] 3152 1 ## q[2118] 3152 1 ## q[2119] 3152 1 ## q[2120] 3582 1 ## q[2121] 3582 1 ## q[2122] 3152 1 ## q[2123] 3260 1 ## q[2124] 3152 1 ## q[2125] 3260 1 ## q[2126] 3152 1 ## q[2127] 3582 1 ## q[2128] 3152 1 ## q[2129] 3152 1 ## q[2130] 3260 1 ## q[2131] 3582 1 ## q[2132] 3582 1 ## q[2133] 3152 1 ## q[2134] 3260 1 ## q[2135] 3152 1 ## q[2136] 3260 1 ## q[2137] 3582 1 ## q[2138] 3152 1 ## q[2139] 3152 1 ## q[2140] 3152 1 ## q[2141] 3582 1 ## q[2142] 2905 1 ## q[2143] 3075 1 ## q[2144] 3364 1 ## q[2145] 3364 1 ## q[2146] 2905 1 ## q[2147] 3075 1 ## q[2148] 3075 1 ## q[2149] 3075 1 ## q[2150] 3075 1 ## q[2151] 2905 1 ## q[2152] 3075 1 ## q[2153] 3075 1 ## q[2154] 2905 1 ## q[2155] 3075 1 ## q[2156] 2905 1 ## q[2157] 3364 1 ## q[2158] 2905 1 ## q[2159] 2905 1 ## q[2160] 3075 1 ## q[2161] 3075 1 ## q[2162] 3364 1 ## q[2163] 3075 1 ## q[2164] 3364 1 ## q[2165] 2905 1 ## q[2166] 3075 1 ## q[2167] 3075 1 ## q[2168] 2905 1 ## q[2169] 3075 1 ## q[2170] 3075 1 ## q[2171] 2905 1 ## q[2172] 3075 1 ## q[2173] 2610 1 ## q[2174] 2610 1 ## q[2175] 2441 1 ## q[2176] 2816 1 ## q[2177] 2610 1 ## q[2178] 2610 1 ## q[2179] 2610 1 ## q[2180] 2610 1 ## q[2181] 2816 1 ## q[2182] 2441 1 ## q[2183] 2441 1 ## q[2184] 2441 1 ## q[2185] 2610 1 ## q[2186] 2441 1 ## q[2187] 2610 1 ## q[2188] 2610 1 ## q[2189] 2610 1 ## q[2190] 2610 1 ## q[2191] 2610 1 ## q[2192] 2610 1 ## q[2193] 2816 1 ## q[2194] 2610 1 ## q[2195] 2610 1 ## q[2196] 2716 1 ## q[2197] 2716 1 ## q[2198] 2716 1 ## q[2199] 2716 1 ## q[2200] 3109 1 ## q[2201] 3314 1 ## q[2202] 3109 1 ## q[2203] 3314 1 ## q[2204] 2716 1 ## q[2205] 2716 1 ## q[2206] 2716 1 ## q[2207] 2716 1 ## q[2208] 2716 1 ## q[2209] 3109 1 ## q[2210] 2716 1 ## q[2211] 2716 1 ## q[2212] 3109 1 ## q[2213] 2716 1 ## q[2214] 2716 1 ## q[2215] 2716 1 ## q[2216] 3314 1 ## q[2217] 3109 1 ## q[2218] 3109 1 ## q[2219] 3109 1 ## q[2220] 3109 1 ## q[2221] 2716 1 ## q[2222] 3314 1 ## q[2223] 3314 1 ## q[2224] 2716 1 ## q[2225] 3314 1 ## q[2226] 3314 1 ## q[2227] 2716 1 ## q[2228] 2716 1 ## q[2229] 3109 1 ## q[2230] 3109 1 ## q[2231] 3314 1 ## q[2232] 2716 1 ## q[2233] 2716 1 ## q[2234] 2716 1 ## q[2235] 3314 1 ## q[2236] 3314 1 ## q[2237] 2716 1 ## q[2238] 3314 1 ## q[2239] 3109 1 ## q[2240] 2716 1 ## q[2241] 3109 1 ## q[2242] 2716 1 ## q[2243] 2716 1 ## q[2244] 2716 1 ## q[2245] 2716 1 ## q[2246] 3314 1 ## q[2247] 2331 1 ## q[2248] 2323 1 ## q[2249] 2323 1 ## q[2250] 2323 1 ## q[2251] 2038 1 ## q[2252] 2323 1 ## q[2253] 2038 1 ## q[2254] 2323 1 ## q[2255] 2323 1 ## q[2256] 2323 1 ## q[2257] 2038 1 ## q[2258] 2038 1 ## q[2259] 2323 1 ## q[2260] 2323 1 ## q[2261] 2038 1 ## q[2262] 2038 1 ## q[2263] 2038 1 ## q[2264] 2323 1 ## q[2265] 2038 1 ## q[2266] 2331 1 ## q[2267] 2323 1 ## q[2268] 2038 1 ## q[2269] 3044 1 ## q[2270] 2867 1 ## q[2271] 2867 1 ## q[2272] 3044 1 ## q[2273] 3044 1 ## q[2274] 3328 1 ## q[2275] 3044 1 ## q[2276] 2867 1 ## q[2277] 3044 1 ## q[2278] 3044 1 ## q[2279] 2867 1 ## q[2280] 2867 1 ## q[2281] 3044 1 ## q[2282] 2867 1 ## q[2283] 3328 1 ## q[2284] 2867 1 ## q[2285] 3044 1 ## q[2286] 2867 1 ## q[2287] 3328 1 ## q[2288] 3328 1 ## q[2289] 3044 1 ## q[2290] 3044 1 ## q[2291] 3328 1 ## q[2292] 2867 1 ## q[2293] 2867 1 ## q[2294] 3328 1 ## q[2295] 2867 1 ## q[2296] 3044 1 ## q[2297] 3044 1 ## q[2298] 3044 1 ## q[2299] 3044 1 ## q[2300] 2867 1 ## q[2301] 2867 1 ## q[2302] 3044 1 ## q[2303] 2867 1 ## q[2304] 3328 1 ## q[2305] 3044 1 ## q[2306] 3044 1 ## q[2307] 3014 1 ## q[2308] 3148 1 ## q[2309] 2558 1 ## q[2310] 2558 1 ## q[2311] 3148 1 ## q[2312] 2558 1 ## q[2313] 2558 1 ## q[2314] 2558 1 ## q[2315] 3014 1 ## q[2316] 3148 1 ## q[2317] 3014 1 ## q[2318] 2558 1 ## q[2319] 2558 1 ## q[2320] 3014 1 ## q[2321] 2558 1 ## q[2322] 3148 1 ## q[2323] 2558 1 ## q[2324] 2558 1 ## q[2325] 3014 1 ## q[2326] 2558 1 ## q[2327] 3148 1 ## q[2328] 3014 1 ## q[2329] 2558 1 ## q[2330] 3219 1 ## q[2331] 2829 1 ## q[2332] 2829 1 ## q[2333] 2829 1 ## q[2334] 2829 1 ## q[2335] 3219 1 ## q[2336] 3219 1 ## q[2337] 2971 1 ## q[2338] 3219 1 ## q[2339] 3219 1 ## q[2340] 2971 1 ## q[2341] 2829 1 ## q[2342] 2829 1 ## q[2343] 2829 1 ## q[2344] 2829 1 ## q[2345] 3219 1 ## q[2346] 2971 1 ## q[2347] 2829 1 ## q[2348] 2829 1 ## q[2349] 2829 1 ## q[2350] 2829 1 ## q[2351] 3219 1 ## q[2352] 2829 1 ## q[2353] 3219 1 ## q[2354] 2829 1 ## q[2355] 3219 1 ## q[2356] 3219 1 ## q[2357] 2829 1 ## q[2358] 2829 1 ## q[2359] 2829 1 ## q[2360] 2829 1 ## q[2361] 3219 1 ## q[2362] 3219 1 ## q[2363] 2829 1 ## q[2364] 2971 1 ## q[2365] 2829 1 ## q[2366] 3219 1 ## q[2367] 2829 1 ## q[2368] 2829 1 ## q[2369] 3219 1 ## q[2370] 2971 1 ## q[2371] 2971 1 ## q[2372] 2829 1 ## q[2373] 3219 1 ## q[2374] 2829 1 ## q[2375] 3219 1 ## q[2376] 2829 1 ## q[2377] 2971 1 ## q[2378] 2829 1 ## q[2379] 2829 1 ## q[2380] 2829 1 ## q[2381] 2829 1 ## q[2382] 2829 1 ## q[2383] 3219 1 ## q[2384] 2971 1 ## q[2385] 3219 1 ## q[2386] 3219 1 ## q[2387] 2829 1 ## q[2388] 2829 1 ## q[2389] 2829 1 ## q[2390] 3219 1 ## q[2391] 2971 1 ## q[2392] 3219 1 ## q[2393] 2829 1 ## q[2394] 2829 1 ## q[2395] 2971 1 ## q[2396] 2971 1 ## lp__ 1724 1 ## ## Samples were drawn using NUTS(diag_e) at Mon May 18 16:55:44 2020. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). 檢查模型參數的收斂情況 library(bayesplot) color_scheme_set(\u0026quot;mix-brightblue-gray\u0026quot;) posterior5.5 \u0026lt;- rstan::extract(fit1, inc_warmup = TRUE, permuted = FALSE) p \u0026lt;- mcmc_trace(posterior5.5, n_warmup = 0, pars = c(\u0026quot;b[1]\u0026quot;, \u0026quot;b[2]\u0026quot;, \u0026quot;b[3]\u0026quot;, \u0026quot;lp__\u0026quot;), facet_args = list(nrow = 2, labeller = label_parsed)) p Figure 1: 用 bayesplot包數繪製的模型5-5的MCMC鏈式軌跡圖 (trace plot)。 p \u0026lt;- mcmc_acf_bar(posterior5.5, pars = c(\u0026quot;b[1]\u0026quot;, \u0026quot;b[2]\u0026quot;, \u0026quot;b[3]\u0026quot;, \u0026quot;lp__\u0026quot;)) p Figure 2: 用 bayesplot包數繪製的事後樣本自相關圖(autocorrelation)。 p \u0026lt;- mcmc_dens_overlay(posterior5.5, pars = c(\u0026quot;b[1]\u0026quot;, \u0026quot;b[2]\u0026quot;, \u0026quot;b[3]\u0026quot;, \u0026quot;OR1\u0026quot;, \u0026quot;OR2\u0026quot;, \u0026quot;lp__\u0026quot;), color_chains = T) p Figure 3: 用 bayesplot包數繪製的事後樣本密度分佈圖。 檢查模型的擬合情況 ms \u0026lt;- rstan::extract(fit1) set.seed(123) logistic \u0026lt;- function(x) 1/(1+exp(-x)) X \u0026lt;- 30:200 q_qua \u0026lt;- logistic(t(sapply(1:length(X), function(i) { q_mcmc \u0026lt;- ms$b[,1] + ms$b[,3]*X[i]/200 quantile(q_mcmc, probs=c(0.1, 0.5, 0.9)) }))) d_est \u0026lt;- data.frame(X, q_qua) colnames(d_est) \u0026lt;- c(\u0026#39;X\u0026#39;, \u0026#39;p10\u0026#39;, \u0026#39;p50\u0026#39;, \u0026#39;p90\u0026#39;) d$A \u0026lt;- as.factor(d$A) p \u0026lt;- ggplot(d_est, aes(x=X, y=p50)) p \u0026lt;- p + theme_bw(base_size=18) p \u0026lt;- p + geom_ribbon(aes(ymin=p10, ymax=p90), fill=\u0026#39;black\u0026#39;, alpha=2/6) p \u0026lt;- p + geom_line(size=1) p \u0026lt;- p + geom_point(data=subset(d, A==0 \u0026amp; Weather==\u0026#39;A\u0026#39;), aes(x=Score, y=Y, color=A), position=position_jitter(w=0, h=0.1), size=1) p \u0026lt;- p + labs(x=\u0026#39;Score\u0026#39;, y=\u0026#39;q\u0026#39;) p \u0026lt;- p + scale_color_manual(values=c(\u0026#39;black\u0026#39;)) p \u0026lt;- p + scale_y_continuous(breaks=seq(0, 1, 0.2)) p \u0026lt;- p + xlim(30, 200) p Figure 4: 不喜歡打工，且天氣晴天的情況下，分數在 30-200 點之間的學生的出勤概率 q 和它的 80% 可信區間範圍。圖中黑色實線是預測概率的事後分佈的中央值，灰色帶是可信區間範圍。 # ggsave(file=\u0026#39;output/fig5-9.png\u0026#39;, plot=p, dpi=300, w=4.5, h=3) 圖4試圖把分數範圍在 30-200 之間的學生中，通過模型計算獲得的，在天氣晴朗，且不愛打工的孩子們的事後出勤概率的預測值(黑色實線)，和它的事後概率80%可信區間，以及對應的實際觀測值的結果(黑點)。但是，當預測變量越來越多，模型結果的可視化變得越來越困難。下面我們介紹兩種常見的評價邏輯回歸擬合結果的可視化圖。\n首先是圖 5 顯示的事後出勤概率，和實際觀察出勤結果之間的關係圖。在這個圖中，橫軸是 \\(q[i]\\) 的事後分佈的中央值(每名學生都有自己的事後出勤概率預測，它的中央值)，縱軸是該名學生實際是否在該次課上出勤的觀察結果。如果模型擬合的理想的話，那麼在 \\(Y=0\\)，也就是圖中的下半部分，大多數的預測點應該靠近概率較低的部分(也就是靠近左側)，同時，\\(y = 1\\) 的部分預測概率應該大多數在靠近左側的部分。此圖其實提示我們該模型的擬合效果不理想。不能明顯地將出勤與不出勤較爲準確地區分開來。\nset.seed(123) ms \u0026lt;- rstan::extract(fit1) d_qua \u0026lt;- t(apply(ms$q, 2, quantile, prob=c(0.1, 0.5, 0.9))) colnames(d_qua) \u0026lt;- c(\u0026#39;p10\u0026#39;, \u0026#39;p50\u0026#39;, \u0026#39;p90\u0026#39;) d_qua \u0026lt;- data.frame(d, d_qua) d_qua$Y \u0026lt;- as.factor(d_qua$Y) d_qua$A \u0026lt;- as.factor(d_qua$A) p \u0026lt;- ggplot(data=d_qua, aes(x=Y, y=p50)) p \u0026lt;- p + theme_bw(base_size=18) p \u0026lt;- p + coord_flip() p \u0026lt;- p + geom_violin(trim=FALSE, size=1.5, color=\u0026#39;grey80\u0026#39;) p \u0026lt;- p + geom_point(aes(color=A), position=position_jitter(w=0.4, h=0), size=1) p \u0026lt;- p + scale_color_manual(values=c(\u0026#39;grey5\u0026#39;, \u0026#39;grey50\u0026#39;)) p \u0026lt;- p + labs(x=\u0026#39;Y\u0026#39;, y=\u0026#39;q\u0026#39;) p Figure 5: 把計算獲得的事後概率的中央值作爲每名學生是否出勤的概率預測(x 軸)，和實際觀察的出勤結果(y 軸)，繪製的散點圖。其中，灰色的小提琴圖其實是根據獲得的事後概率的中央值的概率密度曲線，繪製的上下對稱的圖形，形似小提琴。 ","date":1549152000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549152000,"objectID":"42d7d2c2085c29cd274a0a9ef00460d4","permalink":"https://wangcc.me/post/logistic-rstan2/","publishdate":"2019-02-03T00:00:00Z","relpermalink":"/post/logistic-rstan2/","section":"post","summary":"Rstan 學習筆記 Chapter 5.3","tags":["Bayesian","Medical Statistics"],"title":"Rstan Wonderful R-(5)","type":"post"},{"authors":null,"categories":["Bayesian","R techniques","statistics"],"content":" 邏輯回歸模型的 Rstan 貝葉斯實現 確定分析目的 確認數據分佈 寫下數學模型表達式 確認收斂效果 邏輯回歸模型的 Rstan 貝葉斯實現 本小節使用的數據，和前一節的出勤率數據很類似:\nd \u0026lt;- read.table(\u0026quot;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap05/input/data-attendance-2.txt\u0026quot;, sep = \u0026quot;,\u0026quot;, header = T) head(d) ## PersonID A Score M Y ## 1 1 0 69 43 38 ## 2 2 1 145 56 40 ## 3 3 0 125 32 24 ## 4 4 1 86 45 33 ## 5 5 1 158 33 23 ## 6 6 0 133 61 60 其中，\nPersonID: 是學生的編號； A, Score: 和之前一樣用來預測出勤率的兩個預測變量，分別是表示是否喜歡打工的 A，和表示對學習本身是否喜歡的評分 (滿分200)； M: 過去三個月內，該名學生一共需要上課的總課時數； Y: 過去三個月內，該名學生實際上出勤的課時數。 確定分析目的 需要回答的問題依然是，\\(A, Score\\) 分別在多大程度上預測學生的出勤率？另外，我們希望知道的是，當需要修的課時數固定的事後，這兩個預測變量能準確提供 \\(Y\\) 的多少信息？\n確認數據分佈 library(ggplot2) library(GGally) set.seed(1) d \u0026lt;- d[, -1] # d \u0026lt;- read.csv(file=\u0026#39;input/data-attendance-2.txt\u0026#39;)[,-1] d$A \u0026lt;- as.factor(d$A) d \u0026lt;- transform(d, ratio=Y/M) N_col \u0026lt;- ncol(d) ggp \u0026lt;- ggpairs(d, upper=\u0026#39;blank\u0026#39;, diag=\u0026#39;blank\u0026#39;, lower=\u0026#39;blank\u0026#39;) for(i in 1:N_col) { x \u0026lt;- d[,i] p \u0026lt;- ggplot(data.frame(x, A=d$A), aes(x)) p \u0026lt;- p + theme_bw(base_size=14) p \u0026lt;- p + theme(axis.text.x=element_text(angle=40, vjust=1, hjust=1)) if (class(x) == \u0026#39;factor\u0026#39;) { p \u0026lt;- p + geom_bar(aes(fill=A), color=\u0026#39;grey20\u0026#39;) } else { bw \u0026lt;- (max(x)-min(x))/10 p \u0026lt;- p + geom_histogram(aes(fill=A), color=\u0026#39;grey20\u0026#39;, binwidth=bw) p \u0026lt;- p + geom_line(eval(bquote(aes(y=..count..*.(bw)))), stat=\u0026#39;density\u0026#39;) } p \u0026lt;- p + geom_label(data=data.frame(x=-Inf, y=Inf, label=colnames(d)[i]), aes(x=x, y=y, label=label), hjust=0, vjust=1) p \u0026lt;- p + scale_fill_manual(values=alpha(c(\u0026#39;white\u0026#39;, \u0026#39;grey40\u0026#39;), 0.5)) ggp \u0026lt;- putPlot(ggp, p, i, i) } zcolat \u0026lt;- seq(-1, 1, length=81) zcolre \u0026lt;- c(zcolat[1:40]+1, rev(zcolat[41:81])) for(i in 1:(N_col-1)) { for(j in (i+1):N_col) { x \u0026lt;- as.numeric(d[,i]) y \u0026lt;- as.numeric(d[,j]) r \u0026lt;- cor(x, y, method=\u0026#39;spearman\u0026#39;, use=\u0026#39;pairwise.complete.obs\u0026#39;) zcol \u0026lt;- lattice::level.colors(r, at=zcolat, col.regions=grey(zcolre)) textcol \u0026lt;- ifelse(abs(r) \u0026lt; 0.4, \u0026#39;grey20\u0026#39;, \u0026#39;white\u0026#39;) ell \u0026lt;- ellipse::ellipse(r, level=0.95, type=\u0026#39;l\u0026#39;, npoints=50, scale=c(.2, .2), centre=c(.5, .5)) p \u0026lt;- ggplot(data.frame(ell), aes(x=x, y=y)) p \u0026lt;- p + theme_bw() + theme( plot.background=element_blank(), panel.grid.major=element_blank(), panel.grid.minor=element_blank(), panel.border=element_blank(), axis.ticks=element_blank() ) p \u0026lt;- p + geom_polygon(fill=zcol, color=zcol) p \u0026lt;- p + geom_text(data=NULL, x=.5, y=.5, label=100*round(r, 2), size=6, col=textcol) ggp \u0026lt;- putPlot(ggp, p, i, j) } } for(j in 1:(N_col-1)) { for(i in (j+1):N_col) { x \u0026lt;- d[,j] y \u0026lt;- d[,i] p \u0026lt;- ggplot(data.frame(x, y, gr=d$A), aes(x=x, y=y, fill=gr, shape=gr)) p \u0026lt;- p + theme_bw(base_size=14) p \u0026lt;- p + theme(axis.text.x=element_text(angle=40, vjust=1, hjust=1)) if (class(x) == \u0026#39;factor\u0026#39;) { p \u0026lt;- p + geom_boxplot(aes(group=x), alpha=3/6, outlier.size=0, fill=\u0026#39;white\u0026#39;) p \u0026lt;- p + geom_point(position=position_jitter(w=0.4, h=0), size=2) } else { p \u0026lt;- p + geom_point(size=2) } p \u0026lt;- p + scale_shape_manual(values=c(21, 24)) p \u0026lt;- p + scale_fill_manual(values=alpha(c(\u0026#39;white\u0026#39;, \u0026#39;grey40\u0026#39;), 0.5)) ggp \u0026lt;- putPlot(ggp, p, i, j) } } ggp Figure 1: 三個變量的分佈觀察圖，相比之前增加了 \\(ratio = Y/M\\) 列。 從圖 1 還可以看出，由於總課時數越多，學生實際出勤的課時數也會越多所以 \\(M, Y\\) 兩者之間理應有很強的正相關。另外可能可以推測的是 \\(Ratio\\) 和是否愛學習的分數之間大概有可能有正相關，和是否喜歡打工之間大概可能有負相關。\n寫下數學模型表達式 在 Stan 的語法中，使用的是反邏輯函數 (inverse logit): inv_logit 來描述下面的邏輯回歸模型 5-4。\n\\[ \\begin{array}{l} q[n] = \\text{inv_logit}(b_1 + b_2 A[n] + b_3Score[n]) \u0026amp; n = 1, 2, \\dots, N \\\\ Y[n] \\sim \\text{Binomial}(M[n], q[n]) \u0026amp; n = 1, 2, \\dots, N \\\\ \\end{array} \\]\n上面的數學模型，可以被翻譯成下面的 Stan 語言:\ndata { int N; int\u0026lt;lower=0, upper=1\u0026gt; A[N]; real\u0026lt;lower=0, upper=1\u0026gt; Score[N]; int\u0026lt;lower=0\u0026gt; M[N]; int\u0026lt;lower=0\u0026gt; Y[N]; } parameters { real b1; real b2; real b3; } transformed parameters { real q[N]; for (n in 1:N) { q[n] = inv_logit(b1 + b2*A[n] + b3*Score[n]); } } model { for (n in 1:N) { Y[n] ~ binomial(M[n], q[n]); } } generated quantities { real y_pred[N]; for (n in 1:N) { y_pred[n] = binomial_rng(M[n], q[n]); } } 下面的 R 代碼用來實現對上面 Stan 模型的擬合:\nlibrary(rstan) d \u0026lt;- read.csv(file=\u0026#39;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap05/input/data-attendance-2.txt\u0026#39;, header = T) data \u0026lt;- list(N=nrow(d), A=d$A, Score=d$Score/200, M=d$M, Y=d$Y) fit \u0026lt;- stan(file=\u0026#39;stanfiles/model5-4.stan\u0026#39;, data=data, seed=1234) ## ## SAMPLING FOR MODEL \u0026#39;model5-4\u0026#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 1.9e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.19 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.148218 seconds (Warm-up) ## Chain 1: 0.146931 seconds (Sampling) ## Chain 1: 0.295149 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL \u0026#39;model5-4\u0026#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 1e-05 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 2: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 2: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 2: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 2: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 2: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 2: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 2: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 2: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 2: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 2: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 2: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.140054 seconds (Warm-up) ## Chain 2: 0.148312 seconds (Sampling) ## Chain 2: 0.288366 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL \u0026#39;model5-4\u0026#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 9e-06 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 3: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 3: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 3: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 3: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 3: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 3: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 3: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 3: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 3: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 3: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 3: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.151245 seconds (Warm-up) ## Chain 3: 0.135369 seconds (Sampling) ## Chain 3: 0.286614 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL \u0026#39;model5-4\u0026#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 8e-06 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 4: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 4: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 4: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 4: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 4: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 4: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 4: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 4: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 4: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 4: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 4: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.144055 seconds (Warm-up) ## Chain 4: 0.144297 seconds (Sampling) ## Chain 4: 0.288352 seconds (Total) ## Chain 4: fit ## Inference for Stan model: model5-4. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% ## b1 0.09 0.01 0.23 -0.33 -0.08 0.08 0.25 0.53 ## b2 -0.62 0.00 0.10 -0.82 -0.69 -0.62 -0.56 -0.44 ## b3 1.91 0.01 0.37 1.19 1.66 1.91 2.17 2.61 ## q[1] 0.68 0.00 0.02 0.63 0.66 0.68 0.70 0.73 ## q[2] 0.70 0.00 0.02 0.67 0.69 0.70 0.71 0.73 ## q[3] 0.78 0.00 0.01 0.76 0.77 0.78 0.79 0.81 ## q[4] 0.57 0.00 0.02 0.53 0.56 0.57 0.59 0.61 ## q[5] 0.73 0.00 0.02 0.69 0.71 0.73 0.74 0.76 ## q[6] 0.80 0.00 0.01 0.77 0.79 0.80 0.80 0.82 ## q[7] 0.76 0.00 0.01 0.73 0.75 0.76 0.77 0.78 ## q[8] 0.70 0.00 0.02 0.67 0.69 0.70 0.72 0.74 ## q[9] 0.81 0.00 0.01 0.79 0.81 0.82 0.82 0.84 ## q[10] 0.81 0.00 0.01 0.79 0.80 0.81 0.82 0.84 ## q[11] 0.69 0.00 0.02 0.66 0.68 0.69 0.70 0.72 ## q[12] 0.80 0.00 0.01 0.78 0.79 0.80 0.81 0.83 ## q[13] 0.64 0.00 0.02 0.61 0.63 0.64 0.65 0.67 ## q[14] 0.76 0.00 0.01 0.73 0.75 0.76 0.77 0.78 ## q[15] 0.76 0.00 0.01 0.73 0.75 0.76 0.76 0.78 ## q[16] 0.60 0.00 0.02 0.56 0.59 0.60 0.61 0.64 ## q[17] 0.76 0.00 0.01 0.74 0.76 0.76 0.77 0.79 ## q[18] 0.70 0.00 0.02 0.67 0.69 0.70 0.72 0.74 ## q[19] 0.86 0.00 0.02 0.83 0.85 0.86 0.88 0.89 ## q[20] 0.72 0.00 0.02 0.69 0.71 0.72 0.73 0.76 ## q[21] 0.57 0.00 0.02 0.53 0.56 0.57 0.59 0.61 ## q[22] 0.62 0.00 0.02 0.59 0.61 0.62 0.63 0.65 ## q[23] 0.62 0.00 0.02 0.58 0.61 0.62 0.63 0.65 ## q[24] 0.70 0.00 0.02 0.67 0.69 0.70 0.71 0.73 ## q[25] 0.64 0.00 0.02 0.61 0.63 0.64 0.65 0.67 ## q[26] 0.67 0.00 0.01 0.64 0.66 0.67 0.68 0.69 ## q[27] 0.77 0.00 0.01 0.75 0.76 0.77 0.78 0.80 ## q[28] 0.77 0.00 0.01 0.75 0.76 0.77 0.78 0.80 ## q[29] 0.84 0.00 0.01 0.81 0.83 0.84 0.85 0.86 ## q[30] 0.76 0.00 0.01 0.74 0.75 0.76 0.77 0.79 ## q[31] 0.74 0.00 0.02 0.70 0.73 0.74 0.75 0.78 ## q[32] 0.54 0.00 0.03 0.49 0.52 0.54 0.56 0.60 ## q[33] 0.69 0.00 0.02 0.66 0.68 0.69 0.70 0.72 ## q[34] 0.66 0.00 0.01 0.63 0.65 0.66 0.67 0.69 ## q[35] 0.78 0.00 0.01 0.76 0.78 0.78 0.79 0.81 ## q[36] 0.79 0.00 0.01 0.77 0.78 0.79 0.80 0.82 ## q[37] 0.62 0.00 0.02 0.58 0.60 0.62 0.63 0.65 ## q[38] 0.76 0.00 0.01 0.73 0.75 0.76 0.77 0.78 ## q[39] 0.72 0.00 0.02 0.69 0.71 0.72 0.73 0.75 ## q[40] 0.72 0.00 0.02 0.68 0.70 0.72 0.73 0.75 ## q[41] 0.79 0.00 0.01 0.77 0.78 0.79 0.80 0.81 ## q[42] 0.80 0.00 0.01 0.77 0.79 0.80 0.80 0.82 ## q[43] 0.78 0.00 0.01 0.75 0.77 0.78 0.79 0.80 ## q[44] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.84 ## q[45] 0.86 0.00 0.02 0.83 0.85 0.86 0.87 0.89 ## q[46] 0.75 0.00 0.01 0.72 0.74 0.75 0.76 0.78 ## q[47] 0.64 0.00 0.03 0.58 0.62 0.64 0.66 0.70 ## q[48] 0.82 0.00 0.01 0.79 0.81 0.82 0.83 0.85 ## q[49] 0.74 0.00 0.02 0.71 0.73 0.74 0.75 0.77 ## q[50] 0.60 0.00 0.02 0.56 0.59 0.60 0.61 0.64 ## y_pred[1] 29.07 0.06 3.26 23.00 27.00 29.00 31.00 35.00 ## y_pred[2] 39.29 0.06 3.57 32.00 37.00 39.00 42.00 46.00 ## y_pred[3] 25.03 0.04 2.38 20.00 23.00 25.00 27.00 29.00 ## y_pred[4] 25.78 0.06 3.48 19.00 23.00 26.00 28.00 32.00 ## y_pred[5] 23.85 0.04 2.62 19.00 22.00 24.00 26.00 29.00 ## y_pred[6] 48.57 0.05 3.17 42.00 47.00 49.00 51.00 54.00 ## y_pred[7] 37.24 0.05 3.03 31.00 35.00 37.00 39.00 43.00 ## y_pred[8] 53.52 0.07 4.13 45.00 51.00 54.00 56.00 61.00 ## y_pred[9] 63.60 0.06 3.57 56.00 61.00 64.00 66.00 70.00 ## y_pred[10] 52.03 0.05 3.21 46.00 50.00 52.00 54.00 58.00 ## y_pred[11] 23.56 0.05 2.73 18.00 22.00 24.00 25.00 29.00 ## y_pred[12] 35.20 0.04 2.67 30.00 33.00 35.00 37.00 40.00 ## y_pred[13] 34.12 0.06 3.61 27.00 32.00 34.00 37.00 41.00 ## y_pred[14] 30.40 0.04 2.79 25.00 29.00 30.50 32.00 36.00 ## y_pred[15] 42.26 0.05 3.28 36.00 40.00 42.00 45.00 48.00 ## y_pred[16] 35.48 0.06 3.84 28.00 33.00 36.00 38.00 43.00 ## y_pred[17] 29.07 0.04 2.72 23.00 27.00 29.00 31.00 34.00 ## y_pred[18] 31.72 0.05 3.20 25.00 30.00 32.00 34.00 38.00 ## y_pred[19] 38.83 0.04 2.41 34.00 37.00 39.00 41.00 43.00 ## y_pred[20] 55.53 0.07 4.16 47.00 53.00 56.00 58.00 63.00 ## y_pred[21] 40.00 0.07 4.41 31.00 37.00 40.00 43.00 49.00 ## y_pred[22] 47.90 0.07 4.38 39.00 45.00 48.00 51.00 56.00 ## y_pred[23] 38.95 0.06 4.00 31.00 36.00 39.00 42.00 46.00 ## y_pred[24] 47.35 0.06 3.91 39.00 45.00 47.00 50.00 55.00 ## y_pred[25] 32.06 0.05 3.41 25.00 30.00 32.00 34.00 39.00 ## y_pred[26] 34.00 0.06 3.48 27.00 32.00 34.00 36.00 41.00 ## y_pred[27] 22.38 0.04 2.33 17.00 21.00 22.00 24.00 27.00 ## y_pred[28] 28.58 0.04 2.66 23.00 27.00 29.00 30.00 34.00 ## y_pred[29] 15.06 0.03 1.56 12.00 14.00 15.00 16.00 18.00 ## y_pred[30] 37.36 0.05 3.04 31.00 35.00 37.00 39.00 43.00 ## y_pred[31] 55.39 0.07 4.07 47.00 53.00 56.00 58.00 63.00 ## y_pred[32] 6.48 0.03 1.72 3.00 5.00 6.00 8.00 10.00 ## y_pred[33] 15.74 0.03 2.23 11.00 14.00 16.00 17.00 20.00 ## y_pred[34] 24.36 0.05 2.90 18.98 22.00 24.00 26.00 30.00 ## y_pred[35] 46.30 0.05 3.26 40.00 44.00 46.00 49.00 52.00 ## y_pred[36] 43.46 0.05 3.05 37.00 41.00 44.00 46.00 49.00 ## y_pred[37] 54.13 0.08 4.80 45.00 51.00 54.00 57.00 63.00 ## y_pred[38] 35.67 0.05 3.05 29.00 34.00 36.00 38.00 41.00 ## y_pred[39] 15.85 0.04 2.13 11.00 14.00 16.00 17.00 20.00 ## y_pred[40] 29.35 0.05 2.99 23.00 27.00 29.00 31.00 35.00 ## y_pred[41] 45.02 0.05 3.19 39.00 43.00 45.00 47.00 51.00 ## y_pred[42] 25.48 0.04 2.30 21.00 24.00 26.00 27.00 30.00 ## y_pred[43] 41.32 0.05 3.11 35.00 39.00 41.00 43.00 47.00 ## y_pred[44] 25.32 0.04 2.25 21.00 24.00 25.00 27.00 29.00 ## y_pred[45] 19.79 0.03 1.74 16.00 19.00 20.00 21.00 23.00 ## y_pred[46] 38.19 0.05 3.23 31.00 36.00 38.00 40.00 44.00 ## y_pred[47] 14.04 0.04 2.40 9.00 12.00 14.00 16.00 19.00 ## y_pred[48] 31.19 0.04 2.42 26.00 30.00 31.00 33.00 36.00 ## y_pred[49] 16.95 0.03 2.12 12.00 16.00 17.00 18.00 21.00 ## y_pred[50] 40.32 0.07 4.27 32.00 37.00 40.00 43.00 48.03 ## lp__ -1389.42 0.04 1.21 -1392.48 -1390.00 -1389.12 -1388.51 -1387.97 ## n_eff Rhat ## b1 1238 1.01 ## b2 1892 1.00 ## b3 1161 1.01 ## q[1] 1504 1.01 ## q[2] 2126 1.00 ## q[3] 2604 1.00 ## q[4] 1454 1.00 ## q[5] 1799 1.00 ## q[6] 2443 1.00 ## q[7] 2449 1.00 ## q[8] 2065 1.00 ## q[9] 1992 1.00 ## q[10] 2036 1.00 ## q[11] 2258 1.00 ## q[12] 2332 1.00 ## q[13] 2512 1.00 ## q[14] 2449 1.00 ## q[15] 2389 1.00 ## q[16] 1727 1.00 ## q[17] 2528 1.00 ## q[18] 1667 1.00 ## q[19] 1350 1.01 ## q[20] 1839 1.00 ## q[21] 1454 1.00 ## q[22] 2083 1.00 ## q[23] 1989 1.00 ## q[24] 2191 1.00 ## q[25] 2480 1.00 ## q[26] 2618 1.00 ## q[27] 2611 1.00 ## q[28] 2611 1.00 ## q[29] 1588 1.00 ## q[30] 2504 1.00 ## q[31] 1684 1.00 ## q[32] 1329 1.01 ## q[33] 2360 1.00 ## q[34] 2628 1.00 ## q[35] 2591 1.00 ## q[36] 2494 1.00 ## q[37] 1945 1.00 ## q[38] 2419 1.00 ## q[39] 1808 1.00 ## q[40] 1785 1.00 ## q[41] 2539 1.00 ## q[42] 2443 1.00 ## q[43] 2622 1.00 ## q[44] 1910 1.00 ## q[45] 1369 1.00 ## q[46] 2260 1.00 ## q[47] 1389 1.01 ## q[48] 1840 1.00 ## q[49] 2073 1.00 ## q[50] 1727 1.00 ## y_pred[1] 3373 1.00 ## y_pred[2] 3405 1.00 ## y_pred[3] 4073 1.00 ## y_pred[4] 3876 1.00 ## y_pred[5] 3753 1.00 ## y_pred[6] 4028 1.00 ## y_pred[7] 3672 1.00 ## y_pred[8] 3782 1.00 ## y_pred[9] 3937 1.00 ## y_pred[10] 3629 1.00 ## y_pred[11] 3662 1.00 ## y_pred[12] 3836 1.00 ## y_pred[13] 3918 1.00 ## y_pred[14] 3986 1.00 ## y_pred[15] 3819 1.00 ## y_pred[16] 3824 1.00 ## y_pred[17] 4008 1.00 ## y_pred[18] 3713 1.00 ## y_pred[19] 3338 1.00 ## y_pred[20] 3292 1.00 ## y_pred[21] 3463 1.00 ## y_pred[22] 3943 1.00 ## y_pred[23] 3884 1.00 ## y_pred[24] 3653 1.00 ## y_pred[25] 4050 1.00 ## y_pred[26] 3863 1.00 ## y_pred[27] 3952 1.00 ## y_pred[28] 3686 1.00 ## y_pred[29] 3824 1.00 ## y_pred[30] 3953 1.00 ## y_pred[31] 3577 1.00 ## y_pred[32] 3957 1.00 ## y_pred[33] 4101 1.00 ## y_pred[34] 3888 1.00 ## y_pred[35] 3968 1.00 ## y_pred[36] 3967 1.00 ## y_pred[37] 3709 1.00 ## y_pred[38] 3809 1.00 ## y_pred[39] 3665 1.00 ## y_pred[40] 3418 1.00 ## y_pred[41] 3991 1.00 ## y_pred[42] 4050 1.00 ## y_pred[43] 4147 1.00 ## y_pred[44] 3604 1.00 ## y_pred[45] 3786 1.00 ## y_pred[46] 3640 1.00 ## y_pred[47] 2938 1.00 ## y_pred[48] 3762 1.00 ## y_pred[49] 4067 1.00 ## y_pred[50] 3501 1.00 ## lp__ 1107 1.00 ## ## Samples were drawn using NUTS(diag_e) at Mon May 18 16:53:57 2020. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). 把獲得的參數事後樣本的均值代入上面的數學模型中可得:\n\\[ \\begin{array}{l} q[n] = \\text{inv_logit}(0.09 - 0.62 A[n] + 1.90Score[n]) \u0026amp; n = 1, 2, \\dots, N \\\\ Y[n] \\sim \\text{Binomial}(M[n], q[n]) \u0026amp; n = 1, 2, \\dots, N \\\\ \\end{array} \\]\n確認收斂效果 library(bayesplot) color_scheme_set(\u0026quot;mix-brightblue-gray\u0026quot;) posterior2 \u0026lt;- rstan::extract(fit, inc_warmup = TRUE, permuted = FALSE) p \u0026lt;- mcmc_trace(posterior2, n_warmup = 0, pars = c(\u0026quot;b1\u0026quot;, \u0026quot;b2\u0026quot;, \u0026quot;b3\u0026quot;, \u0026quot;lp__\u0026quot;), facet_args = list(nrow = 2, labeller = label_parsed)) p Figure 2: 用 bayesplot包數繪製的模型5-3的MCMC鏈式軌跡圖 (trace plot)。 ms \u0026lt;- rstan::extract(fit) d_qua \u0026lt;- t(apply(ms$y_pred, 2, quantile, prob=c(0.1, 0.5, 0.9))) colnames(d_qua) \u0026lt;- c(\u0026#39;p10\u0026#39;, \u0026#39;p50\u0026#39;, \u0026#39;p90\u0026#39;) d_qua \u0026lt;- data.frame(d, d_qua) d_qua$A \u0026lt;- as.factor(d_qua$A) p \u0026lt;- ggplot(data=d_qua, aes(x=Y, y=p50, ymin=p10, ymax=p90, shape=A, fill=A)) p \u0026lt;- p + theme_bw(base_size=18) + theme(legend.key.height=grid::unit(2.5,\u0026#39;line\u0026#39;)) p \u0026lt;- p + coord_fixed(ratio=1, xlim=c(5, 70), ylim=c(5, 70)) p \u0026lt;- p + geom_pointrange(size=0.8, color=\u0026#39;grey5\u0026#39;) p \u0026lt;- p + geom_abline(aes(slope=1, intercept=0), color=\u0026#39;black\u0026#39;, alpha=3/5, linetype=\u0026#39;31\u0026#39;) p \u0026lt;- p + scale_shape_manual(values=c(21, 24)) p \u0026lt;- p + scale_fill_manual(values=c(\u0026#39;white\u0026#39;, \u0026#39;grey70\u0026#39;)) p \u0026lt;- p + labs(x=\u0026#39;Observed\u0026#39;, y=\u0026#39;Predicted\u0026#39;) p \u0026lt;- p + scale_x_continuous(breaks=seq(from=0, to=70, by=20)) p \u0026lt;- p + scale_y_continuous(breaks=seq(from=0, to=70, by=20)) p Figure 3: 觀測值(x)，和預測值(y)的散點圖，以及預測值的80%預測區間。 ","date":1548547200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548547200,"objectID":"05b06ad2a042d3b6eda5c7d9e5ff383f","permalink":"https://wangcc.me/post/logistic-rstan/","publishdate":"2019-01-27T00:00:00Z","relpermalink":"/post/logistic-rstan/","section":"post","summary":"Rstan 學習筆記 Chapter 5.2","tags":["Bayesian","Medical Statistics"],"title":"Rstan Wonderful R-(4)","type":"post"},{"authors":null,"categories":["Bayesian","R techniques","statistics"],"content":" 多重回歸 multiple regression Step 1. 確認數據分佈 Step 2. 寫下數學模型 Step 3. 看圖確認模型擬合狀況 Step 4. MCMC 樣本的散點圖矩陣 多重回歸 multiple regression 本章使用的數據，大學生出勤記錄也是架空的數據。\n有大學記錄了50名大學生的出勤狀況：\nA,Score,Y 0,69,0.286 1,145,0.196 0,125,0.261 1,86,0.109 1,158,0.23 0,133,0.35 0,111,0.33 1,147,0.194 0,146,0.413 0,145,0.36 1,141,0.225 0,137,0.423 1,118,0.186 0,111,0.287 ... 0,99,0.268 1,99,0.234 其中，\n\\(A\\): 是學生大學二年級時進行的問卷調查時回答是否喜歡打零工的結果（0:不喜歡打工；1:喜歡打工） \\(Score\\): 是大學二年級時進行的問卷調查時計算的該學生對學習是否感興趣的數值評分(200分滿分，分數越高，該學生越熱愛學習) \\(Y\\): 是該學生一年內的出勤率 在本次分析範例中，把\\(Y\\)出勤率當作是連續型結果變量，我們來用Stan實施多重回歸分析，回答學生喜歡打零工與否，和學生對學習的熱情程度兩個變量能解釋多少出勤率。\nStep 1. 確認數據分佈 # The following figure codes come from the authors website: # https://github.com/MatsuuraKentaro/RStanBook/blob/master/chap05/fig5-1.R library(ggplot2) library(GGally) set.seed(123) d \u0026lt;- read.csv(file=\u0026#39;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap05/input/data-attendance-1.txt\u0026#39;, header = T) d$A \u0026lt;- as.factor(d$A) N_col \u0026lt;- ncol(d) ggp \u0026lt;- ggpairs(d, upper=\u0026#39;blank\u0026#39;, diag=\u0026#39;blank\u0026#39;, lower=\u0026#39;blank\u0026#39;) for(i in 1:N_col) { x \u0026lt;- d[,i] p \u0026lt;- ggplot(data.frame(x, A=d$A), aes(x)) p \u0026lt;- p + theme_bw(base_size=14) p \u0026lt;- p + theme(axis.text.x=element_text(angle=40, vjust=1, hjust=1)) if (class(x) == \u0026#39;factor\u0026#39;) { p \u0026lt;- p + geom_bar(aes(fill=A), color=\u0026#39;grey5\u0026#39;) } else { bw \u0026lt;- (max(x)-min(x))/10 p \u0026lt;- p + geom_histogram(binwidth=bw, aes(fill=A), color=\u0026#39;grey5\u0026#39;) #繪製柱狀圖 p \u0026lt;- p + geom_line(eval(bquote(aes(y=..count..*.(bw)))), stat=\u0026#39;density\u0026#39;) #添加概率密度曲線 } p \u0026lt;- p + geom_label(data=data.frame(x=-Inf, y=Inf, label=colnames(d)[i]), aes(x=x, y=y, label=label), hjust=0, vjust=1) p \u0026lt;- p + scale_fill_manual(values=alpha(c(\u0026#39;white\u0026#39;, \u0026#39;grey40\u0026#39;), 0.5)) ggp \u0026lt;- putPlot(ggp, p, i, i) } zcolat \u0026lt;- seq(-1, 1, length=81) zcolre \u0026lt;- c(zcolat[1:40]+1, rev(zcolat[41:81])) for(i in 1:(N_col-1)) { for(j in (i+1):N_col) { x \u0026lt;- as.numeric(d[,i]) y \u0026lt;- as.numeric(d[,j]) r \u0026lt;- cor(x, y, method=\u0026#39;spearman\u0026#39;, use=\u0026#39;pairwise.complete.obs\u0026#39;) zcol \u0026lt;- lattice::level.colors(r, at=zcolat, col.regions=grey(zcolre)) textcol \u0026lt;- ifelse(abs(r) \u0026lt; 0.4, \u0026#39;grey20\u0026#39;, \u0026#39;white\u0026#39;) ell \u0026lt;- ellipse::ellipse(r, level=0.95, type=\u0026#39;l\u0026#39;, npoints=50, scale=c(.2, .2), centre=c(.5, .5)) p \u0026lt;- ggplot(data.frame(ell), aes(x=x, y=y)) p \u0026lt;- p + theme_bw() + theme( plot.background=element_blank(), panel.grid.major=element_blank(), panel.grid.minor=element_blank(), panel.border=element_blank(), axis.ticks=element_blank() ) p \u0026lt;- p + geom_polygon(fill=zcol, color=zcol) p \u0026lt;- p + geom_text(data=NULL, x=.5, y=.5, label=100*round(r, 2), size=6, col=textcol) ggp \u0026lt;- putPlot(ggp, p, i, j) } } for(j in 1:(N_col-1)) { for(i in (j+1):N_col) { x \u0026lt;- d[,j] y \u0026lt;- d[,i] p \u0026lt;- ggplot(data.frame(x, y, gr=d$A), aes(x=x, y=y, fill=gr, shape=gr)) p \u0026lt;- p + theme_bw(base_size=14) p \u0026lt;- p + theme(axis.text.x=element_text(angle=40, vjust=1, hjust=1)) if (class(x) == \u0026#39;factor\u0026#39;) { p \u0026lt;- p + geom_boxplot(aes(group=x), alpha=3/6, outlier.size=0, fill=\u0026#39;white\u0026#39;) p \u0026lt;- p + geom_point(position=position_jitter(w=0.4, h=0), size=2) } else { p \u0026lt;- p + geom_point(size=2) } p \u0026lt;- p + scale_shape_manual(values=c(21, 24)) p \u0026lt;- p + scale_fill_manual(values=alpha(c(\u0026#39;white\u0026#39;, \u0026#39;grey40\u0026#39;), 0.5)) ggp \u0026lt;- putPlot(ggp, p, i, j) } } ggp Figure 1: 三個變量的分佈觀察圖，對角線上是三個變量各自的柱狀圖 (histogram) 和計算獲得的概率密度函數曲線；左下角三個圖是三個變量的箱式圖和散點圖；右上角三個圖是三個變量兩兩計算獲得的 Spearman 秩相關乘以100之後的數值。對角線上及左下角三個圖中數據點和形狀的不同分別表示學生喜歡(三角形)和不喜歡(圓形)打工。右上角表示秩相關的數值越接近0，顏色越白圖形越接近圓形，相關係數的絕對值越接近1，則顏色越深，橢圓越細長。 # png(file=\u0026#39;output/fig5-1.png\u0026#39;, w=1600, h=1600, res=300) # print(ggp, left=0.3, bottom=0.3) # dev.off() Step 2. 寫下數學模型 Model can be written as (Model5-1):\n\\[ \\begin{array}{l} Y[n] = b_1 + b_2A[n] + b_3Sore[n] + \\varepsilon [n]\u0026amp; n = 1,2,\\dots,N \\\\ \\varepsilon[n] \\sim \\text{Normal}(0, \\sigma) \u0026amp; n = 1,2,\\dots,N \\\\ \\end{array} \\]\n其中，\n\\(N\\) 表示學生的人數，\\(n\\)則是學生編號的下標； \\(b_1\\) 是回歸直線的截距； \\(b_2\\) 是\\(Score\\)保持不變時，\\(A\\)從\\(0\\rightarrow 1\\)時出勤率的變化(增加，或者減少)； \\(b_3\\) 是\\(A\\)保持不變時，\\(Score\\)增加一個單位時出勤率的變化(增加，或者減少)。 Model can also be written as (Model5-2):\n\\[ \\begin{array}{l} Y[n] \\sim \\text{Normal}(b_1 + b_2A[n] + b_3Score[n], \\sigma) \u0026amp; n = 1,2,\\dots,N \\\\ \\end{array} \\]\n如果認爲\\(A\\)和\\(Score\\)所能預測的出勤率有一個基礎的均值 \\(\\mu[n]\\)，剩下的每名學生的出勤率服從這個均值和標準差爲 \\(\\sigma\\) 的正態分佈，那麼模型又可以繼續改寫成爲下面的 Model 5-3:\n\\[ \\begin{array}{l} \\mu[n] = b_1 + b_2A[n] + b_3Sore[n] \u0026amp; n = 1,2,\\dots,N \\\\ Y[n] \\sim \\text{Normal}(\\mu[n], \\sigma) \u0026amp; n = 1,2,\\dots,N \\\\ \\end{array} \\]\n下面的 Stan 模型是按照 Model 5-3 寫的，它的模型參數有四個，\\(b_1, b_2, b_3, \\sigma\\)，\\(\\mu[n]\\)通過 transformed parameter 計算獲得:\ndata { int N; int\u0026lt;lower=0, upper=1\u0026gt; A[N]; real\u0026lt;lower=0, upper=1\u0026gt; Score[N]; real\u0026lt;lower=0, upper=1\u0026gt; Y[N]; } parameters { real b1; real b2; real b3; real\u0026lt;lower=0\u0026gt; sigma; } transformed parameters { real mu[N]; for (n in 1:N) { mu[n] = b1 + b2*A[n] + b3*Score[n]; } } model { for (n in 1:N) { Y[n] ~ normal(mu[n], sigma); } } generated quantities { real y_pred[N]; for (n in 1:N) { y_pred[n] = normal_rng(mu[n], sigma); } } 下面的 R 代碼用來實現對上面 Stan 模型的擬合:\nlibrary(rstan) d \u0026lt;- read.csv(file=\u0026#39;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap05/input/data-attendance-1.txt\u0026#39;, header = T) data \u0026lt;- list(N=nrow(d), A=d$A, Score=d$Score/200, Y=d$Y) fit \u0026lt;- stan(file=\u0026#39;stanfiles/model5-3.stan\u0026#39;, data=data, seed=1234) ## ## SAMPLING FOR MODEL \u0026#39;model5-3\u0026#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 1.8e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.120626 seconds (Warm-up) ## Chain 1: 0.137671 seconds (Sampling) ## Chain 1: 0.258297 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL \u0026#39;model5-3\u0026#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 6e-06 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 2: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 2: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 2: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 2: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 2: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 2: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 2: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 2: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 2: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 2: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 2: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.11864 seconds (Warm-up) ## Chain 2: 0.14427 seconds (Sampling) ## Chain 2: 0.26291 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL \u0026#39;model5-3\u0026#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 7e-06 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 3: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 3: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 3: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 3: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 3: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 3: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 3: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 3: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 3: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 3: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 3: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.125027 seconds (Warm-up) ## Chain 3: 0.122722 seconds (Sampling) ## Chain 3: 0.247749 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL \u0026#39;model5-3\u0026#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 7e-06 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 4: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 4: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 4: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 4: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 4: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 4: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 4: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 4: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 4: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 4: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 4: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.129317 seconds (Warm-up) ## Chain 4: 0.13154 seconds (Sampling) ## Chain 4: 0.260857 seconds (Total) ## Chain 4: fit ## Inference for Stan model: model5-3. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## b1 0.12 0.00 0.03 0.06 0.10 0.12 0.15 0.19 1803 1 ## b2 -0.14 0.00 0.01 -0.17 -0.15 -0.14 -0.13 -0.11 2486 1 ## b3 0.32 0.00 0.05 0.22 0.29 0.33 0.36 0.43 1751 1 ## sigma 0.05 0.00 0.01 0.04 0.05 0.05 0.05 0.06 2328 1 ## mu[1] 0.24 0.00 0.02 0.20 0.22 0.24 0.25 0.27 2014 1 ## mu[2] 0.22 0.00 0.01 0.19 0.21 0.22 0.22 0.24 2589 1 ## mu[3] 0.33 0.00 0.01 0.31 0.32 0.33 0.33 0.35 3512 1 ## mu[4] 0.12 0.00 0.02 0.09 0.11 0.12 0.13 0.15 2338 1 ## mu[5] 0.24 0.00 0.02 0.21 0.23 0.24 0.25 0.27 2333 1 ## mu[6] 0.34 0.00 0.01 0.32 0.33 0.34 0.35 0.36 3369 1 ## mu[7] 0.30 0.00 0.01 0.28 0.30 0.30 0.31 0.32 3110 1 ## mu[8] 0.22 0.00 0.01 0.19 0.21 0.22 0.23 0.24 2544 1 ## mu[9] 0.36 0.00 0.01 0.34 0.35 0.36 0.37 0.38 2891 1 ## mu[10] 0.36 0.00 0.01 0.34 0.35 0.36 0.37 0.38 2926 1 ## mu[11] 0.21 0.00 0.01 0.18 0.20 0.21 0.22 0.23 2683 1 ## mu[12] 0.35 0.00 0.01 0.33 0.34 0.35 0.35 0.37 3228 1 ## mu[13] 0.17 0.00 0.01 0.15 0.16 0.17 0.18 0.19 2964 1 ## mu[14] 0.30 0.00 0.01 0.28 0.30 0.30 0.31 0.32 3110 1 ## mu[15] 0.30 0.00 0.01 0.28 0.29 0.30 0.31 0.32 3017 1 ## mu[16] 0.14 0.00 0.01 0.12 0.13 0.14 0.15 0.17 2577 1 ## mu[17] 0.31 0.00 0.01 0.29 0.30 0.31 0.31 0.33 3244 1 ## mu[18] 0.26 0.00 0.01 0.23 0.25 0.26 0.27 0.28 2168 1 ## mu[19] 0.42 0.00 0.02 0.38 0.41 0.42 0.44 0.46 2180 1 ## mu[20] 0.23 0.00 0.01 0.20 0.22 0.23 0.24 0.26 2367 1 ## mu[21] 0.12 0.00 0.02 0.09 0.11 0.12 0.13 0.15 2338 1 ## mu[22] 0.16 0.00 0.01 0.13 0.15 0.16 0.16 0.18 2789 1 ## mu[23] 0.15 0.00 0.01 0.13 0.14 0.15 0.16 0.18 2743 1 ## mu[24] 0.21 0.00 0.01 0.19 0.20 0.21 0.22 0.24 2635 1 ## mu[25] 0.17 0.00 0.01 0.15 0.16 0.17 0.18 0.19 2953 1 ## mu[26] 0.19 0.00 0.01 0.16 0.18 0.19 0.20 0.21 2961 1 ## mu[27] 0.32 0.00 0.01 0.30 0.31 0.32 0.32 0.34 3426 1 ## mu[28] 0.32 0.00 0.01 0.30 0.31 0.32 0.32 0.34 3426 1 ## mu[29] 0.38 0.00 0.01 0.36 0.38 0.39 0.39 0.41 2484 1 ## mu[30] 0.31 0.00 0.01 0.29 0.30 0.31 0.31 0.33 3200 1 ## mu[31] 0.25 0.00 0.02 0.22 0.24 0.25 0.26 0.28 2232 1 ## mu[32] 0.10 0.00 0.02 0.07 0.09 0.10 0.11 0.14 2185 1 ## mu[33] 0.20 0.00 0.01 0.18 0.20 0.20 0.21 0.23 2754 1 ## mu[34] 0.18 0.00 0.01 0.16 0.17 0.18 0.19 0.20 2989 1 ## mu[35] 0.33 0.00 0.01 0.31 0.32 0.33 0.33 0.35 3509 1 ## mu[36] 0.34 0.00 0.01 0.32 0.33 0.34 0.34 0.36 3427 1 ## mu[37] 0.15 0.00 0.01 0.13 0.14 0.15 0.16 0.18 2719 1 ## mu[38] 0.30 0.00 0.01 0.28 0.30 0.30 0.31 0.32 3064 1 ## mu[39] 0.27 0.00 0.01 0.24 0.26 0.27 0.28 0.29 2306 1 ## mu[40] 0.27 0.00 0.01 0.24 0.26 0.27 0.27 0.29 2283 1 ## mu[41] 0.33 0.00 0.01 0.31 0.33 0.33 0.34 0.35 3472 1 ## mu[42] 0.34 0.00 0.01 0.32 0.33 0.34 0.35 0.36 3369 1 ## mu[43] 0.32 0.00 0.01 0.30 0.32 0.32 0.33 0.34 3491 1 ## mu[44] 0.36 0.00 0.01 0.34 0.36 0.36 0.37 0.39 2823 1 ## mu[45] 0.42 0.00 0.02 0.38 0.41 0.42 0.43 0.45 2204 1 ## mu[46] 0.29 0.00 0.01 0.27 0.29 0.29 0.30 0.31 2838 1 ## mu[47] 0.21 0.00 0.02 0.17 0.19 0.21 0.22 0.25 1910 1 ## mu[48] 0.37 0.00 0.01 0.34 0.36 0.37 0.38 0.39 2759 1 ## mu[49] 0.28 0.00 0.01 0.26 0.28 0.28 0.29 0.31 2603 1 ## mu[50] 0.14 0.00 0.01 0.12 0.13 0.14 0.15 0.17 2577 1 ## y_pred[1] 0.23 0.00 0.05 0.13 0.20 0.23 0.27 0.34 3730 1 ## y_pred[2] 0.22 0.00 0.05 0.11 0.18 0.22 0.25 0.32 3977 1 ## y_pred[3] 0.32 0.00 0.05 0.23 0.29 0.32 0.36 0.43 3856 1 ## y_pred[4] 0.12 0.00 0.05 0.01 0.08 0.12 0.15 0.22 3896 1 ## y_pred[5] 0.24 0.00 0.05 0.13 0.20 0.24 0.27 0.34 3804 1 ## y_pred[6] 0.34 0.00 0.05 0.24 0.31 0.34 0.38 0.44 3865 1 ## y_pred[7] 0.30 0.00 0.05 0.20 0.27 0.30 0.34 0.41 3757 1 ## y_pred[8] 0.22 0.00 0.05 0.11 0.18 0.22 0.25 0.33 3784 1 ## y_pred[9] 0.36 0.00 0.05 0.26 0.33 0.36 0.40 0.47 3939 1 ## y_pred[10] 0.36 0.00 0.05 0.25 0.32 0.36 0.39 0.46 3743 1 ## y_pred[11] 0.21 0.00 0.05 0.10 0.17 0.21 0.25 0.32 4113 1 ## y_pred[12] 0.35 0.00 0.05 0.24 0.31 0.35 0.38 0.45 4149 1 ## y_pred[13] 0.17 0.00 0.05 0.07 0.14 0.17 0.21 0.28 3806 1 ## y_pred[14] 0.30 0.00 0.05 0.20 0.27 0.30 0.34 0.41 3944 1 ## y_pred[15] 0.30 0.00 0.05 0.20 0.27 0.30 0.34 0.41 3930 1 ## y_pred[16] 0.14 0.00 0.05 0.03 0.11 0.14 0.18 0.24 3833 1 ## y_pred[17] 0.31 0.00 0.05 0.21 0.27 0.31 0.34 0.41 3981 1 ## y_pred[18] 0.26 0.00 0.05 0.15 0.22 0.26 0.29 0.37 3718 1 ## y_pred[19] 0.43 0.00 0.06 0.32 0.39 0.43 0.46 0.53 3690 1 ## y_pred[20] 0.23 0.00 0.05 0.13 0.20 0.23 0.27 0.34 2926 1 ## y_pred[21] 0.12 0.00 0.05 0.01 0.08 0.12 0.15 0.23 3751 1 ## y_pred[22] 0.16 0.00 0.05 0.05 0.12 0.16 0.19 0.26 3993 1 ## y_pred[23] 0.15 0.00 0.05 0.04 0.11 0.15 0.19 0.26 3990 1 ## y_pred[24] 0.21 0.00 0.05 0.11 0.17 0.21 0.25 0.32 3766 1 ## y_pred[25] 0.17 0.00 0.05 0.06 0.13 0.17 0.21 0.27 3800 1 ## y_pred[26] 0.19 0.00 0.05 0.08 0.15 0.19 0.22 0.30 3781 1 ## y_pred[27] 0.32 0.00 0.05 0.22 0.28 0.32 0.35 0.42 4073 1 ## y_pred[28] 0.32 0.00 0.05 0.21 0.28 0.32 0.35 0.42 4155 1 ## y_pred[29] 0.38 0.00 0.05 0.28 0.35 0.38 0.42 0.49 3869 1 ## y_pred[30] 0.31 0.00 0.05 0.20 0.27 0.31 0.34 0.41 3708 1 ## y_pred[31] 0.25 0.00 0.06 0.14 0.21 0.25 0.29 0.35 4005 1 ## y_pred[32] 0.10 0.00 0.06 -0.01 0.06 0.10 0.14 0.21 3479 1 ## y_pred[33] 0.20 0.00 0.05 0.10 0.17 0.20 0.24 0.31 3656 1 ## y_pred[34] 0.18 0.00 0.05 0.08 0.15 0.18 0.22 0.28 4111 1 ## y_pred[35] 0.33 0.00 0.05 0.22 0.29 0.33 0.36 0.43 3866 1 ## y_pred[36] 0.34 0.00 0.05 0.23 0.30 0.34 0.37 0.44 4043 1 ## y_pred[37] 0.15 0.00 0.05 0.04 0.12 0.15 0.19 0.25 3830 1 ## y_pred[38] 0.30 0.00 0.05 0.20 0.27 0.30 0.34 0.41 3814 1 ## y_pred[39] 0.27 0.00 0.05 0.16 0.23 0.27 0.30 0.37 3792 1 ## y_pred[40] 0.27 0.00 0.05 0.16 0.23 0.27 0.30 0.37 3793 1 ## y_pred[41] 0.33 0.00 0.05 0.23 0.30 0.33 0.37 0.44 3748 1 ## y_pred[42] 0.34 0.00 0.05 0.23 0.30 0.34 0.38 0.44 3811 1 ## y_pred[43] 0.32 0.00 0.05 0.22 0.29 0.32 0.35 0.42 4268 1 ## y_pred[44] 0.36 0.00 0.05 0.26 0.33 0.36 0.40 0.47 4015 1 ## y_pred[45] 0.42 0.00 0.05 0.31 0.38 0.42 0.45 0.53 3658 1 ## y_pred[46] 0.29 0.00 0.05 0.19 0.26 0.29 0.33 0.40 3993 1 ## y_pred[47] 0.21 0.00 0.06 0.10 0.17 0.21 0.24 0.32 3671 1 ## y_pred[48] 0.37 0.00 0.05 0.26 0.33 0.37 0.40 0.47 3988 1 ## y_pred[49] 0.28 0.00 0.05 0.18 0.25 0.28 0.32 0.39 3596 1 ## y_pred[50] 0.14 0.00 0.05 0.03 0.10 0.14 0.18 0.24 4027 1 ## lp__ 120.85 0.04 1.43 117.36 120.12 121.19 121.90 122.68 1610 1 ## ## Samples were drawn using NUTS(diag_e) at Mon May 18 16:52:55 2020. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). 上述代碼中值得注意的是我們對 \\(Score\\) 進行了全部除以 \\(200\\) 的數據縮放調整 (scaling)。這樣有助於我們的模型在進行 MCMC 計算時加速其達到收斂時所需要的時間。\n把計算獲得的事後模型參數平均值代入模型 Model 5-3:\n\\[ \\begin{array}{l} \\mu[n] = 0.12 - 0.14A[n] + 0.32Sore[n] \u0026amp; n = 1,2,\\dots,N \\\\ Y[n] \\sim \\text{Normal}(\\mu[n], 0.05) \u0026amp; n = 1,2,\\dots,N \\\\ \\end{array} \\]\n從輸出的結果報告來看，所有的 Rhat 都小於1.1，可以認爲採樣已經達到收斂效果，再來確認一下軌跡圖：\nlibrary(bayesplot) color_scheme_set(\u0026quot;mix-brightblue-gray\u0026quot;) posterior2 \u0026lt;- rstan::extract(fit, inc_warmup = TRUE, permuted = FALSE) p \u0026lt;- mcmc_trace(posterior2, n_warmup = 0, pars = c(\u0026quot;b1\u0026quot;, \u0026quot;b2\u0026quot;, \u0026quot;b3\u0026quot;, \u0026quot;sigma\u0026quot;, \u0026quot;lp__\u0026quot;), facet_args = list(nrow = 2, labeller = label_parsed)) p Figure 2: 用 bayesplot包數繪製的模型5-3的MCMC鏈式軌跡圖 (trace plot)。 收斂效果很不錯，下面來解釋回歸係數的事後均值的涵義：\nb3的事後均值是\\(0.32\\)，所以，\\(Score=150\\)和\\(Score=50\\)的兩名學生，當他們同時都是喜歡或者同時都不喜歡打工時，\\(Score = 150\\)的學生的出勤率平均比 \\(Score = 50\\) 的學生的出勤率高 \\(0.32 \\times (150-50)/200 = 0.16\\)。 b2的事後均值是\\(-0.14\\)，所以，同樣地，\\(Score\\)相同的兩名學生，喜歡打工的學生比不喜歡打工的學生出勤率平均要低 \\(0.14\\)。 Step 3. 看圖確認模型擬合狀況 下圖繪製了上面貝葉斯多重線性回歸模型計算獲得的事後貝葉斯預測區間，和觀測值\\(Y\\)出勤率之間的直觀關係：\nsource(\u0026quot;commonRstan.R\u0026quot;) ms \u0026lt;- rstan::extract(fit) Score_new \u0026lt;- 50:200 N_X \u0026lt;- length(Score_new) N_mcmc \u0026lt;- length(ms$lp__) set.seed(1234) y_base_mcmc \u0026lt;- as.data.frame(matrix(nrow=N_mcmc, ncol=N_X)) y_base_a0_mcmc \u0026lt;- as.data.frame(matrix(nrow = N_mcmc, ncol = N_X)) y_mcmc \u0026lt;- as.data.frame(matrix(nrow=N_mcmc, ncol=N_X)) y_a0_mcmc \u0026lt;- as.data.frame(matrix(nrow=N_mcmc, ncol=N_X)) for (i in 1:N_X) { y_base_mcmc[,i] \u0026lt;- ms$b1 + ms$b2 + ms$b3 * Score_new[i]/200 y_base_a0_mcmc[] \u0026lt;- ms$b1 + ms$b2*0 + ms$b3 * Score_new[i]/200 y_mcmc[,i] \u0026lt;- rnorm(n=N_mcmc, mean=y_base_mcmc[,i], sd=ms$sigma) y_a0_mcmc[,i] \u0026lt;- rnorm(n=N_mcmc, mean=y_base_a0_mcmc[,i], sd=ms$sigma) } customize.ggplot.axis \u0026lt;- function(p) { p \u0026lt;- p + labs(x=\u0026#39;Score\u0026#39;, y=\u0026#39;Y\u0026#39;) p \u0026lt;- p + scale_y_continuous(breaks=seq(from=-0.2, to=0.8, by=0.2)) p \u0026lt;- p + coord_cartesian(xlim=c(50, 200), ylim=c(-0.2, 0.6)) return(p) } d_est \u0026lt;- data.frame.quantile.mcmc(x=Score_new, y_mcmc=y_mcmc) d_esta0 \u0026lt;- data.frame.quantile.mcmc(x=Score_new, y_mcmc=y_a0_mcmc) # p \u0026lt;- ggplot.5quantile(data=d_est) # p2 \u0026lt;- ggplot.5quantile(data = d_esta0) # p \u0026lt;- p + geom_point(data=d[d$A==1, ], aes(x=Score, y=Y), shape=24, size=5) # p2 \u0026lt;- p2 + geom_point(data=d[d$A==0, ], aes(x=Score, y=Y), shape=1, size=5) # p \u0026lt;- customize.ggplot.axis(p) # p2 \u0026lt;- customize.ggplot.axis(p2) visuals = rbind(d_est,d_esta0) visuals$A=c(rep(1,151),rep(0,151)) # 151 points of each flavour qn \u0026lt;- colnames(visuals)[-1] p \u0026lt;- ggplot(data=visuals, aes(x=X, y=p50, group = A)) p \u0026lt;- p + my_theme() p \u0026lt;- p + geom_ribbon(aes_string(ymin=qn[1], ymax=qn[5]), fill=\u0026#39;black\u0026#39;, alpha=1/6) p \u0026lt;- p + geom_ribbon(aes_string(ymin=qn[2], ymax=qn[4]), fill=\u0026#39;black\u0026#39;, alpha=2/6) p \u0026lt;- p + geom_line(size=1) p \u0026lt;- p + geom_point(data=d[d$A==1, ], aes(x=Score, y=Y), shape=24, size=5) p \u0026lt;- p + geom_point(data=d[d$A==0, ], aes(x=Score, y=Y), shape=20, size=5) p \u0026lt;- customize.ggplot.axis(p) p Figure 3: 黑色原點(不喜歡打工)，和無色三角形(喜歡打工)的學生的出勤率，和模型計算獲得的貝葉斯事後預測區間。黑色線是中位數，灰色帶是50%預測區間和95%預測區間。 上述觀察預測值區間和實際觀測之間的關係的視覺化圖形，在多重線性回歸模型只有兩個預測變量的事後還較爲容易獲得，當模型中有三個或以上的預測變量時，可視化變得困難重重。\n此時我們推薦繪製“實際觀測值和預測值”，以及模型給出的每個預測值的隨機誤差\\(\\varepsilon\\)分佈範圍，相結合的圖形來判斷模型擬合程度。\nd_qua \u0026lt;- t(apply(ms$y_pred, 2, quantile, prob=c(0.1, 0.5, 0.9))) colnames(d_qua) \u0026lt;- c(\u0026#39;p10\u0026#39;, \u0026#39;p50\u0026#39;, \u0026#39;p90\u0026#39;) d_qua \u0026lt;- data.frame(d, d_qua) d_qua$A \u0026lt;- as.factor(d_qua$A) p \u0026lt;- ggplot(data=d_qua, aes(x=Y, y=p50, ymin=p10, ymax=p90, shape=A, fill=A)) p \u0026lt;- p + theme_bw(base_size=18) + theme(legend.key.height=grid::unit(2.5,\u0026#39;line\u0026#39;)) p \u0026lt;- p + coord_fixed(ratio=1, xlim=c(0, 0.5), ylim=c(0, 0.5)) p \u0026lt;- p + geom_pointrange(size=0.8, color=\u0026#39;grey5\u0026#39;) p \u0026lt;- p + geom_abline(aes(slope=1, intercept=0), color=\u0026#39;black\u0026#39;, alpha=3/5, linetype=\u0026#39;31\u0026#39;) p \u0026lt;- p + scale_shape_manual(values=c(21, 24)) p \u0026lt;- p + scale_fill_manual(values=c(\u0026#39;white\u0026#39;, \u0026#39;grey70\u0026#39;)) p \u0026lt;- p + labs(x=\u0026#39;Observed\u0026#39;, y=\u0026#39;Predicted\u0026#39;) p \u0026lt;- p + scale_x_continuous(breaks=seq(from=0, to=0.5, by=0.1)) p \u0026lt;- p + scale_y_continuous(breaks=seq(from=0, to=0.5, by=0.1)) p Figure 4: 觀測值(x)，和預測值(y)的散點圖，以及預測值的80%預測區間。 從上圖中可以看出，大多數的觀測點和預測點以及預測的80%區間基本都在 \\(y = x\\) 這條對角線上。大致可以認爲本次貝葉斯多重線性回歸擬合效果尚且能夠接受。\n隨機誤差 \\(\\varepsilon[n]\\) 被認爲服從 \\(\\text{Normal}(0, \\sigma)\\) 的正態分佈。從模型中可以計算獲得每個學生出勤率的預測值和實際觀測值之間的差，這就是隨機誤差。貝葉斯框架之下，我們實際獲得的會是每名學生隨機誤差的分佈：\nN_mcmc \u0026lt;- length(ms$lp__) d_noise \u0026lt;- data.frame(t(-t(ms$mu) + d$Y)) colnames(d_noise) \u0026lt;- paste0(\u0026#39;noise\u0026#39;, 1:nrow(d)) d_est \u0026lt;- data.frame(mcmc=1:N_mcmc, d_noise) d_melt \u0026lt;- reshape2::melt(d_est, id=c(\u0026#39;mcmc\u0026#39;), variable.name=\u0026#39;X\u0026#39;) d_mode \u0026lt;- data.frame(t(apply(d_noise, 2, function(x) { dens \u0026lt;- density(x) mode_i \u0026lt;- which.max(dens$y) mode_x \u0026lt;- dens$x[mode_i] mode_y \u0026lt;- dens$y[mode_i] c(mode_x, mode_y) }))) colnames(d_mode) \u0026lt;- c(\u0026#39;X\u0026#39;, \u0026#39;Y\u0026#39;) p \u0026lt;- ggplot() p \u0026lt;- p + theme_bw(base_size=18) p \u0026lt;- p + geom_line(data=d_melt, aes(x=value, group=X), stat=\u0026#39;density\u0026#39;, color=\u0026#39;black\u0026#39;, alpha=0.4) p \u0026lt;- p + geom_segment(data=d_mode, aes(x=X, xend=X, y=Y, yend=0), color=\u0026#39;black\u0026#39;, linetype=\u0026#39;dashed\u0026#39;, alpha=0.4) p \u0026lt;- p + geom_rug(data=d_mode, aes(x=X), sides=\u0026#39;b\u0026#39;) p \u0026lt;- p + labs(x=\u0026#39;value\u0026#39;, y=\u0026#39;density\u0026#39;) p Figure 5: 每名學生的出勤率隨機誤差的分佈 實際上我們只需要選取每名學生模型計算獲得的事後隨機誤差的代表值，比如可以是平均值，中央值，或者是MAP值（事後確率最大推定値，maximum a posteriori estimate），來觀察就可以了：\ns_dens \u0026lt;- density(ms$s) s_MAP \u0026lt;- s_dens$x[which.max(s_dens$y)] bw \u0026lt;- 0.01 p \u0026lt;- ggplot(data=d_mode, aes(x=X)) p \u0026lt;- p + theme_bw(base_size=18) p \u0026lt;- p + geom_histogram(binwidth=bw, color=\u0026#39;black\u0026#39;, fill=\u0026#39;white\u0026#39;) p \u0026lt;- p + geom_density(eval(bquote(aes(y=..count..*.(bw)))), alpha=0.5, color=\u0026#39;black\u0026#39;, fill=\u0026#39;gray20\u0026#39;) p \u0026lt;- p + stat_function(fun=function(x) nrow(d)*bw*dnorm(x, mean=0, sd=s_MAP), linetype=\u0026#39;dashed\u0026#39;) p \u0026lt;- p + labs(x=\u0026#39;value\u0026#39;, y=\u0026#39;count\u0026#39;) p \u0026lt;- p + xlim(range(density(d_mode$X)$x)) p ## Warning: Removed 2 rows containing missing values (geom_bar). Figure 6: 每名學生事後出勤率隨機誤差的MAP值的柱狀圖，和相應的概率密度函數（灰色鐘罩），點狀虛線是均值爲0，標準差是模型計算的事後隨機誤差標準差的 MAP 值的正態分佈的形狀。 Step 4. MCMC 樣本的散點圖矩陣 library(ggplot2) library(GGally) library(hexbin) d \u0026lt;- data.frame(b1=ms$b1, b2=ms$b2, b3=ms$b3, sigma=ms$sigma, mu1=ms$mu[,1], mu50=ms$mu[,50], lp__=ms$lp__) N_col \u0026lt;- ncol(d) ggp \u0026lt;- ggpairs(d, upper=\u0026#39;blank\u0026#39;, diag=\u0026#39;blank\u0026#39;, lower=\u0026#39;blank\u0026#39;) label_list \u0026lt;- list(b1=\u0026#39;b1\u0026#39;, b2=\u0026#39;b2\u0026#39;, b3=\u0026#39;b3\u0026#39;, sigma=\u0026#39;sigma\u0026#39;, mu1=\u0026#39;mu[1]\u0026#39;, mu50=\u0026#39;mu[50]\u0026#39;, lp__=\u0026#39;lp__\u0026#39;) for(i in 1:N_col) { x \u0026lt;- d[,i] bw \u0026lt;- (max(x)-min(x))/10 p \u0026lt;- ggplot(data.frame(x), aes(x)) p \u0026lt;- p + theme_bw(base_size=14) p \u0026lt;- p + theme(axis.text.x=element_text(angle=60, vjust=1, hjust=1)) p \u0026lt;- p + geom_histogram(binwidth=bw, fill=\u0026#39;white\u0026#39;, color=\u0026#39;grey5\u0026#39;) p \u0026lt;- p + geom_line(eval(bquote(aes(y=..count..*.(bw)))), stat=\u0026#39;density\u0026#39;) p \u0026lt;- p + geom_label(data=data.frame(x=-Inf, y=Inf, label=label_list[[colnames(d)[i]]]), aes(x=x, y=y, label=label), hjust=0, vjust=1) ggp \u0026lt;- putPlot(ggp, p, i, i) } zcolat \u0026lt;- seq(-1, 1, length=81) zcolre \u0026lt;- c(zcolat[1:40]+1, rev(zcolat[41:81])) for(i in 1:(N_col-1)) { for(j in (i+1):N_col) { x \u0026lt;- as.numeric(d[,i]) y \u0026lt;- as.numeric(d[,j]) r \u0026lt;- cor(x, y, method=\u0026#39;spearman\u0026#39;, use=\u0026#39;pairwise.complete.obs\u0026#39;) zcol \u0026lt;- lattice::level.colors(r, at=zcolat, col.regions=grey(zcolre)) textcol \u0026lt;- ifelse(abs(r) \u0026lt; 0.4, \u0026#39;grey20\u0026#39;, \u0026#39;white\u0026#39;) ell \u0026lt;- ellipse::ellipse(r, level=0.95, type=\u0026#39;l\u0026#39;, npoints=50, scale=c(.2, .2), centre=c(.5, .5)) p \u0026lt;- ggplot(data.frame(ell), aes(x=x, y=y)) p \u0026lt;- p + theme_bw() + theme( plot.background=element_blank(), panel.grid.major=element_blank(), panel.grid.minor=element_blank(), panel.border=element_blank(), axis.ticks=element_blank() ) p \u0026lt;- p + geom_polygon(fill=zcol, color=zcol) p \u0026lt;- p + geom_text(data=NULL, x=.5, y=.5, label=100*round(r, 2), size=6, col=textcol) ggp \u0026lt;- putPlot(ggp, p, i, j) } } for(j in 1:(N_col-1)) { for(i in (j+1):N_col) { x \u0026lt;- d[,j] y \u0026lt;- d[,i] p \u0026lt;- ggplot(data.frame(x, y), aes(x=x, y=y)) p \u0026lt;- p + theme_bw(base_size=14) p \u0026lt;- p + theme(axis.text.x=element_text(angle=60, vjust=1, hjust=1)) p \u0026lt;- p + geom_hex() p \u0026lt;- p + scale_fill_gradientn(colours=gray.colors(7, start=0.1, end=0.9)) ggp \u0026lt;- putPlot(ggp, p, i, j) } } ggp Figure 7: MCMC樣本的事後矩陣。對角線上是各個參數事後樣本的柱狀圖和相應的概率密度曲線。右上角是各個參數事後樣本之間的 Spearman 秩相關係數。左下角是兩兩的散點圖。 ","date":1548115200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548115200,"objectID":"6cd5d4ec8fa1804972559f6668a49f87","permalink":"https://wangcc.me/post/rstan-wonderful-r3/","publishdate":"2019-01-22T00:00:00Z","relpermalink":"/post/rstan-wonderful-r3/","section":"post","summary":"Rstan 學習筆記 Chapter 5.1","tags":["Bayesian","Medical Statistics"],"title":"Rstan Wonderful R-(3)","type":"post"},{"authors":null,"categories":["R techniques","statistics","Bayesian"],"content":" Step 1, 確認數據分佈 Step 2, 描述線性模型 Step 3, 寫下Stan模型 Step 4, 診斷Stan貝葉斯模型的收斂程度 Step 5，修改MCMC條件設定 Step 6, 並行（平行）計算的設定 Step 7, 計算貝葉斯可信區間和貝葉斯預測區間 練習題 數據 data-salary.txt是架空的。\n某公司社員的年齡 \\(X\\)（歲），和年收入 \\(Y\\)（萬日元）的數據如下：\nX,Y 24,472 24,403 26,454 32,575 33,546 35,781 38,750 40,601 40,814 43,792 43,745 44,837 48,868 52,988 56,1092 56,1007 57,1233 58,1202 59,1123 59,1314 年收入 \\(Y\\) 被認爲是由基本年收 \\(y_{base}\\) 和其他影響因素 \\(\\varepsilon\\) 構成。由於該公司是典型的年功序列式的日本傳統企業，所以基本年收本身和社員年齡成正比例。 \\(\\varepsilon\\) 則被認爲是由該員工當年的業績等隨機誤差造成的，但是所有員工的 \\(\\varepsilon\\) 的均值被認爲是零。\ng分析目的：\n借用這個數據來分析並回答如下的問題：在該公司如果採用了一名50歲的員工，他/她的年收入的預期值會是多少。 Step 1, 確認數據分佈 Salary \u0026lt;- read.table(\u0026quot;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap04/input/data-salary.txt\u0026quot;, sep = \u0026quot;,\u0026quot;, header = T) library(ggplot2) ggplot(Salary, aes(x = X, y = Y)) + geom_point(shape = 1, size = 4) + theme(plot.subtitle = element_text(vjust = 1), plot.caption = element_text(vjust = 1), axis.line = element_line(colour = \u0026quot;bisque4\u0026quot;, size = 0.2, linetype = \u0026quot;solid\u0026quot;), axis.ticks = element_line(size = 0.7), axis.title = element_text(size = 16), axis.text = element_text(size = 16, colour = \u0026quot;gray0\u0026quot;), panel.background = element_rect(fill = \u0026quot;gray98\u0026quot;)) + scale_y_continuous(limits = c(200, 1400), breaks = c(200, 600, 1000, 1400)) Figure 1: 橫軸爲 \\(X\\)，縱軸爲 \\(Y\\) 的散點圖 從這個散點圖的特徵可以看出年收入確實似乎和年齡呈線性正相關。\nStep 2, 描述線性模型 這個簡單線性回歸模型的數學表達式可以描述如下：\n\\[ \\begin{array}{l} Y[n] = y_{base}[n] + \\varepsilon [n]\u0026amp; n = 1,2,\\dots,N \\\\ y_{base}[n] = a + bX[n] \u0026amp; n = 1,2,\\dots,N \\\\ \\varepsilon[n] \\sim \\text{Normal}(0, \\sigma) \u0026amp; n = 1,2,\\dots,N \\\\ \\end{array} \\]\n同樣的模型你可以簡化描述成爲：\n\\[ Y[n] \\sim \\text{Normal}(a + bX[n], \\sigma)\\;\\; n = 1,2,\\dots,N \\]\n那麼如果一個統計師只有經過傳統概率論觀點的訓練，他/她會在R裏面這樣來分析這個數據：\nres_lm \u0026lt;- lm(Y ~ X, data = Salary) summary(res_lm) ## ## Call: ## lm(formula = Y ~ X, data = Salary) ## ## Residuals: ## Min 1Q Median 3Q Max ## -155.471 -51.523 -6.663 52.822 141.349 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -119.697 68.148 -1.756 0.096 . ## X 21.904 1.518 14.428 2.47e-11 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 79.1 on 18 degrees of freedom ## Multiple R-squared: 0.9204, Adjusted R-squared: 0.916 ## F-statistic: 208.2 on 1 and 18 DF, p-value: 2.466e-11 # 用這個線性回歸模型來對上面模型中的參數作出預測： X_new \u0026lt;- data.frame(X=23:60) conf_95 \u0026lt;- predict(res_lm, X_new, interval = \u0026quot;confidence\u0026quot;, level = 0.95) pred_95 \u0026lt;- predict(res_lm, X_new, interval = \u0026quot;prediction\u0026quot;, level = 0.95) temp_var \u0026lt;- predict(res_lm, interval=\u0026quot;prediction\u0026quot;) ## Warning in predict.lm(res_lm, interval = \u0026quot;prediction\u0026quot;): predictions on current data refer to _future_ responses new_df \u0026lt;- cbind(Salary, temp_var) ggplot(new_df, aes(x = X, y = Y)) + geom_point(shape = 1, size = 4) + theme(plot.subtitle = element_text(vjust = 1), plot.caption = element_text(vjust = 1), axis.line = element_line(colour = \u0026quot;bisque4\u0026quot;, size = 0.2, linetype = \u0026quot;solid\u0026quot;), axis.ticks = element_line(size = 0.7), axis.title = element_text(size = 16), axis.text = element_text(size = 16, colour = \u0026quot;gray0\u0026quot;), panel.background = element_rect(fill = \u0026quot;gray98\u0026quot;)) + geom_smooth(method = lm, se=TRUE, size = 0.3)+ scale_y_continuous(limits = c(200, 1400), breaks = c(200, 600, 1000, 1400)) + geom_line(aes(y=lwr), color = \u0026quot;red\u0026quot;, linetype = \u0026quot;dashed\u0026quot;)+ geom_line(aes(y=upr), color = \u0026quot;red\u0026quot;, linetype = \u0026quot;dashed\u0026quot;) Figure 2: 用簡單線性回歸模型計算的基本年收的信賴區間(灰色陰影)和預測區間(紅色點線)。 Step 3, 寫下Stan模型 data { int N; real X[N]; real Y[N]; } parameters { real a; real b; real\u0026lt;lower=0\u0026gt; sigma; } model { for(n in 1:N) { Y[n] ~ normal(a + b*X[n], sigma); } } 參數部分 real\u0026lt;lower=0\u0026gt; sigma 的代碼表示標準差不可採集負數作爲樣本。\n實際運行上面的Stan代碼：\nlibrary(rstan) data \u0026lt;- list(N=nrow(Salary), X=Salary$X, Y = Salary$Y) fit \u0026lt;- sampling(model4_5, data, seed = 1234) ## ## SAMPLING FOR MODEL \u0026#39;5b73686886069c0bad70513d4ea4141a\u0026#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 1.5e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.093322 seconds (Warm-up) ## Chain 1: 0.054914 seconds (Sampling) ## Chain 1: 0.148236 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL \u0026#39;5b73686886069c0bad70513d4ea4141a\u0026#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 4e-06 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 2: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 2: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 2: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 2: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 2: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 2: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 2: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 2: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 2: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 2: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 2: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.098838 seconds (Warm-up) ## Chain 2: 0.061329 seconds (Sampling) ## Chain 2: 0.160167 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL \u0026#39;5b73686886069c0bad70513d4ea4141a\u0026#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 4e-06 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 3: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 3: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 3: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 3: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 3: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 3: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 3: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 3: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 3: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 3: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 3: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.091017 seconds (Warm-up) ## Chain 3: 0.053765 seconds (Sampling) ## Chain 3: 0.144782 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL \u0026#39;5b73686886069c0bad70513d4ea4141a\u0026#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 4e-06 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 4: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 4: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 4: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 4: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 4: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 4: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 4: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 4: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 4: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 4: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 4: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.100685 seconds (Warm-up) ## Chain 4: 0.062005 seconds (Sampling) ## Chain 4: 0.16269 seconds (Total) ## Chain 4: print(fit) ## Inference for Stan model: 5b73686886069c0bad70513d4ea4141a. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## a -117.45 1.93 71.31 -257.66 -164.65 -119.17 -71.98 23.17 1358 1.00 ## b 21.86 0.04 1.60 18.68 20.83 21.89 22.91 24.97 1331 1.00 ## sigma 84.51 0.41 15.21 61.09 73.72 82.41 93.18 120.03 1381 1.01 ## lp__ -93.61 0.04 1.26 -96.86 -94.19 -93.27 -92.69 -92.14 1164 1.01 ## ## Samples were drawn using NUTS(diag_e) at Mon May 18 14:21:36 2020. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). 輸出結果的前三行，是該次MCMC的設定條件，其中模型名稱是Rmarkdown文件中隨機產生的。\n第二行則說明的是該次MCMC進行了4條鏈的採樣，每條鏈2000次，其中前1000次被當作是 burn-in (或者叫 warmup)。可以看到一共獲得了4000個事後樣本。\n接下來的五行是參數的事後樣本的事後分析總結，一共有11列。\n第1列是參數名稱，最後一個 lp__是Stan特有的算法得到的產物，具體解釋爲對數事後概率 (log posterior)，當然它也需要得到收斂才行。 第2列是獲得的4000個參數的事後樣本的事後平均值(posterior mean)。例如b（回歸直線的斜率）的事後平均值是21.96，也就是說年齡每增加一歲，基本年收入平均增加21.96萬日元。你可以和之前的概率論算法相比較(b = 21.904)。 第3列se_mean是事後平均值的標準誤(standard error of posterior mean)。說白了是MCMC事後樣本的方差除以第10列的有效樣本量n_eff之後取根號獲得的值。 第4列sd是MCMC事後樣本的標準差(standard deviation of posterior MCMC sample)。 第5-9列是MCMC事後樣本的四分位點。也就是貝葉斯統計算法獲得的事後可信區間。 第10列n_eff是Stan在基於事後樣本自相關程度來判斷的有效事後樣本量大小。爲了有效地計算和繪製事後分佈的統計量，這個有效樣本量需要至少有100個以上吧（作者觀點）。如果報告給出的事後有效樣本量過小的話也是模型收斂不佳的表現之一。 第11列Rhat\\((\\hat R)\\)是主要用於判斷模型是否達到收斂的重要指標，每個參數都會被計算一個Rhat值。當MCMC鏈條數在3以上，且同時所有的模型參數的 Rhat \u0026lt; 1.1的話，可以認爲模型達到了良好的收斂。 Step 4, 診斷Stan貝葉斯模型的收斂程度 library(ggmcmc) ggmcmc(ggs(fit, inc_warmup = TRUE, stan_include_auxiliar = TRUE), plot = \u0026quot;traceplot\u0026quot;, dev_type_html = \u0026quot;png\u0026quot;, file = \u0026quot;trace.html\u0026quot;) 上面的代碼，會自動生成四個模型參數的軌跡MCMC鏈式圖報告。\nFigure 3: 用ggmcmc函數製作而成的MCMC鏈式圖報告。 library(bayesplot) color_scheme_set(\u0026quot;mix-brightblue-gray\u0026quot;) posterior2 \u0026lt;- rstan::extract(fit, inc_warmup = TRUE, permuted = FALSE) p \u0026lt;- mcmc_trace(posterior2, n_warmup = 0, facet_args = list(nrow = 2, labeller = label_parsed)) p Figure 4: 用 bayesplot包數繪製的MCMC鏈式圖。 p \u0026lt;- mcmc_acf_bar(posterior2) p Figure 5: 用 bayesplot包數繪製的事後樣本自相關圖(autocorrelation)。 p \u0026lt;- mcmc_dens_overlay(posterior2, color_chains = T) p Figure 6: 用 bayesplot包數繪製的事後樣本密度分佈圖。 Step 5，修改MCMC條件設定 進行貝葉斯模型擬合的過程中，常常需要不停地修改模型的條件，例如縮短warm-up等。下面的Rstan代碼可以實現簡便地頻繁修改MCMC條件設定：\n# library(rstan) uncomment if run for the first time data \u0026lt;- list(N=nrow(Salary), X=Salary$X, Y = Salary$Y) fit2 \u0026lt;- sampling( model4_5, data = data, pars = c(\u0026quot;b\u0026quot;, \u0026quot;sigma\u0026quot;), init = function(){ list(a = runif(1, -10, 10), b = runif(1, 0, 10), sigma = 10) }, seed = 123, chains = 3, iter = 1000, warmup = 200, thin = 2 ) ## ## SAMPLING FOR MODEL \u0026#39;5b73686886069c0bad70513d4ea4141a\u0026#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 1.2e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 1: Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 1: Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 1: Iteration: 201 / 1000 [ 20%] (Sampling) ## Chain 1: Iteration: 300 / 1000 [ 30%] (Sampling) ## Chain 1: Iteration: 400 / 1000 [ 40%] (Sampling) ## Chain 1: Iteration: 500 / 1000 [ 50%] (Sampling) ## Chain 1: Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 1: Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 1: Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 1: Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 1: Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.05316 seconds (Warm-up) ## Chain 1: 0.036218 seconds (Sampling) ## Chain 1: 0.089378 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL \u0026#39;5b73686886069c0bad70513d4ea4141a\u0026#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 2e-06 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 2: Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 2: Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 2: Iteration: 201 / 1000 [ 20%] (Sampling) ## Chain 2: Iteration: 300 / 1000 [ 30%] (Sampling) ## Chain 2: Iteration: 400 / 1000 [ 40%] (Sampling) ## Chain 2: Iteration: 500 / 1000 [ 50%] (Sampling) ## Chain 2: Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 2: Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 2: Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 2: Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 2: Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.052191 seconds (Warm-up) ## Chain 2: 0.036956 seconds (Sampling) ## Chain 2: 0.089147 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL \u0026#39;5b73686886069c0bad70513d4ea4141a\u0026#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 2e-06 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 3: Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 3: Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 3: Iteration: 201 / 1000 [ 20%] (Sampling) ## Chain 3: Iteration: 300 / 1000 [ 30%] (Sampling) ## Chain 3: Iteration: 400 / 1000 [ 40%] (Sampling) ## Chain 3: Iteration: 500 / 1000 [ 50%] (Sampling) ## Chain 3: Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 3: Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 3: Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 3: Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 3: Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.052083 seconds (Warm-up) ## Chain 3: 0.042977 seconds (Sampling) ## Chain 3: 0.09506 seconds (Total) ## Chain 3: print(fit2) ## Inference for Stan model: 5b73686886069c0bad70513d4ea4141a. ## 3 chains, each with iter=1000; warmup=200; thin=2; ## post-warmup draws per chain=400, total post-warmup draws=1200. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## b 21.90 0.06 1.56 18.56 20.96 21.92 22.91 24.83 587 1.00 ## sigma 85.49 0.58 15.60 61.11 74.29 83.15 94.85 122.67 727 1.01 ## lp__ -93.62 0.05 1.29 -96.82 -94.22 -93.30 -92.70 -92.16 609 1.00 ## ## Samples were drawn using NUTS(diag_e) at Mon May 18 14:27:30 2020. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). 其中fit的最後一行是修改各種條件的示例：\nchains至少要三條； iter一開始可以設定在500~1000左右，確定模型可以收斂以後，再加大這個數值以獲得穩定的事後統計量，多多益善； warmup，也就MCMC採樣開始後多少樣本可以丟棄。這個數值需要參考trace plot； thin，通常只需要保持默認值 1。和WinBUGS, JAGS相比Stan算法採集的事後樣本自相關比較低。 Step 6, 並行（平行）計算的設定 如果你寫出來的貝葉斯模型需要很長時間的計算和收斂，可以充分利用你的計算機的多核計算，把每條MCMC鏈單獨進行計算加速這個過程：\nparallel::detectCores() #我的桌上型電腦有8個核可以用於平行計算 ## [1] 8 但是平行計算時如果計算中出錯則由於每條鏈都是相互獨立地進行，報錯就減少了。所以如果要使用多核同時計算的話，建議先減少採樣數，確認不會報錯以後再用多核平行計算增加採樣量。\nrstan_options(auto_write = TRUE) options(mc.cores = parallel::detectCores()) Step 7, 計算貝葉斯可信區間和貝葉斯預測區間 這一步就又回到一開始提出的研究問題上來，我們來計算基本年收的貝葉斯可信區間和貝葉斯預測區間。\nms \u0026lt;- rstan::extract(fit) quantile(ms$b, probs = c(0.025, 0.975)) ## 2.5% 97.5% ## 18.67987 24.97108 d_mcmc \u0026lt;- data.frame(a = ms$a, b = ms$b, sigma = ms$sigma) head(d_mcmc) ## a b sigma ## 1 -103.92295 22.04743 70.39829 ## 2 -30.41656 20.16582 74.35210 ## 3 -95.35165 21.32534 75.19714 ## 4 -10.88849 19.01547 68.02757 ## 5 -177.04183 23.49984 81.40161 ## 6 -146.45260 22.73838 95.97706 p1 \u0026lt;- ggplot(d_mcmc, aes(x = a, y = b)) + geom_point(shape = 1, size = 4) ggExtra::ggMarginal( p = p1, type = \u0026#39;density\u0026#39;, margins = \u0026#39;both\u0026#39;, size = 4, colour = \u0026#39;black\u0026#39;, fill = \u0026#39;#2D077A\u0026#39; ) Figure 7: MCMC樣本的兩個模型參數的事後散點圖，及它們之間的邊緣分佈密度圖。 從圖7中可觀察到該貝葉斯線性模型獲得的事後模型參數樣本中，截距a，和斜率b之間呈極強的負相關關係。也就是說，截距是工資的起點（年齡爲0歲時），這個起點的理論值越低，斜率越大（歲年齡增加工資上升的速度越大）。\n根據上面分析的結果，下面的R代碼可以計算一名50歲的人被這家公司採用的時候，她/他的預期基本年收入的分佈（中獲得的MCMC樣本），和她/他的預期總年收的預測分佈（中獲得的MCMC樣本）。\nN_mcmc \u0026lt;- length(ms$lp__) y50_base \u0026lt;- ms$a + ms$b*50 y50 \u0026lt;- rnorm(n = N_mcmc, mean = y50_base, sd = ms$sigma) d_mcmc \u0026lt;- data.frame(a = ms$a, b = ms$b, sigma = ms$sigma, y50_base, y50) head(d_mcmc) ## a b sigma y50_base y50 ## 1 -103.92295 22.04743 70.39829 998.4488 953.4024 ## 2 -30.41656 20.16582 74.35210 977.8746 861.2176 ## 3 -95.35165 21.32534 75.19714 970.9152 1076.7587 ## 4 -10.88849 19.01547 68.02757 939.8852 877.2139 ## 5 -177.04183 23.49984 81.40161 997.9499 1109.8183 ## 6 -146.45260 22.73838 95.97706 990.4664 1063.0656 # the following codes are also available from the author\u0026#39;s page: # https://github.com/MatsuuraKentaro/RStanBook/blob/master/chap04/fig4-8.R # library(ggplot2) source(\u0026#39;commonRstan.R\u0026#39;) # load(\u0026#39;output/result-model4-5.RData\u0026#39;) ms \u0026lt;- rstan::extract(fit) X_new \u0026lt;- 23:60 N_X \u0026lt;- length(X_new) N_mcmc \u0026lt;- length(ms$lp__) set.seed(1234) y_base_mcmc \u0026lt;- as.data.frame(matrix(nrow=N_mcmc, ncol=N_X)) y_mcmc \u0026lt;- as.data.frame(matrix(nrow=N_mcmc, ncol=N_X)) for (i in 1:N_X) { y_base_mcmc[,i] \u0026lt;- ms$a + ms$b * X_new[i] y_mcmc[,i] \u0026lt;- rnorm(n=N_mcmc, mean=y_base_mcmc[,i], sd=ms$sigma) } customize.ggplot.axis \u0026lt;- function(p) { p \u0026lt;- p + labs(x=\u0026#39;X\u0026#39;, y=\u0026#39;Y\u0026#39;) p \u0026lt;- p + scale_y_continuous(breaks=seq(from=200, to=1400, by=400)) p \u0026lt;- p + coord_cartesian(xlim=c(22, 61), ylim=c(200, 1400)) return(p) } d_est \u0026lt;- data.frame.quantile.mcmc(x=X_new, y_mcmc=y_base_mcmc) p \u0026lt;- ggplot.5quantile(data=d_est) p \u0026lt;- p + geom_point(data=Salary, aes(x=X, y=Y), shape=1, size=3) p \u0026lt;- customize.ggplot.axis(p) # ggsave(file=\u0026#39;output/fig4-8-left.png\u0026#39;, plot=p, dpi=300, w=4, h=3) p Figure 8: MCMC樣本計算獲得的基本年收的貝葉斯可信區間。 d_est \u0026lt;- data.frame.quantile.mcmc(x=X_new, y_mcmc=y_mcmc) p \u0026lt;- ggplot.5quantile(data=d_est) p \u0026lt;- p + geom_point(data=Salary, aes(x=X, y=Y), shape=1, size=3) p \u0026lt;- customize.ggplot.axis(p) p Figure 9: MCMC樣本計算獲得的預期總年收的貝葉斯預測區間。（顏色較深的是50%預測區間帶，黑線是事後樣本的中央值） # ggsave(file=\u0026#39;output/fig4-8-right.png\u0026#39;, plot=p, dpi=300, w=4, h=3) 練習題 用模擬數據來嘗試進行貝葉斯t檢驗\nset.seed(123) N1 \u0026lt;- 30 N2 \u0026lt;- 20 Y1 \u0026lt;- rnorm(n=N1, mean=0, sd=5) Y2 \u0026lt;- rnorm(n=N2, mean=1, sd=4) 請繪製上面代碼生成的兩組數據的示意圖 d1 \u0026lt;- data.frame(group=1, Y=Y1) d2 \u0026lt;- data.frame(group=2, Y=Y2) d \u0026lt;- rbind(d1, d2) d$group \u0026lt;- as.factor(d$group) p \u0026lt;- ggplot(data=d, aes(x=group, y=Y, group=group, col=group)) p \u0026lt;- p + geom_boxplot(outlier.size=0) p \u0026lt;- p + geom_point(position=position_jitter(w=0.4, h=0), size=2) p Figure 10: 隨機生成的兩組數據的散點圖和箱式圖。 #ggsave(file=\u0026#39;fig-ex1.png\u0026#39;, plot=p, dpi=300, w=4, h=3) 寫下相當於t檢驗的數學式，表示各組之間方差或者標準差如果相等時，均值比較的檢驗模型。 hypotheses:\nobservations in each group follow a normal distribution all observations are independent The two population variance/standard deviations are known (and can be considered equal) \\[ \\text{H}_0: \\mu_2 - \\mu_1 = 0 \\\\ \\text{H}_1: \\mu_2 - \\mu_1 \\neq 0 \\\\ \\text{If H}_0 \\text{ is true, then:} \\\\ Z=\\frac{\\bar{Y_2} - \\bar{Y_1}}{\\sqrt{(\\sigma_2^2/n_2) + (\\sigma_1^2/n_1)}} \\\\ \\text{follows a standard normal distribution with zero mean} \\\\ \\Rightarrow \\text{ if two variances are considered the same}\\\\ Y_1[n] \\sim N(\\mu_1, \\sigma) \\;\\; n = 1,2,\\dots,N \\\\ Y_2[n] \\sim N(\\mu_2, \\sigma) \\;\\; n = 1,2,\\dots,N \\]\n寫下上一步模型的Stan代碼，並嘗試在R裏運行 Stan代碼如下：\ndata { int N1; int N2; real Y1[N1]; real Y2[N2]; } parameters { real mu1; real mu2; real\u0026lt;lower=0\u0026gt; sigma; } model { for (n in 1:N1) Y1[n] ~ normal(mu1, sigma); for (n in 1:N2) Y2[n] ~ normal(mu2, sigma); } R代碼如下：\nlibrary(rstan) ## Loading required package: StanHeaders ## Loading required package: ggplot2 ## rstan (Version 2.19.3, GitRev: 2e1f913d3ca3) ## For execution on a local, multicore CPU with excess RAM we recommend calling ## options(mc.cores = parallel::detectCores()). ## To avoid recompilation of unchanged Stan programs, we recommend calling ## rstan_options(auto_write = TRUE) data \u0026lt;- list(N1=N1, N2=N2, Y1=Y1, Y2=Y2) exe13 \u0026lt;- stan_model(file = \u0026quot;stanfiles/ex3.stan\u0026quot;) fit \u0026lt;- sampling(exe13, data=data, seed=1234) ## ## SAMPLING FOR MODEL \u0026#39;ex3\u0026#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 1.5e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.027413 seconds (Warm-up) ## Chain 1: 0.022648 seconds (Sampling) ## Chain 1: 0.050061 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL \u0026#39;ex3\u0026#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 4e-06 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 2: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 2: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 2: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 2: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 2: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 2: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 2: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 2: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 2: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 2: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 2: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.028538 seconds (Warm-up) ## Chain 2: 0.021555 seconds (Sampling) ## Chain 2: 0.050093 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL \u0026#39;ex3\u0026#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 5e-06 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 3: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 3: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 3: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 3: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 3: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 3: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 3: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 3: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 3: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 3: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 3: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.026656 seconds (Warm-up) ## Chain 3: 0.021916 seconds (Sampling) ## Chain 3: 0.048572 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL \u0026#39;ex3\u0026#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 3e-06 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 4: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 4: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 4: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 4: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 4: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 4: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 4: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 4: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 4: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 4: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 4: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.027425 seconds (Warm-up) ## Chain 4: 0.020884 seconds (Sampling) ## Chain 4: 0.048309 seconds (Total) ## Chain 4: fit ## Inference for Stan model: ex3. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## mu1 -0.24 0.01 0.84 -1.93 -0.80 -0.23 0.30 1.40 3805 1 ## mu2 1.59 0.02 0.99 -0.32 0.92 1.61 2.25 3.50 3739 1 ## sigma 4.46 0.01 0.46 3.66 4.14 4.42 4.76 5.45 3402 1 ## lp__ -97.74 0.03 1.23 -100.89 -98.32 -97.42 -96.84 -96.33 1879 1 ## ## Samples were drawn using NUTS(diag_e) at Mon May 18 16:51:40 2020. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). 從獲取到的事後參數的MCMC樣本計算 \\(\\text{Prob}[\\mu_1 \u0026lt; \\mu_2]\\)： ms \u0026lt;- extract(fit) prob \u0026lt;- mean(ms$mu1 \u0026lt; ms$mu2) #=\u0026gt; 0.932 prob ## [1] 0.9235 所以可以認爲地一組均值，小於第二組均值的事後概率是93.2%\n如果不能認爲兩組的方差相等的話，模型又該改成什麼樣子？ \\[ Y_1[n] \\sim N(\\mu_1, \\sigma_1) \\;\\; n = 1,2,\\dots,N \\\\ Y_2[n] \\sim N(\\mu_2, \\sigma_2) \\;\\; n = 1,2,\\dots,N \\]\ndata { int N1; int N2; real Y1[N1]; real Y2[N2]; } parameters { real mu1; real mu2; real\u0026lt;lower=0\u0026gt; sigma1; real\u0026lt;lower=0\u0026gt; sigma2; } model { for (n in 1:N1) Y1[n] ~ normal(mu1, sigma1); for (n in 1:N2) Y2[n] ~ normal(mu2, sigma2); } 下面的代碼相當於實施Welch的t檢驗：\nlibrary(rstan) data \u0026lt;- list(N1=N1, N2=N2, Y1=Y1, Y2=Y2) exe15 \u0026lt;- stan_model(file = \u0026quot;stanfiles/ex5.stan\u0026quot;) fit \u0026lt;- sampling(exe15, data=data, seed=1234) ## ## SAMPLING FOR MODEL \u0026#39;ex5\u0026#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 1.2e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.03039 seconds (Warm-up) ## Chain 1: 0.023831 seconds (Sampling) ## Chain 1: 0.054221 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL \u0026#39;ex5\u0026#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 4e-06 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 2: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 2: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 2: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 2: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 2: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 2: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 2: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 2: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 2: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 2: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 2: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.028121 seconds (Warm-up) ## Chain 2: 0.026473 seconds (Sampling) ## Chain 2: 0.054594 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL \u0026#39;ex5\u0026#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 1.6e-05 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 3: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 3: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 3: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 3: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 3: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 3: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 3: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 3: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 3: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 3: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 3: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.030293 seconds (Warm-up) ## Chain 3: 0.027682 seconds (Sampling) ## Chain 3: 0.057975 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL \u0026#39;ex5\u0026#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 3e-06 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 4: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 4: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 4: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 4: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 4: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 4: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 4: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 4: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 4: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 4: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 4: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.030591 seconds (Warm-up) ## Chain 4: 0.0252 seconds (Sampling) ## Chain 4: 0.055791 seconds (Total) ## Chain 4: fit ## Inference for Stan model: ex5. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## mu1 -0.22 0.01 0.94 -2.07 -0.85 -0.23 0.40 1.68 4084 1 ## mu2 1.63 0.01 0.82 0.04 1.09 1.62 2.15 3.25 4068 1 ## sigma1 5.12 0.01 0.71 3.94 4.62 5.05 5.53 6.74 3863 1 ## sigma2 3.63 0.01 0.64 2.66 3.18 3.55 3.98 5.08 3002 1 ## lp__ -95.33 0.03 1.44 -98.86 -96.05 -95.04 -94.28 -93.52 1916 1 ## ## Samples were drawn using NUTS(diag_e) at Mon May 18 16:52:15 2020. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). ms \u0026lt;- rstan::extract(fit) prob \u0026lt;- mean(ms$mu1 \u0026lt; ms$mu2) #=\u0026gt; 0.93725 prob ## [1] 0.9345 ","date":1547510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547510400,"objectID":"6cf15125b94e4c44644469ee9cce390f","permalink":"https://wangcc.me/post/simple-linear-regression-using-rstan/","publishdate":"2019-01-15T00:00:00Z","relpermalink":"/post/simple-linear-regression-using-rstan/","section":"post","summary":"Rstan 學習筆記 Chapter 4.4","tags":["Bayesian","Medical Statistics"],"title":"Simple linear regression using Rstan--Rstan Wonderful R-(2)","type":"post"},{"authors":null,"categories":["To do"],"content":" 2018-12-03 Learn Emacs; day 1 done Return the email from LP; learn reversible R markdown /MS word documents package; Pay the you-know-what ticket; Read Evidence and Evolution 1% 2018-12-04 Prepare the manuscript for AJCN 10% left; FINALLY!!!!! Learn Emacs; day 2\u0026amp;3 done; 2018-12-05 Read the victim book, and keep memo to P151; Learn Emacs; day 4\u0026amp;5 done; 2018-12-06 Move all todo to Org-mode page ","date":1543795200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543795200,"objectID":"e8494962d5e661d0ae4208fdf80c5a23","permalink":"https://wangcc.me/post/2018-12-todo/","publishdate":"2018-12-03T00:00:00Z","relpermalink":"/post/2018-12-todo/","section":"post","summary":"todo list","tags":["To do"],"title":"2018-12 todo","type":"post"},{"authors":null,"categories":["R techniques","statistics"],"content":" P16\n事後分布 \\(p(\\theta | Y)\\)の値が最大になる点\\(\\theta^*\\)を事後確率最大推定値 (maximum a posteriori estimate)と呼ぶ．略してMAP推定値 (MAP estimate)．\n我們把能夠將事後概率分布取極大值的參數點 \\(\\theta^*\\) 稱爲事後概率的最大似然估計值 (maximum a posteriori estimate)，簡稱 MAP估計值 (MAP estimate)。\nP19\n統計建模的一般順序\n確定分析目的 確定數據分布 想象數據產生本身的機制：思考數據與數據之間可能的關系 寫下你所認爲的數據模型的數學表達式 用 R 模擬(simulation)並確認前一步寫下的數學模型的性質，特點 用 Stan 實際進行模型參數的推斷 獲得推斷結果，解釋其事後概率分布的意義，繪制易於理解的模型示意圖 繪制成功之後的模型示意圖和最先使用的模型之間進行比對，重新查缺補漏 P23\nただいたずらにモデルを複雑化させるのは解釈のしにくさを招く．\nP30\n最初にmodel ブロックの尤度の部分（と事前分布の部分）を書く．その尤度の部分に登場した変数のうち，データの変数をdataブロックに，残りの変数をparametersブロックに書いていく．\nStan的基本文法構成\ndata { 數據描述 } parameters { 想要進行MCMC事後樣本採集的參數描述 } model { p(Y|theta) 似然的描述 先驗概率分布 p(theta) 的描述 } 把下面的模型\n\\[ \\begin{aligned} Y[n] \u0026amp; \\sim \\text{Normal}(\\mu, 1) \\;\\; n = 1, \\dots, N \\\\ \\mu \u0026amp; \\sim \\text{Normal}(0, 100) \\end{aligned} \\]\n翻譯成爲 Stan 模型語言是：\ndata { int N; real Y[N]; } parameters { real mu; } model { for (n in 1:N) { Y[n] ~ normal(mu, 1); } mu ~ normal(0, 100); } 其中我們按照實際模型書寫的順序 model -\u0026gt; data -\u0026gt; parameter 來逐個解釋：\nmodel 模塊中 for (n in 1:N) 開始的循環部分（三行）對應數學模型的 $Y[n] (, 1) n = 1, , N $　部分。 Stan 語言中，每一行描述的結尾需要用分號 ; 來結束。 mu ~ normal(0,100) 則對應數學模型中寫的先驗概率 \\(\\mu \\sim \\text{Normal}(0, 100)\\) 部分。這裏給均值的先驗概率分佈是一個方差很大的無信息先驗概率分佈 (noninformative prior)。事實上在 Stan 軟件語言中，如果不特別指出先驗概率分佈，系統會默認給參數以無信息的先驗概率分佈，這樣即使沒有這一行，模型也是可以跑的。 data 模塊中寫明的是 model 模塊中描述的模型將要使用的數據。它包括宣示數據的個數（樣本量 \\(N\\)），以及數據本身。其中 int N 意爲樣本量的數量是整數個 (integer)，real Y[N] 則宣示實數有 N 個作爲數據。 parameter 模塊是告訴軟件需要採樣且關注的未知參數 (parameter) 是 mu 在 Stan 語言中，還可以和其他語言一樣爲模型加註解釋的文字，只需要在想要做註釋的文字最開始的部分增加 //，如果註釋的文字超過一行，那麼在註釋的模塊前後加上 /* 和 */ 即可。 另外，目前爲止主流的貝葉斯模型軟件中使用精確度 (precision) ，也就是方差的倒數來描述正態分佈 normal(mean, 1/variance) ，但是在Stan的語法中使用的是 normal(mean, sd)，也就是用標準差來描述正態分佈。 寫Stan（或者說寫大多數的代碼）時，請遵守以下的原則：\n適當縮進，以便於閱讀； 表示數據的部分用大寫字母，表示參數的部分，用小寫字母； 每個部分之間至少使用一個空行加以區分； 請不要用camelCase這樣的方式（單詞之間用大寫隔開），請在單詞之間用下劃線 camel_case 的標記方法； ~或者=前後用一個字符大小的空格來隔開。 最低限度的話，也請依照1,2兩個標準來書寫你的Stan代碼。不爲他人，也爲自己將來再讀代碼時能快速理解其涵義。往Stan的官方論壇投稿時，也必須遵守它們在手冊裏提供的 “Stan Program Style Guide” 代碼書寫規則，也是對其他寫，讀代碼的人的尊重。\n","date":1542844800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542844800,"objectID":"5c09356656c39b88998de58a977fa0cd","permalink":"https://wangcc.me/post/rstan-wonderful-r/","publishdate":"2018-11-22T00:00:00Z","relpermalink":"/post/rstan-wonderful-r/","section":"post","summary":"Rstan 學習筆記","tags":["Medical Statistics","Bayesian"],"title":"Rstan Wonderful R-(1)","type":"post"},{"authors":null,"categories":["To do"],"content":" 2018-11-05 Paper comments to Lin; Cancel the tsumitate; Ask about the insurance; Submit nenmatsu; Modify the poster (adding logo), etc; 2018-11-06 Bayesian statistics practical 6 done; Prepare AJCN template (Rmd) file; Prepare AJCN draft (.docx) file; MLCA analyses done stratified by men and women; 2018-11-07 fix the racket before Wed. Bayesian statistics Chapter 7 done 20%; Check the problem of the gh-page on github. 2018-11-08~11 Search for guideline of eradication among adolescents of H. pylori in the world; Protocol comments to TMS; Read the victim book, and keep memo; Prepare the manuscript for AJCN 10%; Modify the Presentation Slides 2018-11-12 Bayesian statistics Chapter 7 done 80%; Read the victim book, and keep memo; Comment the Presentation Slides from LP; Labeling the learning note bookdown book. 2018-11-13 Bayesian statistics Chapter 7 done 100%; Prepare the manuscript for AJCN 25%; Comment the manuscript from Lin; Domestic travel paper work 2018-11-14 Read the victim book, and keep memo; Prepare the manuscript for AJCN 40%; 2018-11-15~16 Bayesian statistics practical 7 done 100%; Prepare the manuscript for AJCN the tables, figures done; 2018-11-19 Prepare the manuscript for AJCN 80%; Confirm the feedback of the project; 2018-11-20 Comment KS application form (2018-11-21). Perform the analyses using sex as an interaction, combine three tables. done 20% Read the victim book, and keep memo; Bayesian statistics Chapter 8 20% done; 2018-11-21 Learn Stan for Bayesian stats 1%; Perform the analyses using sex as an interaction, combine three tables. done; Bayesian statistics Chapter 8 30% done; 2018-11-22 Learn Stan for Bayesian stats 5%; Bayesian statistics Chapter 8 35% done; Manuscript to AMU about the overseas study 2018-11-24 Learn Stan for Bayesian stats 6%; Review the JAT meta-ana paper (deadline: 2018-11-28); 2018-11-25~12-02 (annual leave) ","date":1541376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541376000,"objectID":"70f7044c5a49220add6d632e070ce6f7","permalink":"https://wangcc.me/post/2018-11-todo/","publishdate":"2018-11-05T00:00:00Z","relpermalink":"/post/2018-11-todo/","section":"post","summary":"todo list","tags":["To do"],"title":"2018-11 todo","type":"post"},{"authors":null,"categories":["To do"],"content":" 2018-10-02 ~ 2018-10-17 Prepare the proposal to JSPS. 2018-10-02 Background, key questions objective, importance 2018-10-09 Bayesian statistics Chapter 1 done; 2018-10-10 Bayesian statistics Chapter 2 done; 2018-10-11~12 Bayesian statistics Chapter 3 done; 2018-10-13 Review of the paper about HIV; 2018-10-15 Wait for the comments from LP, SA; 2018-10-16 Modification of the proposal; Submission of the proposal; Learn how to apply for the data usage of Japan National Nutrional Survey; 2018-10-17~19 Bayesian statistics Chapter 4 done; 2018-10-23 Bayesian statistics Chapter 5 done; 2018-10-24 Learn how to use Stan (just a beginning); 2018-10-25 Some preliminary analysis using vague prior to do MabeRCT; Bayesian statistics Chapter 6 done; Talk with LP AS, the next steps of writing the carb paper; 2018-10-26~31 Prepare poster for conference. LCA analyses done stratified by men and women; ","date":1538438400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538438400,"objectID":"34b37cdaf7e4084adef1ce81f1752210","permalink":"https://wangcc.me/post/2018-10-todo/","publishdate":"2018-10-02T00:00:00Z","relpermalink":"/post/2018-10-todo/","section":"post","summary":"todo list","tags":["To do"],"title":"2018_10 todo","type":"post"},{"authors":["Masahiro Nakatochi","Yingsong Lin","Hidemi Ito","Kazuo Hara","Fumie Kinoshita","Yumiko Kobayashi","Hiroshi Ishii","Masato Ozaka","Takashi Sasaki","Naoki Sasahira","Manabu Morimoto","Satoshi Kobayashi","Makoto Ueno","Shinichi Ohkawa","Naoto Egawa","Sawako Kuruma","Mitsuru Mori","Haruhisa Nakao","Chaochen Wang","Takeshi Nishiyama","Takahisa Kawaguchi","Meiko Takahashi","Fumihiko Matsuda","Shogo Kikuchi","Keitaro Matsuo"],"categories":null,"content":"Abstract Genome-wide association studies (GWASs) have identified many single nucleotide polymorphisms (SNPs) that are significantly associated with pancreatic cancer susceptibility. We sought to replicate the associations of 61 GWAS-identified SNPs at 42 loci with pancreatic cancer in Japanese and to develop a risk model for the identification of individuals at high risk for pancreatic cancer development in the general Japanese population. The model was based on data including directly determined or imputed SNP genotypes for 664 pancreatic cancer case and 664 age- and sex-matched control subjects. Stepwise logistic regression uncovered five GWAS-identified SNPs at five loci that also showed significant associations in our case-control cohort. These five SNPs were included in the risk model and also applied to calculation of the polygenic risk score (PRS). The area under the curve determined with the leave-one-out cross-validation method was 0.63 (95% confidence interval, 0.60–0.66) or 0.61 (0.58–0.64) for versions of the model that did or did not include cigarette smoking and family history of pancreatic cancer in addition to the five SNPs, respectively. Individuals in the lowest and highest quintiles for the PRS had odds ratios of 0.62 (0.42–0.91) and 1.98 (1.42–2.76), respectively, for pancreatic cancer development compared with those in the middle quintile. We have thus developed a risk model for pancreatic cancer that showed moderately good discriminatory ability with regard to differentiation of pancreatic cancer patients from control individuals. Our findings suggest the potential utility of a risk model that incorporates replicated GWAS-identified SNPs and established demographic or environmental factors for the identification of individuals at increased risk for pancreatic cancer development.\n","date":1536278400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536278400,"objectID":"23ec6ecf170c41fed726ccf86a8e81d3","permalink":"https://wangcc.me/publication/journal-article/predictionpancreaticgwas/","publishdate":"2018-09-07T00:00:00Z","relpermalink":"/publication/journal-article/predictionpancreaticgwas/","section":"publication","summary":"Abstract Genome-wide association studies (GWASs) have identified many single nucleotide polymorphisms (SNPs) that are significantly associated with pancreatic cancer susceptibility. We sought to replicate the associations of 61 GWAS-identified SNPs at 42 loci with pancreatic cancer in Japanese and to develop a risk model for the identification of individuals at high risk for pancreatic cancer development in the general Japanese population. The model was based on data including directly determined or imputed SNP genotypes for 664 pancreatic cancer case and 664 age- and sex-matched control subjects.","tags":null,"title":"Prediction model for pancreatic cancer risk in the general Japanese population","type":"publication"},{"authors":null,"categories":["study abroad","statistics","To do"],"content":" Data analysis finish by 2018-07-2431 Paper structure confirm by 2018-08-01 Paper draft complete by 2018-08-16 2018-06-24 Read and try to repeat Rll\u0026rsquo;s method in R and familarize the dataset ASAP Two papers applying Repeated Measures LCA 2018-06-25 Meeting with supervisor and Susanna Confirm the cutoff of carborhydrate consumption Talk with Rll ask about the methodology and dataset 2018-06-26 Send the summarised memo of meeting to Supervisor and etc. Read the first part fundamentals of LCA. 2018-06-27 London R in UCL Germany lost their game against South Korea, UNBELIEVEABLE 2018-06-28 Read the book collins2010latent - Latent Class and Latent Transition Analysis: With Applications in the Social, Behavioral, and Health Sciences (Done until 4.2) Learn how to do LCA in R 2018-06-29 Read the book collins2010latent - Latent Class and Latent Transition Analysis: With Applications in the Social, Behavioral, and Health Sciences (Done until 4.3) Data management for NDNS 8 years data (70%) Learn how to do LCA in R Start to analysis the data according to the discussion on 25th(30%) Day1 data analysis results summary 2018-07-01 Relax and do nothing Buy some drink to enjoy the night with classmates(HB) 2018-07-02/03 Send some preliminary results to co-authors Japan lost the game to Belgium, but they are the glory of Asia\u0026ndash;heartbreaking 2018-07-04 \u0026ldquo;consider separating weekdays from weekends if we are not averaging the four days?\u0026quot; 2018-07-05 Test and confirm the availability of LCA in SAS Learn how to do LCA in SAS with NDNS data 2018-07-06 Learn how to do LCA with random effects in SAS Find whether there is any possibility of conducting the same method in R or STATA (no there is no way) 2018-07-07~09 \u0026ldquo;Maybe we should try with the threshold at 25% only as per the existing guidelines (although those are per meal)?\u0026quot; 2018-07-10 ~~Meet with tutor; Start writing about the methodology; Try to start writing about the introduction; 2018-07-11 Try to summarise the meeting memo yesterday; Re-analyse the data with new cut-off values (25, 50, 75); Re-analyse the data with new cut-off values (50); 2018-07-12~22 Use latent class growth analysis; Use multilevel latent class analysis; Think about the mathmatical theory behind the mixed LCA, write to PROC LCA group if necessary; 2018-07-23~25 Learn about the survey package in R Finish writing about the methodology; Write some introduction; 2018-07-26 Let\u0026rsquo;s finish analysis of the classes and health outcomes. Read about the carbo-fibre ratio references. 2018-08-15 PM review Finish most of the discussion outlines and 2 pages of them. 2018-08-31 Finish revising the report according to comments from LP and SAM; Read RT\u0026rsquo;s report and send the comments; Confirm the deadline for funding applications; Prepare the abstract for conferences (UK and JP); Start preparing the paper for submit (MLCA part alone); Think about the schedules and plans after leaving London; Finish the post of Scotland trip. ","date":1529798400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1529798400,"objectID":"c54bccaba37559688dd82685f35290df","permalink":"https://wangcc.me/post/summer-project-schedule/","publishdate":"2018-06-24T00:00:00Z","relpermalink":"/post/summer-project-schedule/","section":"post","summary":"todo list for summer project","tags":["LSHTM","To do","Medical Statistics"],"title":"Summer Project Schedule","type":"post"},{"authors":null,"categories":["diary"],"content":"本次高地旅行（2018-06-13～2018-06-20）值得紀念，我們一行三人，在蘇格蘭自駕開了一個圈。沿路看過的高山流水，懸崖峭壁，狂風暴雨，和那些雨過天晴，都讓人久久難以忘懷。我們還經歷了頭一天輪胎爆胎，最後一天在愛丁堡城堡被偷錢包等各種奇葩的經歷。深感有必要在此簡單記錄下這段旅程，為了將來還記得這一年在英國的腳印。\nYour browser does not support the video tag. Your browser does not support the video tag. 2018-06-13 倫敦出發 出發之前，秀一下我們訂的20英鎊的機票（比機場市區往返的火車還便宜\u0026hellip;\u0026hellip;）：\n當天無事，我們降落愛丁堡時蘇格蘭展示了北方的大風夾着冰涼的雨水，警告我們即使是夏季的夜晚也是那麼的寒冷。順利在機場取了車（其實花了一個半小時，牆裂不推薦蘇格蘭當地的租車公司 Greenmotion，效率低下而且合同上陷阱不少，最後我們實在是覺得很煩，直接上了最高等級的整車安心保險\u0026ndash;後來的事實證明我們當初的選擇是那麼的正確），我們終於踏上了蘇格蘭之行的第一步。先在愛丁堡大學附近和多年未見的好友見面聊天吃吃吃了之後，我們找到了本次旅途中的第一家 Airbnb 房東在海邊的家。當晚其實下了一夜的狂風驟雨，透過閣樓的房間房頂上的玻璃，我們可以直接看到外面不停被風吹亂猙獰的樹枝。第二天一早風依然很大，但是我們無法克制想立刻看到大海的慾望，頂着狂風步行到了沙灘，看到那令人窒息的海邊的愛丁堡：\n這是我們租來的寶獅（标致）3008 SUV，7天保險全包，每天差不多63英鎊（約10000日元/560軟妹幣）：\n這輛車表面上看起來瀟洒，其實個人感覺不太耐用，華而不實。\n白天離開房東家以後我們車停在愛丁堡車站南的一個立體停車場，之後趁着天氣晴朗（但是大風四起，把許多路人吹得很凌亂），我們參觀了愛丁堡大學神學院：\n天氣雖然好，大風讓愛丁堡城堡關閉了遊客參觀，但是並不妨礙我們感受這座古老城市的藝術氣息：\n麥克尤恩大禮堂(McEwan Hall, The University of Edinburgh) 聖基爾斯大教堂(St Giles\u0026rsquo; Cathedral) 斯科特紀念塔(Scott Monument) 路邊的行為藝術： 登上卡爾頓山 (Calton Hill)俯瞰這座蘇格蘭的首府，安靜而令人嚮往。 2018-06-14 爱丁堡到格拉斯哥 傍晚離開了愛丁堡，我們向西驅車開往另一座老城，格拉斯哥。\n這一晚我們預定的 Airbnb 在格拉斯哥的郊區，是一排連排別墅中的一棟。這天入住的時候，由於沒有仔細閱讀房東在 Airbnb 上留下的入住須知，導致用鑰匙打開門之後警報聲大作，直至隔壁鄰居實在是受不了了跑來幫忙關掉警鈴整個世界才安靜。在此友情提醒諸位使用 Airbnb 時注意閱讀房東留下的入住須知。房東當日不在家，所以我們沒有浪費他們家的廚房設施，從超市採購了一大桶雞腿嘗試了白斬雞的做法（味道好極了！多謝 Linda）。\n這一日主要停留在愛丁堡，晚上我們也很早休息，為了第二天的長途跋涉。\n2018-06-15 從格拉斯哥離開，飛馳在高地去往天空島 一早從格拉斯哥離開後要先經過洛蒙德湖泊 (Loch Lomond)，沿着A82號公路，一路上一會兒下雨，一會兒天晴。在湖邊一個叫做路斯 (Luss) 的小鎮，我們在雨中下來看了看湖邊雲霧繚繞的碼頭，水很清澈，水鴨子們也悠閑地在游泳：\nYour browser does not support the video tag. 樹林里，清澈見底的溪流：\n雲霧中，在停車時發現山上是一個水力發電廠：\n洛蒙德湖在籠罩在水氣中：\n之後，我們在路上就經歷了爆胎事件。其經過其實很簡單:離開路斯往北行進的路上，其實道路是十分狹窄的。在某路段我眼角發現左前方路面上有一個拳頭大小的黑色障礙物 (後來想應該是塊山上掉下來的碎石)，當時如果路足夠寬的話，我當然可以躲閃一下。無奈右側迎面而來是一輛大卡車，無法躲閃，後面又有車跟着，不能剎車，於是就只好硬着頭皮前行，心裏希望只是個不太堅硬的小石子。結果過去以後，\u0026ldquo;咣噹\u0026quot;一聲，左前方的輪胎明顯下沉，握方向盤的雙手明顯有左右不平衡的感覺，於是滑行了幾百米後在一段正好施工路段的中間處停下查看輪胎狀況。不出所料，左前方輪胎癟了。此時距離我們離開格拉斯哥大概只有一個半小時左右的路程。圖中是我們等待了兩次雷雨，三集 Friends 過後，才姍姍來遲的路上救援小哥。幸好該寶獅（标致）SUV的後備箱里有一個備胎。\n換好備胎以後的寶獅。 換備胎的小哥讓我們開車開到前面去找輪胎廠換一個新的輪胎，這樣才不至於一直使用備胎，車速要總是限制在50邁以下。於是我們查了查地圖，確定最近的輪胎店的位置。發現如果往南走要回到格拉斯哥，往北則是越來越荒蕪，只有往西靠海邊有個叫做奧本的小鎮有輪胎修理點。於是我們驅車出發去之前並未計劃要去的這個叫做奧本的海港小鎮。想起來之前在愛丁堡，好友還推薦說你們如果順路可以去奧本的港口吃海鮮路邊攤 (seafood hut)，當時我們還想這樣會繞遠路，就沒有把奧本放進行程裏，結果我們這半路爆胎\u0026quot;因禍得福\u0026rdquo;，來到了這個本來並沒有在行程中的西海岸邊的小漁村。\n抵達奧本，換了兩三家輪胎廠，都說我們租的這輛寶獅款式太新，並沒有庫存的相同輪胎，真的是運氣不佳。於是我們只好停車在港口的碼頭之後去路邊攤邊吃海鮮，邊思考接下來腫麼辦 (其實主要就是來吃海鮮的)。\n一盤大龍蝦，就水裏撈上來直接清煮的那種，大約15鎊左右，吃的是大快朵頤！ 後來我們又點了一只麪包蟹，膏滿肉肥。 搞笑的是我們先飽餐了一頓之後離開海鮮攤位，經過下面的奧本火車站在附近散步一圈，同行的一個小朋友可憐兮兮地說，晚上還能不能再吃海鮮呀。於是我們回過頭第二次又去光顧了同一家 seafood hut. 奧本的海鮮真是讓人一步三回頭。 這是在去往天空島的路上經過的廣闊的高地，我們停車在一個紀念二戰時陣亡英格蘭蘇格蘭將士的廣場:\n紀念雕塑 2018-06-16 離開時而暴雨傾盆，時而天氣晴朗的天空島，去往德內斯 (Durness) 在天空島的早上，天氣陰沉沉，我們開車繞着整個半島走了一圈，也沒等到天晴可以看懸崖的時刻。倒是中途發現了一個極爲清澈的海水池\n之後來到蘇格萊最北的懸崖處，這裏名叫德內斯 一個陰森森的洞窟在懸崖下\n我們步行到深藍色海邊的黑色懸崖上，再往北就是北冰洋\n2018-06-17 在靠近 Shetland 的無路可走的盡頭 確切地說，這裏才能算是蘇格蘭和英格蘭島上地理位置上的最北端。地標在這裏顯示往北是 Shetland 往南則距離倫敦690英里 (1100公里左右)。許多人來到了這個道路終結的地方，在那一排色彩斑斕的小房子前，面朝大海。估計這裏的春暖花開一年也就幾天時間。抵達這裏，意味着我們脫繮的道路到了終點，天公作美，我們到此一遊相片留下之後，終於開始了往南回到現實的旅程。\n晚上到了因弗尼斯 (Inverness)， Airbnb 房東家的廚房又一次發揮了極爲重要的作用:\n房東給準備的早餐堪比4星級酒店\n2018-06-18 Inverness 和尼斯湖水怪 尼斯湖是一個顏色暗黑似乎深度內傷的湖，如果你看地圖，它是又細又長的一個小湖泊，這天天氣很冷 (六月)，寒風逼人，不知冬季時這裏的人要如何度過。無法想象。\n湖岸邊有個據說曾經是這附近土地領主的破敗城堡 Urquhart Castle。 處處可見飄揚的蘇格蘭旗，然而並沒有人因此逼着誰必須表態然後撕破臉，說你搞蘇獨，社會的包容度差距可見一斑。嘴上不用說，每個人內心都應該首先是個\u0026quot;獨立\u0026quot;的人，有獨立思想的個體。記得去臺灣的時候，不光有中華民國的青天白日旗，也有人拿着赤色紅旗，海峽對岸流淌着同樣華人血液的社會早已經進步到可以包容各種思想不同的意見的和諧，這一文明的光芒就如同太平洋上的燈塔。盼望有一天，中國，只有一個名字，沒有別的亂七八糟的定語，也不需要在這兩個字中間加各種各樣標新立異的字眼，只有一個 China 去代表這塊土地和有相同認同感的社會。\n晚上下榻的農家旅社 2018-06-19 回到愛丁堡 回到愛丁堡之後，我們的大廚 Linda 又跟我展示了一手好廚藝，在愛丁堡的中國超市買到了大大的福州魚丸。有一點點那個味道。\n","date":1529798400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1529798400,"objectID":"3a4bed1f1404395d21b76a6e5319ff86","permalink":"https://wangcc.me/post/scotland-highland-road-trip/","publishdate":"2018-06-24T00:00:00Z","relpermalink":"/post/scotland-highland-road-trip/","section":"post","summary":"Wild Scotland","tags":["experience","偶爾感慨","Scotland"],"title":"蘇格蘭高地7日公路之旅","type":"post"},{"authors":null,"categories":["To do"],"content":" 大塚敏美財団メールを返信する PCA analysis learning Cluster analysis learning 建立一個論文日程表格 日程計劃包括每周進度 分析進度 論文寫作進度 學習 Latent Class Analysis 方法; 熟悉 NDNS 數據框架結構，思考分析方法; Comment and response to AACE Test on Ubuntu 18.04 尝试从Ubuntu, 日本語を試す 更新 LSHTM 統計學學習筆記 生存分析章節-Cox-models; 更新 LSHTM 統計學學習筆記，GLM Multinomial logistic regression model; 更新 LSHTM 統計學學習筆記，GLM Oridinal logisitic regression model; 更新 LSHTM 統計學學習筆記，貝葉斯進階章節; 更新 LSHTM 統計學學習筆記，用 STATA 或者 R 分析 SME 流行病學數據的實踐部分; 更新生存分析，更多具體細節及練習[Cox]; 更新生存分析，更多具體細節及練習[AFT]; 辦理法國簽證所需的材料; 法國行程取消(2018-06-20) 大學學生在校證明; 銀行三個月存款證明 歐洲之星(7月)訂票; 巴黎青年旅館訂房 (聯繫下正好在法國的 なっちゃん家?); 旅行保險; BRP 複印; 護照複印; [xa] 簽證申請書; 近三个月内的证件照，尺寸3.5cm x 4.5cm，白底 把 LaTeX 模板 調整到 Bookdownplus 的模板之一 複習 Bayesian 2011年 LSHTM 試題 Paper 1-1; 2011年 LSHTM 試題 Paper 1-3 2011年 LSHTM 試題 Paper 1-4; 2011年 LSHTM 試題 Paper 1-7; 2012年 LSHTM 試題 Paper 1-1; [done 2018-06-03 17:00, mean and variance for data transformation] 2012年 LSHTM 試題 Paper 1-3; [done 2018-06-03 17:30 linear regression, really need to pay attention in reading the question] 2012年 LSHTM 試題 Paper 1-4; 2012年 LSHTM 試題 Paper 1-7; [done 2018-06-03 22:30 Binomial exact test] 2012年 LSHTM 試題 Paper 2-1; [done 2018-06-04 20:00 Mock test done, coefficient and rho, 75% about] 2012年 LSHTM 試題 Paper 2-2; 2012年 LSHTM 試題 Paper 2-3; 2012年 LSHTM 試題 Paper 2-4; [done 2018-06-04 20:00 Mock test done, too much to write as a survival question, about 80%] 2012年 LSHTM 試題 Paper 2-5; [done 2018-06-04 20:00 Mock test done, too much to write about 80%] 2012年 LSHTM 試題 Paper 2-6; 2012年 LSHTM 試題 Paper 2-8; [done 2018-06-04 20:00 Mock test done, GLM about 90%] 2013年 LSHTM 試題 Paper 1-1; 2013年 LSHTM 試題 Paper 2-4; 2014年 LSHTM 試題 Paper 2-4; 2015年 LSHTM 試題 Paper 2-4; 2016年 LSHTM 試題 Paper 2-4; 2017年 LSHTM 試題 Paper 2-4; ","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"48a1239bc54e00599909b655860a299d","permalink":"https://wangcc.me/post/todo-2018-june/","publishdate":"2018-06-01T00:00:00Z","relpermalink":"/post/todo-2018-june/","section":"post","summary":"todo list","tags":["To do"],"title":"Todo 2018 June-September","type":"post"},{"authors":null,"categories":["To do"],"content":"試試看用 Blogdown 來管理自己的待辦事項:\nCausal inference 作業 [done 2018-05-18 12:00] 學習去年的 Hierarchical discrete data modeling Lecture 1-4; [2018-05-08 23:45 done] 學習去年的 Generalised linear mixed effect modelling lecture 5-6; [2018-05-12 16:00 done] 完成 ASM Edmund 作業; [2018-05-18 03:20 done] 寫一篇日志回顧最近 LSHTM 的生活; [2018-05-06 done] 複習 Probablity;(2018-05-16 to 17) 複習 Inference;(2018-05-17 to 18) 複習 Clinical Trial;(2018-05-19 to 20) 複習 Basic Epi; (2018-05-20 to 21) 複習 Analytical Technique;(2018-05-22 to 24) 複習 Regression; (2018-05-24 to 25) 複習 Robust statistics; (2018-05-27) 複習 GLM (2018-05-28 to 29) 複習 Survival Analysis (2018-05-39 to 31) 反省自己爲什麼效率這麼低。。。。。 解決辦法就是把自己的 to do 放在網頁上，刺激自己。(2018-05-06 done) 2008年 LSHTM 試題 Paper 2-2 (survival question); [done 2018-05-29 17:01] 2009 年 LSHTM 試題 Paper 2-3 (survival question); [done 2018-05-29 18:19] 2011年 LSHTM 試題 Paper 1-2; [done 2018-05-24 17:01 40 min used] 2011年 LSHTM 試題 Paper 1-5; [2018-05-18 12:21 done] [2018-06-03 12:51 done again 80% 17 min] 2011年 LSHTM 試題 Paper 1-6; [done 2018-05-27 11:15] 2011年 LSHTM 試題 Paper 1-8; [2018-05-19 22:50 done] 2012年 LSHTM 試題 Paper 1-2; [2018-05-19 23:43 done] [2018-06-03 16:30 Wald test, MLE, compare the inverse of mean] 2012年 LSHTM 試題 Paper 1-5; [2018-05-25 21:32 done 35 min] 2012年 LSHTM 試題 Paper 1-6; [2018-05-27 13:39 done 15 min] 2012年 LSHTM 試題 Paper 1-8; [2018-05-18 22:00 done] [2018-06-04 11:27 done again, when use transformation for MLE, we need to substitute it back to the loglikelihood function to find the standard error of the newly transformed variable] 2012年 LSHTM 試題 Paper 2-7; [done 2018-05-25 35 min but brutally beaten] [done 2018-06-04 20:00 Mock test 90%] 2013年 LSHTM 試題 Paper 1-2; [done 2018-04-30, challenged again 2018-05-25 quadratic term interpration is needed] 2013年 LSHTM 試題 Paper 1-3; [done 2018-05-20 01:00 done] [done again on 2018-06-04 12:22 score test and comparison with lrt, hard way of doing lrt with binomial data, time consuming, about 80% got] 2013年 LSHTM 試題 Paper 1-4; [done 2018-04-30] 2013年 LSHTM 試題 Paper 1-5; [done 2018-05-22 23:38] 2013年 LSHTM 試題 Paper 1-6; [done 2018-04-30] [done again 2018-06-03 15 min 80%] 2013年 LSHTM 試題 Paper 1-7; [done 2018-05-27 14:09 18 min used] 2013年 LSHTM 試題 Paper 1-8; [done 2018-05-22 23:38] 2013年 LSHTM 試題 Paper 2-1 [done 2018-05-07 1800] 2013年 LSHTM 試題 Paper 2-2 [done 2018-05-07 1900] 2013年 LSHTM 試題 Paper 2-3 [done 2018-05-07 2230; challenged again 2018-05-25 20:58 35 min used] 2013年 LSHTM 試題 Paper 2-5; [done 2018-05-28 extreeeeeeeemely^99^ difficult GLM] 2013年 LSHTM 試題 Paper 2-6; [done 2018-05-30 21:21 in 37 min] 2013年 LSHTM 試題 Paper 2-7; [done 2018-05-28 巨難無比，條件邏輯回歸的推導和證明] 2013年 LSHTM 試題 Paper 2-8; [done 2018-05-22 23:38] 2014年 LSHTM 試題 Paper 1-1; [done 2018-04-23] 2014年 LSHTM 試題 Paper 1-2; [done 2018-05-25 27 min but only get about 60% poor and brutal\u0026hellip;\u0026hellip; what should I do\u0026hellip;] 2014年 LSHTM 試題 Paper 1-3; [done 2018-04-23] [done 2018-06-04 done in 29 min 90% got] 2014年 LSHTM 試題 Paper 1-4; [done 2018-05-20 22:56] 2014年 LSHTM 試題 Paper 1-5; [done 2018-05-22 18:33] 2014年 LSHTM 試題 Paper 1-6; [done 2018-05-19 13:43] [done agian 2018-06-03 11:18 17 min 80%] 2014年 LSHTM 試題 Paper 1-7; [done 2018-05-27 15:07,278 min used] 2014年 LSHTM 試題 Paper 1-8; [done 2018-05-21 14:50] 2014年 LSHTM 試題 Paper 2-1; [done 2018-05-24 15:01] 2014年 LSHTM 試題 Paper 2-2; [done 2018-04-23] 2014年 LSHTM 試題 Paper 2-3; [done 2018-05-14 23:18; twice challenge 2018-05-25 17:20 better now] 2014年 LSHTM 試題 Paper 2-5; [done 2018-05-15 01:30; challenged again 2018-05-28 15:47 much better now] 2014年 LSHTM 試題 Paper 2-6; [done 2018-05-15 11:30] 2014年 LSHTM 試題 Paper 2-7; [done 2018-05-28 16:50 got 19 points out of 20 I like this one] 2014年 LSHTM 試題 Paper 2-8; [done 2018-05-30 18:38 36 min used, survival questions] 2015年 LSHTM 試題 Paper 1-1; [done 2018-05-19 14:53] [done again 2018-06-03 00:05 17 min 80%] 2015年 LSHTM 試題 Paper 1-2; [done 2018-05-20 13:46 20 min used] [done 2018-06-04 14:08 12 min used well done] 2015年 LSHTM 試題 Paper 1-3; [done 2018-05-20 22:15 20 min used but only get 50%] 2015年 LSHTM 試題 Paper 1-4; [done 2018-05-23 21:00] 2015年 LSHTM 試題 Paper 1-5; [done 2018-05-25 12:16] 2015年 LSHTM 試題 Paper 1-6; [done 2018-05-22 12:06 25 min used but only get 60%] 2015年 LSHTM 試題 Paper 1-7; [done 2018-05-27 16:57 28 min used] 2015年 LSHTM 試題 Paper 1-8; [done 2018-05-22 12:44] 2015年 LSHTM 試題 Paper 2-1; [done 2018-05-24 13:56] 2015年 LSHTM 試題 Paper 2-2; [done 2018-05-30 16:14 29 min used, somewhat easy I can have about 80%] 2015年 LSHTM 試題 Paper 2-3; [done 2018-05-25 13:58] 2015年 LSHTM 試題 Paper 2-5; [2018-05-28 12:42] 2015年 LSHTM 試題 Paper 2-6; [2018-05-28 14:15 done within 25 min, Poisson regression model is within my range of ability] 2015年 LSHTM 試題 Paper 2-7; [2018-05-30 17:07 done. 40 min used. Quite difficult Weibull model with AFT feature.] 2015年 LSHTM 試題 Paper 2-8; [done 2018-05-22 00:58] 2016年 LSHTM 試題 Paper 1-1; [done 2018-05-02] [done again 2018-06-03 00:04 30 min 80%] 2016年 LSHTM 試題 Paper 1-2; [done 2018-05-02] [done again 2018-06-04 14:40 20 80%] 2016年 LSHTM 試題 Paper 1-3; [done 2018-05-02] 2016年 LSHTM 試題 Paper 1-4; [done 2018-05-02] 2016年 LSHTM 試題 Paper 1-5; [done 2018-05-02 and 2018-05-24 22:58 33 min used] 2016年 LSHTM 試題 Paper 1-6; [done 2018-05-03] [done 2018-06-02 17:41 about 80%] 2016年 LSHTM 試題 Paper 1-7; [done 2018-05-03] [done 2018-06-02 17:41 about 80%] 2016年 LSHTM 試題 Paper 1-8; [done 2018-05-03] 2016年 LSHTM 試題 Paper 2-1; [done 2018-05-23 18:00] 2016年 LSHTM 試題 Paper 2-2; [done 2018-05-30 01:03 very difficult competing risk(subdistribution) model] [done again 2018-06-01 11:59, more than 80% get, answers improved] 2016年 LSHTM 試題 Paper 2-3; [done 2018-05-25 01:38 40 min used] 2016年 LSHTM 試題 Paper 2-5; [done 2018-05-27 23:58 45 min used] 2018-05-31 15:22 challenged again 32 min used, answers improved. 2016年 LSHTM 試題 Paper 2-6; [done 2018-05-28 11:55 matched case-control study] 2018-05-31 16:04 challenged again 27 min used, answers improved. 2016年 LSHTM 試題 Paper 2-7; [done 2018-05-30 13:09 Weibull model with connection to AFT model] [done again 2018-06-01 12:00 more than 80% got, answers improved] 2016年 LSHTM 試題 Paper 2-8; [done 2018-05-21 13:00] [done 2018-06-02 17:40 again, about 80%] 2017年 LSHTM 試題 Paper 1-1; [2018-05-19 20:02 done] [2018-06-03 00:03 17 min done 80% got] 2017年 LSHTM 試題 Paper 1-2; [2018-05-20 16:07 史上最難] [2018-06-04 20 min 90% got, MSE = Variance + (Bias)^2] 2017年 LSHTM 試題 Paper 1-3; [2018-05-20 18:30] 2017年 LSHTM 試題 Paper 1-4; [2018-05-23 01:45 done] 2017年 LSHTM 試題 Paper 1-5; [2018-05-24 21:57 done] 2017年 LSHTM 試題 Paper 1-6; [2018-05-21 02:35 被虐慘了] [2018-06-02 14:49 largely improved] 2017年 LSHTM 試題 Paper 1-7; [2018-05-21 11:09 答案可能有錯的 AT?] [2018-06-02 14:48 done again, Good] 2017年 LSHTM 試題 Paper 1-8; [2018-05-27 12:34 done, 28 min used, but only had 60%. Read your question carefully!!!] 2017年 LSHTM 試題 Paper 2-1; [2018-05-21 12:49 done 17] [2018-06-02 14:48 done again, largely improved] 2017年 LSHTM 試題 Paper 2-2; [done 2018-05-27 23:09 extremely difficult GLM] challenged again 2018-05-31 21 min done with quick smash 2017年 LSHTM 試題 Paper 2-3; [2018-05-27 20:00 very difficult combined with ordinal logistic regression] [2018-05-31 14:35 challenged again, 40 min, still very difficult but improved] 2017年 LSHTM 試題 Paper 2-5; [2018-05-29 20:11 done, not very difficult survival question, but you only got about 50%] [2018-06-01 13:52 challenged again, answers improved, some time-dependent variable in survival analysis is quite interesting] 2017年 LSHTM 試題 Paper 2-6; [2018-05-29 22:47 done, not very difficult, but less than 50% obtained] [2018-06-01 13:55 done again, answers improved.] 2017年 LSHTM 試題 Paper 2-7; [2018-05-24 18:43 done 45 min used. too much time wasted!!!] 2017年 LSHTM 試題 Paper 2-8; [2018-05-20] ","date":1525651200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525651200,"objectID":"8f5cca2cca695f18aabb95617f3b135d","permalink":"https://wangcc.me/post/todotest/","publishdate":"2018-05-07T00:00:00Z","relpermalink":"/post/todotest/","section":"post","summary":"todo list","tags":["To do"],"title":"To do list 2018 May","type":"post"},{"authors":null,"categories":["study abroad"],"content":"媽在電話裏說，今年要是期末考試考不過，明年把妹妹(我女兒)背去倫敦再考。(T_T) 突然我就想起快離開廈門的前一天，帶着兒子去大榕樹底下玩。他興奮地要玩我新買的乒乓球和乒乓球拍。可我心裏舍不得新的球和球拍弄髒了，就故意拿小汽車和其他的東西分散他的注意力。回到家裏了才給了他一個乒乓球玩。其實那天本來還想帶他去買肉包給他吃，可是領了快遞以後我沒有手再拿東西，就直接帶兒子回家了。那天兒子女兒和妻還要坐火車回榕城外婆家，一路顛簸誰也沒想起來，兒子還沒吃早飯。火車上聽說他也一直沒吃東西，不知道他三歲的心裏在想什麼。到了外婆家裏也很晚了，男孩子興奮哭鬧總是比女孩子激烈。我一聽電話裏他哭的聲音，心裏就不由得難受極了。怎麼就舍不得把乒乓球給他一個呢，我真是個自私極了的爸爸。忘了兒子沒吃早飯，也舍不得把他想玩的乒乓球送給他。也許他早就不記得了，但是我總惦記着這一天發生的事。也許在我心裏，那段美好時光在琥珀中靜止在了廈門開往島外的那列送行的地鐵上。\n我又想起在學習因果推斷的時候，每次老師都要強調那三個永世不能忘記的推斷前提:\n無相互幹擾 no interference; 一致性 consistency; 條件可置換性 conditional exchangeability; 每當老師提問說，我們現在的前提是什麼？全班同學總能異口同聲地念出上面那三句咒語，場景仿佛間諜與間諜之間對暗號。又有點像黑幫入會時指天發誓的三句誓言。還有就是那個老師可愛的法文味道的英文，標準誤的英文是 standard error，她總是說 standard \u0026ldquo;唉河\u0026rdquo;。另一個教生存分析的法國人老師就更有趣了，每次舉例子都說，比方說我們拿\u0026ndash;法國做例子，隨機選一個國家嘛。。blablabla\u0026hellip;\n今天，響子同學說要去阿根廷完成自己的碩士課題。我們下午坐在 SOAS 的草地上一邊從作業間隙中休息，一邊喝着咖啡，突然意識到，再過一陣子，新學生就又要來了呢。去年這時候我們都還在世界各地，響子在危地馬拉給 JICA 幹活，說着流利的西班牙語; 我在名古屋一邊給日本學生講課，一邊內心充滿了期待快出生的妹妹和快要出發來倫敦的復雜又忐忑的心情，如今我們竟然已經在討論彼此回程的機票訂了幾號，想起3月我們還在寒風中頂着大雪抱怨着留英這一年碰到數十年最嚴重的大學罷課，這一段時光，竟也這樣偷偷溜走，沒有琥珀可以給它定格。\n直到兩天前，同班同學在因果推斷下課後，復習完了我們每次課上對完的暗號，突然有同學提議說，我們去學校門口拍一張集體照片吧，學校年度學生畫冊 (Yearbook) 的內容我們還沒人提交吶，至少要有一張咱們的集體照片吧！ 於是我們有了封面的那張照片。總算是用 LOMO 的隨手拍記錄下這年我們在 LSHTM 待過的證據。這年，我們這十幾個在 LSHTM 推倒公式，背誦\u0026quot;間諜暗號\u0026quot;，倒騰貝葉斯，糾結着那些回歸模型的殘差，還有那個永遠也搞不懂的似然。一瞬間留在相紙上，一轉眼可能就要各奔四方。傷感不由就從心中涌出，蔓延到大西洋。\n","date":1525564800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525564800,"objectID":"7b8e10f90128fc4aa81d6d687f7434ae","permalink":"https://wangcc.me/post/sleep/","publishdate":"2018-05-06T00:00:00Z","relpermalink":"/post/sleep/","section":"post","summary":"時光在琥珀中靜止","tags":["experience","偶爾感慨"],"title":"你靜靜地睡在琥珀裏","type":"post"},{"authors":null,"categories":[],"content":"倫敦的生活已經過去四個月，每天和統計學公式打交道的我，今天不想在這裏寫任何公式。說說這四個月想更新一直偷懶沒更新的那些在倫敦衛生與熱帶醫學院度過的平凡的日子。\n寒假時去了普利茅斯，和康沃爾。康沃爾是個很有意思的名字 Cornwall。字面意思是玉米牆。我是去自己本來計劃聖誕節想去的 Homestay。HOST UK 本來負責我的人告訴我聖誕節可能有點困難，聖誕節前的週末可以的話就去康沃爾的一對退休的老人家裏去做客吧。於是週五一早踏上了一個多月前就從網上訂好的倫敦的帕丁頓去往卡爾斯托克 (Calstock) 的西大不列顛列車。說來諷刺的是，大英帝國建立了世界上第一條火車，如今我一個老外來到這個國家卻在嫌棄這裏的火車慢如老牛拉車。\n和我一起享受英國農村四天三晚的 Homestay 的還有另一個來自毛里求斯的印度人學生。我們一路同行從倫敦出發。整列火車從離開倫敦時的滿員，乘客隨着車窗外樓房的減少而逐漸減少。腦海裏推算了一下，這絕對是有意義的正相關。 到了普利茅斯只剩下包括我倆在內，絕對只有個位數的人。\n我也沒打算把整個四天三晚都去了哪裏在這裏記流水帳，印象深刻的是我們和老爺爺老太太每晚每晚的長談。還好來自毛里求斯的印度人英文流利，我一個人跟這些老人肯定是無法聊到深夜的。他們聊他們的老當益壯，用腳丈量非洲大陸的那些經歷和記憶，我們侃我們的年輕氣盛和那些無處發泄的憂國憂民。走時，老爺爺把自己收藏了多年的一個據說來自唐朝中國的佛像給了我，說，我希望你帶它回到它來自的地方。我想起我們都站在康沃爾的大西洋沿岸峭壁懸崖，放眼望着法國的方向，腳下全是泥巴。\n我在老人家的留言本上寫下了我在中國和日本的地址電話，中日英三語，生怕他們真的會在中國或者日本迷路。隔天回到了倫敦的房間，我收到老爺爺發來的郵件，淡然如水，卻彷若那些夜晚我們促膝長談時說的話：“你豐富了我們的人生，在你我的道路重新交集前，保重。You have enriched our lives. Until our paths cross again, take care.”\n英國冬季的日照時間短得可憐。白天離開宿舍去大學時天黑着，下午下課離開大學時，天依然是黑着的。加上我們統計系的課許多都在地下的教室裏，我跟其他人打趣說，我現在的生活像一隻土撥鼠。我在地下，推導着讓我內心無比踏實的那些數學公式。\n有時候，我會十分的想念日本的生活。有時候，我又會無比的思念廈門的日子。這些落腳過的地方，只有上海的感覺越來越模糊。不知道我懷念的是名古屋乾淨的街道，是廈門的沙茶面的味道，還是那些夜晚打完工以後路邊的便利店門口騎着腳踏車路過的那時的我，也許還有那個在白城沙灘上可以悠閒地聽海浪拍岸聲的那個無腦少年。不清楚緣由地，只有上海的記憶在大腦中逐漸變得不那麼色彩斑斕。我也很好奇多年以後我會怎樣回憶倫敦？ 也許只剩下記憶裏土撥鼠一樣的無聊日子，還有貴死你不償命的宿舍房租。\n跨年那晚我和幾個同學走在滿目瘡痍的倫敦街頭，焰火散去，人去城空，2018年就已經被我們踩在了腳下。眼看着這新的一年在凌亂中開始，但願過程也不要太過殘酷。這一年唯一的目標是順利完成這沒日沒夜 (說好聽是朝思暮想) 的醫學統計學碩士。還有的話就是希望家人孩子平安，待我回到你們身邊，我們再也不要用小的可憐的手機屏幕來看彼此，我要帶你們去看整個世界。\n","date":1515888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515888000,"objectID":"5352fce07b84c195b187cb77ad2afbc5","permalink":"https://wangcc.me/post/luan/","publishdate":"2018-01-14T00:00:00Z","relpermalink":"/post/luan/","section":"post","summary":"我像一隻土撥鼠，在LSHTM地下的教室裏推導公式","tags":["London","experience","偶爾感慨"],"title":"日落在下午四點","type":"post"},{"authors":null,"categories":["diary","study abroad"],"content":"康沃爾的牛肉餡餅！ 贊！ (2017-12-16 @The Shop in the Square) 看起來美味但是甜到牙痛的聖誕蛋糕 (2017-12-17 @Calstock Homestay) 香腸美味！烤雞美味！洋蔥有點糊但是還是很美味！ (2017-12-16 @Jane\u0026amp;Ian\u0026rsquo;s Homestay) 在中國城買到最贊的國貨！廈門鐵觀音 (2017-12-09 @London China Town) 貴到無法下手的三文魚壽司 (2017-12-06 @Waitrose) 和日本一風堂味道一樣但是貴一倍的豚骨拉麵 (2017-12-02 @London Ippudo) 學校附近和同學一起去喝過最棒的拿鐵，缺點是杯子太小 (2017-12-01 @Tap Caffee) Brunch 在長頸鹿餐廳可以打8折 (2017-11-29 @Giraffe) 約克郡的傳統午餐，美味牛肉 (2017-11-26 @The Judge\u0026rsquo;s Lodging) 謝菲爾德的聖誕街市賣的烤香腸 (2017-11-25 @Sheffield) 日本同學從日本帶來的速食味增湯，美味至極 (2017-11-23 @International Hall) 國王十字車站對面的新加坡華人餐廳，海南雞飯不錯 (2017-11-01 @Chop Chop Noodle Bar) 同學宿舍裏的自制小炒 (2017-10-21 @The Garden Hall) 大英博物館前的中餐館的水煮魚 (2017-10-20 @Chang\u0026rsquo;s Noodle) 大英博物館前的都可茶飲買到的原味珍珠奶茶 £3.5 有學生優惠 (2017-10-20 @Coco) 雞腿不錯但是旁邊的配菜有點像中藥味的壓縮餅乾 (2017-10-08 @IHdining room) 哈利波特特約飲料 Butterbeer （奶油啤酒）(2017-10-07 @Warner Bros. Studio Tour London) 以爲是甜食的派結果裏面包着牛肉的奇怪料理 (2017-10-07 @IHdining room) 羅素廣場地鐵站門口的小攤賣的超划算味道很正的新鮮草莓！(2017-10-05 @Russel Square Station) 价值5镑的食堂素食色拉一盒 (2017-10-02@LSHTM食堂) 新鮮，但是米飯有點夾生。\n味道超讚牛肉披薩 Diavolo (2017-09-28 @Pizza Express in Charlotte Street) 菜單上的說明是這樣滴： Hot spiced beef, pepperoni, mozzarella, tomato, green pepper, red onion and Tabasco, with your choice of hot green, Roquito or jalapeño peppers. Available as Classic or Romana\n羊肉味的奶油夾心三明治 (2017-09-24 @Store Street Espresso) 我點菜之前還故意跟收銀員小妹搭訕，讓她推薦一下今天的特色三明治。 她推薦的這個 Goat Cheese Sandwich。 當然我一開始聽到這名字的時候就有點猶豫。但是想說既然是推薦的應該至少不會有什麼怪味道。結果事實證明了，我的想法是多麼的幼稚。\n看這剛出爐的香噴噴的三明治，我咬下第一口就差點吐了。羊羶味在我喉嚨和鼻腔中打轉。後悔也來不及了。另外我同同時還點了 Espresso。就是特濃咖啡。口味超重！不能喝濃咖啡的一定要慎點！！！！\n酸酸的不知道怎麼形容的麵包 (日期忘了，估計是剛到的第二個早晨的早餐@IHdining room) 這麵包吃起來鬆鬆的，然額，麵的味道有些酸，又不像是過期食品，而像是本來就應該是這樣的味道的酸麵包。讓人不想再嘗試第二次。。\n美味海鮮飯 (2017-09-24 @Ciao Bella) 感謝顏師兄帶領，終於找到了一家可以吃到正常大米的飯店了！且海鮮量超足！\n看起來很奇怪的整魚炸薯條 外觀其實讓人沒什麼食慾的烤魚 酸奶放在白煮雞胸肉上 ","date":1513641600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1513641600,"objectID":"ec3fb7c14a1b7abd0559b8b3b5b2eb6b","permalink":"https://wangcc.me/post/black-meal/","publishdate":"2017-12-19T00:00:00Z","relpermalink":"/post/black-meal/","section":"post","summary":"記錄一下來倫敦以後吃過見過的黑暗料理 or 美食 (不定時更新)","tags":["London","LSHTM","experience"],"title":"萬衆期待，英國黑暗料理","type":"post"},{"authors":null,"categories":null,"content":"","date":1510617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1510617600,"objectID":"fd4586b07e4f04a412da094b83386c84","permalink":"https://wangcc.me/project/lshtmlearningnote/","publishdate":"2017-11-14T00:00:00Z","relpermalink":"/project/lshtmlearningnote/","section":"project","summary":"Learning Notes in London School of Hygiene \u0026 Tropical Medicine","tags":["Statistics","Learning notes"],"title":"Medical Statistics@LSHTM","type":"project"},{"authors":null,"categories":["statistics","study abroad"],"content":" 對數似然比 Log-likelihood ratio 對數似然比的想法來自於將對數似然方程圖形的 \\(y\\) 軸重新調節 (rescale) 使之最大值爲零。這可以通過計算該分佈方程的對數似然比 (log-likelihood ratio) 來獲得：\n\\[llr(\\theta)=\\ell(\\theta|data)-\\ell(\\hat{\\theta}|data)\\]\n由於 \\(\\ell(\\theta)\\) 的最大值在 \\(\\hat{\\theta}\\) 時， 所以，\\(llr(\\theta)\\) 就是個當 \\(\\theta=\\hat{\\theta}\\) 時取最大值，且最大值爲零的方程。很容易理解我們叫這個方程爲對數似然比，因爲這個方程就是將似然比 \\(LR(\\theta)=\\frac{L(\\theta)}{L(\\hat{\\theta})}\\) 取對數而已。\n之前我們也確證了，不包含我們感興趣的參數的方程部分可以忽略掉。還是用上一節 10人中4人患病的例子：\n\\[L(\\pi|X=4)=\\binom{10}{4}\\pi^4(1-\\pi)^{10-4}\\\\ \\Rightarrow \\ell(\\pi)=log[\\pi^4(1-\\pi)^{10-4}]\\\\ \\Rightarrow llr(\\pi)=\\ell(\\pi)-\\ell(\\hat{\\pi})=log\\frac{\\pi^4(1-\\pi)^{10-4}}{0.4^4(1-0.4)^{10-4}}\\]\n其實由上也可以看出 \\(llr(\\theta)\\) 只是將對應的似然方程的 \\(y\\) 軸重新調節了一下而已。形狀是沒有改變的：\npar(mfrow=c(1,2)) x \u0026lt;- seq(0,1,by=0.001) y \u0026lt;- (x^4)*((1-x)^6)/(0.4^4*0.6^6) z \u0026lt;- log((x^4)*((1-x)^6))-log(0.4^4*0.6^6) plot(x, y, type = \u0026quot;l\u0026quot;, ylim = c(0,1.1),yaxt=\u0026quot;n\u0026quot;, frame.plot = FALSE, ylab = \u0026quot;LR(\\U03C0)\u0026quot;, xlab = \u0026quot;\\U03C0\u0026quot;) axis(2, at=seq(0,1, 0.2), las=2) title(main = \u0026quot;Binomial likelihood ratio\u0026quot;) abline(h=1.0, lty=2) segments(x0=0.4, y0=0, x1=0.4, y1=1, lty = 2) plot(x, z, type = \u0026quot;l\u0026quot;, ylim = c(-10, 1), yaxt=\u0026quot;n\u0026quot;, frame.plot = FALSE, ylab = \u0026quot;llr(\\U03C0)\u0026quot;, xlab = \u0026quot;\\U03C0\u0026quot; ) axis(2, at=seq(-10, 0, 2), las=2) title(main = \u0026quot;Binomial log-likelihood ratio\u0026quot;) abline(h=0, lty=2) segments(x0=0.4, y0=-10, x1=0.4, y1=0, lty = 2) 正態分佈數據的最大似然和對數似然比 假設單個樣本 \\(y\\) 是來自一組服從正態分佈數據的觀察值：\\(Y\\sim N(\\mu, \\tau^2)\\)\n那麼有：\n\\[ \\begin{aligned} f(y|\\mu) \u0026amp;= \\frac{1}{\\sqrt{2\\pi\\tau^2}}e^{(-\\frac{1}{2}(\\frac{y-\\mu}{\\tau})^2)} \\\\ \\Rightarrow L(\\mu|y) \u0026amp;=\\frac{1}{\\sqrt{2\\pi\\tau^2}}e^{(-\\frac{1}{2}(\\frac{y-\\mu}{\\tau})^2)} \\\\ \\Rightarrow \\ell(\\mu)\u0026amp;=log(\\frac{1}{\\sqrt{2\\pi\\tau^2}})-\\frac{1}{2}(\\frac{y-\\mu}{\\tau})^2\\\\ omitting\u0026amp;\\;terms\\;not\\;in\\;\\mu \\\\ \u0026amp;= -\\frac{1}{2}(\\frac{y-\\mu}{\\tau})^2 \\\\ \\Rightarrow \\ell^\\prime(\\mu) \u0026amp;= 2\\cdot[-\\frac{1}{2}(\\frac{y-\\mu}{\\tau})\\cdot\\frac{-1}{\\tau}] \\\\ \u0026amp;=\\frac{y-\\mu}{\\tau^2} \\\\ let \\; \\ell^\\prime(\\mu) \u0026amp;= 0 \\\\ \\Rightarrow \\frac{y-\\mu}{\\tau^2} \u0026amp;= 0 \\Rightarrow \\hat{\\mu} = y\\\\ \\because \\ell^{\\prime\\prime}(\\mu) \u0026amp;= \\frac{-1}{\\tau^2} \u0026lt; 0 \\\\ \\therefore \\hat{\\mu} \u0026amp;= y \\Rightarrow \\ell(\\hat{\\mu}=y)_{max}=0 \\\\ llr(\\mu)\u0026amp;=\\ell(\\mu)-\\ell(\\hat{\\mu})=\\ell(\\mu)\\\\ \u0026amp;=-\\frac{1}{2}(\\frac{y-\\mu}{\\tau})^2 \\end{aligned} \\]\n\\(n\\) 個獨立正態分佈樣本的對數似然比 假設一組觀察值來自正態分佈 \\(X_1,\\cdots,X_n\\stackrel{i.i.d}{\\sim}N(\\mu,\\sigma^2)\\)，先假設 \\(\\sigma^2\\) 已知。將觀察數據 \\(x_1,\\cdots, x_n\\) 標記爲 \\(\\underline{x}\\)。 那麼：\n\\[ \\begin{aligned} L(\\mu|\\underline{x}) \u0026amp;=\\prod_{i=1}^nf(x_i|\\mu)\\\\ \\Rightarrow \\ell(\\mu|\\underline{x}) \u0026amp;=\\sum_{i=1}^nlogf(x_i|\\mu)\\\\ \u0026amp;=\\sum_{i=1}^n[-\\frac{1}{2}(\\frac{x_i-\\mu}{\\sigma})^2]\\\\ \u0026amp;=-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x_i-\\mu)^2\\\\ \u0026amp;=-\\frac{1}{2\\sigma^2}[\\sum_{i=1}^n(x_i-\\bar{x})^2+\\sum_{i=1}^n(\\bar{x}-\\mu)^2]\\\\ omitting\u0026amp;\\;terms\\;not\\;in\\;\\mu \\\\ \u0026amp;=-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(\\bar{x}-\\mu)^2\\\\ \u0026amp;=-\\frac{n}{2\\sigma^2}(\\bar{x}-\\mu)^2 \\\\ \u0026amp;=-\\frac{1}{2}(\\frac{\\bar{x}-\\mu}{\\sigma/\\sqrt{n}})^2\\\\ \\because \\ell(\\hat{\\mu}) \u0026amp;= 0 \\\\ \\therefore llr(\\mu) \u0026amp;= \\ell(\\mu)-\\ell(\\hat{\\mu}) = \\ell(\\mu) \\end{aligned} \\]\n\\(n\\) 個獨立正態分佈樣本的對數似然比的分佈 假設我們用 \\(\\mu_0\\) 表示總體均數這一參數的值。要注意的是，每當樣本被重新取樣，似然，對數似然方程，對數似然比都隨着觀察值而變 (即有自己的分佈)。\n考慮一個服從正態分佈的單樣本 \\(Y\\): \\(Y\\sim N(\\mu_0,\\tau^2)\\)。那麼它的對數似然比：\n\\[llr(\\mu_0|Y)=\\ell(\\mu_0)-\\ell(\\hat{\\mu})=-\\frac{1}{2}(\\frac{Y-\\mu_0}{\\tau})^2\\]\n根據卡方分佈的定義：\n\\[\\because \\frac{Y-\\mu_0}{\\tau}\\sim N(0,1)\\\\ \\Rightarrow (\\frac{Y-\\mu_0}{\\tau})^2 \\sim \\mathcal{X}_1^2\\\\ \\therefore -2llr(\\mu_0|Y) \\sim \\mathcal{X}_1^2\\]\n所以，如果有一組服從正態分佈的觀察值：\\(X_1,\\cdots,X_n\\stackrel{i.i.d}{\\sim}N(\\mu_0,\\sigma^2)\\)，且 \\(\\sigma^2\\) 已知的話：\n\\[-2llr(\\mu_0|\\bar{X})\\sim \\mathcal{X}_1^2\\]\n根據中心極限定理，可以將上面的結論一般化： Theorem 1 如果 \\(X_1,\\cdots,X_n\\stackrel{i.i.d}{\\sim}f(x|\\theta)\\)。 那麼當重複多次從參數爲 \\(\\theta_0\\) 的總體中取樣時，那麼統計量 \\(-2llr(\\theta_0)\\) 會漸進於自由度爲 \\(1\\) 的卡方分佈： \\[-2llr(\\theta_0)=-2\\{\\ell(\\theta_0)-\\ell(\\hat{\\theta})\\}\\xrightarrow[n\\rightarrow\\infty]{}\\;\\sim \\mathcal{X}_1^2\\] 似然比信賴區間 如果樣本量 \\(n\\) 足夠大 (通常應該大於 \\(30\\))，根據上面的定理：\n\\[-2llr(\\theta_0)=-2\\{\\ell(\\theta_0)-\\ell(\\hat{\\theta})\\}\\sim \\mathcal{X}_1^2\\]\n所以：\n\\[Prob(-2llr(\\theta_0)\\leqslant \\mathcal{X}_{1,0.95}^2=3.84) = 0.95\\\\ \\Rightarrow Prob(llr(\\theta_0)\\geqslant-3.84/2=-1.92) = 0.95\\]\n故似然比的 \\(95\\%\\) 信賴區間就是能夠滿足 \\(llr(\\theta)=-1.92\\) 的兩個 \\(\\theta\\) 值。\n以二項分佈數據爲例 繼續用本文開頭的例子：\n\\[llr(\\pi)=\\ell(\\pi)-\\ell(\\hat{\\pi})=log\\frac{\\pi^4(1-\\pi)^{10-4}}{0.4^4(1-0.4)^{10-4}}\\]\n如果令 \\(llr(\\pi)=-1.92\\) 在代數上可能較難獲得答案。然而從圖形上，如果我們在 \\(y=-1.92\\) 畫一條橫線，和該似然比方程曲線相交的兩個點就是我們想要求的信賴區間的上限和下限：\nx \u0026lt;- seq(0,1,by=0.001) z \u0026lt;- log((x^4)*((1-x)^6))-log(0.4^4*0.6^6) plot(x, z, type = \u0026quot;l\u0026quot;, ylim = c(-10, 1), yaxt=\u0026quot;n\u0026quot;, frame.plot = FALSE, ylab = \u0026quot;llr(\\U03C0)\u0026quot;, xlab = \u0026quot;\\U03C0\u0026quot; ) axis(2, at=seq(-10, 0, 2), las=2) abline(h=0, lty=2) abline(h=-1.92, lty=2) segments(x0=0.15, y0=-12, x1=0.15, y1=-1.92, lty = 2) segments(x0=0.7, y0=-12, x1=0.7, y1=-1.92, lty = 2) axis(1, at=c(0.15,0.7)) text(0.9, -1, \u0026quot;-1.92\u0026quot;) arrows(0.8, -1.92, 0.8, 0, lty = 1, length = 0.08) arrows( 0.8, 0, 0.8, -1.92, lty = 1, length = 0.08) title(main = \u0026quot;Log-likelihood ratio for binomial example, \\n with 95% likelihood confidence interval shown\u0026quot;) 從上圖中可以讀出，\\(95\\%\\) 對數似然比信賴區間就是 \\((0.15, 0.7)\\)\n以正態分佈數據爲例 本文前半部分證明過， \\(X_1,\\cdots,X_n\\stackrel{i.i.d}{\\sim}N(\\mu,\\sigma^2)\\)，先假設 \\(\\sigma^2\\) 已知。將觀察數據 \\(x_1,\\cdots, x_n\\) 標記爲 \\(\\underline{x}\\)。 那麼：\n\\[llr(\\mu|\\underline{x}) = \\ell(\\mu|\\underline{x})-\\ell(\\hat{\\mu}) = \\ell(\\mu|\\underline{x}) \\\\ =-\\frac{1}{2}(\\frac{\\bar{x}-\\mu}{\\sigma/\\sqrt{n}})^2\\]\n很顯然，這是一個關於 \\(\\mu\\) 的二次方程，且最大值在 MLE \\(\\hat{\\mu}=\\bar{x}\\) 時取值 \\(0\\)。所以可以通過對數似然比法求出均值的 \\(95\\%\\) 信賴區間公式：\n\\[-2\\times[-\\frac{1}{2}(\\frac{\\bar{x}-\\mu}{\\sigma/\\sqrt{n}})^2]=3.84\\\\ \\Rightarrow L=\\bar{x}-\\sqrt{3.84}\\frac{\\sigma}{\\sqrt{n}} \\\\ U=\\bar{x}+\\sqrt{3.84}\\frac{\\sigma}{\\sqrt{n}} \\\\ note: \\;\\sqrt{3.84}=1.96\\]\n注意到這和我們之前求的正態分佈均值的信賴區間公式完全一致。\nExercise Q1 假設十個對象中有三人死亡，用二項分佈模型來模擬這個例子，求這個例子中參數 \\(\\pi\\) 的似然方程和圖形 (likelihood) ? 解 \\(\\begin{aligned} L(\\pi|3) \u0026amp;= \\binom{10}{3}\\pi^3(1-\\pi)^{10-3} \\\\ omitting\\;\u0026amp;terms\\;not\\;in\\;\\mu \\\\ \\Rightarrow \\ell(\\pi|3) \u0026amp;= log[\\pi^3(1-\\pi)^7] \\\\ \u0026amp;= 3log\\pi+7log(1-\\pi)\\\\ \\Rightarrow \\ell^\\prime(\\pi|3)\u0026amp;= \\frac{3}{\\pi}-\\frac{7}{1-\\pi} \\\\ let \\; \\ell^\\prime\u0026amp; =0\\\\ \u0026amp;\\frac{3}{\\pi}-\\frac{7}{1-\\pi} = 0 \\\\ \u0026amp;\\frac{3-10\\pi}{\\pi(1-\\pi)} = 0 \\\\ \\Rightarrow MLE \u0026amp;= \\hat\\pi = 0.3 \\end{aligned}\\)\n計算似然比，並作圖，注意方程圖形未變，\\(y\\) 軸的變化；取對數似然比，並作圖 LR \u0026lt;- L/max(L) ; head(LR) ## [1] 0.0000000000 0.0004191759 0.0031233631 0.0098110584 0.0216286076 ## [6] 0.0392577320 plot(pi, LR, type = \u0026quot;l\u0026quot;, ylim = c(0, 1),yaxt=\u0026quot;n\u0026quot;, col=\u0026quot;darkblue\u0026quot;, frame.plot = FALSE, ylab = \u0026quot;\u0026quot;, xlab = \u0026quot;\\U03C0\u0026quot;) grid(NA, 5, lwd = 1) axis(2, at=seq(0,1,0.2), las=2) title(main = \u0026quot;Binomial likelihood ratio function\\n 3 out of 10 subjects\u0026quot;) logLR \u0026lt;- log(L/max(L)) plot(pi, logLR, type = \u0026quot;l\u0026quot;, ylim = c(-4, 0),yaxt=\u0026quot;n\u0026quot;, col=\u0026quot;darkblue\u0026quot;, frame.plot = FALSE, ylab = \u0026quot;\u0026quot;, xlab = \u0026quot;\\U03C0\u0026quot;) grid(NA, 5, lwd = 1) axis(2, at=seq(-4,0,1), las=2) title(main = \u0026quot;Binomial log-likelihood ratio function\\n 3 out of 10 subjects\u0026quot;) abline(h=-1.92, lty=1, col=\u0026quot;red\u0026quot;) axis(4, at=-1.92, las=0) Q2 與上面用同樣的模型，但是觀察人數變爲 \\(100\\) 人 患病人數爲 \\(30\\) 人，試作對數似然比方程之圖形，與上圖對比： 可以看出，兩組數據的 MLE 都是一致的， \\(\\hat\\pi=0.3\\)，但是對數似然比方程圖形在 樣本量爲 \\(n=100\\) 時比 \\(n=10\\) 時窄很多，由此產生的似然比信賴區間也就窄很多（精確很多）。所以對數似然比方程的曲率（二階導數），反映了觀察獲得數據提供的對總體參數 \\(\\pi\\) 推斷過程中的信息量。而且當樣本量較大時，對數似然比方程也更加接近左右對稱的二次方程曲線。\nQ3 在一個實施了160人年的追蹤調查中，觀察到8個死亡案例。使用泊松分佈模型，繪製對數似然比方程圖形，從圖形上目視推測極大似然比的 \\(95\\%\\) 信賴區間。\n解 \\(\\begin{aligned} d = 8, \\;p \u0026amp;= 160\\; person\\cdot year \\\\ \\Rightarrow D\\sim Poi(\\mu \u0026amp;=\\lambda p) \\\\ L(\\lambda|data) \u0026amp;= Prob(D=d=8) \\\\ \u0026amp;= e^{-\\mu}\\frac{\\mu^d}{d!} \\\\ \u0026amp;= e^{-\\lambda p}\\frac{\\lambda^d p^d}{d!} \\\\ omitting\u0026amp;\\;terms\\;not\\;in\\;\\lambda \\\\ \u0026amp;= e^{-\\lambda p}\\lambda^d \\\\ \\Rightarrow \\ell(\\lambda|data)\u0026amp;= log(e^{-\\lambda p}\\lambda^d) \\\\ \u0026amp;= d\\cdot log(\\lambda)-\\lambda p \\\\ \u0026amp; = 8\\times log(\\lambda) - 160\\times\\lambda \\end{aligned}\\)\nlambda LogLR 0.010 -6.4755033 0.011 -5.8730219 0.012 -5.3369308 0.013 -4.8565892 0.014 -4.4237254 0.015 -4.0317824 0.016 -3.6754743 0.017 -3.3504773 0.018 -3.0532100 0.019 -2.7806722 0.020 -2.5303259 0.021 -2.3000045 0.022 -2.0878444 0.023 -1.8922303 0.024 -1.7117534 0.025 -1.5451774 0.026 -1.3914117 0.027 -1.2494891 0.028 -1.1185480 0.029 -0.9978174 0.030 -0.8866050 0.031 -0.7842864 0.032 -0.6902968 0.033 -0.6041236 0.034 -0.5252998 0.035 -0.4533996 0.036 -0.3880325 0.037 -0.3288407 0.038 -0.2754948 0.039 -0.2276909 0.040 -0.1851484 0.041 -0.1476075 0.042 -0.1148271 0.043 -0.0865831 0.044 -0.0626670 0.045 -0.0428841 0.046 -0.0270529 0.047 -0.0150032 0.048 -0.0065760 0.049 -0.0016217 0.050 0.0000000 0.051 -0.0015790 0.052 -0.0062343 0.053 -0.0138487 0.054 -0.0243117 0.055 -0.0375186 0.056 -0.0533705 0.057 -0.0717739 0.058 -0.0926400 0.059 -0.1158845 0.060 -0.1414275 0.061 -0.1691931 0.062 -0.1991090 0.063 -0.2311062 0.064 -0.2651194 0.065 -0.3010859 0.066 -0.3389461 0.067 -0.3786431 0.068 -0.4201224 0.069 -0.4633320 0.070 -0.5082221 0.071 -0.5547450 0.072 -0.6028551 0.073 -0.6525085 0.074 -0.7036633 0.075 -0.7562791 0.076 -0.8103173 0.077 -0.8657407 0.078 -0.9225134 0.079 -0.9806012 0.080 -1.0399710 0.081 -1.1005908 0.082 -1.1624301 0.083 -1.2254592 0.084 -1.2896497 0.085 -1.3549740 0.086 -1.4214057 0.087 -1.4889191 0.088 -1.5574895 0.089 -1.6270931 0.090 -1.6977067 0.091 -1.7693080 0.092 -1.8418754 0.093 -1.9153881 0.094 -1.9898258 0.095 -2.0651689 0.096 -2.1413985 0.097 -2.2184962 0.098 -2.2964442 0.099 -2.3752252 0.100 -2.4548226 所以從列表數據結合圖形， 可以找到信賴區間的下限在 0.022~0.023 之間， 上限在 0.093～0.094 之間。\n","date":1509840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1509840000,"objectID":"2387d6b536d807c4b83833b0547df294","permalink":"https://wangcc.me/post/log-likelihood-ratio/","publishdate":"2017-11-05T00:00:00Z","relpermalink":"/post/log-likelihood-ratio/","section":"post","summary":"對數似然比，和信賴區間的計算","tags":["Medical Statistics","inference","learning notes","London","LSHTM"],"title":"對數似然比 Log-likelihood ratio","type":"post"},{"authors":["Chaochen Wang","Takeshi Nishiyama","Shogo Kikuchi","Manami Inoue","Norie Sawada","Shoichiro Tsugane","Yingsong Lin"],"categories":null,"content":"Abstract Changing trends in the prevalence of H. pylori infection in the general population over time are thought to be the main driving force behind the declining gastric cancer mortality in Japan. However, whether the prevalence of H. pylori infection itself shows a birth-cohort pattern needs to be corroborated. We performed a systematic review of studies that reported the prevalence of H. pylori infection among Japanese individuals. Meta-regression was conducted in the framework of a generalized additive mixed model (GAMM) to account for heterogeneity in the prevalence of H. pylori infection as a function of birth year. The prevalence of H. pylori infection confirmed a clear birth cohort pattern: the predicted prevalence (%, 95% CI) was 60.9 (56.3-65.4), 65.9 (63.9-67.9), 67.4 (66.0-68.7), 64.1 (63.1-65.1), 59.1 (58.2-60.0), 49.1 (49.0-49.2), 34.9 (34.0-35.8), 24.6 (23.5-25.8), 15.6 (14.0-17.3), and 6.6 (4.8-8.9) among those who were born in the year 1910, 1920, 1930, 1940, 1950, 1960, 1970, 1980, 1990, and 2000, respectively. The present study demonstrated a clear birth-cohort pattern of H. pylori infection in the Japanese population. The decreased prevalence of H. pylori infection in successive generations should be weighed in future gastric cancer control programs.\nClick the graph to see the interactive version Summary statistics from fitting meta-regression in the best model.(click to see the table) Table of risk of bias diagnosis. (click to see the table) ","date":1509062400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1509062400,"objectID":"b794bd5315b2a480dafacbeb62c7c24c","permalink":"https://wangcc.me/publication/journal-article/hpylorimeta/","publishdate":"2017-10-27T00:00:00Z","relpermalink":"/publication/journal-article/hpylorimeta/","section":"publication","summary":"Abstract Changing trends in the prevalence of H. pylori infection in the general population over time are thought to be the main driving force behind the declining gastric cancer mortality in Japan. However, whether the prevalence of H. pylori infection itself shows a birth-cohort pattern needs to be corroborated. We performed a systematic review of studies that reported the prevalence of H. pylori infection among Japanese individuals. Meta-regression was conducted in the framework of a generalized additive mixed model (GAMM) to account for heterogeneity in the prevalence of H.","tags":null,"title":"Changing trends in the prevalence of H. pylori infection in Japan (1908-2003): a systematic review and meta-regression analysis of 170,752 individuals","type":"publication"},{"authors":null,"categories":["statistics","study abroad"],"content":" 最近明顯可以感覺到課程的步驟開始加速。看我的課表：\n手機畫面太小了。早上都是9點半開始，下午基本都是到5點。週一更慘，到7點。週二-週五中午都被統計中心的講座佔據。簡直是非人的生活。\n這周概率論基礎結束。中心極限定理講完以後我們正式進入了 Inference 統計推斷的課程。我們花了一天時間講什麼是樣本估計 (Estimation)，什麼是參數精確度 (Precision)，什麼是自由度 (degree of freedom)，怎樣進行不偏的估計 (unbiased inference)。然後還有似然方程 (likelihood function)。\n今天的更新還是簡單的把概率論掃尾一下。感受一下中心極限定理的偉大。\n協方差 Covariance 之前我們定義過，兩個獨立連續隨機變量 \\(X,Y\\) 之和的方差 Variance ：\n\\[Var(X+Y)=Var(X)+Var(Y)\\]\n然而如果他們並不相互獨立的話：\n\\[\\begin{aligned} Var(X+Y) \u0026amp;= E[((X+Y)-E(X+Y))^2] \\\\ \u0026amp;= E[(X+Y)-(E(X)+E(Y))^2] \\\\ \u0026amp;= E[(X-E(X)) - (Y-E(Y))^2] \\\\ \u0026amp;= E[(X-E(X))^2+(Y-E(Y))^2 \\\\ \u0026amp; \\;\\;\\; +2(X-E(X))(Y-E(Y))] \\\\ \u0026amp;= Var(X)+Var(Y)+2E[(X-E(X))(Y-E(Y))] \\end{aligned}\\] 可以發現在兩者和的方差公式展開之後多了一部分 \\(E[(X-E(X))(Y-E(Y))]\\)。 這個多出來的一部分就說明了二者 \\((X, Y)\\) 之間的關係。它被定義爲協方差 (Covariance): \\[Cov(X,Y) = E[(X-E(X))(Y-E(Y))]\\]\n所以：\n\\[Var(X+Y)=Var(X)+Var(Y)+2Cov(X,Y)\\]\n要記住，協方差只能用於評價(X,Y)之間的線性關係 (Linear Association)。 以下是協方差 (Covariance) 的一些特殊性質：\n\\(Cov(X,X)=Var(X)\\) \\(Cov(X,Y)=Cov(Y,X)\\) \\(Cov(aX,bY)=ab\\:Cov(X,Y)\\) \\(Cov(aR+bS,cX+dY)=ac\\:Cov(R,X)+ad\\:Cov(R,Y)\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;+bc\\:Cov(S,X)+bd\\:Cov(S,Y)\\) \\(Cov(aX+bY,cX+dY)=ac\\:Var(X)+ad\\:Var(Y)\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;+(ad+bc)Cov(X,Y)\\) \\(Cov(X+Y,X-Y)=Var(X)-Var(Y)\\) If \\(X, Y\\) are independent. \\(Cov(X,Y)=0\\) But not vise-versa ! 相關 Correlation 協方差雖然\\(Cov(X,Y)\\) 的大小很大程度上會被他們各自的單位和波動大小左右。 我們將協方差標準化(除以各自的標準差 s.d.) (standardization) 之後，就可以得到相關係數 Corr (\\(-1\\sim1\\)): \\[Corr(X,Y)=\\frac{Cov(X,Y)}{SD(X)SD(Y)}=\\frac{Cov(X,Y)}{\\sqrt{Var(X)Var(Y)}}\\] 中心極限定理 the Central Limit Theory 如果從人羣中多次選出樣本量爲 \\(n\\) 的樣本，並計算樣本均值, \\(\\bar{X}_n\\)。那麼這個樣本均值 \\(\\bar{X}_n\\) 的分佈，會隨着樣本量增加 \\(n\\rightarrow\\infty\\)，而接近正態分佈。\n偉大的中心極限定理告訴我們：\n當樣本量足夠大時，樣本均值 \\(\\bar{X}_n\\) 的分佈爲正態分佈，這個特性與樣本來自的人羣的分佈 \\(X_i\\) 無關。\n再說一遍：\n如果對象是獨立同分佈 i.i.d (identically and independently distributed)。那麼它的總體期望和方差分別是: \\(E(X)=\\mu;\\;Var(X)=\\sigma^2\\)。 根據中心極限定理，可以得到：\n當樣本量增加，樣本均值的分佈服從正態分佈： \\[\\bar{X}_n\\sim N(\\mu, \\frac{\\sigma^2}{n})\\] 也可以寫作，當樣本量增加： \\[\\sum_{i=1}^nX_i \\sim N(n\\mu,n\\sigma^2)\\] 有了這個定理，我們可以拋開樣本空間(\\(X\\))的分佈，也不用假定它服從正態分佈。 但是樣本的均值，卻總是服從正態分佈的。簡直是太完美了！！！！！！ ","date":1508371200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1508371200,"objectID":"b9a9948fbc248dbeee1206fbe6db9ff9","permalink":"https://wangcc.me/post/central-limit-theory/","publishdate":"2017-10-19T00:00:00Z","relpermalink":"/post/central-limit-theory/","section":"post","summary":"協方差，相關，和中心極限定理","tags":["basic mathematics","learning notes","inference","LSHTM","Medical Statistics"],"title":"偉大的中心極限定理","type":"post"},{"authors":null,"categories":["statistics","study abroad"],"content":" 二項分佈的概念 Binomial distribution 二項分佈在醫學研究中至關重要，一組二項分佈的數據，指的通常是 \\(n\\) 次相互獨立的成功率爲 \\(\\pi\\) 的伯努利實驗 (\\(n\\) independent Bernoulli trials) 中成功的次數。\n當 \\(X\\) 服從二項分佈，記爲 \\(X \\sim binomial(n, \\pi)\\) 或\\(X \\sim bin(n, \\pi)\\)。它的(第 \\(x\\) 次實驗的)概率被定義爲：\n\\[\\begin{align} P(X=x) \u0026amp;= ^nC_x\\pi^x(1-\\pi)^{n-x} \\\\ \u0026amp;= \\binom{n}{x}\\pi^x(1-\\pi)^{n-x} \\\\ \u0026amp; for\\;\\; x = 0,1,2,\\dots,n \\end{align}\\]\n二項分佈的期望和方差 期望 \\(E(X)\\) 若 \\(X \\sim bin(n,\\pi)\\)，那麼 \\(X\\) 就是這一系列獨立伯努利實驗中成功的次數。 用 \\(X_i, i =1,\\dots, n\\) 標記每個相互獨立的伯努利實驗。 那麼我們可以知道 \\(X=\\sum_{i=1}^nX_i\\)。 \\[\\begin{align} E(X) \u0026amp;= E(\\sum_{i=1}^nX_i)\\\\ \u0026amp;= E(X_1+X_2+\\cdots+X_n) \\\\ \u0026amp;= E(X_1)+E(X_2)+\\cdots+E(X_n)\\\\ \u0026amp;= \\sum_{i=1}^nE(X_i)\\\\ \u0026amp;= \\sum_{i=1}^n\\pi \\\\ \u0026amp;= n\\pi \\end{align}\\] 方差 \\(Var(X)\\) \\[\\begin{align} Var(X) \u0026amp;= Var(\\sum_{i=1}^nX_i) \\\\ \u0026amp;= Var(X_i+X_2+\\cdots+X_n) \\\\ \u0026amp;= Var(X_i)+Var(X_2)+\\cdots+Var(X_n) \\\\ \u0026amp;= \\sum_{i=1}^nVar(X_i) \\\\ \u0026amp;= n\\pi(1-\\pi) \\\\ \\end{align}\\] 超幾何分佈 hypergeometric distribution 假設我們從總人數爲 \\(N\\) 的人羣中，採集一個樣本 \\(n\\)。假如已知在總體人羣中(\\(N\\))有 \\(M\\) 人患有某種疾病。請問採集的樣本 \\(X=n\\) 中患有這種疾病的人，服從怎樣的分佈？\n從人羣(\\(N\\))中取出樣本(\\(n\\))，有 \\(^NC_n\\) 種方法。 從患病人羣(\\(M\\))中取出患有該病的人(\\(x\\))有 \\(^MC_x\\) 種方法。 樣本中不患病的人(\\(n-x\\))被採樣的方法有 \\(^{N-M}C_{n-x}\\) 種。 採集一次 \\(n\\) 人作爲樣本的概率都一樣。因此： \\[P(X=x)=\\frac{\\binom{M}{x}\\binom{N-M}{n-x}}{\\binom{N}{n}}\\]\n樂透中獎概率問題： 從數字 \\(1\\sim59\\) 中選取 \\(6\\) 個任意號碼 開獎時從 \\(59\\) 個號碼球中隨機抽取 \\(6\\) 個 如果六個號碼全部猜中(不分順序)，你可以成爲百萬富翁。請問一次猜中全部 \\(6\\) 個號碼的概率是多少？ 從 \\(59\\) 個號碼中隨機取出任意 \\(6\\) 個號碼的方法有 \\(^{59}C_6\\) 種。 \\[^{59}C_6=\\frac{59!}{6!(59-6)!}=45,057,474\\]\n每次選取六個號碼做爲一組的可能性相同，所以，你買了一組樂透號碼，能中獎的概率就是 \\(1/45,057,474 = 0.00000002219\\)。你還會再去買彩票麼？\n如果我只想中其中的 \\(3\\) 個號碼，概率有多大？ 用超幾何分佈的概率公式：\n\\[\\begin{align} P(X=3) \u0026amp;= \\frac{^6C_3\\times ^{53}C_3}{^{59}C_6} \\\\ \u0026amp;= 0.010 \\end{align}\\]\n你有 \\(1\\%\\) 的可能中獎。換句話說，如果中三個以上的數字算中獎的話，你買的彩票中獎的概率低於 \\(1\\%\\)。是不是覺得下次送錢給博彩公司的時候還不如跟我一起喝一杯咖啡划算？\n泊松分佈 Poisson Distribution 當一個事件，在一段時間 (\\(T\\)) 中可能發生的次數是 \\(\\lambda\\) 。那麼我們可以認爲，經過時間 \\(T\\)，該時間發生的期望次數是 \\(E(X)=\\lambda T\\)。 利用微分思想，將這段時間 \\(T\\) 等分成 \\(n\\) 個時間段，當 \\(n\\rightarrow\\infty\\) 直到每個微小的時間段內最多發生一次該事件。 那麼\n每個微小的時間段，可以視爲是一個伯努利實驗（有事件發生或者沒有） 那麼這整段時間 \\(T\\) 內發生的事件可以視爲是一個二項分佈實驗。 令 \\(X=\\) 一次事件發生時所經過的所有時間段。\n\\(X \\sim Bin(n, \\pi)\\)，其中 \\(n\\rightarrow\\infty\\)，\\(n\\) 爲時間段。 在每個分割好的時間段內，事件發生的概率都是：\\(\\pi=\\frac{\\lambda T}{n}\\) 期望 \\(\\mu=\\lambda T \\Rightarrow \\pi=\\mu/n\\) 所以 \\(X\\) 的概率方程就是： \\[\\begin{align} P(X=x) \u0026amp;= \\binom{n}{x}\\pi^x(1-\\pi)^{n-x} \\\\ \u0026amp;= \\binom{n}{x}(\\frac{\\mu}{n})^x(1-\\frac{\\mu}{n})^{n-x} \\\\ \u0026amp;= \\frac{n!}{x!(n-x)!}(\\frac{\\mu}{n})^x(1-\\frac{\\mu}{n})^{n-x} \\\\ \u0026amp;=\\frac{n!}{n^x(n-x)!}\\frac{\\mu^x}{x!}(1-\\frac{\\mu}{n})^{n-x}\\\\ 當 n\\rightarrow\\infty \u0026amp;\\; x \\ll n (x遠小於n) 時\\\\ \\frac{n!}{n^x(n-x)!} \u0026amp;=\\frac{n(n-1)\\dots(n-x+1)}{n^x} \\rightarrow 1\\\\ (1-\\frac{\\mu}{n})^{n-x} \u0026amp;\\approx (1-\\frac{\\mu}{n})^n \\rightarrow e^{-\\mu}\\\\ 所以 我們可\u0026amp;以得到泊松分佈的概率公式： \\\\ P(X=x) \u0026amp;\\rightarrow \\frac{\\mu^x}{x!}e^{-\\mu} \\end{align}\\] 當數據服從泊松分佈時，記爲 \\(X\\sim Poisson(\\mu=\\lambda T)\\;\\; or\\;\\; X\\sim Poi(\\mu)\\)\n證明泊松分佈的參數特徵： \\(E(X)=\\mu\\) \\[\\begin{align} E(X) \u0026amp;= \\sum_{x=0}^\\infty xP(X=x) \\\\ \u0026amp;= \\sum_{x=0}^\\infty x\\frac{\\mu^x}{x!}e^{-\\mu} \\\\ \u0026amp;= 0+ \\sum_{x=1}^\\infty x\\frac{\\mu^x}{x!}e^{-\\mu} \\\\ \u0026amp;= \\sum_{x=1}^\\infty \\frac{\\mu^x}{(x-1)!}e^{-\\mu} \\\\ \u0026amp;= \\mu\\sum_{x=1}^\\infty \\frac{\\mu^{x-1}}{(x-1)!}e^{-\\mu} \\\\ 這個時候我們用i\u0026amp;=x-1 替換掉所有的 x \\\\ \u0026amp;= \\mu\\sum_{i=0}^\\infty \\frac{\\mu^{i}}{i!}e^{-\\mu} \\\\ 注意到右半部分 \u0026amp;\\sum_{i=0}^\\infty \\frac{\\mu^{i}}{i!}e^{-\\mu}=1 是一個\\\\泊松分佈的所有\u0026amp;概率和 \\\\ \u0026amp;= \\mu \\end{align}\\]\n\\(Var(x)=\\mu\\) 爲了找到 \\(Var(X)\\)，我們用公式 \\(Var(X)=E(X^2)-E(X)^2\\) 我們需要找到 \\(E(X^2)\\)\n\\[\\begin{align} E(X^2) \u0026amp;= \\sum_{x=0}^\\infty x^2\\frac{\\mu^x}{x!}e^{-\\mu} \\\\ \u0026amp;= \\mu \\sum_{x=1}^\\infty x\\frac{\\mu^{x-1}}{(x-1)!}e^{-\\mu} \\\\ 這個時候我們用i\u0026amp;=x-1 替換掉所有的 x \\\\ \u0026amp;= \\mu \\sum_{i=0}^\\infty (i+1)\\frac{\\mu^{i}}{i!}e^{-\\mu} \\\\ \u0026amp;= \\mu(\\sum_{i=0}^\\infty i\\frac{\\mu^i}{i!}e^{-\\mu} + \\sum_{i=0}^\\infty \\frac{\\mu^i}{i!}e^{-\\mu}) \\\\ \u0026amp;= \\mu(E(X)+1) \\\\ \u0026amp;= \\mu^2+\\mu \\\\ 因此，代入上面\u0026amp;提到的方差公式： \\\\ Var(X) \u0026amp;= E(X^2) - E(X)^2 \\\\ \u0026amp;= \\mu^2 + \\mu -\\mu^2 \\\\ \u0026amp;= \\mu \\end{align}\\]\n","date":1507680000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1507680000,"objectID":"3eed3ea4df8f88d6ee3b1cab722485d2","permalink":"https://wangcc.me/post/probability3/","publishdate":"2017-10-11T00:00:00Z","relpermalink":"/post/probability3/","section":"post","summary":"簡單的離散分佈：二項分佈，泊松分佈","tags":["basic mathematics","learning notes","Medical Statistics","LSHTM"],"title":"你買的彩票中獎概率到底有多少？","type":"post"},{"authors":null,"categories":["statistics","study abroad"],"content":" 概率密度曲線 probability density function， PDF 一個隨機連續型變量 \\(X\\) 它的性質由一個對應的概率密度方程 (probability density function, PDF) 決定。\n在給定的範圍區間內，如 \\(a\\sim b, (a \u0026lt; b)\\)，它的概率滿足:\n\\[P(a\\leqslant X \\leqslant b) = \\int_a^bf(x)dx\\]\n這個相關的方程，在 \\(a\\sim b\\) 區間內的積分，就是這個連續變量在這個區間內取值的概率。 # R codes for drawing a standard normal distribution by using ggplot2 library(ggplot2) p \u0026lt;- ggplot(data.frame(x=c(-3,3)), aes(x=x)) + stat_function(fun = dnorm) p + annotate(\u0026quot;text\u0026quot;, x=2, y=0.3, parse=TRUE, label=\u0026quot;frac(1, sqrt(2*pi)) * e ^(-z^2/2)\u0026quot;) + theme(plot.subtitle = element_text(vjust = 1), plot.caption = element_text(vjust = 1), axis.text.x = element_text(size = 12), axis.text.y = element_text(size = 12), plot.title = element_text(size = 10, face = \u0026quot;bold\u0026quot;, hjust = 0.5), panel.background = element_rect(fill = \u0026quot;ivory\u0026quot;)) + labs(title = \u0026quot;Probability density functions \\n for standard normal distribution\u0026quot;, x = NULL, y = NULL) + stat_function(fun = dnorm, xlim = c(-1.3,0.4), geom = \u0026quot;area\u0026quot;,fill=\u0026quot;#00688B\u0026quot;, alpha= 0.2) 注意：整個方程的曲線下面積等於 \\(1\\)： \\[\\int_{-\\infty}^\\infty f(x)dx=1\\]\n期望 \\(E(X)=\\int_{-\\infty}^\\infty xf(x)dx\\) 方差 \\(Var(X)=\\int_{-\\infty}^\\infty (x-\\mu)^2f(x)dx\\) 正態分佈 如果一組數據服從正態分佈，我們通常用它的期望（或者叫平均值）\\(\\mu\\)，和它的方差 \\(\\sigma^2\\)，來描述這組數據。記爲：\n\\[X \\sim N(\\mu, \\sigma^2)\\]\n它的概率密度方程可以表述爲： \\[f(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp(-\\frac{(x-\\mu)^2}{2\\sigma^2})\\]\n\\(E(x) =\\mu\\) \\(Var(x)=\\sigma^2\\) 標準正態分佈 標準正態分佈的期望（或者均值）爲 \\(0\\)，方差爲 \\(1\\)\n記爲：\\(Z \\sim N(0,1)\\) 它的概率密度方程表述爲： \\[\\frac{1}{\\sqrt{2\\pi}}exp(-\\frac{z^2}{2})\\]\n它的累積分佈方程 (cumulative distribution function， CDF)，是將概率密度方程 (PDF) 積分以後獲得的方程。通常我們記爲 \\(\\Phi(z)\\) 再看一下標準正態分佈的概率密度方程曲線：\n95% 的曲線下面積在標準差 standard deviation \\(-1.96\\sim1.96\\) 之間的區域。 而且，\\(\\phi(-x)=1-\\phi(x)\\) 任何一個正態分佈都可以通過下面的公式，標準化成爲標準正態分佈： \\[Z=\\frac{X-\\mu}{\\sigma}\\]\n","date":1507680000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1507680000,"objectID":"1b31ef40092f8aa91876f40cf28342a3","permalink":"https://wangcc.me/post/normal-distribution/","publishdate":"2017-10-11T00:00:00Z","relpermalink":"/post/normal-distribution/","section":"post","summary":"正態分佈","tags":["basic mathematics","learning notes","Medical Statistics","probability"],"title":"正態分佈","type":"post"},{"authors":null,"categories":["statistics","study abroad"],"content":" Bayes 理論的概念 許多時候，我們需要將概率中的條件相互對調。 例如： 在已知該人羣中有20%的人有吸菸習慣(\\(P(S)\\))，吸菸的人有9%的概率有哮喘(\\(P(A|S)\\))，不吸菸的人有7%的概率有哮喘(\\(P(A|\\bar{S})\\))的前提下，有個人前來門診，發現是哮喘患者，那麼這個人有多大的概率是一個菸民？也就是要求 \\(P(S|A)\\)\n這裏先引入貝葉斯的概念：\n我們可以將 \\(P(A\\cap S)\\) 寫成： \\[P(A\\cap S)=P(A|S)P(S)\\\\or\\\\ P(A\\cap S)=P(S|A)P(A)\\] 這兩個等式是完全等價的。我們將他們連起來：\n\\[P(S|A)P(A)=P(A|S)P(S)\\\\ \\Rightarrow P(S|A)=\\frac{P(A|S)P(S)}{P(A)}\\]\n是不是看起來又像是寫了一堆廢話？ 沒錯，你看出來是一堆廢話的時候，證明你也同意這背後的簡單邏輯。\n再繼續，我們可以利用另外一個廢話：\\(\\because S+\\bar{S}=1\\\\ \\therefore P(A)=P(A\\cap S)+P(A\\cap\\bar{S})\\)\n用上面的公式替換掉 \\(P(A\\cap S)+P(A\\cap\\bar{S}） \\\\ \\therefore P(A)=P(A|S)P(S)+P(A|\\bar{S})P(\\bar{S})\\)\n可以得到貝葉斯理論公式：\n\\[P(S|A)=\\frac{P(A|S)P(S)}{P(A|S)P(S)+P(A|\\bar{S})P(\\bar{S})}\\]\n回到上面說到的哮喘人中有多少比例吸菸的問題。可以繼續使用概率樹來方便的計算：\n\\[\\begin{align} P(S|A) \u0026amp;= \\frac{P(A|S)P(S)}{P(A|S)P(S)+P(A|\\bar{S})P(\\bar{S})} \\\\ \u0026amp;= \\frac{0.09\\times0.2}{0.09\\times0.2+0.07\\times0.8} \\\\ \u0026amp;= 0.24 \\end{align}\\]\n所以我們的結論就是，在已知該人羣中有20%的人有吸菸習慣(\\(P(S)\\))，吸菸的人有9%的概率有哮喘(\\(P(A|S)\\))，不吸菸的人有7%的概率有哮喘(\\(P(A|\\bar{S})\\))的前提下，有個人前來門診，發現是哮喘患者，那麼這個人有24% 的概率是一個菸民(\\(P(S|A)\\))。\n期望 Expectation (或均值 or mean) 和 方差 Variance 期望（或均值）是用來描述一組數據中心位置的指標（另一個是中位數 Median）。 對於離散型隨機變量 \\(X\\) (discrete random variables)，它的期望被定義爲：\n\\[E(X)=\\sum_x xP(X=x)\\]\n所以就是將所有 \\(X\\) 可能取到的值乘以相應的概率後求和。這個期望（或均值）常常用希臘字母 \\(\\mu\\) 來標記。\n方差 Variance 是衡量一組數據變化幅度(dispersion/variability)的指標之一。 方差的定義是：\n\\[Var(X)=E((X-\\mu)^2)\\\\其中，\\mu=E(x)\\]\n實際上我們更加常用的是它的另外一個公式：\n\\[Var(X)=E(X^2)-E(X)^2\\]\n證明 上面兩個方差公式相等 \\[\\begin{align} Var(x) \u0026amp;= E((X-\\mu)^2) \\\\ \u0026amp;= E(X^2-2X\\mu+\\mu^2)\\\\ \u0026amp;= E(X^2) - 2\\mu E(X) + \\mu^2\\\\ \u0026amp;= E(X^2) - 2\\mu^2 + \\mu^2 \\\\ \u0026amp;= E(X^2) - \\mu^2 \\\\ \u0026amp;= E(X^2) - E(X)^2 \\end{align}\\]\n方差的性質： \\(Var(X+b)=Var(X)\\) \\(Var(aX)=a^2Var(X)\\) \\(Var(aX+b)=a^2Var(X)\\) 伯努利分佈 Bernoulli distribution 伯努利分佈，說的就是一個簡單的二分變量 (1, 0)，它取1時的概率如果是 \\(\\pi\\)。那麼我們可以計算這個分佈的期望值:\n\\[\\begin{align} E(X) \u0026amp;=\\sum_x xP(X=x) \\\\ \u0026amp;=1\\times\\pi + 0\\times(1-\\pi)\\\\ \u0026amp;=\\pi \\end{align}\\]\n由於 \\(x=x^2\\)，因爲 \\(x=0,1\\), 所以 \\(E[X^2]=E[X]\\)，那麼方差爲：\n\\[\\begin{align} Var(X) \u0026amp;=E[X^2]-E[X]^2 \\\\ \u0026amp;=E[X]-E[X]^2 \\\\ \u0026amp;=\\pi - \\pi^2 \\\\ \u0026amp;=\\pi(1-\\pi) \\end{align}\\]\n證明，\\(X,Y\\) 爲互爲獨立的隨機離散變量時，\na) \\(E(XY)=E(X)E(Y)\\) ; b) \\(Var(X+Y)=Var(X)+Var(Y)\\) 證明 \\[\\begin{align} E(XY) \u0026amp;= \\sum_x\\sum_y xyP(X=x, Y=y) \\\\ \\because \u0026amp;\\; X,Y are\\;independent\\;to\\;each\\;other \\\\ \\therefore \u0026amp;= \\sum_x\\sum_y xyP(X=x)P(Y=y)\\\\ \u0026amp;=\\sum_x xP(X=x)\\sum_y yP(Y=y)\\\\ \u0026amp;=E(X)E(Y) \\end{align}\\]\n證明 根據方差的定義： \\[\\begin{align} Var(X+Y) \u0026amp;= E((X+Y)^2)-E(X+Y)^2 \\\\ \u0026amp; \\; Expand \\\\ \u0026amp;=E(X^2+2XY+Y^2)-(E(X)+E(Y))^2\\\\ \u0026amp;=E(X^2)+E(Y^2)+2E(XY)\\\\ \u0026amp;\\;\\;\\; - E(X)^2-E(Y)^2-2E(X)E(Y)\\\\ \u0026amp;\\; We\\;just\\;showed\\; E(XY)=E(X)E(Y)\\\\ \u0026amp;=E(X^2)-E(X)^2+E(Y^2)-E(Y)^2 \\\\ \u0026amp;=Var(X)+Var(Y) \\end{align}\\] ","date":1507593600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1507593600,"objectID":"5abed355cf9c4552b932ea0b934a729b","permalink":"https://wangcc.me/post/probability2-4/","publishdate":"2017-10-10T00:00:00Z","relpermalink":"/post/probability2-4/","section":"post","summary":"Bayes 概念; 伯努利分佈 Bernoulli distribution ; 期望值; 方差","tags":["basic mathematics","learning notes","Medical Statistics","LSHTM"],"title":"概率論2","type":"post"},{"authors":null,"categories":["statistics","study abroad"],"content":" 轉眼我已經進入課程的第二週了，總體來說，我們一半的時間都在電腦房練習 Stata 的數據清理和簡單的描述統計 (descriptive statistics)。從我個人的經驗來說，數據分析的過程，其實一大半的時間是消耗在 data cleaning 上的，即使手頭拿到了所謂的乾淨的數據，到真正要分析的時候就會發現一大堆的問題在裏面，需要重新整理，重新添加標記以使之變得更加讓人類可以讀懂。電腦是機器，他是不管你的數據是否乾淨的。只要你放了數據進去，邏輯還可以，沒有編程上的語法錯誤，它總歸會出來一些報告和結果的。如果就這麼直接用的話，大部分的人就會掉進陷阱。畢竟數據不光會說出事實真相，更多的情況下還會把真相給掩蓋住了。\n我的其餘大部分時間都用在了複習高等數學的微積分上了。感覺好似回到了高中時代。其實大學的時候線性代數得分還是接近滿分的。後來多年不用，生疏了。剛打開複習的書的時候，許多微分積分的規則都已經忘記。通過這一週的辛苦練習，終於是找回了一點狀態。如果你也想有空的時候複習以下高中數學知識，這本書可以推薦給你：\nQuick Calculus: Short Manual of Self-instruction\n上面這本書的內容可以一邊閱讀，一邊練習。實在是複習的一本好書。我花了一週的課餘時間，從頭到尾把裏面的習題和解答全部完成。收穫很大。感覺年輕時的數學思維又開始在大腦裏復甦了。一身輕鬆。\n下面想介紹一下上週學習的概率的基礎問題。\n首先是最基礎的三個概率的公理： 對於任意事件 \\(A\\)，它發生的概率 \\(P(A)\\) 滿足這樣的不等式： \\(0 \\leqslant P(A) \\leqslant 1\\) \\(P(\\Omega)=1\\) , \\(\\Omega\\) 是全樣本空間 (total sample space) 對於互斥（相互獨立）的事件 \\(A_1, A_2, \\dots, A_n\\) 有如下的等式關係： \\(P(A_1\\cup A_2 \\cup \\cdots \\cup A_n)=P(A_1)+P(A_2)+\\cdots+P(A_n)\\) 你是不是覺得上面三條公理都是廢話。 不用擔心，我也是這麼覺得的。因爲所有人都認同的道理，才能成爲公理 (axiom)，因爲它們是不需要證明的自然而然形成的人人都接受的觀念。(axiom: a saying that is widely accepted on its own merits; its truth is assumed to be self-evident)\n然而，正是這樣顯而易見的道理，確是拿來建築理論的基石，千萬不能小看了他們。例如，我們看下面這個看似也應該成爲公理的公式，你能證明嗎：\n\\(P(A_1\\cup A_2) = P(A_1) + P(A_2) - P(A_1 \\cap A_2)\\)\n證明： 先考慮 \\(A_1 \\cup A_2\\) 是什麼（拆分成三個互斥事件）\n\\(A_1 \\cup A_2 = (A_1\\cap \\bar{A_2})\\cup(\\bar{A_1}\\cap A_2)\\cup(A_1\\cap A_2)\\)\n運用上面的公理2 3\n\\(\\therefore P(A_1 \\cup A_2) = P(A_1\\cap \\bar{A_2}) + P(\\bar{A_1}\\cap A_2) + P(A_1\\cap A_2) \\;\\;\\;\\;\\;\\;(1)\\)\n再考慮 \\(A_1=(A_1\\cap A_2)\\cup(A_1\\cap\\bar{A_2})\\) 繼續拆分成兩個互斥事件\n\\(\\therefore P(A_1)=P(A_1\\cap A_2)+P(A_1\\cap\\bar{A_2})\\) 整理一下：\n\\(P(A_1\\cap\\bar{A_2})=P(A_1)-P(A_1\\cap A_2)\\)\n同理可得: \\(P(\\bar{A_1}\\cap A_2)=P(A_2)-P(A_1\\cap A_2)\\)\n代入上面第(1)式可得：\n\\(P(A_1 \\cup A_2) =P(A_1)-P(A_1\\cap A_2)\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;+P(A_2)-P(A_1\\cap A_2)\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;+P(A_1\\cap A_2)\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=P(A_1) + P(A_2) - P(A_1 \\cap A_2)\\)\n條件概率 Conditional probability \\(P(A|S)=\\frac{P(A\\cap S)}{P(S)}\\) \\(P(A\\cap S) = P(A|S)P(S)\\) 獨立 (independence) 的定義 兩個事件定義爲互爲獨立時 (\\(A\\) and \\(B\\) are said to be independent if and only if) \\[P(A\\cap B)=P(A)P(B)\\] 因爲從條件概率的概念我們已知\n\\(P(A\\cap B) = P(A|B)P(B)\\) 所以\\(P(A|B)=P(A)\\) 即：事件 \\(B\\) 無法提供事件 \\(A\\) 的任何有效訊息 (\\(A, B\\) 互相獨立) 賭博問題 終於來到本次話題的重點了。我要扣題了哦。語文老師快在此加分。\n假設你在一個電視遊戲節目。有上圖一樣的三扇門。其中一扇門後面有一輛保時捷，另兩扇門後面則是(味道奇特的)山羊。遊戲規則是主持人會讓你先選擇其中一扇門（先不打開你選的這扇門）。主持人隨後打開另外兩扇門中的一扇沒有保時捷的門。主持人問你，你要堅持選擇之前選中的那扇門，還是要改變主意換一扇門去猜是否可以猜中保時捷。 請問，堅持選擇之前選中的門猜中保時捷的概率高，還是主持人打開一扇門以後改變主意猜中保時捷的概率更高呢？\n答案明天揭曉。\n","date":1507420800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1507420800,"objectID":"1fa12a866db18b3c23fa1a9b13eb0047","permalink":"https://wangcc.me/post/probability-gambling/","publishdate":"2017-10-08T00:00:00Z","relpermalink":"/post/probability-gambling/","section":"post","summary":"回顧一下最近學習的情況","tags":["probability","inference"],"title":"你會用概率論來賭博嗎？","type":"post"},{"authors":null,"categories":["statistics"],"content":" Basic Definition and notations: An \\(m\\times n\\) matrix \\(A\\) is a rectangular array of numbers with \\(m\\) rows and \\(n\\) columns.\nThe elements of a matrix \\(A_{m\\times n}\\) are \\(a_{ij}\\)\nThe order of a matrix is the number of rows by the number of columns, i.e. \\(m\\times n\\)\nA column vector with \\(m\\) elements, \\(y = \\left( \\begin{array}{c} y_1\\\\ y_2\\\\ \\vdots\\\\ y_n \\end{array} \\right)\\), is a matrix with only one column i.e. an \\(m\\times 1\\) matrix.\nA row vector with \\(n\\) elements, \\(x=(x_1,x_2,x_3, \\cdots, x_n)\\), is a matrix with only one row, i.e. an \\(1\\times n\\) matrix.\nTransposed matrix \\(A^T\\) (or \\(A\u0026#39;\\)) arises from the matrix \\(A\\) by interchanging the column vectors and the row vectors i.e. \\(a_{ij}^T = a_{ji}\\) (so a column vector is converted into a row vector and vise versa)\nA partitioned matrix is a matrix written in terms of sub-matrices. \\(A=\\left( \\begin{array}{c} A_{11} \u0026amp; A_{12}\\\\ A_{21} \u0026amp; A_{22}\\\\ \\end{array} \\right)\\), where \\(A_{11},A_{12},A_{21},A_{22}\\) are sub-matrices\n\\(A_{11}, A_{21}\\) have the same number of columns, so do \\(A_{12}, A_{22}\\) \\(A_{11}, A_{12}\\) have the same number of rows, so do \\(A_{21}, A_{22}\\) partitioning is not restricted to dividing a matrix into just four sub-matrices A square matrix has exactly as many rows as it has columns i.e. the order of the matrix is \\(n\\times n\\)\nThe main diagonal (or leading diagnonal) of a square matrix \\(A (n\\times n)\\) are the elements lying on the diagnoal from top left to bottom right. \\(a_{11},a_{22},a_{33},\\cdots,a_{nn}\\) i.e. all \\(a_{ii}, i= 1,\\cdots, n\\)\nThe trace of a square matrix is the sum of the diagonal elements \\(tr(A)=a_{11}+a_{22}+\\cdots+a_{nn}=\\sum_{i=1}^na_{ii}\\)\nSpecial matrices A symmetric matrix is a square matrix for which the following is true for all the off diagonal elements. \\(a_{ij}=a_{ji}\\) i.e. \\(A^T=A\\) Diagonal matrix is a square matrix having zero for all the non-diagonal elements i.e. \\(A=\\left( \\begin{array}{c} a_{11} \u0026amp; \\cdots \u0026amp; 0\\\\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots\\\\ 0 \u0026amp; \\cdots \u0026amp; a_{nn} \\end{array} \\right)\\) Zero matrix (null matrix) is a matrix whose all elements are zero Identity matrix (or unit matrix) is a diagonal matrix having all diagonal elements equal to 1 and off diagonal elements equal to zero. i.e. \\(I=\\left( \\begin{array}{c} 1 \u0026amp; \\cdots \u0026amp; 0\\\\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots\\\\ 0 \u0026amp; \\cdots \u0026amp; 1 \\end{array} \\right)\\) “Summing vector” is a vector whose every element is 1 i.e. \\(1_{n}=(1\\cdots1)\\) “J matrix” is a matrix (not necessarily square) whose every element is 1 i.e. \\(J_{m\\times n}=\\left( \\begin{array}{c} 1 \u0026amp; 1 \u0026amp; \\cdots \u0026amp; 1\\\\ 1 \u0026amp; 1 \u0026amp; \\cdots \u0026amp; 1 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots\\\\ 1 \u0026amp; 1 \u0026amp; \\cdots \u0026amp; 1 \\end{array} \\right)\\) Basic Operations Addition (Substraction) can take place only when the matrices involved are of the same order. i.e. Two matrices can be added (subtracted) only if they have the same numbers of rows and the same numbers of columns. \\(A+B=B+A\\) \\((A+B)+C=A+(B+C)\\) \\(A+0=0+A=A\\) \\(A+(-A)=0\\) \\((A+B)^T=A^T+B^T\\) Multiplication by scalar: - \\(cA=Ac\\) - \\(c(dA)=(cd)A\\) - \\((c\\pm d)A=cA\\pm dA\\) - \\(c(A\\pm B)=cA \\pm cB\\)\nMultiplication of an \\(2\\times2\\) matrix by a column vector which has 2 rows yields a column vector with \\(2\\) rows. \\[Ax=\\left( \\begin{array}{c} a_{11} \u0026amp; a_{12}\\\\ a_{21} \u0026amp; a_{22}\\\\ \\end{array} \\right)\\left( \\begin{array}{c} x_{1}\\\\ x_{2}\\\\ \\end{array} \\right)=\\left( \\begin{array}{c} a_{11}x_1+a_{12}x_2\\\\ a_{21}x_1+a_{22}x_2\\\\ \\end{array} \\right)\\]\nGenerally: Multiplication of an \\(m\\times n\\) matrix by a column vector which has \\(n\\) rows yields a column vector with \\(m\\) rows. \\[Ax=\\left( \\begin{array}{c} a_{11} \u0026amp; \\cdots \u0026amp; a_{1n} \\\\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ a_{m1} \u0026amp; \\cdots \u0026amp; a_{mn} \\end{array} \\right)\\left( \\begin{array}{c} x_{1}\\\\ x_{2}\\\\ \\vdots \\\\ x_{n} \\end{array} \\right)=\\left( \\begin{array}{c} a_{11}x_{1}+a_{12}x_2+\\cdots+a_{1n}x_n\\\\ \\vdots \\\\ a_{m1}x_{1}+a_{m2}x_2+\\cdots+a_{mn}x_n \\end{array} \\right)=y \\\\ i.e. y_i=\\sum_{j=1}^na_{ij}x_j, \\; i=1,\\cdots, m\\]\nMultiplication of matrices: The product \\(AB=C\\) is defined only when \\(A_{m\\times r}\\) has exactly as many columns as \\(B_{r\\times n}\\) has rows. And the elements of \\(C_{m\\times n}\\) are given as \\[c_{ij}=\\sum_{l=1}^na_{il}b_{lj}, \\;\\; i=1,\\cdots,m \\; and \\; j=1,\\cdots, n\\]\n\\(AB \\neq BA\\) \\((AB)C=A(BC)=ABC\\) \\(A(B+C)=AB+AC\\) \\((B+C)A=BA+CA\\) \\(IA=AI=A\\) Further definitions The determinant of a second order square matrix is \\(det(A)=|A|=\\begin{vmatrix} a_{11} \u0026amp; a_{12} \\\\ a_{21} \u0026amp; a_{22} \\end{vmatrix}=a_{11}a_{22}-a_{12}a_{21}\\)\nThe inverse of a matrix \\(A\\), \\(A^{-1}\\) if it exists, is a matrix whose product with \\(A\\) is the identity matrix i.e. \\(AA^{-1}=A^{-1}A=I\\). (Note: both \\(A\\) and \\(A^{-1}\\) have to be square) For second order matrices:\\(A^{-1}=\\frac{1}{det(A)}\\left( \\begin{array}{c} a_{22} \u0026amp; -a_{12}\\\\ -a_{21} \u0026amp; a_{11}\\\\ \\end{array} \\right)\\)\nSingular or non-invertible matrix: \\(det(A)=0\\)\nIdempotent matrices(冪等矩陣) are square and the following is true: \\(AA=A^2=A\\)\nOrthogonal matrices have the following property: \\(AA^T=A^TA=I\\)\n","date":1506729600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1506729600,"objectID":"64ddb68a4f727024de4cc92a2c2fd7de","permalink":"https://wangcc.me/post/matrix-revision/","publishdate":"2017-09-30T00:00:00Z","relpermalink":"/post/matrix-revision/","section":"post","summary":"統計學中常用的簡單數學規則 (2)","tags":["basic mathematics","linear algebra","Medical Statistics","LSHTM","London"],"title":"Matrix Revisions","type":"post"},{"authors":null,"categories":["diary","study abroad"],"content":"我們學科 (Medical Statistics) 是爲數不多的在 Orientation 周就有一半以上的時間在上課的學科。別的學科像 Epidemiology 這周還集體去劍橋大學見學啥的。幾乎都是第二周，也就是10月2日週一開始的時候才有大量的必修和選修課。所以不見得再有太多時間寫見聞和體驗。（也沒有時間出去玩了\u0026hellip;..）不過學習的內容還是會來更新一下，給各位有個印象，也讓大家都來判斷以下，這裏的碩士課程的內容和質量到底如何。\n9月22日 ISW \u0026amp; UCL \u0026amp; 福爾摩斯博物館（的紀念品商店） 忘了交代，9月21日和22日都是 Internatioanal Student Welcome (留學生歡迎會, ISW) 的日子。充滿對這一年的期待，和但是是否自己能最終倖存下來的不安，坐在這樣壓力巨大的梯形教室裏，我和這麼多來自世界各地的年輕人成爲了同學： 這個教室就是大名鼎鼎的 John Snow Lecture Theatre。我就知道你以爲是這個人： 其實歷史上的 Snow 同學可是奠定了近代流行病學基礎的巨人。比這個 bastard 強$^{9999999}$ 多了好麼，快去維基百科自學去。\n這個大名鼎鼎的水泵就是他拆的！ 閒話少敘，註冊參加時，排隊的樣子： 領到自己的卡，寫上自己的名，和課程名稱 (原諒我的粗鄙的筆跡)： 看這梯形教室有多陡峭：（據說梯形教室陡峭的程度和學習壓力成正比）\n學校還請來了倫敦警察（可愛壞）薯熟來跟大家講我以前感覺只有在中國才會有的防盜防火防學長的故事：\n22日中午結束以後我就步行在大學附近閒逛，UCL就在我們大學 Keppel Street 往北一點點。連3分鐘都不到。看這大學，真有大學的感覺：\n爲數不多的晴天在這一週已經碰到兩天了。是不是應該買張彩票試試看？（笑） UCL是以前高中同班同學待過（一個已經回廈門），和正在待的地方（一個正在博士後/Research Assistant?）。\n大學對面是UCL的醫學院：\n有那麼一點點霍格沃茨的感覺。我在校內略逛了以下，起身前去貝克街。如果你聽說過貝克街，那你一定聽說過221號B。因爲這是小說裏福爾摩斯和華生的住址。來之前我就查過了，距離倫敦大學步行半個小時左右，中途還會經過杜莎夫人蠟像館（聽起來就令人覺得索然無味的地方）。\n當我來到貝克街的時候，路邊有個福爾摩斯的雕像，許多人駐足和心目中世界上最聰明的男人合影留念：\n旁邊還有一個貝克街的公共汽車站證明了這可是真實存在的地址哦：\n但是其實這裏只是貝克街的起點。走到 Baker Street 221 B 之後我被門口排的長隊驚呆了：\n於是我趕緊跟在隊的最後面排隊，大概五分鐘過後有個老頭過來問說，你買了門票了嗎？ 我說我還以爲這裏就是排隊買門票呢。他說這些人都是排隊進福爾摩斯紀念館的。旁邊的小房間現在是紀念品商店，可以在那裏買到門票。所以我就轉身進入了紀念品商店。\n商店裏還真是應有盡有。你能想到的關於福爾摩斯的任何東西。而且大多數價格都不便宜：\n這個杯子賣八鎊，低下寫着英國製造。\n這頂帽子，全羊毛手工製作，也寫着英國製造，49鎊一頂：\n這兩個東西正好也是我現在需要的。所以我就買了下來。但是進入紀念館的門票竟然要16英鎊。我想還是等我有學生優惠了以後再來問問看吧。說不定可以便宜一些。打道回府的路上又經過UCL，在這棟標誌性的建築物門口的長椅上坐了許久，休息，沉思。側面的角度還是很有感覺的呢。\n","date":1506470400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1506470400,"objectID":"0b83f2fceecc9d479067895adb1330c0","permalink":"https://wangcc.me/post/first-impression-of-london2/","publishdate":"2017-09-27T00:00:00Z","relpermalink":"/post/first-impression-of-london2/","section":"post","summary":"倫敦 Day34567","tags":["experience","London"],"title":"倫敦城漫步 (2)","type":"post"},{"authors":null,"categories":["diary","study abroad"],"content":"在倫敦生活過完了第一周，似乎快要步入正軌了。因為許許多多的前輩告誡說，第一周是唯一可以輕鬆度過的時間，一定要珍惜好好利用。多看看倫敦，多四處走走。\n於是我很聽話的每天都四處走走。接下來打算把這幾天去過的地方盡量根據回憶都列舉一下：\n9月19日 大英博物館初體驗 國王十字車站附近的郵局 → 大英博物館\n從宿舍往北步行十五分鐘左右，就是大英圖書館，圖書館的旁邊就能看到國王十字車站 (King\u0026rsquo;s Cross Station)。所有剛到英國的留學生（簽證在6個月以上的吧？），都要去自己申請簽證時登記的郵局領取BRP卡。BRP卡就相當於日本的在留卡，也就是登記一下外國人的個人信息，住址和在留期限。我當時登記的是這個最近的郵局，叫做國王十字車站郵局 (Kings Cross Post Office)。\n去郵局的路上看到了兩個騎著高頭大馬的帥氣警察。不敢從正面拍，所以走到了後面才趕緊拍了一張： 然後你也會看到一個非常有巴洛克風的建築物： 這個叫做 St. Pancras Renaissance Hotel London，中文名應該是聖潘克拉斯萬麗酒店。如果你玩過刺客信條梟雄。一定會對這樣的建築印象深刻。\n郵局內部真的是破爛不堪，連中國三線城市的郵政儲蓄營業廳都不如，櫃檯工作的好多是黑人和印度人，口音很重，態度蠻橫。要做好心理準備。而且標識十分不明，你必須問周圍的人我現在排的隊是幹什麼的。\n領完我的BRP卡之後，又往回走，到宿舍以南步行也是10分鐘左右，進入大英博物館。從正門走時，因為每個進入大英博物館的人如果有帶包都要打開給警備員查看。所以入口處隊伍非常的長。後來聽說從後門走的話人就少很多。不過這天不是週末，所以我得以很快的通過安檢進入博物館。\n最重要的是，大英博物館是免費的！ 大英博物館外觀：\n進入大英博物館的玄關以後，視野一下子就開闊起來:\n果然從世界各地搶來的東西展覽起來就是有底氣！ 呵呵！\n當然英國人對世界人類瑰寶的保存還是花了不少力氣。我花了一個小時左右看了古埃及部分，一點點古希臘，還有一個專門展覽人類各種貨幣的展廳。牆裂推薦！\n9月20日 暴走倫敦城 谷歌事無鉅細記錄了我一天的行程。早晨我起床吃了早飯就離開宿舍前往天空花園 (Sky Garden)。\n出發來倫敦之前，通過閱讀學校給的關於倫敦的簡單介紹，知道了這個地方。應該是類似新加坡 Sky Park。的空中花園。天氣晴朗的時候可以俯瞰全城的地標性建築物。 不同的是倫敦的天空花園是免費，且要預約的。如果你打開上面天空花園的網頁鏈接，就能看見預約的方法。我提前預約了週三早晨10點半進入參觀的門票。如果我沒有記錯的話，新加坡的空中花園不用預約，但是需要在門口乘電梯的地方購買門票，一個人應該是15新幣的樣子。\nSky Garden 的外觀： 週三天氣很不好，看不清楚太遠的地方。不過畢竟登高望遠，泰晤士河兩岸，倫敦塔橋等建築物還是能看得見：\n霧濛濛的是不是感覺比北京天氣還糟糕？！ 這麼說有點不公平，因為我沒在北京生活過。客觀點說倫敦空氣質量還不錯，天氣很不好。據說到了冬天抑鬱症的人就會增加。\n雖然預約的是10點半到11點半一個小時，不過整個花園不大，天氣不好所以也沒什麼看頭，十五分鐘我就下來了。看著地圖又繼續往泰晤士河邊走。中途路過倫敦大火紀念碑：\n經過倫敦橋(你以為倫敦橋有多壯觀？看了你會失望)以後你會發現這種橋在黃浦江上可能還根本算不上是一座橋。也就跟我們村里小河邊看櫻花的那個橋差不太多。。。\n可能倫敦“城裡人”會不同意我的話哈哈，請多見諒，您是城里人嘛。\n到了泰晤士河南岸，往西走，去塔橋 (Tower Bridge) 的路上會看見一艘軍艦停靠在港口：\n估計和停靠在珍珠港的軍艦一樣曾經在戰爭中服役。只不過從外觀規模上來看這艘軍艦顯然小很多。門票有點小貴，所以我也沒有花錢進去。經過軍艦以後是個不大不小的廣場。讓我想起了廈門輪渡碼頭。不過，再往前就是壯觀的塔橋了 (Tower Bridge)：\n塔橋在大船要進入泰晤士河的時候是可以從中間舉起，讓船通過然後再放下的。之前都只在風景明信片裡面看過的建築物，我終於有幸在上面留下我的足跡。走上塔橋意味著就離開南岸，又回到北岸了。\n回宿舍的路上，我注意觀察了路邊的ATM機器。絕大部分都是只在牆壁上挖了一個洞，放個機器。讓人超級沒有安全感的。密碼被周圍的人看了咋辦？ 取了多少錢都被周圍的人看見了，有人見財起意咋辦？ 英國人都不在乎這些嗎？ 好像銀行的建築物裡面的ATM機器可能可以多少讓人有點安全感吧。\n機器上方的字的意思是取錢不收手續費。這非常的好。於是我在一個車站的較為隱蔽的角落裡的 ATM 嘗試著用 manepartners 的卡著取了些現金。後來手機一查，還是扣了我 1.5 鎊的手續費。估計多半是 manepartners 扣去了的。\n回到宿舍，我暴走了近 10 公里的腳都已不聽使喚，我還盤算著這樣白天足夠累了的話，夜裡就能多睡睡，把時差快點調整過來。誒，調整時差也應該有個人區別，也許我就是屬於不太容易調整時差的那類吧。\n","date":1506384000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1506384000,"objectID":"55a06daadd49fe6db10179b96a54d4be","permalink":"https://wangcc.me/post/first-impression-of-london/","publishdate":"2017-09-26T00:00:00Z","relpermalink":"/post/first-impression-of-london/","section":"post","summary":"倫敦 Day1 \u0026 Day2","tags":["London","LSHTM"],"title":"漫步倫敦城 (1)","type":"post"},{"authors":null,"categories":["study abroad","diary"],"content":"離開 飛機翱翔在俄羅斯上空。我在機艙內拿出手機，再一次看老友記第四季的最後一集。講的是 Ross 和 Emily 在倫敦的婚禮。 熟悉的歡樂劇情，不熟悉的城市。如果你對老友記像我一樣熟悉，快來跟我做朋友吧！\n十三個小時的飛行，我一直無法休息。我在憧憬這一年如痴如醉的傲遊知識海洋嗎？我離開家，離開愛妻，離開親愛的孩子們，是多麼的捨不得。我才剛踏上旅程，思念就如同潮水在心中湧起。在中部機場和妻告別時，她拿出一個小小的粉紅色信封。依依不捨的告訴我說上了飛機再看。想來和妻在一起這幾年，這是她第一次這樣細膩又柔軟的感情表達。可是我卻把整個家留在了身後，全部交給了她。這一年，要辛苦你了。\n就在離開日本的前一日，名古屋還在18號颱風的正面襲擊之下。夜晚狂風驟雨，摧枯拉朽地吹散整個城市的思念。早晨醒來，天空還是陰沉沉的，颱風仍然沒有完全過去。全家人擔心著飛機的起飛是否被影響。我們還是毅然決然地開著心愛的小西沖向了機場。在日航的櫃檯等待行李寄存時，抬頭看見大屏幕上的航班信息，名古屋-沖繩 取消， 名古屋-札幌 取消，名古屋-成田（東京） 取消。。。。 一整個屏幕都是航班取消的紅色信號。可是最後，名古屋-羽田（東京）的航班竟然顯示的是 \u0026lsquo;計劃\u0026rsquo;。拿到機票進入候機大樓以後才看見，這時天空的烏雲已經開始逐漸散去。原來，我的航班真的可以按時離開了。\n抵達 同行的本田是一個日本人小兒科醫生。她跟我很早就通過 Facebook 聯絡，並且發現我們恰好訂了同一天的航班，宿舍距離也不遠。於是順理成章地，我們到了希思羅機場以後準備一起乘 Uber 去我們各自的宿舍。希思羅機場乘 Uber 時要先去出發的航站樓的停車場，才能順利上車。推著行李過去停車場的一路上我們是真切的感受到了9月倫敦的氣溫是多麼的冷。我趕快拿出放在包裏的羽絨服披上才算沒有被凍到。\n不過奇怪的是我的 uber 賬號本來在美國西雅圖，新加坡等地方都用的好好的，在倫敦卻一直提示我支付用的信用卡信息有誤。就算我立刻更新了信用卡信息，或者是從 mastercard 換成 visa 均不能成功。真是尷尬死了。正在此時，旁邊另一個留學生模樣的女生湊過來說，“我可以借一下你的 wifi 熱點嗎？”。原來此人來自香港，在 LSE （倫敦政經學院）做交換留學生一年。真是巧了。宿舍也離我們的不太遠。所以果斷把我租的小米全球上網分享給她：\n三個剛剛踏上留學生活的陌生人，就這樣乘了同一輛車進入這個陌生的大城市。倫敦，I am coming。\n順利抵達我的宿舍 International Hall，領了房間卡之後，住進了我此生租過的最貴的每週200鎊的單身無廁所無浴室學習房間 (Study Room) :\n大學附贈了一套被褥床單和浴巾。房間的暖氣片暫時還不能使用。夜裡的溫度已經降到10度以下了呢！！！衣櫃裡的鏡子已經碎了，不過我很喜歡這個特別長的書桌，上面的書櫃，還有牆壁上的這塊掛墊：(我的兩個小寶貝的照片被我第一時間貼了上去。)\n宿舍附近就有 Tesco，是個24小時開門的小超市。類似在日本的7/11。也是應有盡有，甚至還能買到新鮮果蔬：\n我的宿舍之所以這麼貴，是有原因的！因為住在這裡，旁邊就是大英博物館：\n進去走馬觀花看了一個半小時，又在附近逛了逛，和一個正在 LSHTM 讀博士學位的日本人見面送了東西，然後又和闊別多年的高中同學吃了午餐。\n壽司果然在這裡價格不菲： 當然更不能錯過美好的天氣，還有我將要奉獻一年時間的夢想中的大學： 倫敦衛生學與熱帶醫學學院 (London School of Hygiene and Tropical Medicine)\n初來乍到的幾天，時差還根本轉不過來，晚上8點多就困的不省人事，現在半夜三點又精神抖擻。希望能快點適應這裡的生活。\n","date":1505865600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1505865600,"objectID":"2d8f9cb5957cacc2edd5c917392beb35","permalink":"https://wangcc.me/post/london-life-started/","publishdate":"2017-09-20T00:00:00Z","relpermalink":"/post/london-life-started/","section":"post","summary":"我的倫敦生活從此開始了。","tags":["experience","LSHTM","London"],"title":"London Baby !","type":"post"},{"authors":null,"categories":["study abroad"],"content":"我在近一個月前去了大阪的英國簽證中心申請了去英國的簽證。\n當時大英帝國的簽證申請就給我留下極差的印象。結果後來我的等待才是最漫長的。\n理一下時間線：\n7月31日 提交簽證材料 8月16日 收到郵件提示，護照抵達馬尼拉 8月23日 打電話給大英帝國高貴的移民局(UK Visas and Immigration) (下面詳述) 8月23日 收到郵件提示，護照抵達日本大阪簽證申請中心 8月24日 下午查詢郵局訂單號發現已經到了我所在的城市，晚上，配送完畢，簽證到手: 當時提交簽證材料時，告訴我的是，提交之日起15個工作日，所以我的預計是8月中旬能入手簽證。結果，實際上8月中旬才剛剛寄到馬尼拉。也許是因爲這個時期有許多學生簽證申請，比較擁擠。我又不樂意去花錢買他們所謂的加急服務。因爲付錢等於我承認了這樣的做法是合理的。反而會助長這種依靠金錢來獲取方便的惡習。（當然我相信他們也不缺我一個人） 申請的順序，爲什麼不是按照申請的時間順序來呢？有人晚來了，付了錢加急就可以排到我前面去，這哪裏公平了？這在日本根本無法想象。最可恨的是，23日那天我實在是等不及了，早晨給移民局打電話去詢問簽證狀態，電話接通以後是機器聲音，提示先輸入有效的信用卡或者借記卡號碼，方便他們收取諮詢費用(1.37 £/min)。\n這一趟簽證申請體驗下來，只能令人感嘆大英帝國真的是沒落了。做事情效率之低下，每一個環節透露出來的全部都是赤裸裸的金錢至上主義。\n回想起當日在大阪簽證中心提交材料時，電視屏幕上循環播放着大英帝國的宣傳片。介紹着英國的方方面面，從工業革命，到互聯網的發明，以及優越先進的醫療和社會制度，無處沒有英國在其中起到的關鍵或者領導式的作用。這不是讓人覺得極爲諷刺嗎？過去的強盛，給他們留下的只有傲慢嗎？\n可能上面的體驗只是我還沒出發之前，體會到的十分侷限的部份，但願接下來一年英國風調雨順。\n大概我除了讀書一無是處，所以畢業應該（希望）是沒有問題！\n還有一個月不到就要出發了，倫敦，我來也！\n","date":1503964800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1503964800,"objectID":"8681cd24ce484c652e913748c01b5f45","permalink":"https://wangcc.me/post/uk-visa-succeed/","publishdate":"2017-08-29T00:00:00Z","relpermalink":"/post/uk-visa-succeed/","section":"post","summary":"算一算英國簽證花了多久時間","tags":["London"],"title":"UK visa succeed","type":"post"},{"authors":null,"categories":["conference","diary"],"content":"ACC@Tokyo 8月18日-8月22日期間，我先是去了東京參加學習了 ACC (Asia Cohort Consortium)，內容主要是亞洲地區進行過的，以及正在進行的隊列研究的大集合。當然還有很多隊列由於各種原因不能加入這個大家庭。 這之後又離開東京去了琦玉的大宮，參加那裏舉辦的第21屆國際流行病學會。\n在東京的ACC其實挺豐富的，最近的課題彙報，還有就是來自亞洲各地的老師們暢所欲言，提供分析或者論文討論的建議。印象深刻的如：\n(1) 臺灣Biobank的近況介紹 (2) 來自范德堡大學 (Vanderbilt University) 的Wei Zheng教授的課題內容更新: 照片的老師是來自新加坡國立大學的Chia Kee Seng教授。\n大名鼎鼎的BMI和總死亡的關係的論文： (3) 當然不能錯過我們研究室林老師的發表啦： (4) 休息時間還不忘來一個日本拉麵的報告（笑）： 結束了ACC以後，晚上去往宴會的路上我們還能看見之前築地市場火災發生後的痕跡：\n發生火災的店鋪是個拉麵店，之前我和西山來癌症中心的時候有路過，當時還有不少人排隊吃麵，我們也曾猶豫過，不過後來還是吃了壽司。當然沒有人想到幾周之後就在同一個地方起火燒掉了許多老店鋪。\nIEA@Omiya Day 1 第二天離開了東京以後乘上電車去往埼玉縣，大宮市。\n原本我應該乘坐的電車時間和線路是這樣的:\n可是等到11點半我發現外面越來越荒涼，好像不太對勁，因爲我一邊在電車裏，一邊聽着有的沒的廣播和播客，所以很有可能聽漏了報站。打開地圖一看我已經超過好多站了，於是決定等到下一個停靠站就下來坐車再往回走。於是摘掉了耳機認真聽廣播報的站名，聽到報下一站站名的時候一時被愣住了，一時沒緩過神來：\n下一站是，桶川。\n這個站名太熟悉了，瞬間想起之前剛看過的一些列文章。講的就是發生在這裏的故事吧。\n到站下車以後隨手拍了一下站牌：\n跑步去了對面站臺，上了回頭車後，不到15分鍾回到了大宮站：\n午飯在車站裏的餐廳解決，美味的鹿肉漢堡：\n飽餐以後找到了我租的Airbnb（距離車站有點遠，推着行李箱步行過去差點被曬成狗），放了行李，我便來到會場，Sonic City。也許是來得太早了。完全沒有國際會議的感覺，會場十分空曠：\n等到下午正式開會，會長中村好一致辭時，也未見增加多少參會者：\n第一日成功和曾經在倫敦衛校留學的潘師兄見面。遙想三年前在阿拉斯加，真是時光如梭，當時我還不知自己今日會有機會去倫敦留學。如今潘師兄已經是孩子爸了，也如願以償在同濟醫學院做了老師。所以我來之前就決定，一定要抓住這次機會跟潘老師多了解了解在倫敦的生活。\n","date":1503532800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1503532800,"objectID":"91349966b3f2e9388317750017c6de3b","permalink":"https://wangcc.me/post/iea-in-saitama/","publishdate":"2017-08-24T00:00:00Z","relpermalink":"/post/iea-in-saitama/","section":"post","summary":"國際流行病學會，琦玉縣，大宮市，感想 \u0026 這幾天的體驗","tags":["IEA","Presentation","Saitama","conference"],"title":"ACC in Tokyo \u0026 IEA in Saitama","type":"post"},{"authors":null,"categories":["statistics"],"content":" 克萊姆法則 Cramer’s Formula 當 \\(X\\) 爲正則矩陣（\\(|X|\\neq0\\)）時 連立一次方程式：\\(X\\underline{a}=\\underline{y}\\) 的解可以寫作：\n\\[a_j=\\frac{|X_j|}{|X|} (j=1,2,\\cdots, n)\\]\n其中： \\(|X_j|\\) 爲矩陣 \\(X\\) 的第 \\(j\\) 列替換爲 \\(\\underline{y}\\) 以後的矩陣的行列式。\n練習 解下列連立一次方程式 \\[\\begin{align} \\left\\{ \\begin{array}{ll} a_1+2a_2+a_3 = 2\\\\ 2a_1+a_2+a_3 = 3\\\\ a_1+a_2+2a_3 = 3 \\end{array} \\right. \\end{align}\\]\n解 \\[X=\\left( \\begin{array}{c} 1 \u0026amp; 2 \u0026amp; 1 \\\\ 2 \u0026amp; 1 \u0026amp; 1 \\\\ 1 \u0026amp; 1 \u0026amp; 2 \\end{array} \\right), \\underline{a}=\\left( \\begin{array}{c} a_1 \\\\ a_2 \\\\ a_3 \\\\ \\end{array} \\right), \\underline{y}=\\left( \\begin{array}{c} 2 \\\\ 3 \\\\ 3 \\\\ \\end{array} \\right)\\]\n其中 \\(|X|=-4\\) (三次行列式的計算)\n\\(X\\) 的第一列置換成 \\(\\underline{y}\\) 則: \\[|X_1|=\\begin{vmatrix} 2 \u0026amp; 2 \u0026amp; 1\\\\ 3 \u0026amp; 1 \u0026amp; 1\\\\ 3 \u0026amp; 1 \u0026amp; 2\\\\ \\end{vmatrix}=-4\\]\n\\(X\\) 的第二列置換成 \\(\\underline{y}\\) 則:\n\\[|X_2|=\\begin{vmatrix} 1 \u0026amp; 2 \u0026amp; 1\\\\ 2 \u0026amp; 3 \u0026amp; 1\\\\ 1 \u0026amp; 3 \u0026amp; 2\\\\ \\end{vmatrix}=0\\]\n\\(X\\) 的第三列置換成 \\(\\underline{y}\\) 則:\n\\[|X_3|=\\begin{vmatrix} 1 \u0026amp; 2 \u0026amp; 2\\\\ 2 \u0026amp; 1 \u0026amp; 3\\\\ 1 \u0026amp; 1 \u0026amp; 3\\\\ \\end{vmatrix}=-4\\]\n\\[\\therefore a_1=\\frac{|X_1|}{|X|}=1, \\\\ a_2=\\frac{|X_2|}{|X|}=0, \\\\ a_3=\\frac{|X_3|}{|X|}=1\\]\n","date":1502236800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1502236800,"objectID":"0c828988b69fa6244b930bfcd0bbaab3","permalink":"https://wangcc.me/post/cramers-formula/","publishdate":"2017-08-09T00:00:00Z","relpermalink":"/post/cramers-formula/","section":"post","summary":"第5章　行列式 determinant P91-92 克萊姆法則法解連立方程式","tags":["basic mathematics","linear algebra","Medical Statistics"],"title":"「統計解析のための線形代数」復習筆記 25","type":"post"},{"authors":null,"categories":["statistics"],"content":" 逆矩陣法解連立一次方程式 \\(X\\) 為正則矩陣時(\\(|X|\\neq0\\))，給 \\(X\\underline{a}=\\underline{y}\\) 等式兩邊同時乘以 \\(X^{-1}\\)，可以得到 \\(X^{-1}X\\underline{a}=X^{-1}\\underline{y}\\rightarrow E\\underline{a}=X^{-1}\\underline{y}\\)。由此方法可以得到 \\(\\underline{a}=X^{-1}\\underline{y}\\)。\n練習 解下列連立一次方程式 \\[\\begin{align} \\left\\{ \\begin{array}{ll} a_1+2a_2+a_3 = 2\\\\ 2a_1+a_2+a_3 = 3\\\\ a_1+a_2+2a_3 = 3 \\end{array} \\right. \\end{align}\\]\n解 元連立方程式可以寫作\\(X\\underline{a}=\\underline{y}\\)，其中 \\[X=\\left( \\begin{array}{c} 1 \u0026amp; 2 \u0026amp; 1 \\\\ 2 \u0026amp; 1 \u0026amp; 1 \\\\ 1 \u0026amp; 1 \u0026amp; 2 \\end{array} \\right), \\underline{a}=\\left( \\begin{array}{c} a_1 \\\\ a_2 \\\\ a_3 \\\\ \\end{array} \\right), \\underline{y}=\\left( \\begin{array}{c} 2 \\\\ 3 \\\\ 3 \\\\ \\end{array} \\right)\\] 之前我們已經用行的基本變形法和逆矩陣法分別計算過了 \\(X^{-1}\\) ： \\[X^{-1}=\\left(\\begin{array}{c} -1/4 \u0026amp; 3/4 \u0026amp; -1/4\\\\ 3/4 \u0026amp; -1/4 \u0026amp; -1/4\\\\ -1/4 \u0026amp; -1/4 \u0026amp; -3/4\\\\ \\end{array}\\right)\\]\n\\(\\therefore\\)\n\\[\\begin{align} \\underline{a} \u0026amp; =X^{-1}\\underline{y} \\\\ \u0026amp; =\\left(\\begin{array}{c} -1/4 \u0026amp; 3/4 \u0026amp; -1/4\\\\ 3/4 \u0026amp; -1/4 \u0026amp; -1/4\\\\ -1/4 \u0026amp; -1/4 \u0026amp; 3/4\\\\ \\end{array}\\right)\\left( \\begin{array}{c} 2 \\\\ 3 \\\\ 3 \\\\ \\end{array} \\right)\\\\ \u0026amp;=\\left( \\begin{array}{c} -1/4\\times2+3/4\\times3-1/4\\times3 \\\\ 3/4\\times1+(-1/4)\\times3-1/4\\times3 \\\\ -1/4\\times2-1/4\\times3+3/4\\times3 \\\\ \\end{array} \\right) \\\\ \u0026amp; = \\left( \\begin{array}{c} 1 \\\\ 0 \\\\ 1 \\\\ \\end{array} \\right) \\end{align} \\]\n","date":1501977600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501977600,"objectID":"bc3e5f8c7ab7302f9a2d0b857ac3c754","permalink":"https://wangcc.me/post/inverse-matrix-method/","publishdate":"2017-08-06T00:00:00Z","relpermalink":"/post/inverse-matrix-method/","section":"post","summary":"第5章　行列式 determinant P90-91 逆矩陣法解連立一次方程式","tags":["basic mathematics","linear algebra","Medical Statistics"],"title":"「統計解析のための線形代数」復習筆記 24","type":"post"},{"authors":null,"categories":["statistics"],"content":"正方形矩陣 $A$ 的行列式滿足 $|A| \\neq 0$ 時，逆矩陣可以表達爲(當 $|A|=0$ 時，正方形矩陣 $A$ 沒有逆矩陣)： $$A^{-1}=\\frac{1}{|A|}adj(A)=\\frac{1}{|A|}(A_{ij})^t$$\n$$=\\frac{1}{|A|}\\lbrace(-1)^{i+j}D_{ij}\\rbrace^t$$\n其中:\n$adj(A)$ 爲餘因子矩陣 $A_{ij}$ 爲餘因子 $D_{ij}$ 爲小行列式 (1) 之前舉過的例子再拿來試試看：\n$$X=\\left( \\begin{array}{c} 1 \u0026amp; 2 \u0026amp; 1 \\newline 2 \u0026amp; 1 \u0026amp; 1 \\newline 1 \u0026amp; 1 \u0026amp; 2 \\end{array} \\right)=\\left(\\begin{array}{c} x_{11} \u0026amp; x_{12} \u0026amp; x_{13} \\newline x_{21} \u0026amp; x_{22} \u0026amp; x_{23} \\newline x_{31} \u0026amp; x_{32} \u0026amp; x_{33} \\end{array}\\right)$$ 元素 $x_{ij}$ 的餘因子 $X_{ij}(i,j=1,2,3)$ 爲：\n$$X_{11}=(-1)^{1+1}\\left| \\begin{array}{c} 1 \u0026amp; 1 \\newline 1 \u0026amp; 2 \\end{array}\\right|=1$$\n$$X_{12}=(-1)^{1+2}\\left| \\begin{array}{c} 2 \u0026amp; 1 \\newline 1 \u0026amp; 2 \\end{array}\\right|=-3$$\n$$X_{13}=(-1)^{1+3}\\left| \\begin{array}{c} 2 \u0026amp; 1 \\newline 1 \u0026amp; 1 \\end{array}\\right|=1$$\n$$X_{21}=(-1)^{2+1}\\left| \\begin{array}{c} 2 \u0026amp; 1 \\newline 1 \u0026amp; 2 \\end{array}\\right|=-3$$\n$$X_{22}=(-1)^{2+2}\\left| \\begin{array}{c} 1 \u0026amp; 1 \\newline 1 \u0026amp; 2 \\end{array}\\right|=1$$\n$$X_{23}=(-1)^{2+3}\\left| \\begin{array}{c} 1 \u0026amp; 2 \\newline 1 \u0026amp; 1 \\end{array}\\right|=1$$\n$$X_{31}=(-1)^{3+1}\\left| \\begin{array}{c} 2 \u0026amp; 1 \\newline 1 \u0026amp; 1 \\end{array}\\right|=1$$\n$$X_{32}=(-1)^{3+2}\\left| \\begin{array}{c} 1 \u0026amp; 1 \\newline 2 \u0026amp; 1 \\end{array}\\right|=1$$\n$$X_{33}=(-1)^{3+3}\\left| \\begin{array}{c} 1 \u0026amp; 2 \\newline 2 \u0026amp; 1 \\end{array}\\right|=-3$$\n因此餘因子矩陣爲：$adj(X)=\\left( \\begin{array}{c} 1 \u0026amp; -3 \u0026amp; 1 \\newline -3 \u0026amp; 1 \u0026amp; 1 \\newline 1 \u0026amp; 1 \u0026amp; -3 \\end{array} \\right)^t=\\left( \\begin{array}{c} 1 \u0026amp; -3 \u0026amp; 1 \\newline -3 \u0026amp; 1 \u0026amp; 1 \\newline 1 \u0026amp; 1 \u0026amp; -3 \\end{array} \\right)$\n我們看見這個餘因子矩陣是一個對稱矩陣，這是由於原矩陣 $X$ 本身就是一個對稱矩陣。另外，行列式爲：\n$$\\begin{align}|X|\u0026amp;=1\\times X_{11}+2\\times X_{12}+1\\times X_{13}\\newline\u0026amp;=1\\times1+2\\times(-3)+1\\times1\\newline\u0026amp;=-4\\end{align}$$\n因此所求的逆矩陣爲：\n$$\\begin{align}X^{-1}\u0026amp;=\\frac{1}{|X|}adj(X)\\newline \u0026amp;=\\frac{1}{-4}\\left( \\begin{array}{c} 1 \u0026amp; -3 \u0026amp; 1 \\newline -3 \u0026amp; 1 \u0026amp; 1 \\newline 1 \u0026amp; 1 \u0026amp; -3 \\end{array} \\right)\\newline \u0026amp;=\\left( \\begin{array}{c} -\\frac{1}{4} \u0026amp; \\frac{3}{4} \u0026amp; -\\frac{1}{4} \\newline \\frac{3}{4} \u0026amp; -\\frac{1}{4} \u0026amp; -\\frac{1}{4} \\newline -\\frac{1}{4} \u0026amp; -\\frac{1}{4} \u0026amp; \\frac{3}{4} \\end{array} \\right)\\end{align}$$\n(2) 試求矩陣 $A=\\left( \\begin{array}{c} 1 \u0026amp; 2 \u0026amp; 1 \\newline 2 \u0026amp; 3 \u0026amp; 1 \\newline 1 \u0026amp; 2 \u0026amp; 2 \\end{array} \\right)=\\left( \\begin{array}{c} a_{11} \u0026amp; a_{12} \u0026amp; a_{13} \\newline a_{21} \u0026amp; a_{22} \u0026amp; a_{23} \\newline a_{31} \u0026amp; a_{32} \u0026amp; a_{33} \\end{array} \\right)$ 的逆矩陣 $A^{-1}$:\n$$\\begin{array} =A_{11}=6-2=4, \u0026amp; A_{12}=-(4-1)=-3, \u0026amp; A_{13}=4-3=1 \\newline A_{21}=-(4-2)=-2, \u0026amp; A_{22}=2-1=1, \u0026amp; A_{23}=-(2-2)=0 \\newline A_{31}=2-3=-1, \u0026amp; A_{32}=-(1-2)=1, \u0026amp; A_{33}=3-4=-1 \\end{array}$$\n$$adj(A)=\\left( \\begin{array}{c} 4 \u0026amp; -3 \u0026amp; 1 \\newline -2 \u0026amp; 1 \u0026amp; 0 \\newline -1 \u0026amp; 1 \u0026amp; -1 \\end{array} \\right)^t=\\left( \\begin{array}{c} 4 \u0026amp; -2 \u0026amp; -1 \\newline -3 \u0026amp; 1 \u0026amp; 1 \\newline 1 \u0026amp; 0 \u0026amp; -1 \\end{array} \\right)$$\n$$\\begin{align} |A| \u0026amp;=1\\times A_{11}+2\\times A_{12}+1\\times A_{13} \\newline \u0026amp;=1\\times4+2\\times(-3)+1\\times1 \\newline \u0026amp;=4-6+1 \\newline \u0026amp;=-1 \\end{align}$$\n$$ \\therefore \\begin{align} A^{-1} \u0026amp;= \\frac{1}{(-1)}\\left( \\begin{array}{c} 4 \u0026amp; -2 \u0026amp; -1 \\newline -3 \u0026amp; 1 \u0026amp; 1 \\newline 1 \u0026amp; 0 \u0026amp; -1 \\end{array} \\right) \\newline \u0026amp;=\\left( \\begin{array}{c} -4 \u0026amp; 2 \u0026amp; 1 \\newline 3 \u0026amp; -1 \u0026amp; -1 \\newline -1 \u0026amp; 0 \u0026amp; 1 \\end{array} \\right) \\end{align}$$\n","date":1501718400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501718400,"objectID":"04115705c4594131e3b9a06cc9634d17","permalink":"https://wangcc.me/post/inverse-matrix/","publishdate":"2017-08-03T00:00:00Z","relpermalink":"/post/inverse-matrix/","section":"post","summary":"第5章　行列式 determinant P87-89 逆矩陣的公式","tags":["basic mathematics","learning notes","Medical Statistics"],"title":"「統計解析のための線形代数」復習筆記22","type":"post"},{"authors":null,"categories":["study abroad"],"content":"來說說7月31日去大阪交簽證材料的事情。\n那叫一個鬱悶。 總之，我在日本還是第一次遇見如此糟糕的接待。另外，大阪的簽證辦理中心，非常不好找，建議第一次去的人挑一個不那麼熱的日子去。這樣也不會像我一樣在馬路上被曬成狗。\n當然我才不會告訴你我早上起晚了差點趕不上我訂的火車票（9：00）。而且近鐵列車的城市快線 (urban liner) 開得不是很平穩，我半路感覺暈車不適還跑去洗手間把早飯吐了以後才能舒服一點得繼續坐到大阪難波車站。從難波車站再換乘御堂筋線到心斎橋，之後就出車站步行差不多10個被暴曬的街區到VFS簽證代理處。不過並非領事館或者大使館，所以我想應該不會像美國領事館那樣戒備森嚴。結果上到10樓VFS辦公室的地方，有提示語告訴我先按門鈴讓保安檢查攜帶行李。按了門鈴，出來一個身材矮小的保安問我預約的時間和簽證申請種類。我把我的預約郵件在手機上打開給他看確認以後才肯帶我進門。\n進去之後就能看到一個還沒有我辦公室大的房間被隔板隔開成幾個部分。保安同學說不能用手機拍照，攜帶的筆記本拿出來給他確認關機。然後材料拿出來先交給保安。他又煞有介事地讓我把揹包每一層都打開給他看確認我沒帶炸彈。這些都搞定了以後又像機場安檢一樣全身掃描一遍確認我身上沒有綁着炸彈。。。\n此時已經比我預約的時間晚了10分鐘。當然我不怪他。他只是認真完成任務。之後讓我在等待區域等待。不久之後來了一個接待員用機器人一樣的語調和口吻告訴我說：“把所有的材料按照不同類別分好類，一定要使用牆壁上掛着的那些分類用的不同顏色的紙張。” 我擡頭一看牆壁上掛着的文件架子上有紅色藍色黃色等不同顏色的A4紙。她又接着機械地說：“如果你發現自己不知道怎麼歸類整理這些文檔的話，我也可以幫你，但是要收取1920日元的服務費。” 我艹，連看文件整理都要收費，我心想。接待員小姐估計聽見我的心裏話了，漫不經心地又說，“你當然可以自己整理，不過責任自負喲。”\n你這是在威脅我嗎？我心裏又想。後來才在他們的網站上看到這樣的收費提示：\nApplication and Document Checks\nThe Application and Document Checks service is for applicants who have already applied and have printed out their application form to check that an application is complete before sending it to the UK Visas and Immigration in Manila.\nThe service will be charged an additional fee of JPY 1,920 per applicant. The fee can only be paid at Visa Application Centre in person by cash.\nVFS staff will check mandatory required documents have been submitted, but not the actual content of the documents.\n我想省這點錢來着，看着他們的分類文件，毫無提示，簡直就像在嘲諷我“你真的是個PhD嗎？”。隨便分類了以後我想就試試看算了去交材料時，她才說，你這個沒有清單表格不行，你這裏有一張是絕對不能使用的。我心裏十萬頭草泥馬奔騰而過，這些事你們爲啥不能寫在通知預約的郵件裏面呢？爲啥不能算在網上交的簽證費用裏面呢？\n然後那小妞又告訴我說，“反正你要用郵寄服務收護照的話，你可以付錢我幫你整理阿，這兩個服務可以合算起來給你打九折！”。她是不是還期待我感激一下有折扣這件事呢？ “W！T！F！”三個字明確的寫在我的臉上，我想在日本這麼多年了，我TM還是第一次感覺回到了中國大陸。\n強忍不爽的我無奈極了，說，那好吧你幫我整理材料吧。\n總之，什麼都是我自己負責，什麼都要從我身上掏錢，出錯了什麼都該自認倒楣的這種噁心頭頂的感覺從一開始進門一直到錄完指紋，拍好照片準備要離開的時刻。下午1點多，十幾張材料才遞交完。\n不知道是不是因爲接觸太多日本的無微不至的服務突然覺得有心理落差。但願以後去了英國不是這樣的服務。\n兩週以後能否安全收到簽證呢？ 拭目以待。\n","date":1501632000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501632000,"objectID":"74b35f669296e6cb4bd60524d32c9480","permalink":"https://wangcc.me/post/visa-application/","publishdate":"2017-08-02T00:00:00Z","relpermalink":"/post/visa-application/","section":"post","summary":"大阪申請 Tier 4 (general) student 簽證","tags":["LSHTM","London"],"title":"大英帝國簽證申請","type":"post"},{"authors":null,"categories":["study abroad"],"content":"言而總之，總而言之，我的4月5月6月7月在無盡的等待中度過。期間投稿了一篇論文。和西山一起進行了磕磕絆絆的GWAS數據分析。\n本來以爲我的 offer 條件僅僅衹是把我原先名古屋大學的博士學位證書，中英文的原本郵寄給 LSHTM 負責確認就可以了。\n結果6月8日那天收到郵件催促我快點滿足 offer 條件：\n可以看到資金證明是我必須提供的條件。所以，我立刻開始著手資金的準備，存款全部移到一個賬戶中去，然後開了一個存款證明。結果就是這個新開的存款證明，後來拖了我一個多月的腿。差點害我以爲可能這次留學計劃就要泡湯了。我原本告訴 LSHTM 的簽證詢問小組（visa-enquiries）說，我的生活費由我的大學支付的工資來做擔保，然後大學還有資助我的一部分旅費和住宿費。因此我還要求我工作的大學給我速速給我開具了上述證明。結果後來被證明這些都不如一張自己賬戶上有錢的證明來得簡單。\n因爲英國留學簽證(Tier 4 student)對 sponsor (資金贊助者)極爲嚴格：\nFor visa purposes, an Official Financial Sponsor is only one of the following: Her Majesty’s Government, your home government, the British Council or any international organisation, international company, university or an Independent School\n我原以爲我開的三個證明完全足夠了吧。結果過了一個月告訴我說：\nYour documents didn’t meet the requirements because: The salary expectancy is not admissible The statement you have provided only shows the balance on a single day and we therefore recommend a bank letter to show funds held for 28 days. Please find attached an example bank letter. The bank statements did not include the bank name and logo. 不知道爲什麽，未來的工資單證明不被接受，然後資金證明必須證明說我擁有足夠的資金并且保持了4周時間。而且還要求資金證明上面有銀行的logo。WTF!\n這些都好說。可是日本的銀行，沒有這種類型的證明書（我也是第一次知道日本銀行不給開這樣子的證明）。所以許多人的解決辦法是讓銀行開一個月的流水賬單，要命的是這個證明不能開英文的，然後再去找翻譯公司翻譯流水。當然我也可以這麽辦。衹是，當我知道我的三個證明書都不能作爲有效的資金證明的時候，我離7月31日祗剩下不到2周時間了。在此奉勸后來者，一定要先準備好自己的資金證明書。最好能按照下面的樣本，讓銀行開具類似的證明書：\n我在接到證明書不滿足條件的郵件的第二天，立刻去了銀行，接待我的銀行經理先是打報告給總部請示。毫無意外被擋回來。說如果是客人自己要求的樣式的證明書，無法給加銀行logo，也不能蓋章，衹能簽字。在我一個多小時的軟磨硬泡以後，經理鬆動了。竟然主動想辦法，她提議說，可以辦理bank statement，不過我看了他們給的bank statement樣本也是一個時間點的賬戶存款而已，無法滿足28天的資金維持證明。看我面有難色，日本人經理還是挺善解人意的，說，我可以把日期改成，從xx月xx日-xx月xx日（28天）的最低資金證明。這樣就能解決問題了。而且bank statement本身自帶銀行logo。謝天謝地，一項死板不能變通的日本人讓我從此刮目相看。解決了我的燃眉之急。也不必再去找翻譯公司翻譯賬戶流水了。有驚無險。第二天我拿到開好的證明，立刻掃描PDF郵件發給LSHTM，期待他們能馬上給開來 CAS (Confirmation of Acceptance of Studies)。等了一周，還是左等不來右等不來，距離7月31日還剩下不到10天了。終於無法忍耐等待的我，打電話去倫敦詢問我的情況。對方接電話的是個年輕女聲，優雅的倫敦音告訴我，不要着急，先無視學校的提醒滿足條件的郵件吧。我們會儘快看你的檔案。無奈我衹好作罷，挂了電話繼續等待。\n結果第二天晚上就收到了確認函，說你的CAS很快就能發給你了。oh yeah！半夜裏我就收到了發來的新鮮剛出爐的CAS號碼以及新的無條件錄取證明：\n我很早以前就在visa4uk上註冊好了全部的信息，就等着學校發來 CAS 的文件了。於是我再花了半個小時把 CAS 上的內容填寫到簽證申請的網站上去。在申請的網站上，會中途跳出來讓你支付一年醫療保險的頁面（£150），付完保險費以後會收到自己的保險號碼。估計以後在英國如果需要看病的話報自己的保險號碼就OK了。於是乎我以迅雷不及掩耳之勢立刻預約了7月31日去大阪的簽證申請中心遞交簽證材料。\n等待去辦簽證的過程中，又收到好消息，宿舍抽籤中了。於是我就成了倫敦準市民之一拉。哈哈哈哈。今兒真高興阿，今兒真高興。\n我抽中的是International Hall的單人間。仔細閱讀了條款後發現，每週2百鎊的房租確實有點小貴，但是呢，確是包了早餐晚餐和週末的四餐的。我想這將會大大減輕時間和金錢的成本。畢竟只有一年的留學時間。將就將就吧，每天都是炸魚和薯條估計吃一週就會讓人瘋了誒。。。先做好心理準備。對伙食不應有太高期待。\n萬事具備，只差簽證了。 於是就到了預約機票的時候，查了半天各種中介的網站，結果都是什麼中轉三四次的，要不就是繞地球一大圈的，才能有價格比較便宜的。索性打電話去日本航空詢問有沒有給留學生準備的往返一年，時間靈活的機票。果然不問不知道，一問嚇一跳阿，電話接線員小哥樂呵呵:-)說，哎呀你這電話打的太是時候了，我們日本航空正好最近上線了歐洲航線的特價機票，而且專門針對你這樣要待三個月以上的客戶。一問價格，我的媽呀，出發行程已定，歸程未定的叫做半靈活機票 (semi-flexi)，日本航空的這個折扣價爲12萬日元。比全日空便宜了一半，比其他的可疑航空減少了飛行時間，還有什麼好說的，果斷就訂了。結果呢，準備付錢了小哥告訴我說，您現在先別付定金，我這裏已經幫你把機票預留好了，您等8月1日以後再上網站上打開訂單支付，因爲8月1日後的燃油稅機場時用費等雜費由於匯率等變化會再便宜一萬日元左右。W!T!F! 感動得熱淚盈眶有沒有，簡直就想穿過電話線去擁抱這位小哥了。\n這一週簡直了，從前幾個月的無盡等待到讓人懷疑人生，懷疑自己還能不能去英國，瞬間轉到材料全備齊，訂了飛機票，而且還額外中了一個獎學金（日本的財團）。快要樂不攏嘴了。。。\n","date":1501286400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501286400,"objectID":"08add528fa877048163351966129ec7e","permalink":"https://wangcc.me/post/unconditional/","publishdate":"2017-07-29T00:00:00Z","relpermalink":"/post/unconditional/","section":"post","summary":"驚心動魄，坐過山車一般的一周。","tags":["LSHTM","London","Medical Statistics"],"title":"無條件 offer, CAS, 和宿舍抽籤結果","type":"post"},{"authors":null,"categories":["statistics"],"content":" 行的基本變形 Theorem 1 (行的基本變形) 對矩陣進行下列操作的過程，被稱爲是行的基本變形（行的基本操作, elementary row operations）。\n給任意一行乘以/除以一個非零的數。 給任意一行加上/減去另外任意行的倍數。 將任意兩行的對應元素互換。 練習基本變形： 用行的基本變形求矩陣 \\(X=\\left(\\begin{array}{c} 1\u0026amp; 2\u0026amp; 1\\\\ 2\u0026amp; 1\u0026amp; 1\\\\ 1\u0026amp; 1\u0026amp; 2\\\\ \\end{array}\\right)\\) 的逆矩陣 \\(X^{-1}\\) 首先，將矩陣 \\(X\\) 和同次單位矩陣 \\(E_3\\) 的元素寫成如下的左右並列的形式（用點隔開）\\((X, E)\\)。數字 (1) (2) (3) 表示行數：\n\\[\\left(\\begin{array}{c} 1\u0026amp; 2\u0026amp; 1 \u0026amp; \\vdots \u0026amp; 1 \u0026amp; 0 \u0026amp; 0\\\\ 2\u0026amp; 1\u0026amp; 1 \u0026amp; \\vdots \u0026amp; 0 \u0026amp; 1 \u0026amp; 0\\\\ 1\u0026amp; 1\u0026amp; 2 \u0026amp; \\vdots \u0026amp; 0 \u0026amp; 0 \u0026amp; 1\\\\ \\end{array}\\right) \\begin{align} \\left\\{ \\begin{array}{rr} (1)\\\\ (2)\\\\ (3) \\end{array} \\right. \\end{align}\\]\n可以變形成爲下面的形式：\n\\[\\left(\\begin{array}{c} 1\u0026amp; 2\u0026amp; 1 \u0026amp; \\vdots \u0026amp; 1 \u0026amp; 0 \u0026amp; 0\\\\ 0\u0026amp; -3\u0026amp; -1 \u0026amp; \\vdots \u0026amp; -2 \u0026amp; 1 \u0026amp; 0\\\\ 0\u0026amp; -1\u0026amp; 1 \u0026amp; \\vdots \u0026amp; -1 \u0026amp; 0 \u0026amp; 1\\\\ \\end{array}\\right) \\begin{align} \\left\\{ \\begin{array}{l} (1)\\\\ (2)=(2)-2\\times(1)\\\\ (3)=(3)-(1) \\end{array} \\right. \\end{align}\\]\n繼續變形成如下的形式：\n\\[\\left(\\begin{array}{c} 1\u0026amp; 0\u0026amp; 3 \u0026amp; \\vdots \u0026amp; -1 \u0026amp; 0 \u0026amp; 2\\\\ 0\u0026amp; -4\u0026amp; 0 \u0026amp; \\vdots \u0026amp; -3 \u0026amp; 1 \u0026amp; 1\\\\ 0\u0026amp; 1\u0026amp; -1 \u0026amp; \\vdots \u0026amp; 1 \u0026amp; 0 \u0026amp; -1\\\\ \\end{array}\\right) \\begin{align} \\left\\{ \\begin{array}{l} (1)=(1)+2\\times(3)\\\\ (2)=(2)+(3)\\\\ (3)=-1\\times(3) \\end{array} \\right. \\end{align}\\]\nNext:\n\\[\\left(\\begin{array}{c} 1\u0026amp; 0\u0026amp; 3 \u0026amp; \\vdots \u0026amp; -1 \u0026amp; 0 \u0026amp; 2\\\\ 0\u0026amp; 1\u0026amp; 0 \u0026amp; \\vdots \u0026amp; 3/4 \u0026amp; -1/4 \u0026amp; -1/4\\\\ 0\u0026amp; 1\u0026amp; -1 \u0026amp; \\vdots \u0026amp; 1 \u0026amp; 0 \u0026amp; -1\\\\ \\end{array}\\right) \\begin{align} \\left\\{ \\begin{array}{l} (1)=(1)\\\\ (2)=(2)\\div(-4)\\\\ (3)=(3) \\end{array} \\right. \\end{align}\\]\nNext:\n\\[\\left(\\begin{array}{c} 1\u0026amp; 0\u0026amp; 3 \u0026amp; \\vdots \u0026amp; -1 \u0026amp; 0 \u0026amp; 2\\\\ 0\u0026amp; 1\u0026amp; 0 \u0026amp; \\vdots \u0026amp; 3/4 \u0026amp; -1/4 \u0026amp; -1/4\\\\ 0\u0026amp; 0\u0026amp; -1 \u0026amp; \\vdots \u0026amp; 1/4 \u0026amp; 1/4 \u0026amp; -3/4\\\\ \\end{array}\\right) \\begin{align} \\left\\{ \\begin{array}{l} (1)=(1)\\\\ (2)=(2)\\\\ (3)=(3)-(2) \\end{array} \\right. \\end{align}\\]\nNext:\n\\[\\left(\\begin{array}{c} 1\u0026amp; 0\u0026amp; 0 \u0026amp; \\vdots \u0026amp; -1/4 \u0026amp; 3/4 \u0026amp; -1/4\\\\ 0\u0026amp; 1\u0026amp; 0 \u0026amp; \\vdots \u0026amp; 3/4 \u0026amp; -1/4 \u0026amp; -1/4\\\\ 0\u0026amp; 0\u0026amp; 1 \u0026amp; \\vdots \u0026amp; -1/4 \u0026amp; -1/4 \u0026amp; -3/4\\\\ \\end{array}\\right) \\begin{align} \\left\\{ \\begin{array}{l} (1)=(1)+3\\times(3)\\\\ (2)=(2)\\\\ (3)=-1\\times(3) \\end{array} \\right. \\end{align}\\]\n點 “\\(\\vdots\\)” 的左側變形成爲單位矩陣時，行變形結束。右側便是所求的逆矩陣 \\(X^{-1}\\)。\n\\[X^{-1}=\\left(\\begin{array}{c} -1/4 \u0026amp; 3/4 \u0026amp; -1/4\\\\ 3/4 \u0026amp; -1/4 \u0026amp; -1/4\\\\ -1/4 \u0026amp; -1/4 \u0026amp; 3/4\\\\ \\end{array}\\right)\\]\nQ: 如果有行的基本變形，請問有沒有列的基本變形 (elementary column operations)？ A: 有。把行的基本變形中的定義 (1) 的行改成列，既是列的基本變形的定義。 ","date":1499385600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1499385600,"objectID":"568eb41263183744608d7731f8e59598","permalink":"https://wangcc.me/post/2017-07-07/","publishdate":"2017-07-07T00:00:00Z","relpermalink":"/post/2017-07-07/","section":"post","summary":"第5章　行列式 determinant P85-87 行的基本變形","tags":["learning notes","basic mathematics","linear algebra"],"title":"「統計解析のための線形代数」復習筆記21","type":"post"},{"authors":null,"categories":["statistics"],"content":" 逆矩陣 逆矩陣定義 Theorem 1 如果對於正方形矩陣 \\(A\\)，存在一個正方形矩陣 \\(X\\) 滿足 \\(AX=XA=E\\) (\\(E\\) 爲單位矩陣) 時，這個正方形矩陣 \\(X\\) 被叫做 \\(A\\) 的逆矩陣，寫作 \\(A^{-1}\\)。\n存在逆矩陣 \\((A^{-1})\\) 的 \\(A\\) ，被叫做正則矩陣 (regular matrix, nonsingular matrix)。\n不存在逆矩陣的 \\(A\\)，被叫做奇異矩陣 (singular matrix)。\n滿足 \\(|A|\\neq 0\\) 的矩陣 \\(A\\) 被叫做正則矩陣。滿足 \\(|A|=0\\) 的矩陣 \\(A\\) 被叫做奇異矩陣。\n\\(A\\) 爲正則矩陣時，滿足：\\(A^{-1}A=AA^{-1}=E\\) 。\n顯然，單位矩陣的逆矩陣也是一個單位矩陣: \\[E^{-1}E=EE^{-1}=E, E^{-1}=E\\] 逆矩陣的性質 對於正則矩陣 \\(A, B\\) 有以下性質：\n\\((AB)^{-1}=B^{-1}A^{-1}\\)\n注意此處矩陣 \\(A，B\\) 的順序對調了。 \\((A^{-1})^{-1}=A\\) \\((A^{t})^{-1}=(A^{-1})^t\\) \\((\\lambda A)^{-1}=\\frac{1}{\\lambda}A^{-1} (\\lambda \\ne 0)\\) 對角矩陣 \\(D_n=diag(a_{11},a_{22},\\dotsm,a_{nn})\\) 的逆矩陣寫作： \\(D_n^{-1}=diag(1/a_{11}, 1/a_{22},\\dotsm,1/a_{nn})\\)；\n注意此處的條件爲所有對角成分均非零: \\(a_{11}a_{22}\\dotsm a_{nn}\\neq 0\\) 證明 \\((AB)(AB)^{-1}=E\\) 等式兩邊從左往右乘以 \\(A^{-1}\\)\n\\((A^{-1}A)B(AB)^{-1}=A^{-1}E\\\\ B(AB)^{-1}=A^{-1}\\)\n等式兩邊從左往右乘以 \\(B^{-1}\\)\n\\((B^{-1}B)(AB)^{-1}=B^{-1}A^{-1}\\\\ E(AB)^{-1}=B^{-1}A^{-1}\\)\n根據單位矩陣的性質：\n\\(\\therefore (AB)^{-1}=B^{-1}A^{-1}\\)\n\\(E=E^{-1}=(A^{-1}A)^{-1}=A^{-1}(A^{-1})^{-1}\\)\n等式兩邊從左往右乘以 \\(A\\)\n\\(AE=AA^{-1}(A^{-1})^{-1}\\\\ \\therefore A=(A^{-1})^{-1}\\)\n\\(E=E^t=(A^{-1}A)^t=A^t(A^{-1})^t\\)\n等式兩邊從左往右乘以 \\((A^t)^{-1}\\)\n\\((A^t)^{-1}E=(A^t)^{-1}A^t(A^{-1})^t\\\\ \\therefore (A^t)^{-1}=(A^{-1})^t\\)\n","date":1499299200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1499299200,"objectID":"9c9dc3fb37b6eb954333ab236fbe81a7","permalink":"https://wangcc.me/post/2017-07-06/","publishdate":"2017-07-06T00:00:00Z","relpermalink":"/post/2017-07-06/","section":"post","summary":"第5章　行列式 determinant P84-85 逆矩陣","tags":["learning notes","basic mathematics","linear algebra"],"title":"「統計解析のための線形代数」復習筆記20","type":"post"},{"authors":null,"categories":["dictation"],"content":" Your browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2017-6-12 16:6\t用时：23:04 正确率：93%\t错词：19个 提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nYou may have noticed your summertime electricity bills, when you are you're cranking the A-C. A–C, they are more pricey than your wintertime payments. That's because air-conditioning is an electricity hog. And when a whole city or your region turns down the thermostat, utilities have to meet that increased demand somehow. This is often when we turn on the oldest power plants or the dirtier power plants. Tracey Holloway, an atmosphere atmospheric scientist at the University of Wisconsin in Madison. Some of these old older power plants that may only run on fuel oil or may run on coal , only come on on the hottest days. Using data from the EPA, Holloway and her team studied how air pollutants respond when the temperature goes up. They found that across the eastern U. S. , for every degree of Celsius temperature rise, power plants spell belched out 13000 140,000 metric tons of additional carbon dioxide. And admissions emissions of the pollutant pollutants sulfur dioxides dioxide and nitrogen oxides rose 3 three and a half percent per extra degree of heat, averaged across the region. That's especially bad, because hot summer days are the worse worst days to pump out more pollution. The These hot days, when we turn on the air conditioning across the U. S. or across the states state also happen to be the most chemically reactive days. So every unit of air pollution is that's going into the air is , you know , that much more likely to form ozone. The And ozone itself is a potent air pollutant. The study is in the journal Environmental Science and Technology. Holloway says the answer to the this summertime pollution peak maybe may be an energy source that thrives on hot, sunny days. If we could be getting solar electricity during this peak time , it may all set offset this hot weather midday peak and be a great solution for avoiding having to turn on those peaking power plants. In other words: why not use the sun, to keep cool? . Words worth to be remembered: cranking: 摇动, 起动, 开动 atmospheric: adj. 大气的，大气层的；有情调的，有魅力的 thermostat: n. 恒温（调节）器 hog: n. 猪；贪婪者，象猪般的人 v. （使）拱起 belch: n. 打嗝；喷吐；喷出物 v. 打嗝；喷出 offset: n. 抵销，支派，旁支，平版印刷 v. 弥补，抵销 譯文 或许你已经注意到夏季家里的电费情况，当你开启空调后，电费要比冬季更多些了。这是因为，空调属于耗电大户。而当一整座城市或者地区都调低恒温器的时候，功耗就不得不超过了需求。 这常见于当我们打开最古老的设备或者已经比较脏的发电设备时。这是Tracey Holloway，麦迪逊市Wisconsin 大学的一位研究大气的科学家。 这些古老的发电装备中有些只能依靠燃烧燃油或者煤炭，只在最热的日子里才会开启它们。 利用EPA 的数据， Holloway和她的研究团队对当温度升高时，空气污染物如何反应的问题进行了研究。 他们发现，贯穿美国东部， 温度每升高1摄氏度，发电厂可喷射出140，000吨额外的二氧化碳。而且，整个地区平均，热量每增加一度，污染物硫氧化物和氮氧化物的排放量就上升3.5 了个百分点。 这特别糟糕，因为炎热的夏季是泵出更多污染的最差的日子。 这些炎热的日子， 当美国人开启空调，或者一个州的居民开启空调， 这些日子也是化学反应最激烈的日子 。 每单位空气污染物进入空气很有可能就形成了臭氧。而臭氧本身是一种潜在的空气污染物。该研究已发表在《环境科学与技术》杂志上了。 Holloway说，应对夏季污染高峰的对策或许可以采用一种在炎热的阳光充足的日子里也很充足的能源。 如果我们可以在高峰时间得到 太阳能发电，这或许能抵消炎热的天气中午高峰时段的污染物排放，并且这还是一种避免不得不开启那些用电高峰时候才需要开启的发电厂的好办法。换句话说：为什么不用太阳提供的能量，保持凉爽呢。\n","date":1497225600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1497225600,"objectID":"dd556349d37776bb69ebf83df97f2eb3","permalink":"https://wangcc.me/post/2017-06-12/","publishdate":"2017-06-12T00:00:00Z","relpermalink":"/post/2017-06-12/","section":"post","summary":"[As temperatures rise, energy demands peak, with a corresponding increase in air pollutants.](https://www.scientificamerican.com/podcast/episode/pollution-peaks-when-temperatures-top-out/)","tags":["dictation","English Learning","Listening","60s science"],"title":"溫度，污染，和電費","type":"post"},{"authors":null,"categories":["dictation"],"content":"昨日說到BBC新聞聽寫在滬江外語學習之死。於是去跟他們客服詢問了一下。得到如下答覆： 好吧，內容優化，就是潔淨咱們的大天朝局域網唄，正準備學習外語的小朋友們怎麼能看到BBC，VOA上面對偉大祖國的描述嘛。曾經一直認爲，學習外語是爲了增進與他國的交流，互通有無，如今卻深刻體會到學習外語的真實作用是瞭解真實的中國，瞭解那個我來自的地方。\n最近新聞酸菜館也變成了只賣酸菜的館，内容優化那叫一個好！\n今天的聽寫講的是刊登在進化心理學雜誌上的一篇論文。說有伴的男人更受歡迎。科學家認爲女生更青睞有伴的男人，特別是伴侶如果美豔如花魅力四射，即使他們本身長相一般。也許是因爲這些男人如果不能用外表吸引人，那一定是富有內涵，或者是富可敵國的。總之一句話，男人被另一個女人證明了自己的價值，那女性便可省去自己去試錯的時間和精力成本。\n听写于：2017-5-31 11:24\t用时：19:27 正确率：93%\t错词：14个\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nIt turns out the best-looking thing on a man , maybe may be a good-looking woman. Researchers report that women find a man they see with an attractive woman more desirable than unattached men. That's according to a study in the journal Evolutionary Psychology. Researchers had two groups of female college students rate photos of men . - all of whom had first been rated as being of average attractiveness. The first group of 148 women were was shown pictures of those men with an attractive female. The subjects were told that the women in the photos were either a girlfriend, ex-girlfriend, cousin , or adopted sister. And the subjects rated the men with the girlfriends as more desirable than the men shown with partners described as being exes or relatives. The second study group of 79 97 students were shown average-looking men with an attractive woman who was described as definitely a girlfriend. And again , the women rated the men with the good-looking girls gals as more desirable than the single guys. Plus, the attached men were thought more likely to be more intelligent, trustworthy, humorous, lovely, wealthy and attentive. Researchers deduce deduced that the women probably assume partners that partnered-up men must have those qualities of smarts and personality to be an appropriate match for a good-looking mate. So, turns out what many of us so expected always suspected may be true . - men look better when they are they're a proven commodity.\nWords worth to be remembered: partnered-up: 结为舞伴，有伴的。 gals: n. 女孩儿们（gal的复数，等于girls） 譯文 据说能否成为有吸引力的男人取决另外一个有吸引力的女人。\n据研究报道，女性看到一个身旁有另外一个具有吸引力的女人的男人时会觉得这个男人更具有吸引力。这是跟据发表在进化心理学杂志上的一个研究得出的结论。\n研究人员招募了两组女性大学生对一些男性的照片进行评分——所有那些单身照开始都被评价为具有一般水平的吸引力。第一组有148名女士，给她们出示的照片里的那些男士们身旁都有一位有吸引力的女士陪伴。 这些大学生们被告知，照片中的那些女人可能是照片中男士的女朋友，前女友，堂/表兄妹或者养女。而这些女大学生们对这些有女朋友陪伴的男士的评价是比那些被告知说他们身旁是前任或者亲属的男士更令人满意。\n第二项研究招募的是97名学生，他们被出示的照片中的男性外貌一般，但身旁都有一位十分有吸引力的被告知是女朋友的女伴。结果这些女学生再次对有女朋友陪伴的男士的评价高于对单身男士的评价。而且，附加描述是这些男士被认为很有可能更聪明，可靠，幽默，富有以及细心。\n研究人员的推论是，女性或许对有同伴的男性的评价都是认为他们具有聪明的特质以及适合匹配高颜值伴侣的品格。\n所以，貌似我们中很多人总是怀疑的事也许还真不假——被验证过的男人最有吸引力。\n","date":1496188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1496188800,"objectID":"b4866667d7cbfb3b91c2ddb15c22cbe4","permalink":"https://wangcc.me/post/2017-05-31/","publishdate":"2017-05-31T00:00:00Z","relpermalink":"/post/2017-05-31/","section":"post","summary":"[Women rate a man they see with an attractive woman as more desirable than an unattached man. Erika Beras reports.](https://www.scientificamerican.com/podcast/episode/partnered-up-men-more-attractive-to-women/)","tags":["dictation","English Learning","Listening","60s science"],"title":"男人的魅力來自女人","type":"post"},{"authors":null,"categories":["dictation"],"content":"今日要祭奠BBC新聞聽寫在滬江外語學習之死。 幾周未見，今日登錄滬江準備聽聽久違的BBC。結果發現節目已經被下架： 然後我點擊其他訂閱的節目，結果一大半都**“被下架”**。細看過去全是外語新聞類，政治類的節目。現在連外語學習也已經進入了莫談國是的狀態了嗎？ 可憐的孩子們今後還有什麼資源可以學習英語呢？\n悲哀。\n連馬里蘭的空氣新鮮不新鮮都已經被上升到乳滑不乳滑了。我等草民還是閉嘴爲上策。親愛的朋友們，你們今天都健身了嗎？一切都只是剛剛開始。\n听写于：2017-5-30 11:34\t用时：19:08 正确率：93%\t错词：10个\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nYou can thank your parents for your DNA. Because humans share genes through sexual reproduction, passing DNA from parent to child. It's known as the \" 'vertical transfer\" ' of DNA. Now imagine if you could share just 1 one or 2 two bits of your DNA with an unrelated stranger, through a handshake, or other incidental contact . - and that stranger inserted your DNA into their genome. No sex, . No offspring either. That's called the \" 'horizontal transfer\" ' of DNA. It's obviously not how humans do it. But it's a mainstay of single cell single-celled organisms like bacteria, which use the process to share antibiotic resistance genes, for example. And now French scientists have found that horizontal DNA transfer could be a lot more common than we thought in multicellular organisms, too . - insects, in this case. Because by analyzing 195 insect genomes, they found more than 2,200 cases of this horizontal DNA transfer between unrelated species of flies and butterflies, beatles beetles and wasps. That total quad druples quadruples the number of horizontal DNA transfers previously describes described in all plants animals and fungi. The study is in the Proceedings of the National Academy of Sciences. How exactly this genetic transfer happens is still a mystery. It Might be viruses, or parasites, doing the DNA delivery. But whatever it callsthe cause, it suggests that the evolution of insects, on a molecular level at least, may be something more of share a shared success story. Words worth to be remembered: beetles: n. 甲壳虫；大槌 v. 用槌打；急忙来回，快速移动；突出；逼近，威胁 quadruples: adj. 四重的；四倍的 n. 四倍 v. （使）成四倍 譯文 你能感谢你的父母给你DNA.因为人类通过~youxing~ 有性繁殖分享基因，把基因从父母传到孩子。 这就是众所周知的DNA 的纵向传递。\n现在想象一下如果你可以分享一两个你的ＤＮＡ片段给一个无关的陌生人，就通过握手或者其它偶然的接触——并且这个陌生人就把你的ＤＮＡ片段整合到它们的基因组里去了。无性的。也没有后代。这就是被称为DNA 的水平转移。人类是怎样做到的这件事还不很明确。 但是这确是细菌等单细胞有机物的主要遗传信息活动， 比如说利用这个过程分享它们各自的抵抗抗生素基因。\n现在法国科学家已经发现水平ＤＮＡ转移现象可能比我们认为的在多细胞有机物中存在的还要更常见，也包括——昆虫，在这种情况下。因为通过分析了１９５个昆虫的基因组，它们发现在没有亲缘关系的蝇类与蝴蝶、甲壳虫和黄蜂各物种之间出现的２２００多个水平ＤＮＡ转移现象。\n**总数上，是之前发现的在全部动物、植物和真菌之间出现的水平ＤＮＡ转移现象数目的四倍。**该研究已发表在《国家科学研究进展》杂志上。\n这些遗传物质的转移是如何发现的至今人类还不十分清楚。或许是靠病毒或寄生物，进行的ＤＮＡ传递。但是，无论是什么原因，这件事提示我们，昆虫的进化，至少在分子水平上，或许不只是已知公认的进化历史那么简单。\n","date":1496102400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1496102400,"objectID":"1b510380672b9a89dfe322e1c13982e5","permalink":"https://wangcc.me/post/2017-05-30/","publishdate":"2017-05-30T00:00:00Z","relpermalink":"/post/2017-05-30/","section":"post","summary":"[Bacteria swap DNA among themselves](https://www.scientificamerican.com/podcast/episode/insects-donate-dna-to-unrelated-bugs/). And that process may be more common in multicellular organisms than previously believed. Christopher Intagliata reports.","tags":["dictation","English Learning","Listening","60s science"],"title":"遺傳物質的橫向傳遞","type":"post"},{"authors":["Okuda, Masumi","Mabe, Katsuhiro","Lin, Yingsong","**Wang, Chaochen**","Taniguchi, Yohei","Kato, Mototsugu","Kikuchi, Shogo"],"categories":null,"content":"Background\nHelicobacter pylori infection is associated with gastric cancer; thus, early diagnosis and treatment are crucial. Since H. pylori infection in adolescents or young adults shows few symptoms, screening tests are necessary for this population. In this study, the accuracy of the rapid urine-H. pylori antibody (u-HpAb) test was evaluated and compared to that of urine and serum H. pylori enzyme-linked immunosorbent assays (u-HpELISA and s-HpELISA, respectively) in junior high school students.\nMethods\nAll 1,225 students attending the junior high schools in Sasayama city were invited to participate in this study. Urine and blood samples were assayed for anti-H. pylori immunoglobulin G antibodies, and rapid u-HpAb was performed by three investigators independently. When the judgment of all investigators was in accordance, a positive or negative result was defined. Discrepant judgment was defined as an undetermined result.\nResults\nIn total, 187 students participated in this study and provided both urine and blood samples. Three students showed undetermined results with rapid u-HpAb. Excluding these results, the positivity rate of rapid u-HpAb was 3.3% (6/184), whereas that for u-HpELISA and s-HpELISA was 4.8% (10/187) and 5.9% (11/187), respectively. Using s-HpELISA and u-HpELISA as the standards, the sensitivity, specificity, positive predictive value, and negative predictive value of rapid u-HpAb were 85.7%, 100%, 100%, and 99.4%, respectively when excluding the undetermined results of rapid u-HpAb.\nConclusions\nThe rapid urine-HpAb test had excellent specificity but relatively low sensitivity. This article is protected by copyright. All rights reserved.\n","date":1493164800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493164800,"objectID":"4454f5f7bc9aab6f41579d21cfd59743","permalink":"https://wangcc.me/publication/journal-article/okuda/","publishdate":"2017-04-26T00:00:00Z","relpermalink":"/publication/journal-article/okuda/","section":"publication","summary":"Background\nHelicobacter pylori infection is associated with gastric cancer; thus, early diagnosis and treatment are crucial. Since H. pylori infection in adolescents or young adults shows few symptoms, screening tests are necessary for this population. In this study, the accuracy of the rapid urine-H. pylori antibody (u-HpAb) test was evaluated and compared to that of urine and serum H. pylori enzyme-linked immunosorbent assays (u-HpELISA and s-HpELISA, respectively) in junior high school students.","tags":null,"title":"Rapid urine antibody test for Helicobacter pylori infection in adolescents","type":"publication"},{"authors":["Chaochen Wang","Chifa Chiang","Hiroshi Yatsuya","Esayas Haregot Hilawe","Edolem Ikerdeu","Kaori Honjo","Takashi Mita","Renzhe Cui","Yoshihisa Hirakawa","Sherilynn Madraisau","Hiroyasu Iso","Atsuko Aoyama"],"categories":null,"content":"Abstract The rise of noncommunicable diseases is a serious health burden for Palau. This study described the prevalence of hypertension, and assessed its association with obesity. Surveys following the WHO STEPwise approach to surveillance were conducted in 2529 adults. Multivariate prevalence ratios (PR) of hypertension for body mass index (BMI) categories were calculated by logistic regression models using conditional standardization procedure. Age- and sex-specified analyses were performed. Overall prevalence of obesity and hypertension were 40.4% and 46.8%, respectively. Prevalence of hypertension was positively associated with BMI. However, overweight men had as high prevalence of hypertension as the obese (multivariable-adjusted PR was 1.84 for overweight and 1.91 for obese compared with nonoverweight). The association between hypertension and BMI was similar across age groups. The prevalence of hypertension in women increased gradually with the increase of BMI whereas that in men reached a plateau already in the overweight.\n","date":1492473600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1492473600,"objectID":"361056dc6acffdd17aab1b6bec61052e","permalink":"https://wangcc.me/publication/journal-article/palau/","publishdate":"2017-04-18T00:00:00Z","relpermalink":"/publication/journal-article/palau/","section":"publication","summary":"Abstract The rise of noncommunicable diseases is a serious health burden for Palau. This study described the prevalence of hypertension, and assessed its association with obesity. Surveys following the WHO STEPwise approach to surveillance were conducted in 2529 adults. Multivariate prevalence ratios (PR) of hypertension for body mass index (BMI) categories were calculated by logistic regression models using conditional standardization procedure. Age- and sex-specified analyses were performed. Overall prevalence of obesity and hypertension were 40.","tags":null,"title":"Descriptive Epidemiology of Hypertension and Its Association With Obesity: Based on the WHO STEPwise Approach to Surveillance in Palau","type":"publication"},{"authors":null,"categories":["statistics"],"content":" 行列式的性質 具體的行列式的值，可以通過以下介紹的行列式性質，儘量簡潔地求解。本節也是爲了簡易示範，僅僅使用3次行列式作例子。4次以上的行列式性質依然相同，依此類推即可。\n轉置矩陣的行列式，與轉置前的行列式一致。即：\\(|A^t|=|A|\\)。 \\(|A|=\\begin{vmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 4 \u0026amp; 5 \u0026amp; 6 \\\\ 7 \u0026amp; 8 \u0026amp; 9 \\\\ \\end{vmatrix}\\)\n任意一列（或者任意一行）若乘以 \\(\\lambda\\) 倍，那麼這個矩陣的行列式結果也將是乘以 \\(\\lambda\\) 倍。\n\\(|A|=\\begin{vmatrix} a_{11} \u0026amp; a_{12} \u0026amp; a_{13}\\\\ \\lambda a_{21} \u0026amp;\\lambda a_{22} \u0026amp; \\lambda a_{23}\\\\ a_{31} \u0026amp; a_{32} \u0026amp; a_{33}\\\\ \\end{vmatrix}\\\\ \\;\\;\\;\\;=|A|=\\lambda \\begin{vmatrix} a_{11} \u0026amp; a_{12} \u0026amp; a_{13}\\\\ a_{21} \u0026amp; a_{22} \u0026amp; a_{23}\\\\ a_{31} \u0026amp; a_{32} \u0026amp; a_{33}\\\\ \\end{vmatrix}\\)\n\\(|A|=\\begin{vmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\lambda a_{13}\\\\ a_{21} \u0026amp; a_{22} \u0026amp; \\lambda a_{23}\\\\ a_{31} \u0026amp; a_{32} \u0026amp; \\lambda a_{33}\\\\ \\end{vmatrix}\\\\ \\;\\;\\;\\;=|A|=\\lambda \\begin{vmatrix} a_{11} \u0026amp; a_{12} \u0026amp; a_{13}\\\\ a_{21} \u0026amp; a_{22} \u0026amp; a_{23}\\\\ a_{31} \u0026amp; a_{32} \u0026amp; a_{33}\\\\ \\end{vmatrix}\\)\n任意一列（或者任意一行）的各成分乘以 \\(\\lambda\\) 倍，與其他任意一列（或者任意一行）的各成分進行加運算（或者減運算）獲得的矩陣的行列式與原矩陣的行列式相同。\n\\(\\begin{vmatrix} a_{11} \u0026amp; a_{12} \u0026amp; a_{13}\\\\ a_{21}\\pm \\lambda a_{11} \u0026amp; a_{22}\\pm \\lambda a_{12} \u0026amp; a_{23}\\pm \\lambda a_{13}\\\\ a_{31} \u0026amp; a_{32} \u0026amp; a_{33}\\\\ \\end{vmatrix}，\\\\ \\begin{vmatrix} a_{11} \u0026amp; a_{12}\\pm \\lambda a_{11} \u0026amp; a_{13}\\\\ a_{21} \u0026amp; a_{22}\\pm \\lambda a_{21} \u0026amp; a_{23}\\\\ a_{31} \u0026amp; a_{32}\\pm \\lambda a_{31} \u0026amp; a_{33}\\\\ \\end{vmatrix},\\) \\(\\begin{vmatrix} a_{11} \u0026amp; a_{12} \u0026amp; a_{13}\\\\ a_{21} \\pm \\frac{a_{11}}{\\lambda} \u0026amp; a_{22}\\pm \\frac{a_{12}}{\\lambda} \u0026amp; a_{23}\\pm \\frac{a_{13}}{\\lambda}\\\\ a_{31} \u0026amp; a_{32} \u0026amp; a_{33}\\\\ \\end{vmatrix}, \\\\ \\begin{vmatrix} a_{11} \u0026amp; a_{12}\\pm \\frac{a_{11}}{\\lambda} \u0026amp; a_{13}\\\\ a_{21} \u0026amp; a_{22}\\pm \\frac{a_{21}}{\\lambda} \u0026amp; a_{23}\\\\ a_{31} \u0026amp; a_{32}\\pm \\frac{a_{31}}{\\lambda} \u0026amp; a_{33}\\\\ \\end{vmatrix}\\)\n上述行列式與行列式 \\(\\begin{vmatrix} a_{11} \u0026amp; a_{12} \u0026amp; a_{13}\\\\ a_{21} \u0026amp; a_{22} \u0026amp; a_{23}\\\\ a_{31} \u0026amp; a_{32} \u0026amp; a_{33}\\\\ \\end{vmatrix}\\) 結果相同。\n符合下列條件時，行列式的值爲 \\(0\\)\n任意一行（或者列）的全部成分均爲 \\(0\\) 時。 矩陣中若有兩行（或者兩列）的對應成分全部相同時。 矩陣中若有兩行（或者兩列）的對應成分均成一定比例時。\n\\(\\begin{vmatrix} a_{11} \u0026amp; a_{12} \u0026amp; a_{13}\\\\ a_{21} \u0026amp; a_{22} \u0026amp; a_{23}\\\\ 0 \u0026amp; 0 \u0026amp; 0\\\\ \\end{vmatrix}=0\\\\ \\begin{vmatrix} a \u0026amp; b \u0026amp; c\\\\ a \u0026amp; b \u0026amp; c\\\\ d \u0026amp; e \u0026amp; f\\\\ \\end{vmatrix}=0\\\\ \\begin{vmatrix} a \u0026amp; b \u0026amp; c\\\\ ka \u0026amp; kb \u0026amp; kc\\\\ d \u0026amp; e \u0026amp; f\\\\ \\end{vmatrix}=0\\)\n由於上面的後兩條成立，所以當矩陣中任意兩列（或者兩行）的對應成分幾乎相等，或者比值無限接近時，行列式的值也可以說就接近爲 \\(0\\)。此性質與多重線性迴歸的多重共線性有直接關係。 一個矩陣中其中兩列（或者兩行）的成分交換以後獲得的矩陣，其行列式值爲原矩陣的行列式的值的相反數。（即符號相反）\n\\(\\begin{vmatrix} a_{11} \u0026amp; a_{12} \u0026amp; a_{13}\\\\ a_{21} \u0026amp; a_{22} \u0026amp; a_{23}\\\\ a_{31} \u0026amp; a_{32} \u0026amp; a_{33}\\\\ \\end{vmatrix}=-\\begin{vmatrix} a_{21} \u0026amp; a_{22} \u0026amp; a_{23}\\\\ a_{11} \u0026amp; a_{12} \u0026amp; a_{13}\\\\ a_{31} \u0026amp; a_{32} \u0026amp; a_{33}\\\\ \\end{vmatrix}\\) （第一行和第二行對調成分）\n對角矩陣，上三角矩陣，下三角矩陣的行列式的值，等於對角成分的積\n\\(\\begin{vmatrix} a_{11} \u0026amp; 0 \u0026amp; 0\\\\ 0 \u0026amp; a_{22} \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; a_{33}\\\\ \\end{vmatrix}=a_{11}a_{22}a_{33},\\\\ \\begin{vmatrix} a_{11} \u0026amp; a_{12} \u0026amp; a_{13}\\\\ 0 \u0026amp; a_{22} \u0026amp; a_{23}\\\\ 0 \u0026amp; 0 \u0026amp; a_{33}\\\\ \\end{vmatrix}=a_{11}a_{22}a_{33},\\\\ \\begin{vmatrix} a_{11} \u0026amp; 0 \u0026amp; 0\\\\ a_{21} \u0026amp; a_{22} \u0026amp; 0\\\\ a_{31} \u0026amp; a_{32} \u0026amp; a_{33}\\\\ \\end{vmatrix}=a_{11}a_{22}a_{33}\\)\n矩陣中如果有任意一行（或列），衹有一個成分為非零成分，可以將該矩陣的行列式降次：\n\\(\\begin{vmatrix} a_{11} \u0026amp; 0 \u0026amp; 0\\\\ a_{21} \u0026amp; a_{22} \u0026amp; a_{23}\\\\ a_{31} \u0026amp; a_{32} \u0026amp; a_{33}\\\\ \\end{vmatrix}=a_{11}(-1)^{1+1}\\begin{vmatrix} a_{22} \u0026amp; a_{23} \\\\ a_{32} \u0026amp; a_{33} \\end{vmatrix}\\) \\(\\begin{vmatrix} a_{11} \u0026amp; a_{12} \u0026amp; 0\\\\ a_{21} \u0026amp; a_{22} \u0026amp; a_{23}\\\\ a_{31} \u0026amp; a_{32} \u0026amp; 0\\\\ \\end{vmatrix}=a_{23}(-1)^{1+1}\\begin{vmatrix} a_{11} \u0026amp; a_{12} \\\\ a_{31} \u0026amp; a_{32} \\end{vmatrix}\\)\n\\(A, B\\) 同時都是正方形矩陣時，\\(|AB|=|A|·|B|\\)。\n證明 \\(A=\\left(\\begin{array}{c} 0 \u0026amp; 4 \u0026amp; 2\\\\ -1 \u0026amp; 3 \u0026amp; 7\\\\ 6 \u0026amp; 5 \u0026amp; 9\\\\ \\end{array}\\right)\\)，\\(B=\\left(\\begin{array}{c} 2 \u0026amp; 3 \u0026amp; 4\\\\ -2 \u0026amp; 7 \u0026amp; 1\\\\ 4 \u0026amp; 6 \u0026amp; 0\\\\ \\end{array}\\right)\\) 時， \\(|AB|=|A|·|B|\\) 成立\n解 \\(\\because AB=\\left(\\begin{array}{c} 0 \u0026amp; 40 \u0026amp; 4\\\\ 20 \u0026amp; 60 \u0026amp; -1\\\\ 38 \u0026amp; 107 \u0026amp; 29\\\\ \\end{array}\\right)\\) \\(\\therefore |AB|=\\begin{vmatrix} 0 \u0026amp; 40 \u0026amp; 4\\\\ 20 \u0026amp; 60 \u0026amp; -1\\\\ 38 \u0026amp; 107 \u0026amp; 29\\\\ \\end{vmatrix}\\)\n利用性質3： 第2列 - 第3列 \\(\\times\\) 10 作新的第2列\n\\(=\\begin{vmatrix} 0 \u0026amp; 0 \u0026amp; 4\\\\ 20 \u0026amp; 70 \u0026amp; -1\\\\ 38 \u0026amp; -183 \u0026amp; 29\\\\ \\end{vmatrix}\\)\n利用性質7： 第一行衹有第三個元素非零，可以降次。\n\\(=4(-1)^{1+3}\\begin{vmatrix} 20 \u0026amp; 70 \\\\ 38 \u0026amp; -183\\end{vmatrix}\\) 利用性質2: 第一行所有元素除以10, 將 10 提前。\n\\(=4\\times10\\begin{vmatrix} 2 \u0026amp; 7 \\\\ 38 \u0026amp; -183\\end{vmatrix}\\\\ =40(-366-266)\\\\=-25280\\)\n\\(|A|=\\begin{vmatrix} 0 \u0026amp; 4 \u0026amp; 2\\\\ -1 \u0026amp; 3 \u0026amp; 7\\\\ 6 \u0026amp; 5 \u0026amp; 9\\\\ \\end{vmatrix}\\) 利用性質3: 第2列 - 第3列 \\(\\times\\) 2 作爲新的第二列元素\n\\(=\\begin{vmatrix} 0 \u0026amp; 0 \u0026amp; 2\\\\ -1 \u0026amp; -11 \u0026amp; 7\\\\ 6 \u0026amp; -13 \u0026amp; 9\\\\ \\end{vmatrix}\\)\n利用性質7: 第一行衹有第三個元素非零，降次。\n\\(=2(-1)^{1+3}\\begin{vmatrix} -1 \u0026amp; -11 \\\\ 6 \u0026amp; -13\\end{vmatrix}\\\\=2(13+66)=158\\) \\(|B|=\\begin{vmatrix} 2 \u0026amp; 3 \u0026amp; 4\\\\ -2 \u0026amp; 7 \u0026amp; 1\\\\ 4 \u0026amp; 6 \u0026amp; 0\\\\ \\end{vmatrix}\\) 利用性質3: 第1行 \\(+\\) 第2行作新的第1行； 第3行 - 第1行 \\(\\times\\) 2 作新的第三行\n\\(=\\begin{vmatrix} 0 \u0026amp; 10 \u0026amp; 5\\\\ -2 \u0026amp; 7 \u0026amp; 1\\\\ 0 \u0026amp; 0 \u0026amp; -8\\\\ \\end{vmatrix}\\)\n利用性質7: 第三行衹有第三個元素非零，降次。\n\\(=-8(-1)^{3+3}\\begin{vmatrix} 0 \u0026amp; 10 \\\\ -2 \u0026amp; 7\\end{vmatrix}\\\\=-8\\times20=-160\\)\n綜上可得 \\(|A|·|B|=158\\times(-160)=-25280=|AB|\\)\n試用這一節介紹的行列式性質，求解前一節例(3)的行列式值。 解 \\(\\begin{vmatrix} -2 \u0026amp; 3 \u0026amp;4 \u0026amp; 1\\\\ 4 \u0026amp; 2\u0026amp; 0\u0026amp; 5\\\\ 2 \u0026amp;-3\u0026amp; -4\u0026amp; 2\\\\ 2 \u0026amp; 1\u0026amp; 2\u0026amp; -3 \\end{vmatrix}\\)\n利用性質3\n1. 第1行 \\(+\\) 第3行，作新的第一行； 2. 第2行 \\(-\\) 第3行 \\(\\times\\) 2，作新的第2行； 3. 第4行 \\(-\\) 第3行 作新的第4行\n\\(=\\begin{vmatrix} 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 3\\\\ 0 \u0026amp; 8 \u0026amp; 8 \u0026amp; 1\\\\ 2 \u0026amp; -3 \u0026amp; -4 \u0026amp; 2\\\\ 0 \u0026amp; 4 \u0026amp; 6 \u0026amp; -5 \\end{vmatrix}\\)\n利用性質7: 第1行衹有第4個元素非零，降次。\n\\(=3(-1)^{1+4}\\begin{vmatrix} 0 \u0026amp; 8 \u0026amp; 8 \\\\ 2 \u0026amp; -3 \u0026amp; -4 \\\\ 0 \u0026amp; 4 \u0026amp; 6 \\end{vmatrix}\\)\n利用性質7: 第1列衹有第2個元素非零，降次。\n\\(=-3 \\times 2(-1)^{1+2}\\begin{vmatrix} 8 \u0026amp; 8 \\\\ 4 \u0026amp; 6 \\end{vmatrix}\\\\ =6 \\times (8 \\times 6 - 4\\times 8)\\\\ =96\\)\n","date":1491091200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491091200,"objectID":"0aedaa51e928ba824bd26aa3043d4c34","permalink":"https://wangcc.me/post/2017-04-02/","publishdate":"2017-04-02T00:00:00Z","relpermalink":"/post/2017-04-02/","section":"post","summary":"第5章　行列式 determinant P80-83","tags":["learning notes","basic mathematics","linear algebra"],"title":"「統計解析のための線形代数」復習筆記19","type":"post"},{"authors":null,"categories":["study abroad"],"content":"尋找並確定合適自己的大學，合適的課程 英國，還是美國？ 這是一個問題 我能獲得現在工作的大學的經費（其實就是保留職位，工資照發）支持的條件是，最長的出差/留學不能超過一年。 上面這個條件是最硬的了，沒有銀子，啥都辦不成是吧。美國的碩士基本都是兩年，而且每年的學費都是英國的兩倍左右。真是羨慕嫉妒自費去英美讀書的大陸籍學生們，你們都是行走的美金符號 $。 加上美國目前爲止去了3-4次了，對北美大陸除了加拿大(溫哥華)印象非常好以外，美帝給人的感覺就是一個自由化了的中國大陸。沒有任何親切感，或者吸引我個人再去長久居住的地方。當然去美國的機會以後可能還有。故覺得去正在經歷激盪變幻莫測歷史的英國也是不錯的選擇。脫歐愈演愈烈，不知道英國會不會有什麼波瀾壯闊的變化，如果能碰巧做個見證人，也是不錯的。將來可以跟我兒子說，看當年大英帝國被踢出歐萌的時候，爸爸在那親眼看着呢。 另外就是大學的選擇了。當然可選擇的大學有很多，奈何我之前跟大學申請這個例外項目的時候說的是倫敦大學。因此什麼劍橋牛津都是浮雲了。還好我沒明確說，其實倫敦大學底下一大堆大學，UCL和LSHTM是我的申請重點。因爲論醫學統計學課程，大家可以參考這篇文章^[Pocock, S. J. Life as an academic medical statistician and how to survive it. Statist. Med. 14, 209–222 (1995).] 。儘管時間有點久遠，但是英國國內大學有開設醫學統計課程的大概就那麼幾個，估計沒什麼太大變化，摘錄Pro. Pocock總結的各家特色如下：\n我們可以看到，從最上面的劍橋大學，到最下面的LSHTM(有人翻譯成倫敦衛校😅)按照教學內容偏重理論還是實際進行了排序。所以，LSHTM最偏重實際應用的名氣，是由來已久的。 Theory\n(偏重理論) Cambridge Mathematical Statistics $\\downdownarrows$ Sheffield Statistics $\\downdownarrows$ University College London Applied Stochastic Systems $\\downdownarrows$ Oxford Applied Statistics $\\downdownarrows$ Kent Statistics $\\downdownarrows$ Reading Biometry $\\downdownarrows$ Southampton Statistics with Application in Medicine $\\downdownarrows$ Leicester Medical Statistics \u0026amp; Information Technology Applications (偏重實踐) London School of Hygiene \u0026amp; Tropical Medicine Medical Statitics 確認申請時間，申請要點（雅思成績要求，是否有面試，推薦信） 決定了申請 LSHTM 以後，便要開始準備材料，確定截止時間，以及雅思成績的要求等。\n我之前並無申請歐美大學的經驗，許多都是這次申請過程中自己摸索的。總結一下就是，留學申請這種事，自己來就可以搞定了。經過仔細鑽研LSHTM的醫學統計碩士課程網站，確認雅思成績要求總分不低於7，聽說讀單項最低不低於5.5，寫作不低於6.5以後，便着手開始集中複習英語的計劃。\n至於申請截止時間，網站說的8月1日，沃天。。。9月底開學8月還能申請。不過，6月1日以後的申請要交£100的過遲申請費用。不管怎麼說，越早越好。我是2016年10月開始計劃申請的，那時候自己給自己定下目標，1月7日雅思成績如果達標，1月份之內就完成所有申請步驟。\n面試的情況後面會再多說一些，其他課程不太瞭解，醫學統計學的碩士課程是對有可能成爲學生的人進行面試的 (potential students will be invited to join an interview)。所以估計材料交了以後很久都沒有面試的通知的話，那就可以安心在家當作自己沒有申請過，該幹嘛幹嘛了。儘量保持低調嘛。我還跟他們負責招生的人發郵件確認了，材料遞交6周左右會給面試通知。估計不錄取也是在這個時間點給通知的。\n最後一個就是最重要的推薦人的選擇了。我邀請之前博士階段的導師，以及現在的同事。聽說美國大學要三個推薦人。英國是只要兩人的。關於如何選擇推薦人，LSHTM的網站上說的是，如果申請人正在就學，那就需要兩個都是對你的學業/學術十分瞭解的人。如果申請人已經就業，那就填最高學歷時期的導師一名，及現在的同事一名或者老闆/上司。當然，在把自己要寫的推薦人姓名信息等填入申請表格之前，要跟他們打個招呼才是。\n至於推薦信的內容。我的博士導師收到我的邀請郵件以後欣然同意，然而那之後我並沒有收到他給我的個人評價或者推薦信內容/稿件。我想，大概(有些)認真的日本人認爲這個推薦信對申請人本人來說也應該保密的。不過我對我的導師有充分的信任，不至於在推薦信裏寫我不愛讀書行爲不端之類害我的話。他一直都是實事求是認真做事的人。另外一封推薦信來自我的同事，他對自己英文不太有自信，而且他每天就坐在我隔壁，寫了稿子就讓我看，我又請native speaker幫忙校對了以後提交的。所以我對這個第二封推薦信的內容是掌握的。\n3個月突擊，雅思8分 我以前考過兩次託福。都是裸考。一次是大學期間跟風考的，大夥兒都忙忙碌碌，準備考研啦，準備託福GRE出國拉，所以我也想說考一個，看看這些英語考試都考什麼內容。如果您來我這裏想瞭解託福雅思考試的祕籍，抱歉出門左轉去滬江外語吧。我每日也都是用的他家的APP和資源（主要是聽寫BBC新聞）。另外推薦一個背單詞的軟件：百詞斬。扇貝單詞也不錯。不過個人還是對百詞斬比較偏愛。也許是先入爲主吧。第一次打開時，設置自己的背誦單詞表（雅思詞彙）然後設定好時間，和背詞計劃。我是設定了每日100個單詞。每天堅持一百個，直到考試前一天。百詞斬的app會再每天第一次打開app的時候提醒，並且複習昨天或者最近背誦過的生詞。感覺他們應該是用了一些算法的，大約是根據個人背誦單詞的記錄（傳說中的記憶曲線？），以及錯誤次數來選出每天複習的詞彙的，這一點百詞斬很厲害。\n除了背單詞，就是尋找合適的老師練習寫作和口語了。在此我就不去給某寶作廣告了。我找了兩個雅思作文老師練習，每天都有寫作的作業，一天 task 1 第二天 task 2 這樣。有的老師只提供作文修改和點評，有的還會給你上課，當然費用就比前一種稍微貴一些。能提供授課服務的老師基本上就是具有新東方，環球雅思等授課經驗的作文老師。作文老師推薦的教材可以在此介紹一下: 至於口語，某寶上的口語外教中介等類似商品就更多了。基本上應該都是菲律賓的口語老師。一開始我也抱着忐忑的心情預約試聽了以下，擔心菲律賓的口語老師可能會有類似印度人的難懂口音。後來的事實證明自己完全是多慮了。至少在我聽課的那幾位菲律賓的外教的口音都較爲純正。況且每個人的口音（應該）都是天生的/後天跟父母學的，不必擔心自己上了幾天口語可就變成怪怪的阿三口音。另外記得以前看過文章說英語母語者能辨別很多不同的口音，所以關鍵不是口音影響一個人的表達，而是你到底真的會不會表達。而且如果你的有點異國口音的英語常常還會被認爲很有趣，很性感，或者很有特點。個人認爲典型的中國人的口音其實多數情況下不太性感，但是你也可以變得像下面這個人一樣風趣幽默（點擊圖片可以看到他講的中式腔調的英語笑話，老外一樣被逗得一樂一樂的）: 於是我的口語課就固定爲每天早晨9點鐘開始一個小時，和老師練習過去口語考過的題目。各種常見/不常見話題的切磋和準備。許多話題是根本想不到的。比如，“請描述一次你參加過的婚禮”，或者“請用英語講一個中國歷史上的有趣的故事”這樣的題目，讓我用中文來講述我還要愣上個1分鐘，更不要說在分秒必爭的口語考試中被問道這樣的題目，基本就等於告訴你回去準備再交錢考試了。\n備考雅思是一段辛苦的過程。堅持每日練習才能保持良好的考試/競技狀態，口語和作文是中國人的短板。聽力和寫作常常有不少人（包括我）可以拿到接近滿分。我一開始備考時也是覺得要把過去劍橋雅思的4-11套全真練習題全部過一邊，題海戰術嘛。後來被寫作的老師敲了警鐘。他說：「聽力和閱讀如果每天都花過多的時間去做的話，對於你來說提高很有限，因爲你都只有錯很少的題目，只是自己刷高分滿足自己的虛榮心而已。到頭來短板的作文和口語都沒有時間練習的話，總分還是上不去。」於是我聽從了寫作老師的話，改爲三四天做一套聽力和閱讀。當然每次都是用考試時的標準來。所以其實一直到了考前，我也沒有把4-11的所有過去試題都練習完。只是挑着做了一些。關於考場的真實感受和我的考分。可以看我之前的文章。\n1個月集中，文件準備 考完雅思考試以後等待考試成績公佈的這段時間，我便開始着手準備申請所需要的各種文件。\n最近的大學院（就是我的博士課程）成績單，英文版。\n博士學位證書，和名古屋大學的畢業證書的英文版。\n1和2由於是要開英文版的，聯繫名古屋大學的留學生辦公室，申請郵寄辦理（無手續費），然後附上回信的信封和郵票就可以了。 另外，爲了以防萬一，我又拜託之前本科階段上海交大的指導老師幫忙開具了本科階段的學位證書，畢業證書的英文版，以及當年的成績單。（看了當時的成績單，不禁回想當年在上海求學的日子。曾經有段時間，在中國訪問facebook是不需要任何技巧的。那個時候，我們還有google reader，還有google.cn。。。） 個人簡歷cv\n查找了許多模板，后来选择了(这一款)。Fork过来以后打开Rmd文件，写上自己的内容，knit，pdf就生成了。生活从未如此简单与快乐。告别Micro$oft，你会更轻松。 個人陳述(Personal Statement)寫作，修改，寫作，修改。\n格式依然是用模板，然后内容的写作和修改，确实费了一番脑筋和功夫。先是寫了初稿，然後給了曾經上过LSHTM醫學統計課程的日本人前輩看，然後修改，又給寫推薦信的兩位導師看，然後再修改，之後再給曾經在UCL留學的高中同學，以及他認識的 native speaker 看，之後再修改。此後又给目前在UCL任教的曾經的高中同學看。最后又花錢送去潤色和校對一遍，才決定最後作爲申請文書遞交給LSHTM。六個不同的人給的意見自然會有不同，最終還是要自己作決斷和取捨的。 跟以前的導師，現在的上司請求推薦信\n上面提到個人陳述的時候也說到把稿子給了兩位寫推薦信的導師看。我覺得這一點十分重要。畢竟寫推薦信的導師，他要知道你自己在個人陳述中自我推薦了什麼，才能再在推薦信裏加以強調。深以爲然。 備齊材料，終於可以申請了！ 上面的各種文件備齊了以後，就是直接在線填寫申請表格了。表格中仍有部分內容需要自己填寫的。在此不再贅述。 按下申請按鈕之後，LSHTM發來確認信。估計是系統自動發送的。之後便是等待兩位推薦人在線遞交推薦信了。兩位推薦人交齊了推薦信，已經是我申請提交之後一個月左右的事了。之後該是進入和文書審查階段。\n面試來了！面試真的來了！ 過了兩到三週。課程的聯絡人(Admissions Administrator)發來郵件說安排一下Skype面試的時間。\n我被錄取了！ 補交畢業證書的原件 ","date":1489622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1489622400,"objectID":"871e04c78b7051f9222c884892b9e530","permalink":"https://wangcc.me/post/2017-03-16/","publishdate":"2017-03-16T00:00:00Z","relpermalink":"/post/2017-03-16/","section":"post","summary":"英國留學申請-準備-面試-合格！(to-be-updated)","tags":["LSHTM","London","Medical Statistics"],"title":"留學筆記","type":"post"},{"authors":null,"categories":["statistics"],"content":" 行列式的定義與計算 Theorem 1 (determinant) \\(n\\) 次正方形矩陣 \\(A= (a_{ij})=\\left( \\begin{array}{c} a_{11}\u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n}\\\\ a_{21}\u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n}\\\\ \\vdots\u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots\\\\ a_{n1}\u0026amp; a_{n2} \u0026amp; \\cdots \u0026amp; a_{nn} \\end{array} \\right)\\) 的行列式(determinant)被定義爲是，\\(A\\) 的全部成分 \\(a_{11},a_{12},\\cdots,a_{nn}\\) 的函數，這個函數是一個標量(scalar)。 \\(n\\)次正方形矩陣 \\(A\\) 的行列式(\\(n\\)次行列式)，被記作：\n\\(|A|, |a_{ij}|, \\det(A), \\det(a_{ij})， \\begin{vmatrix} a_{11}\u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n}\\\\ a_{21}\u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n}\\\\ \\vdots\u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots\\\\ a_{n1}\u0026amp; a_{n2} \u0026amp; \\cdots \u0026amp; a_{nn} \\notag \\end{vmatrix}\\)\n1次行列式： \\[\\begin{equation} A=(a_{11}), |A|=a_{11} (\\#eq:determinant1) \\end{equation}\\]\n2次行列式： \\[\\begin{equation} A=\\left( \\begin{array}{} a_{11} \u0026amp; a_{12}\\\\ a_{21} \u0026amp; a_{22}\\\\ \\end{array} \\right), |A|=a_{11}a_{12}-a_{12}a_{21} (\\#eq:determinant2) \\end{equation}\\]\n\\(n-1\\) 次行列式 \\[\\begin{equation} A_{(n-1)\\times(n-1)}, 假設行列式 |A| 有被定義 (\\#eq:determinant3) \\end{equation}\\]\n\\(n\\) 次行列式\n假如(??)成立：\n對於：\\(A_{n\\times n}=\\left( \\begin{array}{c} a_{11}\u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n}\\\\ a_{21}\u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n}\\\\ \\vdots\u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots\\\\ a_{n1}\u0026amp; a_{n2} \u0026amp; \\cdots \u0026amp; a_{nn} \\end{array} \\right)\\\\ |A|=\\begin{vmatrix} a_{11}\u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n}\\\\ a_{21}\u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n}\\\\ \\vdots\u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots\\\\ a_{n1}\u0026amp; a_{n2} \u0026amp; \\cdots \u0026amp; a_{nn} \\notag \\end{vmatrix}\\\\ \\left\\{ \\begin{array}{} (4)\\;=a_{i1}A_{i1}+a_{i2}A_{i2}+\\cdots+a_{ij}A_{ij}+\\cdots+a_{in}A_{in}\\\\ (5)\\;=a_{1j}A_{i1}+a_{2j}A_{2j}+\\cdots+a_{ij}A_{ij}+\\cdots+a_{nj}A_{nj}\\\\ \\end{array} \\right.\\)\n式子 \\((4)\\) 被稱爲行列式 \\(|A|\\) 的第 \\(i\\) 行展開式(expansion of \\(|A|\\) according to elements of row \\(i\\))。同樣的，式子 \\((5)\\) 被稱爲行列式 \\(|A|\\) 的第 \\(j\\) 列展開式。\\(|A_{ij}|(i=1,2,\\cdots,n;j=1,2,\\cdots,n)\\) 被稱爲 成分 \\(a_{ij}\\) 的餘因子(cofactor)，定義如下：\n\\(A_{ij}=(-1)^{i+j}D_{ij}\\\\ \\;\\;\\;\\;=(-1)^{i+j}\\begin{vmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1,j-1} \u0026amp; a_{1,j+1} \u0026amp; \\cdots \u0026amp; a_{1n}\\\\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2,j-1} \u0026amp; a_{2,j+1} \u0026amp; \\cdots \u0026amp; a_{2n}\\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots\\\\ a_{i-1,1} \u0026amp; a_{i-1,2} \u0026amp; \\cdots \u0026amp; a_{i-1,j-1} \u0026amp; a_{i-1,j+1} \u0026amp; \\cdots \u0026amp; a_{i-1,n}\\\\ a_{i+1,1} \u0026amp; a_{i+1,2} \u0026amp; \\cdots \u0026amp; a_{i+1,j-1} \u0026amp; a_{i+1,,j+1} \u0026amp; \\cdots \u0026amp; a_{i+1,n}\\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots\\\\ a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1,j-1} \u0026amp; a_{1,j+1} \u0026amp; \\cdots \u0026amp; a_{1n}\\\\ \\end{vmatrix}\\)\n\\(D_{ij}\\) 正如上面等式最右端所寫，其實是行列式 \\(A_{n\\times n}\\) 剔除了第 \\(i\\) 行和第 \\(j\\) 列的 \\((n-1)\\) 次行列式，又被叫做行列式 \\(A_{n\\times n}\\) 的小行列式(minor)。\n餘因子矩陣 以 \\(A_{n\\times n}\\) 的成分 \\(a_{ij}\\) 的餘因子 \\(A_{ij}\\) 作成分的矩陣的轉置矩陣作被稱爲 \\(A_{n\\times n}\\) 的餘因子矩陣(adjoint matrix, adjugate matrix)。標記爲 \\(adj(A)\\)。也就是說：\n\\(adj(A)=\\left( \\begin{array}{c} A_{11}\u0026amp; A_{12} \u0026amp; \\cdots \u0026amp; A_{1n}\\\\ A_{21}\u0026amp; A_{22} \u0026amp; \\cdots \u0026amp; A_{2n}\\\\ \\vdots\u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots\\\\ A_{n1}\u0026amp; A_{n2} \u0026amp; \\cdots \u0026amp; A_{nn} \\end{array} \\right)^t=\\left( \\begin{array}{c} A_{11}\u0026amp; A_{21} \u0026amp; \\cdots \u0026amp; A_{n1}\\\\ A_{12}\u0026amp; A_{22} \u0026amp; \\cdots \u0026amp; A_{n2}\\\\ \\vdots\u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots\\\\ A_{1n}\u0026amp; A_{2n} \u0026amp; \\cdots \u0026amp; A_{nn} \\end{array} \\right)\\)\n我們來試着計算行列式：\n1. \\(2\\) 次行列式\n以方程(??)的定義計算：\n\\(\\begin{vmatrix} a_{11} \u0026amp; a_{12}\\\\ a_{21} \u0026amp; a_{22}\\\\ \\end{vmatrix}=a_{11}a_{22}-a_{12}a_{21}\\)\n此公式可以用下列 示意圖(薩呂法則, Sarrus’ rule) 來記憶: 也就是，沿着右下方向將所有成分相乘以後用加號 \\(+\\) 號連接起來，沿着左下的方向的所有成分則相乘以後用減號 \\(-\\) 號連接起來。最後將這兩者相加獲得行列式的值。\n練習： 求 \\(A=\\left( \\begin{array}{} 4 \u0026amp; 2\\\\ 1 \u0026amp; 3\\\\ \\end{array} \\right)\\) 的行列式和餘因子矩陣。\n解： \\(|A|=\\begin{vmatrix} 4 \u0026amp; 2\\\\ 1 \u0026amp; 3\\\\ \\end{vmatrix}=4\\times3-2\\times1=10\\) \\(adj(A)=\\left( \\begin{array}{} A_{11} \u0026amp; A_{21}\\\\ A_{12} \u0026amp; A_{22}\\\\ \\end{array} \\right)\\) \\(\\because A_{11}=(-1)^{(1+1)}\\times3\\\\ A_{21}=(-1)^{(2+1)}\\times2\\\\ A_{12}=(-1)^{(1+2)}\\times1\\\\ A_{22}=(-1)^{2+2}\\times4\\\\ \\therefore adj(A)=\\left( \\begin{array}{r} 3 \u0026amp; -2\\\\ -1 \u0026amp; 4\\\\ \\end{array} \\right)\\)\n注意：餘因子矩陣，終究是一個矩陣而非行列式。\n三次矩陣\n\\(A=\\left( \\begin{array}{c} a_{11} \u0026amp; a_{12} \u0026amp; a_{13}\\\\ a_{21} \u0026amp; a_{22} \u0026amp; a_{23}\\\\ a_{31} \u0026amp; a_{32} \u0026amp; a_{33}\\\\ \\end{array} \\right)\\) 的行列式 \\(|A|\\) 要如何用 \\(A\\) 的成分來表示呢？ 我們發現，代入上面第 \\((4)\\) 個式子 \\(n=3\\) 的情況來計算。\n在這裏，我們就按照 \\(i=1\\) 的情況來展開。 (注意：\\(i=2, i=3\\) 的情況展開，結果也是一樣的。)\n\\[\\begin{align} |A| \u0026amp;= a_{11}A_{11}+a_{12}A_{12}+a_{13}A_{13}\\\\ \u0026amp;= a_{11}(-1)^{1+1}D_{11}+a_{12}(-1)^{1+2}D_{12}\\\\ \u0026amp;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;+a_{13}(-1)^{1+3}D_{13}\\\\ \u0026amp;= a_{11}\\begin{vmatrix}a_{22} \u0026amp; a_{23}\\\\ a_{32} \u0026amp; a_{33}\\\\\\end{vmatrix}-a_{12}\\begin{vmatrix}a_{21} \u0026amp; a_{23}\\\\ a_{31} \u0026amp; a_{33}\\\\\\end{vmatrix}\\\\ \u0026amp;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;+a_{13}\\begin{vmatrix}a_{21} \u0026amp; a_{22}\\\\ a_{31} \u0026amp; a_{32}\\\\\\end{vmatrix}\\\\ \u0026amp;= a_{11}(a_{22}a_{23}-a_{23}a_{32})-a_{12}(a_{21}a_{33}-a_{31}a_{23})\\\\ \u0026amp;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;+a_{13}(a_{21}a_{32}-a_{22}a_{31})\\\\ \u0026amp;=a_{11}a_{22}a_{33}+a_{12}a_{23}a_{31}+a_{13}a_{21}a_{32}-a_{11}a_{23}a_{32}\\\\ \u0026amp;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;-a_{12}a_{21}a_{33}-a_{13}a_{22}a_{31} \\end{align}\\]\n我們也可以利用薩呂法則（下圖）來記住計算過程：\n另外，可以得到如下的餘因子：\n\\(A_{11}=\\begin{vmatrix}a_{22} \u0026amp; a_{23}\\\\ a_{32} \u0026amp; a_{33}\\\\\\end{vmatrix}, A_{12}=-\\begin{vmatrix}a_{21} \u0026amp; a_{23}\\\\ a_{31} \u0026amp; a_{33}\\\\\\end{vmatrix}, A_{13}=\\begin{vmatrix}a_{21} \u0026amp; a_{22}\\\\ a_{31} \u0026amp; a_{32}\\\\\\end{vmatrix}\\\\ A_{21}=-\\begin{vmatrix}a_{12} \u0026amp; a_{13}\\\\ a_{32} \u0026amp; a_{33}\\\\\\end{vmatrix}, A_{22}=\\begin{vmatrix}a_{11} \u0026amp; a_{13}\\\\ a_{31} \u0026amp; a_{33}\\\\\\end{vmatrix}, A_{23}=-\\begin{vmatrix}a_{11} \u0026amp; a_{12}\\\\ a_{31} \u0026amp; a_{32}\\\\\\end{vmatrix}\\\\ A_{31}=\\begin{vmatrix}a_{12} \u0026amp; a_{13}\\\\ a_{22} \u0026amp; a_{23}\\\\\\end{vmatrix}, A_{32}=-\\begin{vmatrix}a_{11} \u0026amp; a_{13}\\\\ a_{21} \u0026amp; a_{23}\\\\\\end{vmatrix}, A_{33}=\\begin{vmatrix}a_{11} \u0026amp; a_{12}\\\\ a_{21} \u0026amp; a_{22}\\\\\\end{vmatrix}\\)\n因此餘因子矩陣爲：\n\\(adj(A)=\\left( \\begin{array}{c} A_{11} \u0026amp; A_{12} \u0026amp; A_{13}\\\\ A_{21} \u0026amp; A_{22} \u0026amp; A_{23}\\\\ A_{31} \u0026amp; A_{32} \u0026amp; A_{33}\\\\ \\end{array} \\right)^t=\\left( \\begin{array}{c} A_{11} \u0026amp; A_{21} \u0026amp; A_{23}\\\\ A_{12} \u0026amp; A_{22} \u0026amp; A_{32}\\\\ A_{13} \u0026amp; A_{23} \u0026amp; A_{33}\\\\ \\end{array} \\right)\\) 練習：試求矩陣 \\(A=\\left( \\begin{array}{r} 6 \u0026amp; 1 \u0026amp; -3\\\\ 3 \u0026amp; 5 \u0026amp; 7\\\\ 2 \u0026amp; -1 \u0026amp; 3\\\\ \\end{array} \\right)\\) 的行列式和餘因子矩陣 解： \\(|A|= \\begin{vmatrix} 6 \u0026amp; 1 \u0026amp; -3\\\\ 3 \u0026amp; 5 \u0026amp; 7\\\\ 2 \u0026amp; -1 \u0026amp; 3\\\\ \\end{vmatrix}\\\\ \\;\\;\\;\\;\\:=6\\times5\\times3+1\\times7\\times2+(-3)\\times3\\times(-1)\\\\ \\;\\;\\;\\;\\;\\:\\;\\;\\;\\;\\:-\\{6\\times7\\times(-1)+1\\times3\\times3+(-3)\\times5\\times2\\}\\\\ \\;\\;\\;\\;\\:=113-(-63)=176\\\\ \\\\ A_{11}=\\begin{vmatrix}5 \u0026amp; 7\\\\ -1 \u0026amp; 3\\\\\\end{vmatrix}=15-(-7)=22\\\\ A_{12}=\\begin{vmatrix}3 \u0026amp; 7\\\\ 2\u0026amp; 3\\\\\\end{vmatrix}=9-14=-5\\\\ A_{13}=\\begin{vmatrix}3 \u0026amp; 5\\\\ 2 \u0026amp; -1\\\\\\end{vmatrix}=-3-10=-13\\\\ A_{21}=\\begin{vmatrix}1 \u0026amp; -3\\\\ -1 \u0026amp; 3\\\\\\end{vmatrix}=3-3=0\\\\ A_{22}=\\begin{vmatrix}6 \u0026amp; -3\\\\ 2 \u0026amp; 3\\\\\\end{vmatrix}=18-(-6)=24\\\\ A_{23}=\\begin{vmatrix}6 \u0026amp; 1\\\\ 2 \u0026amp; -1\\\\\\end{vmatrix}=-6-2=-8\\\\ A_{31}=\\begin{vmatrix}1 \u0026amp; -3\\\\5 \u0026amp; 7\\\\\\end{vmatrix}=7-(-15)=22\\\\ A_{32}=\\begin{vmatrix}6 \u0026amp; -3\\\\3 \u0026amp; 7\\\\\\end{vmatrix}=42-(-9)=51\\\\ A_{33}=\\begin{vmatrix}6 \u0026amp; 1\\\\ 3 \u0026amp; 5\\\\\\end{vmatrix}=30-3=27\\\\ \\Longrightarrow adj(A)=\\left( \\begin{array}{r} 22 \u0026amp; 5 \u0026amp; -13 \\\\ 0 \u0026amp; 24\u0026amp; -8 \\\\ 22 \u0026amp; 51\u0026amp; 27 \\\\ \\end{array} \\right)^t=\\left( \\begin{array}{r} 22 \u0026amp; 0 \u0026amp; 22\\\\ 5 \u0026amp; 24 \u0026amp; 51\\\\ -13 \u0026amp; -8 \u0026amp; 27\\\\ \\end{array} \\right)\\) 練習： 求3次矩陣的固有值時(將來敘述)需要的行列式\n\\(\\begin{vmatrix} a-\\lambda \u0026amp; b \u0026amp; c\\\\ d \u0026amp; e-\\lambda \u0026amp; f\\\\ g \u0026amp; h \u0026amp; i-\\lambda \\end{vmatrix}\\)\n展開以後，整理爲關於 \\(\\lambda\\) 的式子：\n解： \\(\\begin{vmatrix} a-\\lambda \u0026amp; b \u0026amp; c\\\\ d \u0026amp; e-\\lambda \u0026amp; f\\\\ g \u0026amp; h \u0026amp; i-\\lambda \\end{vmatrix}\\\\ =(a-\\lambda)(e-\\lambda)(i-\\lambda)+bfg+dhc\\\\ \\;\\;\\;\\;\\;-\\{g(e-\\lambda)c+bd(i-\\lambda)+(a-\\lambda)fh\\}\\\\ =-\\lambda^3+(a+e+i)\\lambda^2+(bd+cg+fh-ae-ei-ai)\\lambda\\\\ \\;\\;\\;\\;\\;+(aei+bfg+cdh-afh-bdi-ecg)\\) 4次行列式：\n試求\\(A=\\left( \\begin{array}{} a_{11}\u0026amp; a_{12}\u0026amp; a_{13}\u0026amp; a_{14}\\\\ a_{21}\u0026amp; a_{22}\u0026amp; a_{23}\u0026amp; a_{24}\\\\ a_{31}\u0026amp; a_{32}\u0026amp; a_{33}\u0026amp; a_{34}\\\\ a_{41}\u0026amp; a_{42}\u0026amp; a_{43}\u0026amp; a_{44} \\end{array} \\right)\\\\ \\;\\;=\\left( \\begin{array}{r} -2 \u0026amp; 3 \u0026amp;4 \u0026amp; 1\\\\ 4 \u0026amp; 2\u0026amp; 0\u0026amp; 5\\\\ 2 \u0026amp;-3\u0026amp; -4\u0026amp; 2\\\\ 2 \u0026amp; 1\u0026amp; 2\u0026amp; -3 \\end{array} \\right)\\) 的行列式 \\(|A|\\)：\n由於第2行有成分 \\(a_{23}=0\\) 我們以第二行展開行列式，因爲 \\(a_{23}=0\\)，所以 \\(a_{23}A_{23}=0\\) 可以省略：\n\\(|A|=a_{21}A_{21}+a_{22}A_{22}+a_{24}A_{24}\\\\ \\;\\;\\;\\;\\:=a_{21}(-1)^{2+1}D_{21}+a_{22}(-1)^{2+2}D_{22}+a_{24}(-1)^{2+4}D_{24}\\)\n\\(\\because A_{21}=(-1)^{2+1}\\begin{vmatrix} a_{12} \u0026amp; a_{13} \u0026amp; a_{14}\\\\ a_{32} \u0026amp; a_{33} \u0026amp; a_{34}\\\\ a_{42} \u0026amp; a_{43} \u0026amp; a_{44}\\\\ \\end{vmatrix}\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;=-\\begin{vmatrix} 3 \u0026amp; 4 \u0026amp; 1\\\\ -3 \u0026amp; -4 \u0026amp; 2\\\\ 1 \u0026amp; 2\u0026amp; -3\\\\ \\end{vmatrix}=6\\\\ A_{22}=(-1)^{2+2}\\begin{vmatrix} a_{11} \u0026amp; a_{13} \u0026amp; a_{14}\\\\ a_{31} \u0026amp; a_{33} \u0026amp; a_{34}\\\\ a_{41} \u0026amp; a_{43} \u0026amp; a_{44}\\\\ \\end{vmatrix}\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;=\\begin{vmatrix} -2 \u0026amp; 4 \u0026amp;1\\\\ 2 \u0026amp; -4 \u0026amp;2\\\\ 2 \u0026amp; 2 \u0026amp;-3\\\\ \\end{vmatrix}=36\\\\ A_{24}=(-1)^{2+4}\\begin{vmatrix} a_{11} \u0026amp; a_{12} \u0026amp; a_{13}\\\\ a_{31} \u0026amp; a_{32} \u0026amp; a_{33}\\\\ a_{41} \u0026amp; a_{42} \u0026amp; a_{43}\\\\ \\end{vmatrix}\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;=-\\begin{vmatrix} -2 \u0026amp; 3 \u0026amp;4\\\\ 2 \u0026amp; -3 \u0026amp;-4\\\\ 2 \u0026amp; 1 \u0026amp;2\\\\ \\end{vmatrix}=0\\\\ \\therefore |A|=4\\times6+2\\times36+5\\times0=96\\)\n然而，4次以上的矩陣的行列式計算，沒有類似薩呂法則的計算方法。 ","date":1489536000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1489536000,"objectID":"5c311ad36df131028f52b1aea55b6eec","permalink":"https://wangcc.me/post/2017-03-15/","publishdate":"2017-03-15T00:00:00Z","relpermalink":"/post/2017-03-15/","section":"post","summary":"第5章　行列式 determinant P73-79","tags":["learning notes","basic mathematics","linear algebra"],"title":"「統計解析のための線形代数」復習筆記18","type":"post"},{"authors":null,"categories":["dictation"],"content":" Your browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2017-3-13 18:20\t用时：24:46 正确率：94%\t错词：14个\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点 In the early part of the last century, speculation about ilfe life on Mars was more florid, particularly by the American astronaut astronomer Percival Lowell. Through his telescope in Arizona, Lowell convinced himself he saw swaths sways of vegetation come and go with the seasons. Of course, he didn't. What he really saw was shifted were shifting areas of sand dunes. But he also believed he'd found evidence of an advanced civilizationcivilisation. \" The canals are of various lengthlengths. Some stretch 2,500 miles from end to end. We are looking upon the results result of the world of some sort of intelligent beings. \" Lowell's canals were just an optical illusion. , the results result of the imperfect image in his telescope and his over-excited imagination. We got a better feeling for Mars as a life-friendly or unfriendly world. , when the first space probes visited in the 1960s and 70s, . It had a really thin atmosphere, and therefore hardly any shielding against radiation coming from space. It was a desert world with no liquid water on the surface, let alone canals. But there was evidence that water had in the distant past, float in great volumes, and that it might still be there as ice undergroundunder ground. So even in 1972, scientists like NASA's Gerald Soffen entertained the possibility of quite fancy organisms alive on the surface today.\nWords worth to be remembered: speculation [,spekjʊ'leɪʃn]: n. 推测；eg:Every induction is a speculation. 所有归纳推理都是一种猜测。 florid ['flɒrɪd] adj. 绚丽的 eg: The senator gave a florid speech.议员作了一番词藻华美的演说。 dune [djuːn] n. （由风吹积而成的）沙丘 eg: Large dunes make access to the beach difficult in places. 在有些地方大沙丘使得靠近海滩很难。 譯文 上个世纪初期，对于火星上存在生命这样的猜测有点言过其词，尤其是美国天文学家帕西瓦尔·罗威尔的猜测。通过他在亚利桑那州的望远镜，他确信自己看到了植物随着季节的变化而变化。显然，他并没有看到什么植物。他真正看到的其实是随风移动的沙丘。但是他相信自己找到了火星上存在先进文明的证据。\n“火星上有不同长度的运河。有些绵延2500英里。我们把这看作是火星世界里存在某种智慧生命才有的结果。”\n罗威尔所说的运河只不过是视觉幻影，因为望远镜里的影像并不完整，还有他的过度兴奋导致了空想。\n在20世纪60年代到70年代，当第一批太空探测器着陆火星，对于火星能否适合生存我们有了更好的了解。它的大气层很薄，因此几乎不能屏蔽来自太空的辐射。它的地表没有水，是一片荒漠，更不要说有运河了。\n但是有证据显示在遥远的过去，火星上有过水的存在，有过大量水的流动，也许这些水还以冰的形式存在地下。所以甚至在1972年，像NASA的杰拉德索芬这样的科学家们都认为如今在火星表面还有相当奇特的生命体存在的可能。\n","date":1489449600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1489449600,"objectID":"80a3fbfcc6a98e5c2d92f028fc81b1f0","permalink":"https://wangcc.me/post/dictation2017-03-13/","publishdate":"2017-03-14T00:00:00Z","relpermalink":"/post/dictation2017-03-13/","section":"post","summary":"Life speculation on Mars","tags":["dictation","English Learning","Listening","BBC"],"title":"绝知此事要躬行","type":"post"},{"authors":null,"categories":["statistics"],"content":" 正定，半正定 (正值，半正值) 對於任意的非零向量 \\(\\underline{x}(\\neq\\underline{0})\\) ，如果2次型 \\(\\underline{x}^tA\\underline{x}\\) 始終滿足 \\(\\underline{x}^tA\\underline{x} \u0026gt; 0\\) 注意此處無等號。我們稱這個2次型爲正定(positive definite)，\\(A\\)爲正定矩陣(positive definite matrix)。另外，如果任意非零向量 \\(\\underline{x}(\\neq\\underline{0})\\) 始終滿足2次型 \\(\\underline{x}^tA\\underline{x} \\geqslant 0\\)， 這個2次型被叫做半正定(positive semi-definite)，\\(A\\)爲半正定矩陣(positive semi-definite matrix)。\n\\(x=\\left( \\begin{array}{} x_1\\\\ x_2\\\\ x_3 \\end{array} \\right), \\ A=\\left( \\begin{array}{} 5 \u0026amp; 2 \u0026amp; 4\\\\ 2 \u0026amp; 2 \u0026amp; 3\\\\ 4 \u0026amp; 3 \u0026amp; 25 \\end{array} \\right)\\)，2次型 \\(\\underline{x}^tA\\underline{x}\\) 是正定。因爲：\n\\(\\underline{x}^tA\\underline{x}=5x_1^2+2x_2^2+25x_3^2\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;+4x_1x_2+8x_1x_3+6x_2x_3\\\\\\;\\;\\;\\;\\;\\;\\;\\;\\;\\:=(2x_1+x_2)^2+(x_2+3x_3)^2+(x_1+4x_3)^2\\\\ \\because \\underline{x}\\neq\\underline{0}=\\left( \\begin{array}{} 0\\\\ 0\\\\ 0 \\end{array} \\right)\\\\ \\therefore \\underline{x}^tA\\underline{x}\u0026gt;0\\)\n\\(x=\\left( \\begin{array}{} x_1\\\\ x_2\\\\ x_3 \\end{array} \\right), \\ A=\\left( \\begin{array}{} 5 \u0026amp; -6 \u0026amp; 3\\\\ -6 \u0026amp; 25 \u0026amp; 32\\\\ 3 \u0026amp; 32 \u0026amp; 73 \\end{array} \\right)\\)，2次型 \\(\\underline{x}^tA\\underline{x}\\) 是半正定。因爲：\n\\(\\underline{x}^tA\\underline{x}=5x_1^2+25x_2^2+73x_3^2\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;-12x_1x_2+6x_2x_3+64x_1x_3\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\:=(2x_1-3x_3)^2+(x_1+3x_3)^2+(4x_2+8x_3)^2\\\\\\)\n\\(\\because \\underline{x}=\\left( \\begin{array}{c} 3\\\\ 2\\\\ -1 \\end{array} \\right)\\) 時 \\(\\underline{x}^tA\\underline{x}=0\\)\n\\(\\therefore \\underline{x}^tA\\underline{x} \\geqslant0\\)\n\\(\\underline{x}=\\left( \\begin{array}{c} x_1\\\\ x_2\\\\ \\vdots\\\\ x_n \\end{array} \\right)(\\neq\\underline{0})\\) 與單位矩陣 \\(E_n\\) 構成的2次型 \\(\\underline{x}^tE_n\\underline{x}=\\underline{x}^t\\underline{x}=\\sum\\limits_{i=1}^nx_i^2\u0026gt;0\\) 是爲正定。\n\\(\\underline{x}=\\left( \\begin{array}{c} x_1\\\\ x_2\\\\ \\vdots\\\\ x_n \\end{array} \\right), \\underline{\\frac{1}{n}}=\\left( \\begin{array}{c} \\frac{1}{n}\\\\ \\frac{1}{n}\\\\ \\vdots \\\\ \\frac{1}{n} \\end{array} \\right)\\) 已知，\\(\\underline{x}\\underline{1/n}=\\sum\\limits_{i=1}^nx_i\\cdot \\frac{1}{n}=\\bar{x}\\) (\\(\\underline{x}\\) 的平均值)，包含了 \\(n\\) 個 \\(\\bar{x}\\) 的橫向量： \\((\\bar{x}，\\bar{x},\\cdots,\\bar{x})\\) 展開以後成爲：\n\\((\\bar{x}，\\bar{x},\\cdots,\\bar{x})\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\:=(x_1, x_2, \\cdots, x_n)\\left( \\begin{array}{c} \\frac{1}{n} \u0026amp; \\frac{1}{n} \u0026amp; \\cdots \u0026amp; \\frac{1}{n}\\\\ \\frac{1}{n} \u0026amp; \\frac{1}{n} \u0026amp; \\cdots \u0026amp; \\frac{1}{n}\\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\frac{1}{n} \u0026amp; \\frac{1}{n} \u0026amp; \\cdots \u0026amp; \\frac{1}{n} \\end{array} \\right)\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\:=\\underline{x}U\\)\n令 \\(U\\) 代表上面第二個等式中有半部分的矩陣。那麼將之從右往左乘以 \\(\\underline{x}\\) 我們可以得到：\n\\(\\underline{x}^tU\\underline{x}=(\\bar{x},\\bar{x},\\cdots,\\bar{x})\\underline{x}=(\\bar{x},\\bar{x},\\cdots,\\bar{x})\\left( \\begin{array}{c} x_1\\\\ x_2\\\\ \\vdots\\\\ x_n \\end{array} \\right)\\\\ =\\sum\\limits_{i=1}^n\\bar{x}x_i=\\bar{x}\\sum\\limits_{i=1}^nx_i=n\\bar{x}^2\\)\n利用上面的式子，我們可以得到，偏差平方和(sum of squared deviation, SS)：\\(E_n\\) 爲 \\(n\\) 次單位矩陣。\n\\(SS=\\sum\\limits_{i=1}^n(x_i-\\bar{x})^2\\\\ \\;\\;\\;\\;\\:=\\sum\\limits_{i=1}^n(x_i^2-2x_i\\bar{x}+\\bar{x}^2)\\\\ \\;\\;\\;\\;\\:=\\sum\\limits_{i=1}^nx_i^2-2\\bar{x}\\sum\\limits_{i=1}^nx_i+n\\bar{x}^2\\\\ \\;\\;\\;\\;\\:=\\sum\\limits_{i=1}^nx_i^2-2n\\bar{x}^2+n\\bar{x}^2\\\\ \\;\\;\\;\\;\\:=\\sum\\limits_{i=1}^nx_i^2-n\\bar{x}^2\\\\ \\;\\;\\;\\;\\:=\\sum\\limits_{i=1}^nx_i^2-\\underline{x}^tU\\underline{x}\\\\ \\;\\;\\;\\;\\:=\\underline{x}^t\\underline{x}-\\underline{x}^tU\\underline{x}\\\\ \\;\\;\\;\\;\\:=\\underline{x}^tE_n\\underline{x}-\\underline{x}^tU\\underline{x}\\\\ \\;\\;\\;\\;\\:=\\underline{x}^t(E_n-U)\\underline{x}\\\\ \\because when \\underline{x}=\\left( \\begin{array}{c} x_1\\\\ x_2\\\\ \\vdots\\\\ x_n \\end{array} \\right)=\\left( \\begin{array}{c} \\bar{x}\\\\ \\bar{x}\\\\ \\vdots\\\\ \\bar{x} \\end{array} \\right), \\\u0026amp; (\\bar{x}\\neq0), SS=0\\\\ \\therefore \\underline{x}^t(E_n-U)\\underline{x}\\;是半正定2次型。\\)\n雙一次型 對於 \\(\\underline{x}=\\left( \\begin{array}{c} x_1\\\\ x_2\\\\ \\vdots\\\\ x_m \\end{array} \\right), A=\\left( \\begin{array}{c} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n}\\\\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n}\\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots\\\\ a_{m1} \u0026amp; a_{m2} \u0026amp; \\cdots \u0026amp; a_{mn} \\end{array} \\right), \\underline{y}=\\left( \\begin{array}{c} y_1\\\\ y_2\\\\ \\vdots\\\\ y_n \\end{array} \\right)\\) 來說，\\(\\underline{x}^tA\\underline{y}=\\sum\\limits_{i=1}^m\\sum\\limits_{j=1}^na_{ij}x_iy_j\\) 既是 \\(\\underline{x}\\) 的1次型，也是 \\(\\underline{y}\\) 的1次型，所以又叫做 \\(\\underline{x}\\underline{y}\\) 的雙1次型(bilinear form)。雙1次型是一個標量(scalar)。\n對於 \\(\\underline{x}=\\left( \\begin{array}{c} x_1\\\\ x_2\\\\ x_3 \\end{array} \\right), B=(b_{ij})=\\left( \\begin{array}{c} 1 \u0026amp; 0 \u0026amp; 0\\\\ 0 \u0026amp; 1 \u0026amp; 1\\\\ 1 \u0026amp; 0 \u0026amp; 1 \\end{array} \\right), \\underline{y}=\\left( \\begin{array}{c} y_1\\\\ y_2\\\\ y_3 \\end{array} \\right)\\)\n\\(\\underline{x}^tB\\underline{y}=\\sum\\limits_{i=1}^3\\sum\\limits_{j=1}^3b_{ij}x_iy_j=(x_1+x_3, x_2, x_2+x_3)\\left( \\begin{array}{c} y_1\\\\ y_2\\\\ y_3 \\end{array} \\right)\\\\ \\;\\;\\;\\;\\:=x_1y_1+x_2y_2+x_2y_3+x_3y_1+x_3y_3\\\\ \\;\\;\\;\\;\\:=x_1y_1+x_2(y_2+y_3)+x_3(y_1+y_3) \\; \\bf (\\underline{x} 的1次型)\\) \\(\\;\\;\\;\\;\\;\\:=y_1(x_1+x_3)+x_2y_2+(x_2+x_3)y_3 \\;\\bf (\\underline{y} 的1次型)\\)\n對於 \\(\\underline{l}=\\left( \\begin{array}{c} l_1\\\\ l_2\\\\ \\end{array} \\right), T=\\left( \\begin{array}{c} t_{11} \u0026amp; t_{12} \u0026amp; t_{13}\\\\ t_{21} \u0026amp; t_{22} \u0026amp; t_{23}\\\\ \\end{array} \\right), \\underline{m}=\\left( \\begin{array}{c} m_1\\\\ m_2\\\\ m_3 \\end{array} \\right)\\) \\(\\underline{l}^tT\\underline{m}=\\sum\\limits_{i=1}^2\\sum\\limits_{j=1}^3t_{ij}l_im_j\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=l_1t_{11}m_1+l_1t_{12}m_2+l_1t_{13}m_3\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;+l_2t_{21}m_1+l_2t_{22}m_2+l_3t_{23}m_3\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=l_1(t_{11}m_1+t_{12}m_2+t_{13}m_3)\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;+l_2(t_{21}m_1+t_{22}m_2+t_{23}m_3) \\;\\;(\\underline{l} \\textbf{的1次型})\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=(t_{11}l_1+t_{21}l_2)m_1+(t_{12}l_1+t_{22}l_2)m_2\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;+(t_{13}l_1+t_{23}l_2)m_3\\;\\;(\\underline{m} \\textbf{的1次型})\\)\n","date":1489363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1489363200,"objectID":"d368e48b5d95133e97f244ab9d410607","permalink":"https://wangcc.me/post/2017-03-13/","publishdate":"2017-03-13T00:00:00Z","relpermalink":"/post/2017-03-13/","section":"post","summary":"第4章　矩陣 matrix P70-72","tags":["learning notes","basic mathematics","linear algebra"],"title":"「統計解析のための線形代数」復習筆記17","type":"post"},{"authors":null,"categories":["statistics"],"content":" 二次型(形式) 對於 \\(\\underline{x}=\\left( \\begin{array}{c} x_{1}\\\\ x_{2}\\\\ \\vdots\\\\ x_{n} \\end{array} \\right), A=\\left( \\begin{array}{c} a_{11}\u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n}\\\\ a_{21}\u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n}\\\\ \\vdots\u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots\\\\ a_{n1}\u0026amp; a_{n2} \u0026amp; \\cdots \u0026amp; a_{nn} \\end{array} \\right)\\) 那麼：\n\\(\\underline{x}^tA\\underline{x}=\\sum\\limits_{i=1}^n\\sum\\limits_{j=1}^na_{ij}x_ix_j\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\:\\:=\\sum\\limits_{i=1}^na_{ii}x_i^2+\\mathop{\\sum\\limits^n\\sum\\limits^n}_{i \\neq j}a_{ij}x_ix_j\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\:\\:=\\sum\\limits_{i=1}^na_{ii}x_i^2+\\mathop{\\sum\\limits^n\\sum\\limits^n}_{i\\ \u0026lt;\\ j}(a_{ij}+a_{ji})x_ix_j\\)\n被稱爲 \\(\\underline{x}\\) 的同次2次式。又被叫做關於 \\(x_1,x_2,\\cdots,x_n\\) 的2次型(quadratic form)。特別的，當 \\(A\\) 爲對稱矩陣時的2次型：\\(\\underline{x}^tA\\underline{x}=\\sum\\limits_{i=1}^na_{ii}x_i^2+2\\mathop{\\sum\\limits^n\\sum\\limits^n}_{i\\ \u0026lt;\\ j}a_{ij}x_ix_j\\) 在多元變量分析中十分重要。\n\\(x=\\left( \\begin{array}{} x_1\\\\ x_2 \\end{array} \\right),\\ A=\\left( \\begin{array}{} a_{11} \u0026amp; a_{12}\\\\ a_{12} \u0026amp; a_{22} \\end{array} \\right)\\), 那麼： \\(\\underline{x}^tA\\underline{x}=a_{11}x_1^2+a_{22}x_2^2+2a_{12}x_1x_2\\)\n\\(x=\\left( \\begin{array}{} x_1\\\\ x_2\\\\ x_3 \\end{array} \\right),\\ A=\\left( \\begin{array}{} a_{11} \u0026amp; a_{12} \u0026amp; a_{13}\\\\ a_{12} \u0026amp; a_{22} \u0026amp; a_{23}\\\\ a_{13} \u0026amp; a_{23} \u0026amp; a_{33} \\end{array} \\right)\\)，那麼： \\(\\underline{x}^tA\\underline{x}=a_{11}x_1^2+a_{22}x_2^2+a_{33}x_3^2+2a_{12}x_1x_2\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;+2a_{13}x_1x_3+2a_{23}x_2x_3\\)\n\\(x=\\left( \\begin{array}{} x_1\\\\ x_2\\\\ x_3 \\end{array} \\right), \\ A=\\left( \\begin{array}{} 3 \u0026amp; 2 \u0026amp; 4\\\\ 6 \u0026amp; 5 \u0026amp; 1\\\\ -2 \u0026amp; 5 \u0026amp; 8 \\end{array} \\right)非對稱矩陣\\)，那麼：\n\\(\\underline{x}^tA\\underline{x}=3x_1^2+5x_2^2+8x_3^2\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;+2x_1x_2+4x_1x_3+6x_2x_1\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;+x_2x_3-2x_3x_1+5x_3x_2\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\:=3x_1^2+5x_2^2+8x_3^2\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;+(2+6)x_1x_2+(4-2)x_1x_3\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;+(5+1)x_2x_3\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\:=3x_1^2+5x_2^2+8x_3^2\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;+8x_1x_2+2x_1x_3+6z_2x_3\\)\n有式子 \\(3x_1^2-5x_2^2+7x_3^2+8x_1x_2+4x_1x_3-12x_2x_3\\) 如果要將它改寫成 \\(\\underline{x}^tA\\underline{x}\\) (\\(A\\)是對稱矩陣) 的話，試求 \\(A=\\left( \\begin{array}{} a_{11} \u0026amp; a_{12} \u0026amp; a_{13}\\\\ a_{22} \u0026amp; a_{22} \u0026amp; a_{23}\\\\ a_{33} \u0026amp; a_{32} \u0026amp; a_{33} \\end{array} \\right)=\\left( \\begin{array}{} a_{11} \u0026amp; a_{12} \u0026amp; a_{13}\\\\ a_{12} \u0026amp; a_{22} \u0026amp; a_{23}\\\\ a_{13} \u0026amp; a_{23} \u0026amp; a_{33} \\end{array} \\right)\\) 的各個成分。\n\\(A\\) 的對角成分：\\(a_{11},a_{22},a_{33}\\) 分別是 \\(x_1^2,x_2^2,x_3^2\\) 的系數： \\(3,-5,7\\)。非對角成分 \\(a_{12}(=a_{21})\\) 等於 \\(x_1x_2\\) 系數的一半：\\(4\\)，\\(a_{13}(=a_{31})\\) 等於 \\(x_1x_3\\) 系數的一半:\\(2\\), \\(a_23(=a_{32})\\) 等於 \\(x_2x_3\\) 系數的一半：\\(-6\\)。\n\\(\\therefore A=\\left( \\begin{array}{} 3 \u0026amp; 4 \u0026amp; 2\\\\ 4 \u0026amp; -5 \u0026amp; -6\\\\ 2 \u0026amp; -6 \u0026amp; 7 \\end{array} \\right)\\)\n","date":1489190400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1489190400,"objectID":"a72b532768213de4ee3d0b654bf69948","permalink":"https://wangcc.me/post/2017-03-11/","publishdate":"2017-03-11T00:00:00Z","relpermalink":"/post/2017-03-11/","section":"post","summary":"第4章　矩陣 matrix P68-69","tags":["learning notes","basic mathematics","linear algebra"],"title":"「統計解析のための線形代数」復習筆記16","type":"post"},{"authors":null,"categories":["statistics"],"content":" 單位矩陣 對角成分全部都是 \\(1\\) (此時我們假定有 \\(n\\) 個)，的對角矩陣被叫做單位矩陣(identity matrix, unit matrix)。寫作： \\(\\left( \\begin{array}{c} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\ddots \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; 1 \\end{array} \\right)=E_n=I_n\\) 下標 \\(n\\) 常被省略。一般的，將 \\(E_n\\) 從左往右乘以 \\(n\\) 次正方形矩陣 \\(A\\)，的結果和從右往左相乘的結果是相等的： \\(E_nA=AE_n=A\\)。\n單位矩陣 \\(E=\\left( \\begin{array}{c} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\\\ \\end{array} \\right)\\) 和矩陣 \\(A=\\left( \\begin{array}{c} a_1 \u0026amp; a_2 \u0026amp; a_3 \\\\ b_1 \u0026amp; b_2 \u0026amp; b_3 \\\\ c_1 \u0026amp; c_2 \u0026amp; c_3 \\\\ \\end{array} \\right)\\) 的積爲：\\(EA=\\left( \\begin{array}{c} a_1 \u0026amp; a_2 \u0026amp; a_3 \\\\ b_1 \u0026amp; b_2 \u0026amp; b_3 \\\\ c_1 \u0026amp; c_2 \u0026amp; c_3 \\\\ \\end{array} \\right)=AE=A\\)，矩陣 \\(A\\) 的所有成分均不變。\n\\(E_nE_n=E_n\\)。像這樣，自己與自己相乘，結果等於自己的矩陣，被叫做冪等矩陣(idempotent matrix, 冪等行列「べきとうぎょうれつ」)。即，\\(HH(=H^2)=H\\) 成立時，\\(H\\) 是冪等矩陣。\n\\(\\underline{x}=E\\underline{x}, \\lambda\\underline{x}=\\lambda E\\underline{x}\\) 此等式會在後面特徵值(eigenvalue, 固有値問題)時使用。\n前一個小節中的對角矩陣(diagonal matrix) \\(D^{\\frac{1}{2}}\\) 則具有這樣的性質： \\(D^{\\frac{1}{2}}D^{-\\frac{1}{2}}=D^{-\\frac{1}{2}}D^{\\frac{1}{2}}=E_n\\)\n逆矩陣 inverse matrix 如果正方形矩陣 \\(A\\) 存在另一個正放心矩陣 \\(X\\) 使得他們滿足 \\(AX=XA=E\\)，即乘積爲一個單位矩陣，那麼我們說 \\(X\\) 是 \\(A\\) 的逆矩陣(inverse matrix)，寫作：\\(A^{-1}\\)。可以將上面的連等式改成：\\(AA^{-1}=A^{-1}A=E\\)。\n如果矩陣 \\(A=\\left( \\begin{array}{c} a \u0026amp; b \\\\ c \u0026amp; d \\\\ \\end{array} \\right)\\) 的成分滿足： \\(ad -bc \\neq 0\\)，那麼有 \\(A^{-1}=\\frac{1}{ad-bc}\\left( \\begin{array}{c} d \u0026amp; -b \\\\ -c \u0026amp; a \\\\ \\end{array} \\right)\\)。如果， \\(ad-bc=0\\) 那麼我們認爲 \\(A\\) 的逆矩陣不存在。\n矩陣 \\(P=\\left( \\begin{array}{c} \\cos \\theta \u0026amp; -\\sin \\theta \\\\ \\sin \\theta \u0026amp; \\cos \\theta \\\\ \\end{array} \\right)\\) 的逆矩陣 \\(P^{-1}=\\left( \\begin{array}{c} \\cos \\theta \u0026amp; \\sin \\theta \\\\ -\\sin \\theta \u0026amp; \\cos \\theta \\\\ \\end{array} \\right)\\) 注意此處出現了逆矩陣的逆矩陣爲元矩陣的例子。\n對稱矩陣(symmetric matrix) \\(A=\\left( \\begin{array}{c} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 4 \u0026amp; 5 \\\\ 3 \u0026amp; 5 \u0026amp; 6 \\\\ \\end{array} \\right)\\) 的逆矩陣 \\(A^{-1}=\\left( \\begin{array}{c} 1 \u0026amp; -3 \u0026amp; 2 \\\\ -3 \u0026amp; 3 \u0026amp; -1 \\\\ 2 \u0026amp; -1 \u0026amp; 0 \\\\ \\end{array} \\right)\\) 注意此處出現了對稱矩陣的逆矩陣還是對稱矩陣的例子。\n矩陣 \\(A=\\left( \\begin{array}{c} -11 \u0026amp; 2 \u0026amp; 2 \\\\ -4 \u0026amp; 0 \u0026amp; 1 \\\\ 6 \u0026amp; -1 \u0026amp; -1 \\\\ \\end{array} \\right)\\) 的逆矩陣 \\(A^{-1}=\\left( \\begin{array}{c} 1 \u0026amp; 0 \u0026amp; 2 \\\\ 2 \u0026amp; -1 \u0026amp; 3 \\\\ 4 \u0026amp; 1 \u0026amp; 8 \\\\ \\end{array} \\right)\\)\n正交矩陣 orthogonal matrix 如果正方形矩陣 \\(P\\) 滿足： \\(PP^t=P^tP=E\\) (單位矩陣)；或者滿足 \\(P^t=P^{-1}\\) 時，我們說這個正方形矩陣 \\(P\\) 爲正交矩陣(orthogonal matrix，直交行列「ちょっこうぎょうれつ」)。正交矩陣如果用列向量來表示，那麼這些組成正交矩陣的列向量被稱爲規範正交系(orthonomal system，正規直交系)\n矩陣 \\(P=\\left( \\begin{array}{c} \\cos \\theta \u0026amp; -\\sin \\theta \\\\ \\sin \\theta \u0026amp; \\cos \\theta \\\\ \\end{array} \\right)\\) 是2次的正交矩陣。如果 \\(\\underline{p}_1=\\left( \\begin{array}{c} \\cos \\theta \\\\ \\sin \\theta \\\\ \\end{array} \\right), \\; \\underline{p}_2=\\left( \\begin{array}{c} -\\sin \\theta \\\\ \\cos \\theta \\\\ \\end{array} \\right)\\)，那麼列向量的長度有：\\(\\| \\underline{p}_1 \\|=\\| \\underline{p}_2 \\|=1\\)，\\(\\underline{p}_1\\cdot\\underline{p}_2=0\\)。因此組成矩陣 \\(P\\) 的兩個列向量 \\(\\underline{p}_1,\\underline{p}_2\\) 構成了一個規範正交系。\n矩陣 \\(P=\\left( \\begin{array}{c} \\frac{1}{\\sqrt{3}} \u0026amp; \\frac{1}{\\sqrt{2}} \u0026amp; \\frac{1}{\\sqrt{6}} \\\\ \\frac{1}{\\sqrt{3}} \u0026amp; -\\frac{1}{\\sqrt{2}} \u0026amp; \\frac{1}{\\sqrt{6}} \\\\ \\frac{1}{\\sqrt{3}} \u0026amp; 0 \u0026amp; -\\frac{2}{\\sqrt{6}} \\\\ \\end{array} \\right)\\) 是個3次正交矩陣。如果 \\(\\underline{p}_1=\\left( \\begin{array}{c} \\frac{1}{\\sqrt{3}}\\\\ \\frac{1}{\\sqrt{3}}\\\\ \\frac{1}{\\sqrt{3}}\\\\ \\end{array} \\right), \\underline{p}_2=\\left( \\begin{array}{c} \\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} \\\\ 0 \\\\ \\end{array} \\right), \\underline{p}_3=\\left( \\begin{array}{c} \\frac{1}{\\sqrt{6}}\\\\ \\frac{1}{\\sqrt{6}}\\\\ -\\frac{2}{\\sqrt{6}}\\\\ \\end{array} \\right)\\) 這三個列向量構成了一個規範正交系。\n三角矩陣 triangular matrix 主對角線的左下部分全部爲 \\(0\\) 的正方形矩陣被叫做：上三角矩陣(upper triangular matrix)，右上部分的成分全部爲 \\(0\\) 的正方形矩陣被叫做： 下三角矩陣(lower triangular matrix)。上三角矩陣，下三角矩陣，統稱爲三角矩陣。有時候左下部分或者右上部分就簡略的只寫一個大的 \\(O\\)。\n類型相同的兩個上三角矩陣的積依然是一個上三角矩陣。兩個類型相同的下三角矩陣的積也依然是一個下三角矩陣。\n\\[ 上三角矩陣： \\left( \\begin{array}{c} a_{11}\u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n}\\\\ 0 \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n}\\\\ \\vdots\u0026amp; \\ddots \u0026amp; \\ddots \u0026amp; \\vdots\\\\ 0 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; a_{nn} \\end{array} \\right) =\\left( \\begin{array}{c} a_{11}\u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n}\\\\ \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n}\\\\ \u0026amp; \u0026amp; \\ddots \u0026amp; \\vdots\\\\ \\Huge{0} \u0026amp; \u0026amp; \u0026amp; a_{nn} \\end{array} \\right)\\]\n\\[ 下三角矩陣：\\left( \\begin{array}{c} a_{11}\u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ a_{21}\u0026amp; a_{22} \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\vdots\u0026amp; \\cdots \u0026amp; \\ddots \u0026amp; 0\\\\ a_{n1}\u0026amp; a_{n2} \u0026amp; \\cdots \u0026amp; a_{nn} \\end{array} \\right)=\\left( \\begin{array}{c} a_{11}\u0026amp; \u0026amp;\u0026amp;\\Huge{0} \\\\ a_{21}\u0026amp; a_{22} \u0026amp; \\\\ \\vdots\u0026amp; \\cdots \u0026amp; \\ddots \u0026amp;\\\\ a_{n1}\u0026amp; a_{n2} \u0026amp; \\cdots \u0026amp; a_{nn} \\end{array} \\right)\\]\n\\(\\left( \\begin{array}{c} 2 \u0026amp; 1 \\\\ 0 \u0026amp; 8 \\\\ \\end{array} \\right), \\left( \\begin{array}{c} -3 \u0026amp; 0 \u0026amp; 6 \\\\ 0 \u0026amp; 5 \u0026amp; 2 \\\\ 0 \u0026amp; 0 \u0026amp; 4 \\\\ \\end{array} \\right), \\left( \\begin{array}{c} 5 \u0026amp; -6 \u0026amp; 3 \u0026amp; 2 \\\\ 0 \u0026amp; 9 \u0026amp;-2 \u0026amp; 4 \\\\ 0 \u0026amp; 0 \u0026amp; 3 \u0026amp; 7 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{array} \\right)\\) 這些都是上三角矩陣。\n\\(\\left( \\begin{array}{c} 2 \u0026amp; 0 \\\\ 5 \u0026amp; 8 \\\\ \\end{array} \\right), \\left( \\begin{array}{c} -3 \u0026amp; 0 \u0026amp; 0 \\\\ 8 \u0026amp; 5 \u0026amp; 0 \\\\ 7 \u0026amp; 2 \u0026amp; 4 \\\\ \\end{array} \\right), \\left( \\begin{array}{c} 5 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 2 \u0026amp; 9 \u0026amp; 0 \u0026amp; 0 \\\\ 3 \u0026amp; 10 \u0026amp; 3 \u0026amp; 0 \\\\ 5 \u0026amp; 1 \u0026amp; 34 \u0026amp; 0 \\end{array} \\right)\\) 這些都是下三角矩陣。\n階梯形矩陣 echelon matrix 如下所示，第1行，第2行，第3行，行數增加的同時，左側的成分中 \\(0\\) 的個數跟着增加的矩陣被叫做階梯形矩陣(echelon matrix)。 \\(\\#\\) 表示非 \\(0\\) 的數， \\(*\\) 表示任意數。\\(\\#\\) 的個數，或者說此矩陣的非零向量的個數被定義爲這個矩陣的階數 (rank)。階梯形矩陣的階數記爲： \\(rank(A)\\)。零矩陣 \\(O\\) 的階數： \\(rank(O)=0\\)。\n\\[\\left( \\begin{array}{c} \\# \u0026amp; * \u0026amp; * \u0026amp; * \u0026amp; * \u0026amp; * \u0026amp; * \u0026amp; * \u0026amp; *\\\\ 0 \u0026amp; \\# \u0026amp; * \u0026amp; * \u0026amp; * \u0026amp; * \u0026amp; * \u0026amp; * \u0026amp; * \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\# \u0026amp; * \u0026amp; * \u0026amp; * \u0026amp; * \u0026amp; * \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\# \u0026amp; * \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\end{array} \\right)\\]\n\\(A=\\left( \\begin{array}{c} 2 \u0026amp; 5 \u0026amp; 6 \u0026amp; 9\\\\ 0 \u0026amp; 5 \u0026amp; -1 \u0026amp; 4\\\\ 0 \u0026amp; 0 \u0026amp; 5 \u0026amp; 0\\\\ \\end{array} \\right)， rank(A)=3\\)\n\\(B=\\left( \\begin{array}{c} 4 \u0026amp; 0 \u0026amp; 6 \u0026amp; 0\\\\ 0 \u0026amp; 5 \u0026amp; 0 \u0026amp; 4\\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 5\\\\ \\end{array} \\right)， rank(B)=3\\)\n\\(C=\\left( \\begin{array}{c} 2 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; -7 \u0026amp; 4\\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; -1\\\\ \\end{array} \\right)， rank(C)=3\\)\n\\(D=\\left( \\begin{array}{c} 4 \u0026amp; 0 \u0026amp; 6 \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 4\\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0\\\\ \\end{array} \\right), rank(D)=2\\)\n\\(F=\\left( \\begin{array}{c} 0 \u0026amp; 2 \u0026amp; 0 \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0\\\\ \\end{array} \\right), rank(F)=1\\)\n\\(O=\\left( \\begin{array}{c} 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0\\\\ \\end{array} \\right), rank(O)=0\\)\n","date":1488931200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488931200,"objectID":"8ae1fa0cbd5f8d0e5e11514f739faf6c","permalink":"https://wangcc.me/post/2017-03-08/","publishdate":"2017-03-08T00:00:00Z","relpermalink":"/post/2017-03-08/","section":"post","summary":"第4章　矩陣 matrix P63-68","tags":["learning notes","basic mathematics","linear algebra"],"title":"「統計解析のための線形代数」復習筆記15","type":"post"},{"authors":["Masaaki Matsunaga","Hiroshi Yatsuya","Hiroyasu Iso","Kentaro Yamashita","Yuanying Li","Kazumasa Yamagishi","Naohito Tanabe","Yasuhiko Wada","Chaochen Wang","Atsuhiko Ota","Koji Tamakoshi","Akiko Tamakoshi","The JACC Study Group"],"categories":null,"content":"Abstract Background and aims Coronary heart disease (CHD) and stroke have common risk factors, but some of these differ in the magnitude or direction of associations between CHD and stroke. We assessed whether the impact of each risk factor differed between CHD and stroke mortality in Asians.\nMethods In total, 104,910 subjects aged 40–79 years without histories of cancer, CHD and stroke at baseline were followed between 1988 and 2009. Competing-risks analysis was used to test for differences in the associations of each risk factor with two endpoints (CHD and stroke). Population attributable fractions (PAFs) were also calculated for these endpoints to estimate the population impact of each risk factor.\nResults During a median 19.1-year follow-up, 1554 died from CHD and 3163 from stroke. The association of hypertension with CHD was similar to that with stroke in terms of the magnitude and direction (multivariable-adjusted hazard ratio for CHD: 1.63 vs. stroke: 1.73 in men and 1.70 vs. 1.66 in women). Conversely, the magnitude of these associations differed for smoking (CHD: 1.95 vs. stroke: 1.23 in men and 2.45 vs. 1.35 in women) and diabetes (1.49 vs. 1.09 in men and 2.08 vs. 1.39 in women). The highest PAF for CHD was caused by smoking in men and by hypertension in women. That for stroke was caused by hypertension in both sexes.\nConclusions Hypertension associations and PAFs were consistent between CHD and stroke, but not for other risk factors. These findings may be useful for optimizing public health intervention strategies.\n","date":1488758400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488758400,"objectID":"d5774fbb0a926b1e3fe87a830569d032","permalink":"https://wangcc.me/publication/journal-article/jaccmatsunaga/","publishdate":"2017-03-06T00:00:00Z","relpermalink":"/publication/journal-article/jaccmatsunaga/","section":"publication","summary":"Abstract Background and aims Coronary heart disease (CHD) and stroke have common risk factors, but some of these differ in the magnitude or direction of associations between CHD and stroke. We assessed whether the impact of each risk factor differed between CHD and stroke mortality in Asians.\nMethods In total, 104,910 subjects aged 40–79 years without histories of cancer, CHD and stroke at baseline were followed between 1988 and 2009. Competing-risks analysis was used to test for differences in the associations of each risk factor with two endpoints (CHD and stroke).","tags":null,"title":"Similarities and differences between coronary heart disease and stroke in the associations with cardiovascular risk factors: The Japan Collaborative Cohort Study","type":"publication"},{"authors":null,"categories":["statistics"],"content":" updated: 2017-03-07 對稱矩陣 Theorem 1 (symmetric matrix) 矩陣 \\(A\\) 如果完全和它的轉置矩陣 \\(A^t\\) 相同，即：\\(A=A^t\\) 成立時，這樣的正方形矩陣被稱爲對稱矩陣(symmetric matrix)。對稱矩陣的成分是以主對角線(main diagonal)對稱的。 \\(A=\\left( \\begin{array}{c} 4 \u0026amp; 3 \u0026amp; 2 \u0026amp; 1 \\\\ 3 \u0026amp; 5 \u0026amp; 6 \u0026amp; 7 \\\\ 2 \u0026amp; 6 \u0026amp; 8 \u0026amp; 9 \\\\ 1 \u0026amp; 7 \u0026amp; 9 \u0026amp; 0 \\end{array} \\right)\\) 是典型的4次對稱矩陣。 數學 物理 化學 數學 \\(1\\) \\(0.72\\) \\(0.62\\) 物理 \\(0.72\\) \\(1\\) \\(-0.55\\) 化學 \\(0.62\\) \\(-0.55\\) \\(1\\) 上表是幾名學生的數學，物理，化學成績得分的相關系數。\n如果提取出數字的部分，左右用圓括號括起來，會得到這樣一個矩陣：\\(R=\\left( \\begin{array}{c} 1 \u0026amp; 0.72 \u0026amp; 0.62 \\\\ 0.72 \u0026amp; 1 \u0026amp; -0.55 \\\\ 0.62 \u0026amp; -0.55 \u0026amp; 1 \\\\ \\end{array} \\right)\\) 這樣類型的矩陣被特別的稱爲相關矩陣(correlation matrix)。類似相關矩陣這樣的明確爲對稱矩陣的情況下，常常像下面這樣簡略的記左下或者右上部分： \\[\\left( \\begin{array}{c} 1 \u0026amp; \u0026amp; \\\\ 0.72 \u0026amp; 1 \u0026amp; \\\\ 0.62 \u0026amp; -0.55 \u0026amp; 1 \\\\ \\end{array} \\right)， \\left( \\begin{array}{c} 1 \u0026amp; 0.72 \u0026amp; 0.62 \\\\ \u0026amp; 1 \u0026amp; -0.55 \\\\ \u0026amp; \u0026amp; 1 \\\\ \\end{array} \\right)\\]\n下面的對稱矩陣，對角成分是方差(variance, 分散)，非對角成分是協方差(covariance, 共分散)，被稱爲方差協方差矩陣(variance-covariance matrix, 分散共分散行列)。\n\\[\\sum=\\left( \\begin{array}{c} \\sigma_{1}^2 \u0026amp; \\sigma_{2} \u0026amp; \\cdots \u0026amp; \\sigma_{1n} \\\\ \\sigma_{12} \u0026amp; \\sigma_{2}^2 \u0026amp; \\cdots \u0026amp; \\sigma_{2n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\sigma_{1n} \u0026amp; \\sigma_{2} \u0026amp; \\cdots \u0026amp; \\sigma_{n}^2 \\end{array} \\right), S=\\left( \\begin{array}{c} s_{1}^2 \u0026amp; s_{2} \u0026amp; \\cdots \u0026amp; s_{1n} \\\\ s_{12} \u0026amp; s_{2}^2 \u0026amp; \\cdots \u0026amp; s_{2n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ s_{1n} \u0026amp; s_{2} \u0026amp; \\cdots \u0026amp; s_{n}^2 \\end{array} \\right)\\]\n矩陣\\(X=\\left( \\begin{array}{c} x_{11} \u0026amp; x_{12} \u0026amp; x_{13} \\\\ x_{21} \u0026amp; x_{22} \u0026amp; x_{23} \\\\ \\end{array} \\right)\\) ，那麼，\n\\(XX^t=\\left( \\begin{array}{c} x_{11} \u0026amp; x_{12} \u0026amp; x_{13} \\\\ x_{21} \u0026amp; x_{22} \u0026amp; x_{23} \\\\ \\end{array} \\right)\\left( \\begin{array}{c} x_{11} \u0026amp; x_{12} \\\\ x_{12} \u0026amp; x_{22} \\\\ x_{13} \u0026amp; x_{13} \\end{array} \\right)\\\\ \\;\\;\\;\\;\\;\\;\\;=\\left( \\begin{array}{c} x_{11}^2+x_{12}^2+x_{13}^2 \u0026amp; x_{11}x_{21}+x_{12}x_{22}+x_{13}x_{23} \\\\ x_{21}x_{11}+x_{22}x_{12}+x_{23}x_{13} \u0026amp; x_{21}^2+x_{22}^2+x_{23}^2 \\\\ \\end{array} \\right)\\) 是一個對稱矩陣。\n\\(X^tX=\\left( \\begin{array}{c} x_{11}^2+x_{21}^2 \u0026amp; x_{11}x_{12}+x_{21}x_{22} \u0026amp; x_{11}x_{13}+x_{21}x_{23} \\\\ x_{12}x_{11}+x_{22}x_{21} \u0026amp; x_{12}^2+x_{22}^2 \u0026amp; x_{12}x_{13}+x_{22}x_{23} \\\\ x_{13}x_{11}+x_{23}x_{21} \u0026amp; x_{13}x_{12}+x_{23}x_{22} \u0026amp; x_{13}^2+x_{23}^2 \\end{array} \\right)\\) 也是一個對稱矩陣。 且，他們的跡(trace)也是一樣的，均爲 \\(X\\) 各個成分的平方和： \\[tr(XX^t)=tr(X^tX)=x_{11}^2+x_{12}^2+x_{13}^2+x_{21}^2+x_{22}^2+x_{23}^2\\]\n對角矩陣 非對角成分(off-diagonal element)均爲零 \\(0\\) 的正方形矩陣被稱爲對角矩陣(diagonal matrix)。寫成如下形式：\n\\[\\left( \\begin{array}{c} a_{11} \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0\\\\ 0 \u0026amp; a_{22} \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; a_{33} \u0026amp; \\cdots \u0026amp; 0\\\\ 0 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; \\ddots \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; a_{nn} \\end{array} \\right)=D_n=\\Delta_n\\\\=diag(a_{11},a_{22},a_{33},\\cdots,a_{nn})\\]\n這樣的矩陣也常把左下部分右上部分的非對角成分用一個大的 \\(0\\) 來表示： \\[ \\left( \\begin{array}{ccccc} a_{11} \\\\ \u0026amp; a_{22} \u0026amp; \u0026amp; \\Huge0 \\\\ \u0026amp; \u0026amp; a_{33} \\\\ \u0026amp; \\Huge 0 \u0026amp; \u0026amp; \\ddots \\\\ \u0026amp; \u0026amp; \u0026amp; \u0026amp; a_{nn} \\end{array} \\right) \\]\n下面也是一個對角矩陣的例子： \\[\\left( \\begin{array}{c} \\sqrt{a_{11}} \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0\\\\ 0 \u0026amp; \\sqrt{a_{22}} \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; \\sqrt{a_{33}} \u0026amp; \\cdots \u0026amp; 0\\\\ 0 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; \\ddots \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; \\sqrt{a_{nn}} \\end{array} \\right)=D_n^{\\frac{1}{2}}=\\Delta_n^{\\frac{1}{2}}\\\\=diag(\\sqrt{a_{11}},\\sqrt{a_{22}},\\sqrt{a_{33}},\\cdots,\\sqrt{a_{nn}})\\]\n對角成分也可以是分母非零的分數： \\[\\left( \\begin{array}{c} 1/a_{11} \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0\\\\ 0 \u0026amp; 1/a_{22} \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; 1/a_{33} \u0026amp; \\cdots \u0026amp; 0\\\\ 0 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; \\ddots \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; 1/a_{nn} \\end{array} \\right)=D_n^{-1}=\\Delta_n^{-1}\\\\=diag(a_{11}^{-1},a_{22}^{-1},a_{33}^{-1},\\cdots,a_{nn}^{-1})\\]\n當然如下的例子也是對角矩陣，默認根號內爲正： \\[\\left( \\begin{array}{c} \\frac{1}{\\sqrt{a_{11}}} \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0\\\\ 0 \u0026amp; \\frac{1}{\\sqrt{a_{22}}} \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; \\frac{1}{\\sqrt{a_{33}}} \u0026amp; \\cdots \u0026amp; 0\\\\ 0 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; \\ddots \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; \\frac{1}{\\sqrt{a_{nn}}} \\end{array} \\right)=D_n^{-\\frac{1}{2}}=\\Delta_n^{-\\frac{1}{2}}\\\\=diag(\\frac{1}{\\sqrt{a_{11}}},\\frac{1}{\\sqrt{a_{22}}},\\frac{1}{\\sqrt{a_{33}}},\\cdots,\\frac{1}{\\sqrt{a_{nn}}})\\]\n當然，上述對角矩陣之間具有這樣的關系：\\(D_n^{\\frac{1}{2}}D_n^{\\frac{1}{2}}=D_n\\)，\\(D_n^{-\\frac{1}{2}}D_n^{-\\frac{1}{2}}=D_n^{-1}\\)。\n矩陣 \\(A\\) 或者向量 \\(\\underline{x}\\) 與對角矩陣 \\(D\\) 從左向右乘時，\\(DA, D\\underline{x}\\) 的第 \\(i\\) 行成分是：\\(A\\) 或 \\(\\underline{x}\\) 的第 \\(i\\) 行乘以 \\(D\\) 的第 \\((i,i)\\) 成分。例如： \\(D=\\left( \\begin{array}{c} \\lambda_1 \u0026amp; 0 \u0026amp; 0\\\\ 0 \u0026amp; \\lambda_2 \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; \\lambda_3\\\\ \\end{array} \\right), A=\\left( \\begin{array}{c} a_1 \u0026amp; a_2 \u0026amp; a_3 \\\\ b_1 \u0026amp; b_2 \u0026amp; b_3 \\\\ c_1 \u0026amp; c_2 \u0026amp; c_3 \\\\ \\end{array} \\right),\\\\ \\underline{x}=\\left( \\begin{array}{c} x_1\\\\ x_2\\\\ x_3\\\\ \\end{array} \\right)\\) 時，\\(DA=\\left( \\begin{array}{c} \\lambda_1a_1 \u0026amp; \\lambda_1a_2 \u0026amp; \\lambda_1a_3 \\\\ \\lambda_2b_1 \u0026amp; \\lambda_2b_2 \u0026amp; \\lambda_2b_3 \\\\ \\lambda_3c_1 \u0026amp; \\lambda_3c_2 \u0026amp; \\lambda_3c_3 \\\\ \\end{array} \\right) \\\\ D\\underline{x}=\\left( \\begin{array}{c} \\lambda_1x_1\\\\ \\lambda_2x_2\\\\ \\lambda_3x_3\\\\ \\end{array} \\right)\\)\n","date":1488326400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488326400,"objectID":"23f59719d8d80359906c8a6ba84d12a9","permalink":"https://wangcc.me/post/2017-03-01/","publishdate":"2017-03-01T00:00:00Z","relpermalink":"/post/2017-03-01/","section":"post","summary":"第4章　矩陣 matrix P60","tags":["learning notes","basic mathematics","linear algebra"],"title":"「統計解析のための線形代数」復習筆記14","type":"post"},{"authors":null,"categories":["statistics"],"content":" 連立一次方程式與矩陣向量的積 連立一次方程式可以改寫爲矩陣與向量的積形成的向量的形式。特別的，以連立方程式的系數作成分的矩陣被叫做系數矩陣(coefficient matrix)。當我們看到連立方程式，應該能立刻條件反射地聯想到其對應的矩陣和向量的積。\n\\(\\begin{align} \\left\\{ \\begin{array}{rr} a_1+2a_2+3a_3 = 3\\\\ 2a_1+4a_2+5a_3 = 5\\\\ 3a_1+5a_2+6a_3 = 7 \\end{array} \\right. \\end{align}\\) 可以改寫成 \\(\\left( \\begin{array}{c} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 4 \u0026amp; 5 \\\\ 3 \u0026amp; 5 \u0026amp; 6 \\end{array} \\right)\\left( \\begin{array}{c} a_1\\\\ a_2\\\\ a_3 \\end{array} \\right)=\\left( \\begin{array}{c} 3\\\\ 5\\\\ 7 \\end{array} \\right)\\) 的形式。\n如果把等號右邊的列向量寫到系數矩陣的右側，形成的矩陣被叫做擴大系數矩陣(augmented coefficient)：\n\\(\\left( \\begin{array}{c} 1 \u0026amp; 2 \u0026amp; 3 \u0026amp; 3 \\\\ 2 \u0026amp; 4 \u0026amp; 5 \u0026amp; 5 \\\\ 3 \u0026amp; 5 \u0026amp; 6 \u0026amp; 7 \\end{array} \\right)\\)\n\\(\\begin{align} \\left\\{ \\begin{array}{rr} a_{11}x_1+a_{12}x_2+a_{13}x_3=0\\\\ a_{21}x_1+a_{22}x_2+a_{23}x_3=0\\\\ \\end{array} \\right. \\end{align}\\) 可以改寫成 \\(\\left( \\begin{array}{c} a_{11} \u0026amp; a_{12} \u0026amp; a_{13} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; a_{23} \\\\ \\end{array} \\right)\\left( \\begin{array}{c} x_1\\\\ x_2\\\\ x_3 \\end{array} \\right)=\\left( \\begin{array}{c} 0\\\\ 0\\\\ \\end{array} \\right)\\)\n\\(\\begin{align} \\left\\{ \\begin{array}{rr} (5-\\lambda)x_1+ x_2+ x_3 = 0\\\\ x_1+(3-\\lambda)x_2+ x_3 = 0\\\\ x_1+ x_2+(3-\\lambda)x_3 = 0 \\end{array} \\right. \\end{align}\\) 可以改寫爲 \\(\\left( \\begin{array}{c} 5-\\lambda \u0026amp; 1 \u0026amp; 1 \\\\ 1 \u0026amp; 3-\\lambda \u0026amp; 1 \\\\ 1 \u0026amp; 1 \u0026amp; 3-\\lambda \\end{array} \\right)\\left( \\begin{array}{c} x_1\\\\ x_2\\\\ x_3 \\end{array} \\right)=\\left( \\begin{array}{c} 0\\\\ 0\\\\ 0 \\end{array} \\right)\\)\n矩形矩陣 列數行數不相等的矩陣，被稱爲矩形矩陣(rectangular matrix)。特別的行數 \\(\u0026gt;\\) 列數的矩陣被叫做垂直型矩形矩陣。行數 \\(\u0026lt;\\) 列數的矩陣被叫做水平型矩形矩陣。多元變量分析時，數據常常被加工稱爲垂直型矩形矩陣的形式。\n個体 体重 \\((kg)\\) 身長 \\((cm)\\) 安倍さん \\(53\\) \\(157\\) 伊藤さん \\(67\\) \\(172\\) 植村さん \\(49\\) \\(163\\) 江川さん \\(80\\) \\(178\\) 小野さん \\(74\\) \\(181\\) 5人的體重和身高數據被表示爲上面的表格： 如果只提取出表格中的數字寫成垂直型矩形矩陣： \\(\\left( \\begin{array}{c} 53 \u0026amp; 157 \\\\ 67 \u0026amp; 172 \\\\ 49 \u0026amp; 163 \\\\ 80 \u0026amp; 178 \\\\ 74 \u0026amp; 181 \\\\ \\end{array} \\right)\\) 正方形矩陣 行數和列數相等的矩陣被稱爲正方形矩陣(sqare matrix)。一個正方形的矩陣如果類型爲 \\((n,n)\\)，又被叫做是 \\(n\\) 次正方矩陣或者 \\(n\\) 次矩陣。\n\\[A_{n\\times n}= \\left( \\begin{array}{c} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ a_{n1} \u0026amp; a_{n2} \u0026amp; \\cdots \u0026amp; a_{nn} \\end{array} \\right)\\]\n從左上角往右下角方向劃一條對角線，這條對角線的名稱爲主對角線(main diagonal)。主對角線上有的成分 \\(a_{11},a_{22},\\cdots, a_{nn}\\)，被叫做對角成分(diagonal element)。其餘的成分被叫做非對角成分(off-diagonal element)。對角成分的和被叫做是該矩陣的跡(trace/spur)，寫作 \\(tr(A)=\\sum\\limits_{i=1}^na_{ii}\\) 。 \\(A=\\left( \\begin{array}{c} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 4 \u0026amp; 5 \u0026amp; 6 \\\\ 7 \u0026amp; 8 \u0026amp; 9 \\end{array} \\right)\\) 是 \\(3\\) 次正方形矩陣， \\(tr(A)=1+5+9=15\\)\n矩陣轉置 \\((m,n)\\) 型矩陣 \\(A_{m\\times n}=(a_{ij})\\) 的行與列互相對調，被叫做轉置(transpose)，形成的新 \\((n,m)\\) 型矩陣，被叫做 \\(A\\) 的轉置矩陣 (transposed matrix) ： \\((a_{ji})\\) 有多種標記方式：\\(A^t, A^\\prime, A^T, ^TA\\) 等，我們今後統一使用 \\(A^t\\)。轉置矩陣具有如下的性質：\n\\((A^t)^t=A\\) \\((AB)^t=B^tA^t\\) 注意： 不是\\(A^tB^t\\) \\((A+B)^t=A^t+B^t\\) \\((kA)^t=kA^t\\) (\\(k\\)爲標量 scalar) 練習 \\(A=\\left( \\begin{array}{c} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 4 \u0026amp; 5 \u0026amp; 6 \\\\ 7 \u0026amp; 8 \u0026amp; 9 \\end{array} \\right)\\) 的轉置矩陣爲：\\(A^t=\\left( \\begin{array}{c} 1 \u0026amp; 4 \u0026amp; 7 \\\\ 2 \u0026amp; 5 \u0026amp; 8 \\\\ 3 \u0026amp; 6 \u0026amp; 9 \\end{array} \\right)\\)\n\\(B=\\left( \\begin{array}{c} 1 \u0026amp; 2 \u0026amp; 3 \u0026amp; 4\\\\ 5 \u0026amp; 6 \u0026amp; 7 \u0026amp; 8\\\\ \\end{array} \\right)\\) 的轉置矩陣爲：\\(B^t=\\left( \\begin{array}{c} 1 \u0026amp; 5 \\\\ 2 \u0026amp; 6 \\\\ 3 \u0026amp; 7 \\\\ 4 \u0026amp; 8 \\end{array} \\right)\\)\n","date":1488240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488240000,"objectID":"a39b2a4058bf8c15eecfd2caab66e39e","permalink":"https://wangcc.me/post/2017-02-28/","publishdate":"2017-02-28T00:00:00Z","relpermalink":"/post/2017-02-28/","section":"post","summary":"第4章　矩陣 matrix P57-59","tags":["learning notes","basic mathematics","linear algebra"],"title":"「統計解析のための線形代数」復習筆記13","type":"post"},{"authors":null,"categories":["statistics"],"content":" 矩陣乘法運算 矩陣乘法定義 Theorem 1 (matrix multiplication) 兩個矩陣 \\(A, B\\) ，只有 \\(A\\) 的列數和 \\(B\\) 的行數相等(這種特徵又被稱爲：矩陣 \\(A,B\\) 可整合的，conformable)時，才有定義：\\(AB\\)。\\(AB\\) 則爲新的矩陣，類型爲 \\(A\\) 的行數， \\(B\\)的列數。即：\\(A_{k\\times l}, \\; B_{m\\times n}\\) 且 \\(l=m\\) 時才能計算乘積: \\(AB_{k\\times n}\\)。 \\(A_{2\\times3}=\\left( \\begin{array}{c} 4 \u0026amp; 6 \u0026amp; 8\\\\ 2 \u0026amp; 1 \u0026amp; 3\\\\ \\end{array} \\right),\\; B_{3\\times2}=\\left( \\begin{array}{c} 0 \u0026amp; 8\\\\ 2 \u0026amp; -1\\\\ 9 \u0026amp; 4 \\\\ \\end{array} \\right)\\) 時，\n“\\(A\\)的列數” \\(=\\) “\\(B\\) 的行數” \\(= 3\\)，因此積 \\(AB\\) 被定義，類型是 \\((2,2)\\) “\\(B\\)的列數” \\(=\\) “\\(A\\) 的行數” \\(= 2\\)，因此積 \\(BA\\) 被定義，類型是 \\((3,3)\\)\n\\(C_{3\\times2}=\\left( \\begin{array}{c} 4 \u0026amp; 2 \\\\ 5 \u0026amp; 6 \\\\ 7 \u0026amp; 3 \\end{array} \\right),\\; D_{2\\times4}=\\left( \\begin{array}{c} 2 \u0026amp; 0 \u0026amp; 9 \u0026amp; -1 \\\\ 4 \u0026amp; 7 \u0026amp; 6 \u0026amp; 5 \\\\ \\end{array} \\right)\\) 時， “\\(C\\)的列數” \\(=\\) “\\(D\\) 的行數” \\(= 2\\)，因此積 \\(CD\\) 被定義，類型是 \\((3,4)\\) “\\(D\\)的列數”\\(= 4\\)，“\\(C\\) 的行數” \\(= 3\\)，因此積 \\(DC\\) 不被定義，\\(DC\\) 不可整合。\n矩陣的積的向量表達形式 矩陣也可以被看做是一個個相同類型（大小，方向）的向量組成。那麼當，下面\\(A, B\\)兩個矩陣滿足：\\(A\\) 的行向量的維度，和\\(B\\)的列向量的維度相等時，\\(AB\\)被定義。\n\\(A=\\left( \\begin{array}{c} \\underline{a}_1^t\\\\ \\underline{a}_2^t\\\\ \\vdots\\\\ \\underline{a}_k^t \\end{array} \\right), \\; B=(\\underline{b}_1,\\underline{b}_2,\\cdots,\\underline{b}_n)\\)\n\\(AB=A(\\underline{b}_1,\\underline{b}_2,\\cdots,\\underline{b}_n)=(A\\underline{b}_1,A\\underline{b}_2,\\cdots,A\\underline{b}_n)\\)\n或者：\n\\(AB=\\left( \\begin{array}{c} \\underline{a}_1^t\\\\ \\underline{a}_2^t\\\\ \\vdots\\\\ \\underline{a}_k^t \\end{array} \\right)B=\\left( \\begin{array}{c} \\underline{a}_1^tB\\\\ \\underline{a}_2^tB\\\\ \\vdots\\\\ \\underline{a}_k^tB \\end{array} \\right)\\)\n特殊情況當\\(A\\)僅有一個行向量 \\(\\underline{a}^t\\) 時，\n\\(AB=\\underline{a}^tB=\\underline{a}^t(\\underline{b}_1,\\underline{b}_2,\\cdots,\\underline{b}_n)=(\\underline{a}^t\\underline{b}_1,\\underline{a}^t\\underline{b}_2,\\cdots,\\underline{a}^t\\underline{b}_n)\\)\n矩陣的積的成分 Theorem 2 (matrix multiplication component) 兩個矩陣 \\(A_{k\\times l}, \\; B_{m\\times n}\\) 的積有被定義時，矩陣 \\(AB_{k\\times n}\\) 的任意成分\\((i,j)\\)，被定義爲：“\\(A\\) 的第 \\(i\\) 行向量與 \\(B\\)的第 \\(j\\) 列向量的內積”。 故：\n\\(AB\\) 的 \\((1,1)\\) 成分是，“\\(A\\) 的第 \\(1\\) 行向量與 \\(B\\)的第 \\(1\\) 列向量的內積”\n\\(AB\\) 的 \\((1,2)\\) 成分是，“\\(A\\) 的第 \\(1\\) 行向量與 \\(B\\)的第 \\(2\\) 列向量的內積”\n\\(\\vdots\\) \\(AB\\) 的 \\((k,n)\\) 成分是，“\\(A\\) 的第 \\(k\\) 行向量與 \\(B\\)的第 \\(ns\\) 列向量的內積”\n練習 \\(A=\\left( \\begin{array}{c} 1 \u0026amp; 2 \\\\ 3 \u0026amp; 4 \\\\ \\end{array} \\right), B=\\left( \\begin{array}{c} 4 \u0026amp; 1 \\\\ 5 \u0026amp; 2 \\\\ \\end{array} \\right)\\)\n\\(AB=\\left( \\begin{array}{c} 1\\times4+2\\times5 \u0026amp; 1\\times1+2\\times2 \\\\ 3\\times4+4\\times5 \u0026amp; 3\\times1+4\\times2 \\\\ \\end{array} \\right)\\\\ \\;\\;\\;\\;\\;=\\left( \\begin{array}{c} 14 \u0026amp; 5 \\\\ 32 \u0026amp; 11 \\\\ \\end{array} \\right)\\) \\(BA=\\left( \\begin{array}{c} 4\\times1+1\\times3 \u0026amp; 4\\times2+1\\times4 \\\\ 5\\times1+2\\times3 \u0026amp; 5\\times2+2\\times4 \\\\ \\end{array} \\right)\\\\ \\;\\;\\;\\;\\;=\\left( \\begin{array}{c} 7 \u0026amp; 12 \\\\ 11 \u0026amp; 18 \\\\ \\end{array} \\right)\\)\n注意： \\(AB\\neq BA\\) 另外：\\(AA=\\left( \\begin{array}{c} 1\\times1+2\\times3 \u0026amp; 1\\times2+2\\times4 \\\\ 3\\times1+4\\times3 \u0026amp; 3\\times2+4\\times4 \\\\ \\end{array} \\right)\\\\ \\;\\;\\;\\;=\\left( \\begin{array}{c} 7 \u0026amp; 10 \\\\ 15 \u0026amp; 22 \\\\ \\end{array} \\right)=AA^2\\)\n\\(A=\\left( \\begin{array}{c} 1 \u0026amp; 2 \\\\ -2 \u0026amp; -4 \\\\ \\end{array} \\right), B=\\left( \\begin{array}{c} 6 \u0026amp; -2 \\\\ -3 \u0026amp; 1 \\\\ \\end{array} \\right)\\) \\(AB=\\left( \\begin{array}{c} 1\\times6+2\\times(-3) \u0026amp; 1\\times(-2)+2\\times1 \\\\ (-2)\\times6+(-4)\\times(-3) \u0026amp; (-2)\\times(-2)+(-4)\\times1 \\\\ \\end{array} \\right)\\\\ \\;\\;\\;\\;=\\left( \\begin{array}{c} 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \\\\ \\end{array} \\right)=\\Large 0\\)\n這裏出現了非零矩陣相乘爲零矩陣 \\(\\large 0\\)的例子。\n\\(A_{2\\times3}=\\left( \\begin{array}{c} a_{11} \u0026amp; a_{12} \u0026amp; a_{13}\\\\ a_{21} \u0026amp; a_{22} \u0026amp; a_{23}\\\\ \\end{array} \\right)=\\left( \\begin{array}{c} \\underline{a}_1^t\\\\ \\underline{a}_2^t\\\\ \\end{array} \\right)=(\\underline{c}_1,\\underline{c}_2,\\underline{c}_3); \\\\ B_{3\\times2}=\\left( \\begin{array}{c} b_{11} \u0026amp; b_{12}\\\\ b_{21} \u0026amp; b_{22}\\\\ b_{31} \u0026amp; b_{32}\\\\ \\end{array} \\right)=(\\underline{b}_1,\\underline{b}_2)=\\left( \\begin{array}{c} \\underline{d}^t_1\\\\ \\underline{d}^t_2\\\\ \\underline{d}^t_3\\\\ \\end{array} \\right)\\) \\(AB_{2\\times2}\\)\\(=\\left( \\begin{array}{c} \\sum\\limits_{k=1}^3a_{1k}b_{k1} \u0026amp;\\sum\\limits_{k=1}^3a_{1k}b_{k2} \\\\ \\sum\\limits_{k=1}^3a_{2k}b_{k1} \u0026amp;\\sum\\limits_{k=1}^3a_{2k}b_{k2} \\\\ \\end{array} \\right)\\\\=\\left( \\begin{array}{c} \\underline{a}^t_1\\underline{b}_1 \u0026amp; \\underline{a}^t_1\\underline{b}_2 \\\\ \\underline{a}^t_2\\underline{b}_1 \u0026amp; \\underline{a}^t_2\\underline{b}_2 \\\\ \\end{array} \\right)\\) \\(BA_{3\\times3}\\)\\(=\\left( \\begin{array}{c} \\sum\\limits_{k=1}^2b_{1k}a_{k1} \u0026amp; \\sum\\limits_{k=1}^2b_{1k}a_{k2} \u0026amp; \\sum\\limits_{k=1}^2b_{1k}a_{k3} \\\\ \\sum\\limits_{k=1}^2b_{2k}a_{k1} \u0026amp; \\sum\\limits_{k=1}^2b_{2k}a_{k2} \u0026amp; \\sum\\limits_{k=1}^2b_{2k}a_{k3} \\\\ \\sum\\limits_{k=1}^2b_{3k}a_{k1} \u0026amp; \\sum\\limits_{k=1}^2b_{3k}a_{k2} \u0026amp; \\sum\\limits_{k=1}^2b_{3k}a_{k3} \\end{array} \\right)\\\\ =\\left( \\begin{array}{c} \\underline{d}^t_1\\underline{c}_1 \u0026amp; \\underline{d}^t_1\\underline{c}_2 \u0026amp; \\underline{d}^t_1\\underline{c}_3 \\\\ \\underline{d}^t_2\\underline{c}_1 \u0026amp; \\underline{d}^t_2\\underline{c}_2 \u0026amp; \\underline{d}^t_2\\underline{c}_3 \\\\ \\underline{d}^t_3\\underline{c}_1 \u0026amp; \\underline{d}^t_3\\underline{c}_2 \u0026amp; \\underline{d}^t_3\\underline{c}_3 \\\\ \\end{array} \\right)\\)\n\\(\\underline{x}=\\left( \\begin{array}{c} x_1\\\\ x_2\\\\ \\vdots\\\\ x_n\\\\ \\end{array} \\right)\\) 時， \\(\\underline{x}\\underline{x}^t=\\left( \\begin{array}{c} x_1^2 \u0026amp; x_1x_2 \u0026amp; \\cdots \u0026amp; x_1x_n \\\\ x_2x_1 \u0026amp; x_2^2 \u0026amp; \\cdots \u0026amp; x_2x_n \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ x_nx_1 \u0026amp; x_nx_2 \u0026amp; \\cdots \u0026amp; x_n^2 \\\\ \\end{array} \\right)\\)，\n\\(\\underline{x}^t\\underline{x}=\\sum\\limits_{i=1}^nx_i^2\\)\n\\(\\underline{a}=\\left( \\begin{array}{c} a_1\\\\ a_2\\\\ a_3\\\\ \\end{array} \\right), \\underline{b}=\\left( \\begin{array}{c} b_1\\\\ b_2\\\\ b_3\\\\ \\end{array} \\right), \\underline{c}=\\left( \\begin{array}{c} c_1\\\\ c_2\\\\ c_3\\\\ \\end{array} \\right)\\\\ A=\\left( \\begin{array}{c} a_1 \u0026amp; b_1 \u0026amp; c_1 \\\\ a_2 \u0026amp; b_2 \u0026amp; c_2 \\\\ a_3 \u0026amp; b_3 \u0026amp; c_3 \\\\ \\end{array} \\right)\\) \\(\\underline{a}\\underline{a}^t+\\underline{b}\\underline{b}^t+\\underline{c}\\underline{c}^t\\\\ \\;\\;=\\left( \\begin{array}{c} a_1^2 \u0026amp; a_1a_2 \u0026amp; a_1a_3 \\\\ a_2a_1 \u0026amp; a_2^2 \u0026amp; a_2a_3 \\\\ a_3a_1 \u0026amp; a_3a_2 \u0026amp; a_3^2 \\\\ \\end{array} \\right)\\\\ \\;\\;\\;\\;\\;\\;+\\left( \\begin{array}{c} b_1^2 \u0026amp; b_1b_2 \u0026amp; b_1b_3 \\\\ b_2b_1 \u0026amp; b_2^2 \u0026amp; b_2b_3 \\\\ b_3b_1 \u0026amp; b_3b_2 \u0026amp; b_3^2 \\\\ \\end{array} \\right)\\\\ \\;\\;\\;\\;\\;\\;+\\left( \\begin{array}{c} c_1^2 \u0026amp; c_1c_2 \u0026amp; c_1c_3 \\\\ c_2c_1 \u0026amp; c_2^2 \u0026amp; c_2c_3 \\\\ c_3c_1 \u0026amp; c_3c_2 \u0026amp; c_3^2 \\\\ \\end{array} \\right)\\\\ =\\left( \\begin{array}{c} a_1 \u0026amp; b_1 \u0026amp; c_1 \\\\ a_2 \u0026amp; b_2 \u0026amp; c_2 \\\\ a_3 \u0026amp; b_3 \u0026amp; c_3 \\\\ \\end{array} \\right)\\left( \\begin{array}{c} a_1 \u0026amp; a_2 \u0026amp; a_3 \\\\ b_1 \u0026amp; b_2 \u0026amp; b_3 \\\\ c_1 \u0026amp; c_2 \u0026amp; c_3 \\\\ \\end{array} \\right)=AA^t\\)\n\\(\\underline{x}=\\left( \\begin{array}{c} x_1\\\\ x_2\\\\ \\vdots\\\\ x_n\\\\ \\end{array} \\right), \\; \\underline{\\frac{1}{n}}=\\left( \\begin{array}{c} \\frac{1}{n}\\\\ \\frac{1}{n}\\\\ \\cdots\\\\ \\frac{1}{n}\\\\ \\end{array} \\right)\\) 時， \\(\\underline{x}^t\\underline{\\frac{1}{n}}=\\sum\\limits_{i=1}^nx_i\\cdot\\frac{1}{n}=\\bar{x}\\) (\\(\\underline{x}\\) 的平均值) 將這樣的 \\(\\bar{x}\\) 寫成 \\(n\\) 個的橫向量：\\((\\bar{x},\\bar{x},\\cdots,\\bar{x})\\) 這個向量如果寫成展開的形式就是：\n\\((\\bar{x},\\bar{x},\\cdots,\\bar{x})=(\\underline{x}^t\\underline{\\frac{1}{n}}, \\underline{x}^t\\underline{\\frac{1}{n}}, \\cdots,\\underline{x}^t\\underline{\\frac{1}{n}})\\\\ \\;\\;\\;\\;\\;\\;=\\underline{x}^t(\\underline{\\frac{1}{n}},\\underline{\\frac{1}{n}},\\cdots,\\underline{\\frac{1}{n}})\\\\ \\;\\;\\;\\;\\;\\;=(x_1,x_2,\\cdots,x_n)\\left( \\begin{array}{c} \\frac{1}{n} \u0026amp; \\frac{1}{n} \u0026amp; \\cdots \u0026amp; \\frac{1}{n} \\\\ \\frac{1}{n} \u0026amp; \\frac{1}{n} \u0026amp; \\cdots \u0026amp; \\frac{1}{n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\frac{1}{n} \u0026amp; \\frac{1}{n} \u0026amp; \\cdots \u0026amp; \\frac{1}{n} \\\\ \\end{array} \\right)\\)\n矩陣積的性質 矩陣 \\(AB\\) 有定義時， \\(BA\\) 並不一定就有定義。無法整合時就沒有定義。 \\(AB=BA\\) 時，\\(A, B\\) 被稱爲可交換 commutative，交換可能矩陣。 當 \\(AB, BA\\) 都有定義時，也不一定就滿足 \\(AB=BA\\)。也就是說，多數情況下， \\(AB\\neq BA\\)。爲了區分二者，\\(AB\\) 被稱爲 \\(A\\) 從右往左乘 \\(B\\) (postmultiplication of \\(A\\) by \\(B\\))，\\(BA\\) 被稱爲 \\(A\\) 從左往右乘 \\(B\\) (postmultiplication of \\(B\\) by \\(A\\))。 相似的， \\(AC=BC\\) 時，應該理解爲： 等式\\(A=B\\)兩邊同時從右往左乘 \\(C\\) \\(CA=CB\\) 就是：等式\\(A=B\\)兩邊同時從左往右乘\\(C\\)。 即使 \\(A\\neq\\Large 0\\) 且 \\(B\\neq\\Large 0\\)，\\(AB\\) 也有可能等於 \\(\\Large 0\\) (零矩陣)，此時我們說， \\(A, B\\) 是零因子 (zero divisor)。 ","date":1487721600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1487721600,"objectID":"fdace18208ddde98c71e6f41192955b7","permalink":"https://wangcc.me/post/2017-02-22/","publishdate":"2017-02-22T00:00:00Z","relpermalink":"/post/2017-02-22/","section":"post","summary":"第4章　矩陣 matrix P53-56","tags":["learning notes","basic mathematics","linear algebra"],"title":"「統計解析のための線形代数」復習筆記12","type":"post"},{"authors":null,"categories":["statistics"],"content":" 矩陣的定義 Theorem 1 (matrix) 將\\(m\\times n\\) 個數 \\(a_{ij} (i=1,2,\\cdots,m; j=1,2,\\cdots,n)\\), 寫成縱 \\(m\\) 行， 橫 \\(n\\) 列的長方形或者正方形，左右用圓括號或者方括號包含在內。我們稱之爲 \\(m\\times n\\) 矩陣(matrix)，或者 \\((m, n)\\) 矩陣。 \\(m\\times n\\) 或者 \\((m,n)\\) 被稱爲是這個矩陣的類型。我們常用大寫字母來標記一個矩陣，如下面的矩陣我們標記爲 \\(A\\)。 如果要特別明示矩陣的類型，可以寫作 \\(\\mathop{A}_{m\\times n}, \\mathop{A}_{(m, n)}, \\; A(m\\times n)\\)。兩個矩陣如果行數相等，列數也相等，我們稱他們爲類型相同的矩陣。構成矩陣的一個個數 \\(a_{11},a_{12},\\cdots,a_{mn}\\) 被叫做矩陣的成分(component, element, entry)。\n第\\(i\\)行，第\\(j\\)列交叉的地方的成分，\\(a_{ij}\\) 被叫做 \\((i,j)\\) 成分。矩陣有時候也會寫成 \\(A=(a_{ij})\\) \\(\\left( \\begin{array}{c} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1j} \u0026amp; \\cdots \u0026amp; a_{1n}\\\\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2j} \u0026amp; \\cdots \u0026amp; a_{2n}\\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots\\\\ a_{i1} \u0026amp; a_{i2} \u0026amp; \\cdots \u0026amp; a_{ij} \u0026amp; \\cdots \u0026amp; a_{in}\\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots\\\\ a_{m1} \u0026amp; a_{m2} \u0026amp; \\cdots \u0026amp; a_{mj} \u0026amp; \\cdots \u0026amp; a_{mn}\\\\ \\end{array} \\right), \\\\ \\left[ \\begin{array}{c} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1j} \u0026amp; \\cdots \u0026amp; a_{1n}\\\\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2j} \u0026amp; \\cdots \u0026amp; a_{2n}\\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots\\\\ a_{i1} \u0026amp; a_{i2} \u0026amp; \\cdots \u0026amp; a_{ij} \u0026amp; \\cdots \u0026amp; a_{in}\\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots\\\\ a_{m1} \u0026amp; a_{m2} \u0026amp; \\cdots \u0026amp; a_{mj} \u0026amp; \\cdots \u0026amp; a_{mn}\\\\ \\end{array} \\right]\\)\n矩陣 \\(\\mathop{A}_{m\\times n}\\) 可以被看做是：\n以第\\(1\\)行爲成分的行向量 \\((a_{11},a_{12},\\cdots,a_{1n})=\\underline{b}_1^t\\)；\n以第\\(2\\)行爲成分的行向量 \\((a_{21},a_{22},\\cdots,a_{2n})=\\underline{b}_2^t\\)；\n\\(\\vdots\\)\n以第 \\(m\\) 行爲成分的行向量 \\((a_{m1},a_{m2},\\cdots,a_{mn})=\\underline{b}_m^t\\)；\n爲成分組成的列向量：\n\\(\\left( \\begin{array}{c} \\underline{b}_1^t\\\\ \\underline{b}_2^t\\\\ \\vdots\\\\ \\underline{b}_m^t\\\\ \\end{array} \\right)\\)\n類似的，矩陣 \\(\\mathop{A}_{m\\times n}\\) 可以被看做是：\n以第\\(1\\)列爲成分的列向量： \\(\\left( \\begin{array}{c} a_{11}\\\\ a_{21}\\\\ \\vdots\\\\ a_{m1}\\\\ \\end{array} \\right)=\\underline{c}_1\\) 以第\\(2\\)列爲成分的列向量：\\(\\left( \\begin{array}{c} a_{12}\\\\ a_{22}\\\\ \\vdots\\\\ a_{m2}\\\\ \\end{array} \\right)=\\underline{c}_2\\) \\(\\vdots\\)\n以第\\(n\\)列爲成分的列向量：\\(\\left( \\begin{array}{c} a_{1n}\\\\ a_{2n}\\\\ \\vdots\\\\ a_{mn}\\\\ \\end{array} \\right)=\\underline{c}_n\\) 爲成分組成的行向量：\\((\\underline{c}_1,\\underline{c}_2,\\cdots,\\underline{c}_n)\\)\n矩陣的運算，和零矩陣 矩陣的和與差 Theorem 2 (matrix plus or minus) 類型(type)相同的矩陣之間的加減法運算，被定義爲各個對應成分的加減法結果作成分的矩陣。 對於\\(A=\\left( \\begin{array}{c} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n}\\\\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n}\\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots\\\\ a_{m1} \u0026amp; a_{m2} \u0026amp; \\cdots \u0026amp; a_{mn}\\\\ \\end{array} \\right),\\\\ B=\\left( \\begin{array}{c} b_{11} \u0026amp; b_{12} \u0026amp; \\cdots \u0026amp; b_{1n}\\\\ b_{21} \u0026amp; b_{22} \u0026amp; \\cdots \u0026amp; b_{2n}\\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots\\\\ b_{m1} \u0026amp; b_{m2} \u0026amp; \\cdots \u0026amp; b_{mn}\\\\ \\end{array} \\right)\\) 有：\\(A\\pm B=\\left( \\begin{array}{c} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n}\\\\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n}\\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots\\\\ a_{m1} \u0026amp; a_{m2} \u0026amp; \\cdots \u0026amp; a_{mn}\\\\ \\end{array} \\right)\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\pm \\left( \\begin{array}{c} b_{11} \u0026amp; b_{12} \u0026amp; \\cdots \u0026amp; b_{1n}\\\\ b_{21} \u0026amp; b_{22} \u0026amp; \\cdots \u0026amp; b_{2n}\\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots\\\\ b_{m1} \u0026amp; b_{m2} \u0026amp; \\cdots \u0026amp; b_{mn}\\\\ \\end{array} \\right)\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=\\left( \\begin{array}{c} a_{11}\\pm b_{11} \u0026amp; a_{12}\\pm b_{12} \u0026amp; \\cdots \u0026amp; a_{1n}\\pm b_{1n}\\\\ a_{21}\\pm b_{21} \u0026amp; a_{22}\\pm b_{22} \u0026amp; \\cdots \u0026amp; a_{2n}\\pm b_{2n}\\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots\\\\ a_{m1}\\pm b_{m1} \u0026amp; a_{m2}\\pm b_{m2} \u0026amp; \\cdots \u0026amp; a_{mn}\\pm b_{mn}\\\\ \\end{array} \\right)(復号同順)\\)\n\\(A=\\left( \\begin{array}{c} 9 \u0026amp; 3 \u0026amp; 1\\\\ -2 \u0026amp; 5 \u0026amp; 8\\\\ \\end{array} \\right)， B=\\left( \\begin{array}{c} 4 \u0026amp; 2 \u0026amp; 1\\\\ 3 \u0026amp; -3 \u0026amp; 5\\\\ \\end{array} \\right)\\) 那麼\n\\(A+B = \\left( \\begin{array}{c} 9+4 \u0026amp; 3+2 \u0026amp; 1+1\\\\ -2+3 \u0026amp; 5+(-3) \u0026amp; 8+5\\\\ \\end{array} \\right)=\\left( \\begin{array}{c} 13 \u0026amp; 5 \u0026amp; 2\\\\ 1 \u0026amp; 2 \u0026amp; 13\\\\ \\end{array} \\right)\\)\n用1.中的矩陣運算：\n\\(A-B=\\left( \\begin{array}{c} 9-4 \u0026amp; 3-2 \u0026amp; 1-1\\\\ -2-3 \u0026amp; 5-(-3) \u0026amp; 8-5\\\\ \\end{array} \\right)=\\left( \\begin{array}{c} 5 \u0026amp; 1 \u0026amp; 0\\\\ -5 \u0026amp; 8 \u0026amp; 3\\\\ \\end{array} \\right)\\)\n矩陣的相等 Theorem 3 (matrix equal) 類型相同的兩個矩陣 \\(A,B\\)，如果他們對應的所有成分，一一相等，我們說這兩個矩陣是相等的。即：\\(A=B\\)。 對於\\(A=\\left( \\begin{array}{c} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n}\\\\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n}\\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots\\\\ a_{m1} \u0026amp; a_{m2} \u0026amp; \\cdots \u0026amp; a_{mn}\\\\ \\end{array} \\right),\\\\ B=\\left( \\begin{array}{c} b_{11} \u0026amp; b_{12} \u0026amp; \\cdots \u0026amp; b_{1n}\\\\ b_{21} \u0026amp; b_{22} \u0026amp; \\cdots \u0026amp; b_{2n}\\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots\\\\ b_{m1} \u0026amp; b_{m2} \u0026amp; \\cdots \u0026amp; b_{mn}\\\\ \\end{array} \\right)\\)\n如果有：\n\\(a_{11}=b_{11},a_{12}=b_{12},\\cdots,a_{mn}=b_{mn}\\)\n那麼 \\(A=B\\)。\n零矩陣 Theorem 4 (zero matrix) 所有的成分均爲數字 \\(0\\) 的 \\(m\\times n\\) 矩陣，\n(共有　\\(m\\times n\\) 個零。)\n\\(\\left( \\begin{array}{c} 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0\\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots\\\\ 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0\\\\ \\end{array} \\right)\\)\n被稱爲零矩陣(zero matrix, null matrix)。寫作：\\(\\large 0, \\mathop{\\large 0}_{m\\times n}, \\mathop{\\large 0}_{(m,n)}\\)。要注意與標量的 \\(0\\) 區分。 矩陣的標量倍數運算 Theorem 5 (scalar times) 矩陣 \\(A\\) 的所有的成分，均乘以一個標量 \\(k\\)，獲得新的矩陣的過程被稱爲矩陣的標量倍數運算。 寫作 \\(kA\\)。 對於 \\(A=\\left( \\begin{array}{c} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n}\\\\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n}\\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots\\\\ a_{m1} \u0026amp; a_{m2} \u0026amp; \\cdots \u0026amp; a_{mn}\\\\ \\end{array} \\right)\\)，\n有：\\(kA = k\\left( \\begin{array}{c} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n}\\\\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n}\\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots\\\\ a_{m1} \u0026amp; a_{m2} \u0026amp; \\cdots \u0026amp; a_{mn}\\\\ \\end{array} \\right)\\\\ =\\left( \\begin{array}{c} ka_{11} \u0026amp; ka_{12} \u0026amp; \\cdots \u0026amp; ka_{1n}\\\\ ka_{21} \u0026amp; ka_{22} \u0026amp; \\cdots \u0026amp; ka_{2n}\\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots\\\\ ka_{m1} \u0026amp; ka_{m2} \u0026amp; \\cdots \u0026amp; ka_{mn}\\\\ \\end{array} \\right)\\)\n特別的，當 \\(k=-1\\) 時， \\((-1)A=-A\\)，\\(k=0\\) 時，\\(0A=\\Large 0\\)。注意 \\(\\Large 0\\) 是與 \\(A\\) 類型相同的零矩陣，而非標量 \\(0\\)。\n對 \\(A=\\left( \\begin{array}{c} a_{11} \u0026amp; a_{12} \u0026amp; a_{13}\\\\ a_{21} \u0026amp; a_{22} \u0026amp; a_{23}\\\\ a_{31} \u0026amp; a_{32} \u0026amp; a_{33}\\\\ \\end{array} \\right)\\)， \\(kA=\\left( \\begin{array}{c} ka_{11} \u0026amp; ka_{12} \u0026amp; ka_{13}\\\\ ka_{21} \u0026amp; ka_{22} \u0026amp; ka_{23}\\\\ ka_{31} \u0026amp; ka_{32} \u0026amp; ka_{33}\\\\ \\end{array} \\right)\\)\n對 \\(B=\\left( \\begin{array}{c} 1 \u0026amp; -2 \u0026amp; 3\\\\ -4 \u0026amp; 5 \u0026amp; -6\\\\ \\end{array} \\right)\\)，\n\\(7B=\\left( \\begin{array}{c} 7\\times1 \u0026amp; 7\\times(-2) \u0026amp; 7\\times3\\\\ 7\\times(-4) \u0026amp; 7\\times5 \u0026amp; 7\\times(-6)\\\\ \\end{array} \\right)\\\\ \\;\\;\\;\\;=\\left( \\begin{array}{c} 7 \u0026amp; -14 \u0026amp; 21\\\\ -28 \u0026amp; 35 \u0026amp; -42\\\\ \\end{array} \\right)\\)\n\\(-B=\\left( \\begin{array}{c} -1 \u0026amp; 2 \u0026amp; -3\\\\ 4 \u0026amp; -5 \u0026amp; 6\\\\ \\end{array} \\right)\\)；\n\\(0B=\\left( \\begin{array}{c} 0 \u0026amp; 0 \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; 0\\\\ \\end{array} \\right)=\\mathop{\\large 0}_{2\\times3}\\)\n","date":1487635200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1487635200,"objectID":"23d2fae80934c54e527a5cf5665c1514","permalink":"https://wangcc.me/post/2017-02-21/","publishdate":"2017-02-21T00:00:00Z","relpermalink":"/post/2017-02-21/","section":"post","summary":"第4章　矩陣 matrix P46-52","tags":["learning notes","basic mathematics","linear algebra"],"title":"「統計解析のための線形代数」復習筆記11","type":"post"},{"authors":null,"categories":["statistics"],"content":" 向量的內積 (inner product) Theorem 1 (vectors inner product) 向量的內積運算，僅限定於維度相同的兩個向量之間。一個向量爲橫向量寫在左側，一個向量爲列向量寫在右側，兩個向量的相對應成分一一相乘，然後將各成分乘積相加的過程，我們稱之爲內積(inner product, scalar product)運算。內積運算結果通常不會是向量，而是標量(scalar)，或正或負，或爲零。向量 \\(\\underline{a}\\) 與向量 \\(\\underline{b}\\) 的內積寫作：\\(\\underline{a}^t\\underline{b}, \\underline{b}^t\\underline{a}\\) 或者寫作： \\(\\underline{a}\\cdot\\underline{b}, (\\underline{a},\\underline{b}), \u0026lt;\\underline{a},\\underline{b}\u0026gt;\\)。內積爲 \\(0\\) 的向量我們稱他們爲正交向量(orthogonal)，寫作：\\(\\underline{a}\\perp\\underline{b}\\)。 內積，與和記號: \\(\\sum\\) 有緊密聯系。我們常常會把 \\(\\sum\\) 式子/量寫成向量的內積形式。 練習 列向量 \\(\\underline{a}=\\left( \\begin{array}{c} a_1\\\\ a_2\\\\ a_3 \\end{array} \\right), \\underline{b}=\\left( \\begin{array}{c} b_1\\\\ b_2\\\\ b_3 \\end{array} \\right)\\) 的內積：\n\\(\\underline{a}^t\\underline{b}=(a_1,a_2,a_3)\\left( \\begin{array}{c} b_1\\\\ b_2\\\\ b_3 \\end{array} \\right)=a_1b_1+a_2b_2+a_3b_3\\\\=\\sum\\limits_{i=1}^3a_ib_i=\\sum\\limits_{i=1}^3b_ia_i=\\underline{b}^t\\underline{a}\\)\n橫向量 \\(\\underline{a}=(a_1,a_2,a_3), \\underline{b}=(b_1,b_2,b_3)\\) 的內積：\n\\(\\underline{a}\\underline{b}^t=(a_1,a_2,a_3)\\left( \\begin{array}{c} b_1\\\\ b_2\\\\ b_3 \\end{array} \\right)=a_1b_1+a_2b_2+a_3b_3\\\\=\\sum\\limits_{i=1}^3a_ib_i=\\sum\\limits_{i=1}^3b_ia_i=\\underline{b}\\underline{a}^t\\)\n完全相同的兩個列向量 \\(\\underline{x}=\\left( \\begin{array}{c} x_1\\\\ x_2\\\\ x_3 \\end{array} \\right),\\;\\underline{x}=\\left( \\begin{array}{c} x_1\\\\ x_2\\\\ x_3 \\end{array} \\right)\\) 的內積：\n\\(\\underline{x}^t\\underline{x}=(x_1,x_2,x_3)\\left( \\begin{array}{c} x_1\\\\ x_2\\\\ x_3 \\end{array} \\right)\\\\=x_1^2+x_2^2+x_3^2=\\sum\\limits_{i=1}^3x_i\\cdot x_i=\\sum\\limits_{i=1}^3x_i^2\\)\n完全相同的兩個橫向量 \\(\\underline{y}=(y_1,y_2,y_3), \\underline{y}=(y_1,y_2,y_3)\\) 的內積：\n\\(\\underline{y}\\underline{y}^t=(y_1,y_2,y_3)\\left( \\begin{array}{c} y_1\\\\ y_2\\\\ y_3 \\end{array} \\right)\\\\=y_1^2+y_2^2+y_3^2=\\sum\\limits_{i=1}^3y_i\\cdot y_i=\\sum\\limits_{i=1}^3y_i^2\\)\n向量 \\(\\underline{a}=(2,0,-1), \\underline{b}=(4,-2,8)\\) 的內積：\n\\(\\underline{a}\\underline{b}^t=(2,0,-1)\\left( \\begin{array}{c} 4\\\\ -2\\\\ 8 \\end{array} \\right)=2\\times4+0\\times(-2)+(-1)\\times8=0\\) 因此我們稱這兩個向量正交。\n\\(\\underline{1}=\\left( \\begin{array}{c} 1\\\\ 1\\\\ 1 \\end{array} \\right), \\underline{x}=\\left( \\begin{array}{c} x_1\\\\ x_2\\\\ x_3 \\end{array} \\right)\\) 時：\n\\(\\underline{1}^t\\underline{x}=1\\cdot x_1+1\\cdot x_2+1\\cdot x_3 =\\sum\\limits_{i=1}^3x_i=\\underline{x}^t\\underline{1}\\) \\(\\underline{1}^t\\underline{1}=\\sum\\limits_{i=1}^31\\cdot 1=3\\) 前者的內積與後者內積的商： \\(\\frac{\\underline{1}^t\\underline{x}}{\\underline{1}^t\\underline{1}}=\\frac{x_1+x_2+x_3}{3}\\) 我們在統計學中用 \\(\\bar{x}\\) (平均值) 來標記。\n問題： 如果，向量 \\(\\underline{a}, \\underline{b}\\) 有內積， 請問有沒有所謂的外積 (outer product) ？ 回答： 有。不過，僅限於3維度的向量： \\(\\underline{a}=\\left( \\begin{array}{c} a_1\\\\ a_2\\\\ a_3 \\end{array} \\right), \\underline{b}=\\left( \\begin{array}{c} b_1\\\\ b_2\\\\ b_3 \\end{array} \\right)\\) 的外積，我們用 \\(\\times\\) 來表示，寫作： \\(\\underline{a}\\times\\underline{b}\\)。 其運算被定義爲：\n\\(\\underline{a}\\times\\underline{b}=\\left( \\begin{array}{c} a_2b_3-a_3b_2\\\\ a_3b_1-a_1b_3\\\\ a_1b_2-a_2b_1 \\end{array}\\right)\\)。與內積不同的是，外積運算的結果仍然是\\(3\\)維度的向量。外積有如下的性質：\n\\(\\underline{a}\\times\\underline{b}=-\\underline{b}\\times\\underline{a}\\)\n向量的長度 (length) Theorem 2 (vector length) 向量 \\(\\underline{a}\\) 的內積 \\(\\underline{a}^t\\underline{a}\\) 的平方根中，非負的量，我們稱之爲向量 \\(\\underline{a}\\) 的長度或者大小。也就是：\\(\\sqrt{\\underline{a}^t\\underline{a}}\\)。記作：\\(\\| \\underline{a} \\|\\)。\n兩個向量 \\(\\underline{a}, \\underline{b}\\) 類型(type：大小，維度)相同時，他們的差 \\(\\underline{a}-\\underline{b}\\) 依然是向量，這個新向量的長度爲：\\(\\| \\underline{a}-\\underline{b} \\| = \\sqrt{(\\underline{a}-\\underline{b})^t(\\underline{a}-\\underline{b})}\\) \\(\\underline{x}=\\left( \\begin{array}{c} x_1\\\\ x_2\\\\ x_3 \\end{array} \\right)\\) 的長度爲： \\(\\| \\underline{x} \\| =\\sqrt{\\underline{x}^t\\underline{x}}=\\sqrt{x_1^2+x_2^2+x_3^2}\\)\n\\(\\underline{a}=(a_1,a_2,a_3)\\) 的長度爲： \\(\\| \\underline{a} \\| =\\sqrt{\\underline{a}\\underline{a}^t}=\\sqrt{a_1^2+a_2^2+a_3^2}\\)\n兩個向量 \\(\\underline{a}, \\underline{b}\\) 的長度和內積有這樣的關系：\n\\(-\\| \\underline{a} \\|\\| \\underline{b} \\|\\leqslant \\underline{a}^t\\underline{b}\\leqslant\\| \\underline{a} \\|\\| \\underline{b} \\|\\)\n證明: 以維度爲 \\(3\\) 的向量爲例進行證明，其他維度的向量，證明思路類似：\n令 \\(\\underline{a}=\\left( \\begin{array}{c} a_1\\\\ a_2\\\\ a_3 \\end{array} \\right), \\underline{b}=\\left( \\begin{array}{c} b_1\\\\ b_2\\\\ b_3 \\end{array} \\right)\\)， \\(t\\) 爲任意實數。平方和：\n\\(\\sum\\limits_{i=1}^3(a_it+b_i)^2 =(a_1t+b_1)^2+(a_2t+b_2)^2+(a_3t+b_3)^2\\\\ \\;\\;\\;\\;\\;\\;\\;=(a_1^2+a_2^2+a_3^2)t^2+2(a_1b_1+a_2b_2+a_3b_3)t\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;+(b_1^2+b_2^2+b_3^2)\\\\ \\;\\;\\;\\;\\;\\;\\;=\\| \\underline{a} \\|^2t^2+2\\underline{a}^t\\underline{b}t+\\| \\underline{b} \\|^2\\geqslant0\\) \\(\\therefore \\| \\underline{a} \\|^2(t+\\frac{\\| \\underline{b} \\|^2}{2\\| \\underline{a} \\|^2})^2+\\| \\underline{b} \\|^2-\\frac{(2\\underline{a}^t\\underline{b})^2}{4\\| \\underline{a} \\|^2}\\geqslant0\\)\n可見這是一個關於 \\(t\\) 的絕對不等式。因此，判別式：\n\\((2\\underline{a}^t\\underline{b})^2-4\\times\\| \\underline{a} \\|^2\\| \\underline{b} \\|^2\\leqslant0\\\\ \\therefore (\\underline{a}^t\\underline{b})^2\\leqslant\\| \\underline{a} \\|^2\\| \\underline{b} \\|^2\\\\ \\therefore -\\| \\underline{a} \\|\\| \\underline{b} \\|\\leqslant \\underline{a}^t\\underline{b}\\leqslant \\| \\underline{a} \\|\\| \\underline{b} \\|\\)\n\\(\\divideontimes\\) 兩向量內積，除以兩向量各自的長度(正)，在統計學中被成爲是相關系數，寫作 \\(r=\\frac{\\underline{a}^t\\underline{b}}{\\| \\underline{a} \\|\\| \\underline{b} \\|}\\)，我們從上面的不等式也可以得出， \\(-1 \\leqslant r \\leqslant 1\\) 另外，兩個向量又可以表示爲兩條射線，這兩條射線構成的角度如果爲 \\(\\theta\\)，\\(\\cos\\theta=r =\\frac{\\underline{a}^t\\underline{b}}{\\| \\underline{a} \\|\\| \\underline{b} \\|}\\)。\n兩個向量 \\(\\underline{a}, \\underline{b}\\) 的和 \\(\\underline{a}+\\underline{b}\\) 也是一個新的向量。這三個向量之間有：\\(\\| \\underline{a}+\\underline{b} \\|\\leqslant\\| \\underline{a} \\|+\\| \\underline{b} \\|\\)。這個關系被稱爲三角不等式，或者三角關系(triangular inequality)。\n證明：此處亦爲了簡便起見使用維度爲 \\(3\\) 的向量，即，前述3.的 \\(\\underline{a}, \\underline{b}\\)：\n\\(\\| \\underline{a}+\\underline{b}\\|^2=(a_1+b_1)^2+(a_2+b_2)^2+(a_3+b_3)^2\\\\ \\;\\;\\;\\;\\;\\;\\;=(a_1^2+a_2^2+a_3^3)+2(a_1b_1+a_2b_2+a_3b_3)+(b_1^2+b_2^2+b_3^2)\\\\ \\;\\;\\;\\;\\;\\;\\;=\\| \\underline{a} \\|^2+2\\underline{a}^t\\underline{b}+\\| \\underline{b} \\|^2\\)\n如果我們把前面問題3.中的不等式代入：\n\\(\\| \\underline{a} \\|^2+2\\underline{a}^t\\underline{b}+\\| \\underline{b} \\|^2\\leqslant \\| \\underline{a} \\|^2+2\\| \\underline{a} \\|\\| \\underline{b} \\|+\\| \\underline{b} \\|^2\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=(\\| \\underline{a} \\|+\\| \\underline{b} \\|)^2\\\\ \\therefore \\| \\underline{a}+\\underline{b}\\|^2 \\leqslant (\\| \\underline{a} \\|+\\| \\underline{b} \\|)^2\\\\ \\therefore \\| \\underline{a}+\\underline{b}\\|\\leqslant\\| \\underline{a} \\|+\\| \\underline{b} \\|\\)\n向量正規化 normalize Theorem 3 (normalize) 長度不爲 \\(0\\) 的任意向量 \\(\\underline{a}(\\neq\\underline{0})\\)，如果將它轉變成長度爲 \\(1\\) 的向量 \\(\\underline{e}_{\\underline{a}}\\)。這個過程被叫做向量的正規化(normalize)。通常只要將向量 \\(\\underline{a}\\) 除以他的長度 \\(\\| \\underline{a} \\|\\) 即可。\n\\(\\underline{e}_{\\underline{a}}=\\frac{\\underline{a}}{\\| \\underline{a} \\|}=\\frac{1}{\\| \\underline{a} \\|}\\underline{a}\\) 例如：\n\\(\\underline{a}=\\left( \\begin{array}{c} a_1\\\\ a_2\\\\ a_3 \\end{array} \\right)\\)， 則有 \\(\\underline{e}_{\\underline{a}}=\\frac{1}{\\sqrt{a_1^2+a_2^2+a_3^2}}\\left( \\begin{array}{c} a_1\\\\ a_2\\\\ a_3 \\end{array} \\right)\\)\n\\(\\underline{b}=\\left( \\begin{array}{c} -2\\\\ 1\\\\ 2 \\end{array} \\right)\\)，則有 \\(\\underline{e}_{\\underline{a}}=\\frac{1}{\\sqrt{(-2)^2+1^2+2^2}}\\left( \\begin{array}{c} -2\\\\ 1\\\\ 2 \\end{array} \\right)=\\left( \\begin{array}{c} -\\frac{2}{3}\\\\ \\frac{1}{3}\\\\ \\frac{2}{3} \\end{array} \\right)\\)\n","date":1487548800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1487548800,"objectID":"e3a2069c65c6e6926d776468ad527c4e","permalink":"https://wangcc.me/post/2017-02-19/","publishdate":"2017-02-20T00:00:00Z","relpermalink":"/post/2017-02-19/","section":"post","summary":"第3章　向量 vector P39-45","tags":["learning notes","basic mathematics","linear algebra"],"title":"「統計解析のための線形代数」復習筆記10","type":"post"},{"authors":null,"categories":["statistics"],"content":" 特殊向量 零向量 (zero vector, null vector) 全部的成分均爲\\(0\\)的向量，我們稱之爲零向量(zero vector, null vector), 寫作： \\(\\underline{0}\\) 注意與標量(scalar) \\(0\\) 相區分。 如果想要加注零向量的維度，我們可以在右下角加上 \\(n\\)：\\(\\underline{0}_n\\) ，意爲 \\(n\\) 維度的零向量。 不是零向量的向量又被叫做，非零向量(non-zero vector, non-null vector)。 例如： 列向量：\\(\\underline{0}_3=\\left( \\begin{array}{c} 0\\\\ 0\\\\ 0\\\\ \\end{array} \\right)\\)， 行向量：\\(\\underline{0}_3^t=(0,0,0)\\)\n\\(1\\) 向量 (vector with all elements 1) 當一個向量的全部成分都是數字 \\(1\\)，我們稱這個向量爲 \\(1\\) 向量。 \\(\\underline{1}\\) 這裏也需要注意與標量 \\(1\\) 相區分。 如果想要加注\\(1\\)向量的維度，我們可以在右下角加上 \\(n\\)：\\(\\underline{1}_n\\) ，意爲 \\(n\\) 維度的\\(1\\)向量。 例如：列向量：\\(\\underline{1}_4=\\left( \\begin{array}{c} 1\\\\ 1\\\\ 1\\\\ 1 \\end{array} \\right)\\)， 行向量：\\(\\underline{1}_4^t=(1,1,1,1)\\)\n第 \\(i\\) 基本向量 Theorem 1 (fundamental vector) \\(n\\) 維度的向量，假如它的第 \\(i\\) 個成分是自然數 \\(1\\)，其他的成分全部都是 \\(0\\)， 我們稱這樣的向量爲第 \\(\\textbf{i}\\) 基本向量 (fundamental vector)。寫作 \\(\\underline{\\smash{e}}_i\\)。 平時我們較少用到一個單獨的基本向量。大多情況下我們用的是由 \\(n\\) 個單獨向量組成的一組向量。這個類型的向量與坐標軸的關系緊密。 例如：維度爲4的第 \\(1\\sim4\\) 基本向量：\\(\\underline{e}_1=\\left( \\begin{array}{c} 1\\\\ 0\\\\ 0\\\\ 0 \\end{array} \\right), \\; \\underline{e}_2=\\left( \\begin{array}{c} 0\\\\ 1\\\\ 0\\\\ 0 \\end{array} \\right), \\; \\underline{e}_3=\\left( \\begin{array}{c} 0\\\\ 0\\\\ 1\\\\ 0 \\end{array} \\right), \\; \\underline{e}_4=\\left( \\begin{array}{c} 0\\\\ 0\\\\ 0\\\\ 1 \\end{array} \\right)\\)\n單位向量 (unit vector) Theorem 2 (unit vector) 求向量的各個成分平方和的正平方根，當結果爲 \\(1\\) 時，這個向量被稱作單位向量(unit vector)。寫作： \\(\\underline{e}\\)。 例如： 因爲 \\(\\sqrt{(\\frac{2}{3})^2+(-\\frac{1}{3})^2+(\\frac{2}{3})^2}=1\\)，所以我們稱向量 \\(\\underline{e}=\\left( \\begin{array}{c} \\frac{2}{3}\\\\ -\\frac{1}{3}\\\\ \\frac{2}{3}\\\\ \\end{array} \\right)\\) 爲單位向量。另外，\\((\\frac{1}{\\sqrt{2}},-\\frac{1}{\\sqrt{2}},0)^t, \\; (\\frac{2}{\\sqrt{6}},\\frac{1}{\\sqrt{6}},-\\frac{1}{\\sqrt{6}})^t\\)，以及前一項的第 \\(i\\) 基本向量，都是單位向量。\n向量的計算，與相等 向量的和與差 Theorem 3 (vectorplus) 類型(type)/成分，維度相同的向量之間的加減運算定義爲：相對應的成分之間的和或差。 例如：\n\\(\\underline{a}=\\left( \\begin{array}{c} a_1\\\\ a_2\\\\ \\vdots\\\\ a_n \\end{array} \\right), \\; \\underline{b}=\\left( \\begin{array}{c} b_1\\\\ b_2\\\\ \\vdots\\\\ b_n \\end{array} \\right)\\)，則有： \\(\\underline{a}\\pm\\underline{b}=\\left( \\begin{array}{c} a_1\\\\ a_2\\\\ \\vdots\\\\ a_n \\end{array} \\right)\\pm\\left( \\begin{array}{c} b_1\\\\ b_2\\\\ \\vdots\\\\ b_n \\end{array} \\right)=\\left( \\begin{array}{c} a_1 \\pm b_1\\\\ a_2 \\pm b_2\\\\ \\vdots\\\\ a_n \\pm b_n \\end{array} \\right)\\) 複号同順\n\\(\\underline{a}= (a_1,a_2,\\cdots,a_n), \\; \\underline{b} = (b_1,b_2,\\cdots,b_n)\\)，則有： \\(\\underline{a}\\pm\\underline{b}=(a_1,a_2,\\cdots,a_n)\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;+(b_1,b_2,\\cdots,b_n)\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=(a_1 \\pm b_1, a_2 \\pm b_2, \\cdots, a_n \\pm b_n)\\) 複号同順\n練習 \\(\\underline{a}=\\left( \\begin{array}{c} 6\\\\ 7\\\\ 8\\\\ \\end{array} \\right),\\) \\(\\underline{b}=\\left( \\begin{array}{c} 1\\\\ 3\\\\ 5\\\\ \\end{array} \\right)\\) 時，\\(\\underline{a}+\\underline{b} =\\left( \\begin{array}{c} 6+1\\\\ 7+3\\\\ 8+5\\\\ \\end{array} \\right)=\\left( \\begin{array}{c} 7\\\\ 10\\\\ 13\\\\ \\end{array} \\right)\\) \\(\\underline{a}-\\underline{b}=\\left( \\begin{array}{c} 6-1\\\\ 7-3\\\\ 8-5\\\\ \\end{array} \\right)=\\left( \\begin{array}{c} 5\\\\ 4\\\\ 3\\\\ \\end{array} \\right)\\)\n\\(\\underline{c}=(6,0,9), \\underline{d}=(7,-3,2)\\) 時，\n\\(\\underline{c}+\\underline{d}=(6+7,0-3,9+2)=(13,-3,11)\\) \\(\\underline{c}-\\underline{d}=(6-7,0-(-3),9-2)=(-1,3,7)\\)\n向量的標量乘法(scalar multiplication) Theorem 4 (scalar multiplication) 向量 \\(\\underline{a}\\) 的所有成分同時乘以標量 \\((k)\\) 以後的向量，我們稱爲 \\(\\underline{a}\\) 的標量 \\(k\\) 倍。寫作： \\(k\\underline{a}\\)。特別地，當 \\(k=1\\) 時，\\(1\\underline{a}=\\underline{a}\\)，\\(k=-1\\) 時，\\((-1)\\underline{a}=-\\underline{a}\\)。另外 \\(k=0\\) 時，\\(0\\underline{a}=\\underline{0}\\)。注意此時\\(\\underline{0}\\)是與\\(\\underline{a}\\)同維度的零向量。不可寫作標量的 \\(0\\)。 \\[k\\underline{a}=k\\left( \\begin{array}{c} a_1\\\\ a_2\\\\ \\vdots\\\\ a_n \\end{array} \\right)=\\left( \\begin{array}{c} ka_1\\\\ ka_2\\\\ \\vdots\\\\ ka_n \\end{array} \\right), \\\\k\\underline{a}=k(a_1,a_2,\\cdots,a_n)=(ka_1,ka_2,\\cdots,ka_n)\\] 練習 \\(k=5, l=\\frac{1}{9}, \\underline{a}=\\left( \\begin{array}{c} 3\\\\ 2\\\\ -7\\\\ \\end{array} \\right)\\) 時，\n\\(k\\underline{a}=5\\left( \\begin{array}{c} 3\\\\ 2\\\\ -7\\\\ \\end{array} \\right)=\\left( \\begin{array}{c} 5\\times3\\\\ 5\\times2\\\\ 5\\times(-7)\\\\ \\end{array} \\right)=\\left( \\begin{array}{c} 15\\\\ 10\\\\ -35\\\\ \\end{array} \\right)\\), \\(l\\underline{a}=\\frac{1}{9}\\left( \\begin{array}{c} 3\\\\ 2\\\\ -7\\\\ \\end{array} \\right)=\\left( \\begin{array}{c} \\frac{1}{9}\\times3\\\\ \\frac{1}{9}\\times2\\\\ \\frac{1}{9}\\times(-7)\\\\ \\end{array} \\right)=\\left( \\begin{array}{c} \\frac{1}{3}\\\\ \\frac{2}{9}\\\\ -\\frac{7}{9}\\\\ \\end{array} \\right)\\)\n\\(\\underline{a}=\\left( \\begin{array}{c} 3\\\\ -2\\\\ 4\\\\ \\end{array} \\right), \\underline{b}=\\left( \\begin{array}{c} 1\\\\ 1\\\\ -3\\\\ \\end{array} \\right), \\underline{c}=\\left( \\begin{array}{c} 0\\\\ 5\\\\ 2\\\\ \\end{array} \\right)\\) 時，\n\\(2\\underline{a}-\\underline{b}+3\\underline{c}=\\left( \\begin{array}{c} 2\\times3\\\\ 2\\times(-2)\\\\ 2\\times4\\\\ \\end{array} \\right)-\\left( \\begin{array}{c} 1\\\\ 1\\\\ -3\\\\ \\end{array} \\right)\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;+\\left( \\begin{array}{c} 3\\times0\\\\ 3\\times5\\\\ 3\\times2\\\\ \\end{array} \\right)=\\left( \\begin{array}{c} 6-1+0\\\\ -4-1+15\\\\ 8-(-3)+6\\\\ \\end{array} \\right)=\\left( \\begin{array}{c} 5\\\\ 10\\\\ 17\\\\ \\end{array} \\right)\\)\n向量相等 equal Theorem 5 (vectors equal) 類型(type)/成分，維度相同的向量 \\(\\underline{a}, \\underline{b}\\)，其對應成分完全一致，我們就稱 \\(\\underline{a}=\\underline{b}\\)，此時有 \\(\\underline{a}-\\underline{b}=\\underline{0}\\) 零向量。 練習： \\(\\underline{a}=\\left( \\begin{array}{c} a_1\\\\ a_2\\\\ a_3\\\\ \\end{array} \\right), \\underline{b}=\\left( \\begin{array}{c} b_1\\\\ b_2\\\\ b_3\\\\ \\end{array} \\right)\\) 如果相等，那麼 \\(\\underline{a}=\\underline{b}\\)，\n即：\\(\\begin{align} \\left\\{ \\begin{array}{ll} a_1 = b_1 \\\\ a_2 = b_2 \\\\ a_3 = b_3 \\end{array} \\right. \\end{align}\\) 等價於：\\(\\underline{a}-\\underline{b}=0\\)，或者\n\\(\\begin{align} \\left\\{ \\begin{array}{ll} a_1 - b_1 =0\\\\ a_2 - b_2 =0\\\\ a_3 - b_3 =0 \\end{array} \\right. \\end{align}\\)\n向量等式：\\(\\left( \\begin{array}{c} a_{11}x_1+a_{12}x_2+a_{13}x_3\\\\ a_{21}x_1+a_{22}x_2+a_{23}x_3\\\\ a_{31}x_1+a_{32}x_2+a_{33}x_3\\\\ \\end{array} \\right)=\\left( \\begin{array}{c} b_1\\\\ b_2\\\\ b_3\\\\ \\end{array} \\right)\\) 等價於三個等式的連立方程：\n\\(\\begin{align} \\left\\{ \\begin{array}{ll} a_{11}x_1+a_{12}x_2+a_{13}x_3= b_1\\\\ a_{21}x_1+a_{22}x_2+a_{23}x_3= b_2\\\\ a_{31}x_1+a_{32}x_2+a_{33}x_3= b_3 \\end{array} \\right. \\end{align}\\)\n求滿足 \\(5\\left( \\begin{array}{c} a_1\\\\ a_2\\\\ a_3\\\\ \\end{array} \\right)+\\left( \\begin{array}{c} 2\\\\ 5\\\\ -1\\\\ \\end{array} \\right)=\\left( \\begin{array}{c} 12\\\\ 25\\\\ 29\\\\ \\end{array} \\right)\\) 的向量 \\(\\underline{a}=\\left( \\begin{array}{c} a_1\\\\ a_2\\\\ a_3\\\\ \\end{array} \\right)\\)。\n解：\\(5\\left( \\begin{array}{c} a_1\\\\ a_2\\\\ a_3\\\\ \\end{array} \\right)=\\left( \\begin{array}{c} 12\\\\ 25\\\\ 29\\\\ \\end{array} \\right)-\\left( \\begin{array}{c} 2\\\\ 5\\\\ -1\\\\ \\end{array} \\right)=\\left( \\begin{array}{c} 10\\\\ 20\\\\ 30\\\\ \\end{array} \\right)\\)\n因此，\\(\\underline{a}=\\left( \\begin{array}{c} a_1\\\\ a_2\\\\ a_3\\\\ \\end{array} \\right)=\\left( \\begin{array}{c} 2\\\\ 4\\\\ 6\\\\ \\end{array} \\right)\\)\n","date":1487376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1487376000,"objectID":"4744e2c21a4f7c224bb334aa43d47d7e","permalink":"https://wangcc.me/post/2017-02-18/","publishdate":"2017-02-18T00:00:00Z","relpermalink":"/post/2017-02-18/","section":"post","summary":"第3章　向量 vector P34-38","tags":["learning notes","basic mathematics","linear algebra"],"title":"「統計解析のための線形代数」復習筆記9","type":"post"},{"authors":null,"categories":["statistics"],"content":" 向量 vector 列向量 column vector 在等號的右側，將數字寫成一列，左右用圓括號或者方括號包含在內的形式，被叫做列向量(column vector)：\n\\(\\underline{a}=\\left( \\begin{array}{c} a_1\\\\ a_2\\\\ \\vdots\\\\ a_i\\\\ \\vdots\\\\ a_n \\end{array} \\right), \\;\\; \\textbf{a}=\\left[ \\begin{array}{c} a_1\\\\ a_2\\\\ \\vdots\\\\ a_i\\\\ \\vdots\\\\ a_n \\end{array} \\right]\\)\n我們接下來將會繼續定義，向量的加減法，標量乘法(scalar multiplication)。把上述的向量用一個文字表示的時候，通常會記爲下劃線 \\(\\underline{a}\\)，或者是加粗的小寫字母： \\(\\bf{a}\\)。\n構成向量的各個數字，被命名爲成分(component, element, entry)，從上往下第 \\(i\\) 個成分稱爲第 \\(i\\) 成分。\n成分的個數爲 \\(n\\)，就被稱爲這個向量具有 \\(n\\) 個維度(次元，dimension)，或者說這個向量的維度爲 \\(n\\)。成分可以是數字，也可以是函數，或者式子。如果兩個列向量的維度一致，我們稱這兩個列向量的型(size, order),或者 類型(type) 一致。\n成分只有一個的向量，被特別稱爲標量(scalar)，原則上不加括號。\n將向量成分全部羅列出來，寫成上面的形式的過程，被稱爲成分表示。在多元變量分析中，我們說到向量，多默認指的就是列向量。\n\\(\\underline{b}=\\left( \\begin{array}{c} 16\\\\ 59\\\\ 80\\\\ \\end{array} \\right)=\\left[ \\begin{array}{c} 16\\\\ 59\\\\ 80\\\\ \\end{array} \\right]=\\textbf{b}\\)\n今後我們都用字母帶下劃線，圓括號包含數字的方式表示向量。\n\\(\\underline{c}=\\left( \\begin{array}{c} \\sin t+\\cos t\\\\ \\cos t+\\tan t-2\\\\ \\tan t + \\sin t\\\\ \\end{array} \\right)\\)\n當 \\(F\\) 爲 \\(a_1,a_2,a_3\\) 的函數時，寫作 \\(F(a_1,a_2,a_3)\\)。 以三個未知數的偏微分爲成分的向量(梯度向量，gradient vector)，寫成下面等式左邊的形式。可以簡略寫作: \\(\\underline{a}=\\left( \\begin{array}{c} a_1\\\\ a_2\\\\ a_3\\\\ \\end{array} \\right)\\)。\\(\\nabla\\)讀作nabla。\n\\(\\left( \\begin{array}{c} \\frac{\\partial F}{\\partial a_1}\\\\ \\frac{\\partial F}{\\partial a_2}\\\\ \\frac{\\partial F}{\\partial a_3}\\\\ \\end{array} \\right)=\\frac{\\partial F}{\\partial \\underline{a}}=\\nabla_{\\underline{a}}F\\)\n橫向量(行向量) row vector 在等號的右側，將數字寫成一行，左右用圓括號或者方括號包含在內的形式，被叫做橫向量(row vector):\n\\(\\underline{a}=(a_1,a_2,\\cdots,a_j,\\cdots,a_n), \\; \\textbf{a}=[a_1,a_2,\\cdots,a_j,\\cdots,a_n]\\)\n成分，維度，類型等的定義與列向量相同。另外注意，維度相同，但是一個是橫向量，一個是列向量的話，這兩個向量是不同類型的。\n\\(\\underline{x}=(x_1,x_2,x_3)\\)\n有時也可以不用逗號分隔成分。 寫作 \\((x_1 \\; x_2 \\;x_3)\\)。下同。 \\((\\frac{\\partial F}{\\partial x_1},\\frac{\\partial F}{\\partial x_2},\\frac{\\partial F}{\\partial x_3})=\\frac{\\partial F}{\\partial \\underline{x}}=\\nabla_{\\underline{x}F}\\) \\(\\underline{u}=(\\sin\\theta\\cos\\phi, \\sin\\theta\\cos\\theta, \\cos\\theta)\\) 向量的轉置 (vector transpose) 將列向量的每個成分，按照從上到下的順序，一字橫着排開寫成橫向量。這個向量稱爲原來列向量的轉置向量(transposed vector)。反之亦然。\n向量 \\(\\underline{a}\\) 的轉置向量，可以標記爲 \\(\\underline{a}^t,\\;\\underline{a}^\\prime,\\;^t\\underline{a},\\;\\underline{a}^T, \\;^T\\underline{a}\\) 各種形式。今後統一用 \\(\\underline{a}^t\\)。\n\\(\\underline{a}=\\left( \\begin{array}{c} a_1\\\\ a_2\\\\ a_3\\\\ \\end{array} \\right)\\) 的轉置向量我們會記爲：\n\\(\\underline{a}^t=\\left( \\begin{array}{c} a_1\\\\ a_2\\\\ a_3\\\\ \\end{array} \\right)^t=(a_1,a_2,a_3)\\) \\(\\underline{x}=(x_1.x_2,x_3)\\) 的轉置向量我們會記爲：\n\\(\\underline{x}^t=(x_1.x_2,x_3)^t=\\left( \\begin{array}{c} x_1\\\\ x_2\\\\ x_3\\\\ \\end{array} \\right)\\) ","date":1487289600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1487289600,"objectID":"caa2595ff4839ea5932aaaf5e1765956","permalink":"https://wangcc.me/post/2017-02-17/","publishdate":"2017-02-17T00:00:00Z","relpermalink":"/post/2017-02-17/","section":"post","summary":"第3章　向量 vector P31-34","tags":["learning notes","basic mathematics","linear algebra"],"title":"「統計解析のための線形代数」復習筆記8","type":"post"},{"authors":null,"categories":["statistics"],"content":" 分解平方和 1 樣本量均爲 \\(n\\) 的兩變量 \\(z, \\hat{z}\\) 如下表，已知這兩個變量滿足條件：\n\\(\\bar{z}=\\frac{1}{n}\\sum\\limits_{i=1}^nz_i=\\frac{1}{n}\\sum\\limits_{i=1}^n\\hat{z}_i=\\bar{\\hat{z}},\\) \\(\\sum\\limits_{i=1}^n(z_i-\\hat{z_i})(\\hat{z_i}-\\bar{z})=0\\)\n個体の番号 変量 \\(z\\) 変量 \\(\\hat{z}\\) \\(1\\) \\(z_1\\) \\(\\hat{z}_1\\) \\(2\\) \\(z_2\\) \\(\\hat{z}_2\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(i\\) \\(z_i\\) \\(\\hat{z}_i\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(n\\) \\(z_n\\) \\(\\hat{z}_n\\) 此時我們有：\n全平方和(全変動，總平方和，總變動， Total sum of Squares)：\n\\(S_T=(z_i-\\bar{z})^2+(z_2-\\bar{z})^2+\\cdots+(z_n-\\bar{z})^2\\\\ \\;\\;\\;\\;=\\sum\\limits_{i=1}^n(z_i-\\bar{z})^2\\) 回歸平方和(回歸變動，Regression sum of Squares)\n\\(S_R=(\\hat{z_1}-\\bar{\\hat{z}})^2+(\\hat{z_2}-\\bar{\\hat{z}})^2+\\cdots+(\\hat{z_n}-\\bar{\\hat{z}})^2\\\\ \\;\\;\\;\\;=\\sum\\limits_{i=1}^n(\\hat{z_i}-\\bar{\\hat{z}})^2=\\sum\\limits_{i=1}^n(\\hat{z_i}-\\bar{z})^2\\) 殘差平方和(誤差平方和，殘差變動，誤差變動，residual sum of Squares)\n\\(S_e=(z_1-\\hat{z_1})^2+(z_2-\\hat{z_2})^2+\\cdots+(z_n-\\hat{z_n})^2\\\\ \\;\\;\\;\\;=\\sum\\limits_{i=1}^n(z_i-\\hat{z_i})^2\\) 上面三個平方和之間，有如下的關系： \\[\\begin{equation} S_T=S_R+S_e \\tag{1} \\end{equation}\\] 既：全平方和等於殘差平方和與回歸平方和之和。(1)式被稱爲平方和的分解(decomposition of sum of squares) 證明(1)式 解： \\[ \\begin{equation} \\begin{split} S_T \u0026amp; = \\sum\\limits_{i=1}^n(z_i-\\bar{z})^2 \\\\ \u0026amp; = \\sum\\limits_{i=1}^n\\left\\{(z_i-\\hat{z_i})+(\\hat{z_i}-\\bar{z})\\right\\}^2\\\\ \u0026amp; = \\sum\\limits_{i=1}^n\\left\\{(z_i-\\hat{z_i})^2+(\\hat{z_i}-\\bar{z})^2+2(z_i-\\hat{z_i})(\\hat{z_i}-\\bar{z})\\right\\}\\\\ \u0026amp; = \\sum\\limits_{i=1}^n(z_i-\\hat{z_i})^2+\\sum\\limits_{i=1}^n(\\hat{z_i}-\\bar{z})^2 + 0\\\\ \u0026amp; = S_e + S_R \\end{split} \\end{equation} \\] 最後一步等式，利用了一開始給出的條件 \\(\\sum\\limits_{i=1}^n(z_i-\\hat{z_i})(\\hat{z_i}-\\bar{z})=0\\)\n這裏的平方和分解與回歸分析有着緊密的聯系。\n分解平方和 2 有樣本量爲 \\(n\\) 的變量 \\(z_1\\) 與樣本量爲 \\(m\\) 的變量 \\(z_2\\) 的數據如下表：\n変量 \\(z_1\\) 変量 \\(z_2\\) \\(z_{11}\\) \\(z_{12}\\) \\(z_{21}\\) \\(z_{22}\\) \\(\\vdots\\) \\(\\vdots\\) \\(z_{i1}\\) \\(z_{i2}\\) \\(\\vdots\\) \\(\\vdots\\) \\(z_{n1}\\) \\(\\vdots\\) \\(z_{m2}\\) 此時我們有：\n樣本平均值： \\(\\bar{z_1}=\\frac{1}{n}\\sum\\limits_{i=1}^nz_{i1}, \\;\\bar{z_2}=\\frac{1}{m}\\sum\\limits_{i=1}^mz_{i2}\\) 樣本總平均值： \\(\\bar{z}=\\frac{1}{n+m}(\\sum\\limits_{i=1}^nz_{i1}+\\sum\\limits_{i=1}^mz_{i2})\\) 全平方和 (全変動，總平方和，總變動, Total sum of Squares)：\n\\(S_T=\\left\\{(z_{11}-\\bar{z})^2+(z_{21}-\\bar{z})^2+\\cdots+(z_{n1}-\\bar{z})^2\\right\\}\\\\ \\;\\;\\;\\;\\;\\;\\;\\;+\\left\\{(z_{12}-\\bar{z})^2+(z_{22}-\\bar{z})^2+\\cdots+(z_{m2}-\\bar{z})^2\\right\\}\\\\ \\;\\;\\;\\;=\\sum\\limits_{i=1}^n(z_{i1}-\\bar{z})^2+\\sum\\limits_{i=1}^m(z_{i2}-\\bar{z})^2\\) 羣內平方和(組內平方和，級內平方和，羣內變動，級內變動，變量內平方和，變量內變動，Within-groups sum of Squares)：\n\\(S_W=\\left\\{(z_{11}-\\bar{z_1})^2+(z_{21}-\\bar{z_1})^2+\\cdots+(z_{n1}-\\bar{z_1})^2\\right\\}\\\\ \\;\\;\\;\\;\\;\\;\\;\\;+\\left\\{(z_{12}-\\bar{z_2})^2+(z_{22}-\\bar{z_2})^2+\\cdots+(z_{m2}-\\bar{z_2})^2\\right\\}\\\\ \\;\\;\\;\\;=\\sum\\limits_{i=1}^n(z_{i1}-\\bar{z_1})^2+\\sum\\limits_{i=1}^m(z_{i2}-\\bar{z_2})^2\\) 羣間平方和(組間平方和，級間平方和，羣間變動，級間變動，變量間平方和，變量間變動，Between-groups sum of Squares)：\n\\(S_B=\\left\\{(\\bar{z_1}-\\bar{z})^2+(\\bar{z_1}-\\bar{z})^2+\\cdots+(\\bar{z_1}-\\bar{z})^2\\right\\}\\\\ \\;\\;\\;\\;\\;\\;\\;\\;+\\left\\{(\\bar{z_2}-\\bar{z})^2+(\\bar{z_2}-\\bar{z})^2+\\cdots+(\\bar{z_2}-\\bar{z})^2\\right\\}\\\\ \\;\\;\\;\\;=\\sum\\limits_{i=1}^n(\\bar{z_1}-\\bar{z})^2+\\sum\\limits_{i=1}^m(\\bar{z_2}-\\bar{z})^2\\\\ \\;\\;\\;\\;=n(\\bar{z_1}-\\bar{z})^2+m(\\bar{z_2}-\\bar{z})^2\\) 上面三個平方和之間有如下的關系： \\[\\begin{equation} S_T=S_W+S_B \\tag{2} \\end{equation}\\] 證明(2)式 解： 注意利用：\\(\\sum\\limits_{i=1}^n(z_{i1}-\\bar{z_1})(\\bar{z_1}-\\bar{z})=(\\bar{z_1}-\\bar{z})\\sum\\limits_{i=1}^n(z_{i1}-\\bar{z_1})\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\:=(\\bar{z_1}-\\bar{z})(\\sum\\limits_{i=1}^nz_{i1}-n\\bar{z_1})\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\:=(\\bar{z_1}-\\bar{z})(n\\bar{z_1}-n\\bar{z_1})=0\\)\n因此\n\\[ \\begin{equation} \\begin{split} S_T \u0026amp; = \\sum_{i=1}^n(z_{i1}-\\bar{z})^2+\\sum_{i=1}^m(z_{i2}-\\bar{z})^2\\\\ \u0026amp; = \\sum_{i=1}^n\\left\\{(z_{i1}-\\bar{z_1})+(\\bar{z_1}-\\bar{z})\\right\\}^2\\\\ \u0026amp;\\;\\;\\;\\;\\; + \\sum_{i=1}^m\\left\\{(z_{i2}-\\bar{z_2})+(\\bar{z_2}-\\bar{z})\\right\\}^2\\\\ \u0026amp; = \\sum_{i=1}^n\\left\\{(z_{i1}-\\bar{z_1})^2+2(z_{i1}-\\bar{z_1})(\\bar{z_1}-\\bar{z})+(\\bar{z_1}-\\bar{z})^2\\right\\}\\\\ \u0026amp;\\;\\;\\;\\;\\; +\\sum_{i=1}^m\\left\\{(z_{i2}-\\bar{z_2})^2+2(z_{i2}-\\bar{z_2})(\\bar{z_2}-\\bar{z})+(\\bar{z_2}-\\bar{z})^2\\right\\}\\\\ \u0026amp; = \\sum_{i=1}^n(z_{i1}-\\bar{z_1})^2 + n(\\bar{z_1}-\\bar{z})^2\\\\ \u0026amp;\\;\\;\\;\\;\\; + \\sum_{i=1}^m(z_{i2}-\\bar{z_2})^2 + m(\\bar{z_2}-\\bar{z})^2\\\\ \u0026amp; = S_W+S_B \\end{split} \\end{equation} \\]\n變量的合成與加權 我們說，將 \\(p\\) 個變量 \\(x_1,x_2,\\cdots,x_p\\) 轉變成一次式：\\(w_1x_1+w_2x_2+\\cdots+w_px_p (=\\hat{y})\\) 的過程稱爲變量的合成 (linear combination of variables) \\(\\hat{y}\\) 被叫做合成變量。系數 \\(w_1,w_2,\\cdots,w_p\\) 被叫做權重 (weight)。假如 \\(x_1,x_2,\\cdots,x_p\\) 是 \\(p\\) 個科目的考試得分，那麼:\n\\(w_1=w_2=\\cdots=w_p=1\\) 時，\\(\\hat{y}\\) 意思就是 \\(p\\) 個科目的總分 \\(w_1=w_2=\\cdots=w_p=\\frac{1}{p}\\) 時，\\(\\hat{y}\\) 意思就是 \\(p\\) 個科目的平均分 多元變量分析中，我們實質上做的許多事就是思考如何合理的決定這個權重。\n","date":1487203200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1487203200,"objectID":"8d4cdc4566f2c20681c65a187e0beb52","permalink":"https://wangcc.me/post/2017-02-16/","publishdate":"2017-02-16T00:00:00Z","relpermalink":"/post/2017-02-16/","section":"post","summary":"第2章　統計の基礎 P26-30","tags":["learning notes","basic mathematics","linear algebra"],"title":"「統計解析のための線形代数」復習筆記7","type":"post"},{"authors":null,"categories":["statistics"],"content":" 數據的變換 平均值附近的偏差: 各個數值 \\(x_i\\) 與樣本平均值 \\(\\bar{x}\\) 的差 \\[x_i^\\prime=x_i-\\bar{x} (i = 1,2,\\cdots,n)\\] 稱爲數據 \\(x_i\\) 在它的平均值 \\(\\bar{x}\\) 附近的偏差(deviation)。通常我們說求偏差，指的是，對數據 \\(x_i\\) 進行偏差轉換。這個過程又被稱作是中心變換(centering) 關於偏差，我們列舉如下兩個有特徵的的概括統計： 樣本平均值： \\[\\begin{equation} \\bar{x}^\\prime=\\frac{1}{n}\\sum_{i=1}^nx_i^\\prime=0 \\tag{1} \\end{equation}\\] 樣本偏差平方和： \\[\\begin{equation} SS^\\prime=\\sum_{i=1}^n(x^\\prime)^2=SS \\tag{2} \\end{equation}\\] 練習：證明(1) 解： 證明(1): \\[\\bar{x}^\\prime=\\frac{\\sum\\limits_{i=1}^n(x_i-\\bar{x})}{n}\\\\ \\;\\;\\;=\\frac{\\sum\\limits_{i=1}^nx_i-n\\bar{x}}{n}\\\\ \\;\\;\\;=\\frac{\\sum\\limits_{i=1}^nx_i}{n}-\\bar{x}\\\\ \\;\\;\\;=\\bar{x}-\\bar{x}=0\\]\n數據的標準化： 將數據 \\(x_i\\) 的平均值 \\(\\bar{x}\\) 附近的偏差除以樣本標準偏差 \\(s\\) 從而獲得下面式子所表示的數據 \\(z_i\\) 的過程，被叫做數據的標準化 (standardization)： \\[\\begin{equation} z_i=\\frac{x_i-\\bar{x}}{s} \\tag{3} \\end{equation}\\] 標準化後的數據 \\(z_i\\) 的概括統計有下列特徵： 樣本平均值： \\[\\begin{equation} \\bar{z}=\\frac{1}{n}\\sum_{i=1}^nz_i=0 \\tag{4} \\end{equation}\\] 樣本方差: \\[\\begin{equation} s_{z}^2=\\frac{1}{n}\\sum_{i=1}^nz_i^2=1 \\tag{5} \\end{equation}\\] 由於標準化數據具有上述兩個非常顯著的特徵，均值爲 \\(0\\)，方差爲 \\(1\\)，因此我們實際分析數據過程中常常對數據進行標準化。標準化以後的數據，單位消失，變成了一組無名數 \\(\\divideontimes\\) 數據的標準化，有時你會看到被定義爲: \\[\\begin{equation} z_i=\\frac{x_i-\\bar{x}}{u} \\tag{6} \\end{equation}\\] 此時的不偏樣本方差爲： \\[\\begin{equation} u_z^2=\\frac{1}{n-1}\\sum_{i=1}{n}z_i^2=1 \\tag{7} \\end{equation}\\] 2變量數據的概括統計： 樣本量同爲 \\(n\\) 的 \\(2\\) 變量 \\(x_1,x_2\\) 的數據，表示爲如下表格： 個体の番号 変量 \\(x_1\\) 変量 \\(x_2\\) \\(1\\) \\(x_{11}\\) \\(x_{12}\\) \\(2\\) \\(x_{21}\\) \\(x_{22}\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(i\\) \\(x_{i1}\\) \\(x_{i2}\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(n\\) \\(x_{n1}\\) \\(x_{n2}\\) 按照變量 \\(x_1,x_2\\) 各自的定義： 樣本平均值：\\(\\bar{x_1}=\\frac{1}{n}\\sum\\limits_{i=1}^nx_{i1}, \\; \\bar{x_2}=\\frac{1}{n}\\sum\\limits_{i=1}^nx_{i2}\\) 樣本偏差平方和: \\(SS_1=\\sum\\limits_{i=1}^n(x_{i1}-\\bar{x_1})^2, \\; SS_2=\\sum\\limits_{i=1}^n(x_{i2}-\\bar{x_2})^2\\) 樣本方差： \\(s_1^2=\\frac{SS_1}{n}, \\; s_2^2=\\frac{SS_2}{n}\\) 樣本標準偏差： \\(s_1=\\sqrt{s_1^2}, \\; s_2=\\sqrt{s_2^2}\\) 不偏樣本方差： \\(u_1^2=\\frac{SS}{n-1}, \\; u_2^2=\\frac{SS_2}{n-1}\\) 不偏樣本方差平方根: \\(u_1=\\sqrt{u_1^2}, \\; u_2=\\sqrt{u_2^2}\\) 對於這樣一對變量 \\(x_1,x_2\\) 來說，我們又可以追加如下的概括統計： 樣本總體平均值： \\(\\bar{x}=\\frac{1}{n+n}(\\sum\\limits_{i-1}^nx_{i1}+\\sum\\limits_{i-1}^nx_{i2})\\)\n樣本方差積和(cross-product)： \\[\\begin{equation} \\begin{split} S_{12} \u0026amp; = \\sum_{i=1}^n(x_{i1}-\\bar{x_1})\\cdot(x_{i2}-\\bar{x_2})\\\\ \u0026amp; = \\sum_{i=1}^n(x_{i1}x_{i2}-\\bar{x_1}x_{i2}-x_{i1}\\bar{x_2}+\\bar{x_1}\\bar{x_2})\\\\ \u0026amp; = \\sum_{i=1}^nx_{i1}x_{i2}-\\bar{x_1}\\sum_{i=1}^nx_{i2}-{\\sum_{i=1}^nx_{i1}}\\bar{x_2}+n\\bar{x_1}\\bar{x_2}\\\\ \u0026amp; = \\sum_{i=1}^nx_{i1}x_{i2}-\\bar{x_1}\\cdot n\\bar{x_2}-n\\bar{x_1}\\cdot\\bar{x_2}+n\\bar{x_1}\\bar{x_2}\\\\ \u0026amp; = \\sum_{i=1}^nx_{i1}x_{i2}-n\\cdot\\bar{x_1}\\cdot\\bar{x_2}\\\\ \u0026amp; = \\sum_{i=1}^nx_{i1}x_{i2}-n\\cdot\\frac{\\sum\\limits_{i=1}^nx_{i1}}{n}\\cdot\\frac{\\sum\\limits_{i=1}^nx_{i2}}{n}\\\\ \u0026amp; = \\sum_{i=1}^nx_{i1}x_{i2}-\\frac{1}{n}(\\sum_{i=1}^nx_{i1})(\\sum_{i=1}^nx_{i2}) = S_{21} \\end{split} \\tag{8} \\end{equation}\\]\n樣本協方差(covariance，共分散)：\\(s_{12}=\\frac{S_{12}}{n}=\\frac{S_{21}}{n}=s_{21}\\)\n樣本相關系數 (correlation coefficient)： \\(r_{11}=r_{22}=1\\) \\[\\begin{equation} \\begin{split} r_{12} \u0026amp; = \\frac{S_{12}}{\\sqrt{SS_1\\cdot SS_2}}\\\\ \u0026amp; = \\frac{\\frac{S_{12}}{n}}{\\sqrt{\\frac{SS_1}{n}}\\cdot\\sqrt{\\frac{SS_2}{n}}}\\\\ \u0026amp; = \\frac{s_{12}}{s_1s_2}=r_{21}\\\\ \\end{split} \\tag{9} \\end{equation}\\]\n此處我們再來證明一下，標準化以後的數據的樣本協方差(covariance)，和標準化以前原來的數據的樣本相關系數(correlation coefficient)是相等的： 假設，\\(x_{i1}\\) 標準化以後爲 \\(z_{i1}=\\frac{x_{i1}-\\bar{x_1}}{s_1}\\)； \\(x_{i2}\\) 標準化以後爲 \\(z_{i2}=\\frac{x_{i2}-\\bar{x_2}}{s_1}\\)。 此時，\\(z_{i1}, z_{i2}\\) 的樣本協方差可以計算如下: \\[\\begin{equation} \\begin{split} s_{z_{12}} \u0026amp; = \\frac{S_{z_{12}}}{n} \\\\ \u0026amp; = \\frac{1}{n}\\cdot\\sum_{i=1}^n(z_{i1}-\\bar{z_1})(z_{i2}-\\bar{z_2})\\\\ \u0026amp; = \\frac{1}{n}\\sum_{i=1}^nz_{i1}z_{i2}\\\\ \u0026amp; = \\frac{1}{n}\\sum_{i=1}^n(\\frac{x_{i1}-\\bar{x_1}}{s_1})(\\frac{x_{i2}-\\bar{x_2}}{s_2})\\\\ \u0026amp; = \\frac{S_{12}}{n}\\cdot\\frac{1}{s_1s_2} = \\frac{s_{12}}{s_1s_2}=r_{12} \\end{split} \\tag{10} \\end{equation}\\]\n","date":1487116800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1487116800,"objectID":"2b2c607784a0afde465076ec4dbbac12","permalink":"https://wangcc.me/post/2017-02-15/","publishdate":"2017-02-15T00:00:00Z","relpermalink":"/post/2017-02-15/","section":"post","summary":"第2章　統計の基礎 P22-25","tags":["learning notes","basic mathematics","linear algebra"],"title":"「統計解析のための線形代数」復習筆記6","type":"post"},{"authors":null,"categories":["statistics"],"content":" 2017-02-15 updated. 數據的種類和尺度 表1. 20歳の若者9名のデータ 性別 健康状態 体温 身長 男 女 極良 良好 普通 不良 極悪 °C cm 1 1 0 1 0 0 0 0 36.9 155 2 0 1 0 1 0 0 0 36.5 190 3 1 0 0 0 0 1 0 36.7 165 4 0 1 0 0 0 0 1 39 155 5 0 1 0 0 1 0 0 38.1 167 6 1 0 0 0 0 1 0 36.2 180 7 1 0 0 0 0 0 1 36.6 178 8 0 1 0 0 1 0 0 36.7 170 9 0 1 1 0 0 0 0 36.5 166 數據按照尺度類型分4種： 定性數據：(qualitative data/categorical data) 名義尺度 (nominal scale): 如番號，性別等，僅用於識別或者區分對象。 順序尺度 (ordinal scale): 如表1中的健康狀態，既具有名義尺度的性質，也具有順序(順位，前後，程度等)意義。 定量數據：(quantitative data) 間隔尺度 (interval scale): 又稱區間尺度，距離尺度。如體溫等數值之間的差具有意義，可以設定原點（零）的尺度。 比例尺度 (ratio scale) 如身高(身長)，不同人的身高差有意義，同時原點（零）又無意義。（身高爲零的人是不存在的） 樣本與總體(標本/サンプルと母集団)： 表格1 中， (1) 全國20歲的年輕人全體的性別，健康狀態，體溫，身高的數據 (2) 任意抽選數名(此處爲9名)年輕人的性別，健康狀態，體溫，身高的數據。 (1) 稱爲總體(population), (2) 稱爲樣本(sample) 一般的，我們稱問卷調查的對象個人爲個體(subject, individual)，人數爲樣本量(size)，我們也常常假設總體有無窮大。在多元變量分析中，如果我們稱數據，通常只能是指樣本數據。我們在統計分析中，一般是通過樣本去推測總體的狀況或者利用一些樣本的參數(parameter)來描述總體，去進行檢驗等。 變量(variable)： 表1 中變量有“性別”，“健康狀態”，“體溫”，“身長”四個。 進一步的，性別又細分了“男”，“女”，健康狀態又分爲“極好”，“良好”，“普通”，“不良”，“極差”五個小項目，這些小項目整個可以視爲一個變量，也可以視爲單獨的變量，如果視爲單獨變量，表1 中就共有9個變量。 性別，健康狀態的各個變量中，我們看到表1 中的數據只有1或者0，這樣的變量稱爲啞變量(dummy variable，ダミー変量)。 特別地，我們可以稱性別和健康狀態的變量爲項目(item)，稱底下的小項目爲分類(category)。我們又稱以這樣有項目，下面有分類的變量爲項目分類型數據(item-categorical data)。 多元變量分析，顧名思義，指的是對一個具有一個以上變量的數據進行統計學分析的過程。 單變量數據 將數據進行總結，分析提取特徵的過程，稱爲概括統計(summary statistics)。 下表中爲樣本量 \\(n\\) 的單變量數據，我們看看該數據可以有哪些概括統計 個体の番号 変量 \\(s\\) \\(1\\) \\(x_1\\) \\(2\\) \\(x_2\\) \\(\\vdots\\) \\(\\vdots\\) \\(i\\) \\(i\\) \\(\\vdots\\) \\(\\vdots\\) \\(n\\) \\(n\\) 樣本平均值(mean)： \\(\\bar{x}=\\frac{x_1+x_2+\\cdots+x_n}{n}=\\frac{1}{n}\\sum\\limits_{i=1}^nx_i\\)\n樣本範圍(range)： \\(R=\\max(x_i) - \\min(x_i)\\)\n樣本平方和(偏差平方和，Sum of Squares):\n\\(SS=(x_1-\\bar{x})^2+(x_2-\\bar{x})^2+\\cdots+(x_n-\\bar{x})^2\\\\ \\;\\;\\;\\;\\:=\\sum\\limits_{i=1}^{n}(x_i-\\bar{x})^2\\\\ \\;\\;\\;\\;\\:=\\sum\\limits_{i=1}^n(x_i^2-2\\bar{x}x_i+\\bar{x}^2)\\\\ \\;\\;\\;\\;\\:=\\sum\\limits_{i=1}^nx_i^2-2\\bar{x}\\sum\\limits_{i=1}^nx_i+\\sum\\limits_{i=1}^{n}\\bar{x}^2\\\\ \\;\\;\\;\\;\\:=\\sum\\limits_{i=1}^nx_i^2-2\\cdot\\frac{\\sum\\limits_{i=1}^nx_i}{n}\\cdot\\sum\\limits_{i=1}^nx_i+n\\cdot(\\frac{\\sum\\limits_{i=1}^nx_i}{n})^2\\\\ \\;\\;\\;\\;\\:=\\sum\\limits_{i=1}^nx_i^2-\\frac{1}{n}(\\sum\\limits_{i=1}^nx_i)^2\\)\n樣本方差(variance, 分散)：\\(s^2=\\frac{SS}{n}\\)\n樣本標準差(standard deviation, S.D., 標準偏差)： \\(s=\\sqrt{s^2}\\)\n無偏樣本方差(unbiased sample variance, 不偏標本分散)：\\(u^2=\\frac{SS}{n-1}\\)\n無偏樣本標準差平方根(不偏標本分散平方根)： \\(u=\\sqrt{u^2}\\)\n通在多元變量分析中，常常利用的思想是將變動(variability)或者是波動(dispersion)最大化，最小化。此處說的變動和波動是指上面提到的無偏樣本方差。\n","date":1487030400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1487030400,"objectID":"d082c51bcefd24a709a1e381064d5a93","permalink":"https://wangcc.me/post/2017-02-13/","publishdate":"2017-02-14T00:00:00Z","relpermalink":"/post/2017-02-13/","section":"post","summary":"第2章　統計の基礎 P19-22","tags":["learning notes","basic mathematics","linear algebra"],"title":"「統計解析のための線形代数」復習筆記5","type":"post"},{"authors":null,"categories":["statistics"],"content":" 連立方程式 (simultaneous equations) 連立方程式，將與第六章談的特徵值問題(固有値問題)有緊密聯系，此處我們一起觀察幾種不同的組合：\n解同次連立1次方程式 \\(\\left\\{ \\begin{array}{ll} (1)\\;a_1+2a_2+3a_3 = 0 \\\\ (2)\\;2a_1+4a_2+5a_3 = 0 \\;\\\\ (3)\\;3a_1+5a_2+6a_3 = 0 \\\\ \\end{array} \\right.\\) 由 \\(2\\times(1)-(2)\\) 可得 \\(a_3=0\\) 。 代入 \\((1),(2),(3)\\) 式後，\\((3)-(2)\\) 可得 \\(a_1=-a_2\\) 。 代入 \\((1)\\) 式可得 \\(a_2=0\\) 。 再代入 \\((4)\\) 式可知 \\(a_1=0\\) 。最終可得 \\(a_1=a_2=a_3=0\\) 其實上述問題不解自明 (trivial solution)。 那麼同次1次連立方程式 (homogeneous system) 除了自明解之外，還有別的解嗎? 我們再看下面一例。\n解 \\(\\left\\{ \\begin{array}{ll} (1)\\;4a_1+3a_2+6a_3 = 0 \\\\ (2)\\;2a_1+a_2+4a_3 = 0 \\;\\\\ (3)\\;a_1+a_2+a_3 = 0 \\\\ \\end{array} \\right.\\) 上述方程表面上看有三個式子，實際上由於 \\((3)=\\left\\{(1)-(2)\\right\\}\\div2\\) 只有2個有意義的方程式。如此這般，有3個未知數，卻只有兩個連立方程組，是無法求解的。如果將三個未知數中的一個例如 \\(a_3\\) 視爲常數(定数) (寫作：\\(s\\) ) 即： \\((4)\\;a_3=s\\) 整理方程組得到新的連立方程 \\(\\left\\{ \\begin{array}{ll} (1^\\prime)\\;4a_1+3a_2 = -6s \\\\ (2^\\prime)\\;2a_1+a_2 = -4s \\;\\\\ \\end{array} \\right.\\) 由 \\((1^\\prime)-2\\times(2^\\prime)\\) 可得 \\(a_2=2s\\) 。代入 \\((2^\\prime)\\) 可得 \\(a_1=-3s\\)。因此我們得到 \\(a_1=-3s,a_2=2s,a_3=s\\) 且 \\(s\\) 爲任意常數，故此連立方程組的解有無數組。當且僅當 \\(s=0\\) 時方程組有自明解， \\(s\\neq0\\) 時此連立方程組的解爲非自明解 (non-trivial solution)。如果將其他未知數視爲常數(定数)時，求得的解會有變化嗎？\n若視 \\(a_2=s\\) 求解連立方程的解時，我們會獲得 \\(a_1=-\\frac{3}{2}s, a_2=s, a_3=-\\frac{1}{2}s\\)。若視 \\(a_1=s\\) 時，計算可得 \\(a_1=s, a_2=-\\frac{2}{3}s,a_3=-\\frac{1}{3}s\\)。\n由此可見，非自明解表面看去各不相同，但是都滿足了 \\(a_1:a_2:a_3=-3:2:1\\) 的本質條件。\n解 \\(\\left\\{ \\begin{array}{ll} (1)\\;4a_1+3a_2+6a_3 = 0 \\\\ (2)\\;2a_1+a_2+4a_3 = 0 \\;\\\\ (3)\\;a_1^2+a_2^2+a_3^2 = 0 \\\\ \\end{array} \\right.\\) 上述方程組其實是將例題2.中的方程 \\((3)\\) 替換成了2次方程。\n\\(3\\times(2)-(1)\\) 可得 \\(a_1=-3a_3\\) \\((1)-2\\times(2)\\) 可得 \\(a_2=2a_3\\) 以上代入 \\((3)\\) 可得， \\(a_3 = \\pm \\frac{1}{\\sqrt{14}}\\)。\n總結一下：　\\(a_1=\\mp \\frac{3}{\\sqrt{14}}, a_2=\\pm \\frac{2}{\\sqrt{14}}, a_3=\\pm\\frac{1}{\\sqrt{14}}\\) (複号同順 double-sign corresponds)\n解 \\(a_1+2a_2-3a_3=0\\) 上面的方程只有一個，並不是連立方程組，將其中兩個未知數視爲常數時就變成了只有一個未知數的方程。例如視，\\(a_2=s, a_3=t\\) 代入上述方程則可以得到: \\(a_1=-2s+3t\\)，因此，此方程的解爲： \\(a_1=-2s+3t, a_2=s, a_3=t\\)，\\(s,t\\) 爲任意常數，有無數組解。\n練習: 解下列連立方程組 \\(\\left\\{ \\begin{array}{ll} (1)\\;2a_1-3a_2 = 0 \\\\ (2)\\;-4a_1+6a_2 = 0 \\;\\\\ \\end{array} \\right.\\)\n\\(\\left\\{ \\begin{array}{ll} (1)\\;2a_1-3a_2 = 0 \\\\ (2)\\;-4a_1+6a_2 = 0 \\;\\\\ (3)\\;a_1^2+a_2^2 = 0 \\\\ \\end{array} \\right.\\) 解 \\(\\because\\) \\((2)=-2\\times(1)\\) 實質上方程組僅有一個方程。\n\\(\\therefore a_1=\\frac{3}{2}s, a_2=s\\)\n只需要求解例題1 中符合方程 \\((3)\\) 的解即可。 \\(\\therefore a_1=\\pm\\frac{3}{\\sqrt{13}}, a_2=\\pm\\frac{2}{\\sqrt{13}}\\) (複号同順 double-sign corresponds)\n","date":1486857600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1486857600,"objectID":"3ce05329de0e85c07f6d158ea34c0c94","permalink":"https://wangcc.me/post/2017-02-12-t/","publishdate":"2017-02-12T00:00:00Z","relpermalink":"/post/2017-02-12-t/","section":"post","summary":"第1章　数学の基礎 P15-17","tags":["learning notes","basic mathematics","linear algebra"],"title":"「統計解析のための線形代数」復習筆記4","type":"post"},{"authors":null,"categories":["statistics"],"content":" 函數的最大值最小值問題 沒有制約條件的情況 函數 \\(F(a_1,a_2,\\dots,a_i,\\dots,a_n)\\) 取最大值或者最小值時，以下的連立方程 \\[\\frac{\\partial F}{\\partial a_1}=0,\\frac{\\partial F}{\\partial a_2}=0，\\frac{\\partial F}{\\partial a_3}=0, \\dots,\\frac{\\partial F}{\\partial a_i}=0, \\dots, \\frac{\\partial F}{\\partial a_n}=0\\] 要成立(必要條件)。\n1.已知下列方程有最小值，求當該方程取最小值時\\(a_1,a_2\\)的值 \\[F(a_1,a_2)=\\left\\{y_1-(a_1+a_2x_1)\\right\\}^2+\\left\\{y_2-(a_1+a_2x_2)\\right\\}^2+\\cdots+\\left\\{y_n-(a_1+a_2x_n)\\right\\}^2\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=\\sum\\limits_{i=1}^n\\left\\{y_i-(a_1+a_2x_i)\\right\\}^2\\\\\\]\n\\[\\begin{align} \\frac{\\partial F}{\\partial a_1}\u0026amp;=-2\\left\\{y_1-(a_1+a_2x_1)\\right\\}-2\\left\\{y_2-(a_1+a_2x_2)\\right\\}-\\cdots-2\\left\\{y_n-(a_1+a_2x_n)\\right\\}\\\\ \u0026amp;= -2\\sum_{i=1}^n\\left\\{y_i-(a_1+a_2x_i)\\right\\}=0 \\Leftrightarrow \\sum_{i=1}^n\\left\\{y_i-(a_1+a_2x_i)\\right\\}=0\\\\ \\Leftrightarrow \\sum_{i=1}^ny_i \u0026amp;= a_1\\cdot n+a_2\\sum_{i=1}^nx_i (1)\\\\ \\\\ \\frac{\\partial F}{\\partial a_2}\u0026amp;=-2x_1\\left\\{y_1-(a_1+a_2x_1)\\right\\}-2x_2\\left\\{y_2-(a_1+a_2x_2)\\right\\}-\\cdots-2x_3\\left\\{y_n-(a_1+a_2x_n)\\right\\}\\\\ \u0026amp;= -2\\sum_{i=1}^nx_i\\left\\{y_i-(a_1+a_2x_i)\\right\\}=0\\\\ \\Leftrightarrow \\sum_{i=1}^nx_iy_i \u0026amp;=a_1\\sum_{i=1}^nx_i+a_2\\sum_{i=1}^nx_i^2 (2)\\\\ \u0026amp;將(1)(2)連立方程求解即可。在回歸分析中，\\\\ \u0026amp;這個連立方程組被稱作正規方程組(Normal \\;equation) \\end{align}\\]\n求下列方程取最大或者最小值時的\\(a_1,a_2,a_3\\)的大小： \\[F(a_1,a_2,a_3)=a_1^2+a_1a_2+a_1a_3+a_2^2+a_2a_3+a_3^2-6a_1-3a_2-7a_3\\] \\[\\begin{align} 解連立方程：\\\\ \\frac{\\partial F}{\\partial a_1} \u0026amp; = 2a_1+a_2+a_3-6=0\\\\ \\frac{\\partial F}{\\partial a_2} \u0026amp; = a_1+2a_2+a_3-3=0\\\\ \\frac{\\partial F}{\\partial a_3} \u0026amp; = a_1+a_2+2a_3-7=0\\\\ 答：\u0026amp; a_1=2, a_2=-1,a_3=3 \\end{align}\\]\n","date":1486684800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1486684800,"objectID":"448bf866d546f3d93218dba16423424a","permalink":"https://wangcc.me/post/2017-02-10/","publishdate":"2017-02-10T00:00:00Z","relpermalink":"/post/2017-02-10/","section":"post","summary":"第1章　数学の基礎 P11-14","tags":["learning notes","basic mathematics","linear algebra"],"title":"「統計解析のための線形代数」復習筆記3","type":"post"},{"authors":null,"categories":["statistics"],"content":" 偏微分 1個變量的函數的微分 公式： 函數 \\(f(a)\\) 關於變量 \\(a\\) 的微分，被定義爲： \\(\\lim\\limits_{h \\to 0} \\frac{f(a+h)-f(a)}{h}\\) , 寫作 \\(\\frac{df}{da}\\), 具有下列性質：\n\\(f(a) = a^n\\) 時， \\(\\frac{df}{da} = na^{n-1}\\) 重要 \\(\\frac{d}{da}\\left\\{kf(a)+lg(a)\\right\\}=k\\frac{df}{da}+l\\frac{dg}{da}\\) (\\(k,l\\) 是常數) \\(\\frac{d}{da}\\left\\{f(a) \\cdot g(a)\\right\\}=\\frac{df}{da}g(a)+f{a}\\frac{dg}{da}\\) \\(\\frac{d}{da}\\left\\{\\frac{f(a)}{g(a)}\\right\\}=\\frac{\\frac{df}{da}g(a)-f(a)\\frac{dg}{da}}{\\left\\{g(a)\\right\\}^2}\\), 特別的有，\\(\\frac{d}{da}\\left\\{\\frac{1}{g(a)}\\right\\}=-\\frac{\\frac{dg}{da}}{\\left\\{g(a)\\right\\}^2}\\) \\(y=f(b), b=g(a)\\) 時， \\(\\frac{dy}{da}=\\frac{dy}{db}\\frac{db}{da}\\) 2次（2階）微分 【二階導數】:\n\\(f(a)\\) 關於常數 \\(a\\) 的微分 \\(\\frac{df}{da}\\) 的二次微分表示爲： \\(\\frac{d^2f}{da^2}\\)\n多個變量的函數的微分 偏微分 包含了 \\(n\\) 個獨立變量 \\(a_1, a_2,a_3,\\cdots,a_i,\\cdots,a_n\\)的函數，即多變量函數 \\(F(a_1, a_2,a_3,\\cdots,a_i,\\cdots,a_n)\\) 關於 \\(a_i (i=1,2,\\cdots,n)\\) 的偏微分 (partial differentiation) 的定義是，把 \\(a_i\\) 以外的獨立變量當做常數（定数），將函數 \\(F\\) 對變量 \\(a_i\\) 求微分，寫作： \\(\\frac{\\partial F}{\\partial a_i}\\)。\n以下爲了便於說明，以三個變量爲例。\n函數 \\(F(a_1,a_2,a_3)=a_1+a_2+a_3=\\sum\\limits_{i=1}^3a_i\\) 對於三個獨立變量分別求偏微分： \\[\\frac{\\partial F}{\\partial a_1}=1，\\frac{\\partial F}{\\partial a_2}=1， \\frac{\\partial F}{\\partial a_3}=1\\]\n函數 \\(F(a_1,a_2,a_3)=a_1b_1+a_2b_2+a_3b_3=\\sum\\limits_{i=1}^3a_ib_i\\) 對於三個獨立變量分別求偏微分： \\[\\frac{\\partial F}{\\partial a_1}=b_1，\\frac{\\partial F}{\\partial a_2}=b_2， \\frac{\\partial F}{\\partial a_3}=b_3\\]\n函數 \\(F(a_1,a_2,a_3)=a_1^2+a_2^2+a_3^2=a_1\\cdot a_1+a_2\\cdot a_2+a_3\\cdot a_3\\\\=\\sum\\limits_{i=1}^3a_i^2=\\sum\\limits_{i=1}^3a_i\\cdot a_i \\;對三個變量分別求偏微分：\\)　\\[\\frac{\\partial F}{\\partial a_1}=2a_1，\\frac{\\partial F}{\\partial a_2}=2a_2， \\frac{\\partial F}{\\partial a_3}=2a_3\\]\n函數 \\(F(a_1,a_2,a_3)=\\lambda_1a_1^2+\\lambda_2a_2^2+\\lambda_3a_3^2=a_1\\cdot\\lambda_1\\cdot a_1+a_2\\cdot\\lambda_2\\cdot a_2+a_3\\cdot\\lambda_3\\cdot a_3\\\\=\\sum\\limits_{i=1}^3\\lambda_ia_i^2=\\sum\\limits_{i=1}^3a_i\\cdot\\lambda_i\\cdot a_i \\; 對三個變量分別求偏微分：\\) \\[\\frac{\\partial F}{\\partial a_1}=2\\lambda_1a_1，\\frac{\\partial F}{\\partial a_2}=2\\lambda_2a_2， \\frac{\\partial F}{\\partial a_3}=2\\lambda_3a_3\\]\n函數 \\(F(a_1,a_2,a_3)=(b_1-\\lambda a_1)^2+(b_2-\\lambda a_2)^2+(b_3-\\lambda a_3)^2\\\\=\\sum\\limits_{i=1}^3(b_i-\\lambda a_i)^2=\\sum\\limits_{i=1}^3(b_i-\\lambda a_i)(b_i-\\lambda a_i)\\;對三個變量求偏微分：\\) \\[\\frac{\\partial F}{\\partial a_1}=-2\\lambda(b_1-\\lambda a_1)，\\frac{\\partial F}{\\partial a_2}=-2\\lambda(b_2-\\lambda a_2)， \\frac{\\partial F}{\\partial a_3}=-2\\lambda(b_3-\\lambda a_3)\\]\n函數 \\(F = a_{11}x_1y_1 + a_{12}x_1y_2 + a_{13}x_1y_3 \\\\ \\;\\;\\;\\;\\;\\;+a_{21}x_2y_1+a_{22}x_2y_2+a_{23}x_2y_3\\\\ \\;\\;\\;\\;\\;\\;+a_{31}x_3y_1+a_{32}x_3y_2+a_{33}x_3y_3\\\\ \\;\\;\\;=\\sum\\limits_{i=1}^3\\sum\\limits_{i=1}^3a_{ij}x_iy_j\\;對三個變量求偏微分：\\)\n\\(F(x_1,x_2,x_3)\\), 即視爲 \\(x_1,x_2,x_3\\) 的函數的時候： \\[ \\frac{\\partial F}{\\partial x_1}=a_{11}y_1+a_{12}y_2+a_{13}y_3=\\sum_{j=1}^3a_{1j}y_j \\\\ \\frac{\\partial F}{\\partial x_2}=a_{21}y_1+a_{22}y_2+a_{23}y_3=\\sum_{j=1}^3a_{2j}y_j \\\\ \\frac{\\partial F}{\\partial x_3}=a_{31}y_1+a_{32}y_2+a_{33}y_3=\\sum_{j=1}^3a_{3j}y_j \\\\ 將上面三個式子總結一下就是: \\\\ \\frac{\\partial F}{\\partial x_i}=\\sum_{j=1}^3a_{ij}y_j (i=1,2,3) \\]\n\\(F(y_1,y_2,y_3)\\), 即視爲 \\(y_1,y_2,y_3\\) 的函數的時候： \\[ \\frac{\\partial F}{\\partial y_1}=a_{11}x_1+a_{21}x_2+a_{31}x_3=\\sum_{i=1}^3a_{i1}x_i \\\\ \\frac{\\partial F}{\\partial y_2}=a_{12}x_1+a_{22}x_2+a_{32}x_3=\\sum_{i=1}^3a_{i2}x_i \\\\ \\frac{\\partial F}{\\partial y_3}=a_{13}x_1+a_{32}x_2+a_{33}x_3=\\sum_{i=1}^3a_{i3}x_i \\\\ 將上面三個式子總結一下就是: \\\\ \\frac{\\partial F}{\\partial x_i}=\\sum_{i=1}^3a_{ij}x_i (j=1,2,3) \\]\n函數 \\(F(x_1,x_2,x_3)=a_{11}x_1x_1+a_{12}x_1x_2+a_{13}x_1x_3 \\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;+a_{12}x_2x_1+a_{12}x_2x_2+a_{23}x_2x_3\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;+a_{13}x_3x_1+a_{23}x_3x_2+a_{33}x_3x_3\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=a_{11}x_1^2+2a_{12}x_1x_2+2a_{13}x_1x_3+a_{22}x_2^2+2a_{23}x_2x_3+a_{33}x_3^2\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=\\sum\\limits_{i=1}^3a_{ii}x_i^2+2\\mathop{\\sum\\limits^3\\sum\\limits^3}\\limits_{i\u0026lt;j}a_{ij}x_ix_j\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;==\\sum\\limits_{i=1}^3x_ia_{ii}x_i+2\\mathop{\\sum\\limits^3\\sum\\limits^3}\\limits_{i\u0026lt;j}x_ia_{ij}x_j\\;對三個變量求偏微分：\\) \\[\\frac{\\partial F}{\\partial x_1}=2(a_{11}x_1+a_{12}x_2+a_{13}x_3)\\\\ \\frac{\\partial F}{\\partial x_2}=2(a_{12}x_1+a_{22}x_2+a_{23}x_3)\\\\ \\frac{\\partial F}{\\partial x_3}=2(a_{13}x_1+a_{23}x_2+a_{33}x_3)\\]\n函數 \\(F(x_1,x_2,x_3)=a_{11}x_1x_1+a_{12}x_1x_2+a_{13}x_1x_3 \\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;+a_{12}x_2x_1+a_{12}x_2x_2+a_{23}x_2x_3\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;+a_{13}x_3x_1+a_{23}x_3x_2+a_{33}x_3x_3\\\\ G(x_1,x_2,x_3)=b_{11}x_1x_1+b_{12}x_1x_2+b_{13}x_1x_3 \\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;+b_{12}x_2x_1+b_{12}x_2x_2+b_{23}x_2x_3\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;+b_{13}x_3x_1+b_{23}x_3x_2+b_{33}x_3x_3\\\\\\;對三個變量求偏微分：\\)\n\\[ \\frac{\\partial}{\\partial x_1}(\\frac{F}{G})=\\frac{\\frac{\\partial F}{\\partial x_1}G(x_1,x_2,x_3)-F(x_1,x_2,x_3)\\frac{\\partial G}{\\partial x_1}}{\\left\\{G(x_1,x_2,x_3)\\right\\}^2}\\\\ =2\\cdot \\frac{(a_{11}x_1+a_{12}x_2+a_{13}x_3)\\cdot G(x_1,x_2,x_3)-F(x_1,x_2,x_3)\\cdot (b_{11}x_1+b_{12}x_2+b_{13}x_3)}{\\left\\{G(x_1,x_2,x_3)\\right\\}^2}\\\\ \\\\ \\frac{\\partial}{\\partial x_2}(\\frac{F}{G})=\\frac{\\frac{\\partial F}{\\partial x_2}G(x_1,x_2,x_3)-F(x_1,x_2,x_3)\\frac{\\partial G}{\\partial x_2}}{\\left\\{G(x_1,x_2,x_3)\\right\\}^2}\\\\ =2\\cdot \\frac{(a_{12}x_1+a_{22}x_2+a_{23}x_3)\\cdot G(x_1,x_2,x_3)-F(x_1,x_2,x_3)\\cdot (b_{12}x_1+b_{22}x_2+b_{23}x_3)}{\\left\\{G(x_1,x_2,x_3)\\right\\}^2}\\\\ \\frac{\\partial}{\\partial x_3}(\\frac{F}{G})=\\frac{\\frac{\\partial F}{\\partial x_3}G(x_1,x_2,x_3)-F(x_1,x_2,x_3)\\frac{\\partial G}{\\partial x_3}}{\\left\\{G(x_1,x_2,x_3)\\right\\}^2}\\\\ =2\\cdot \\frac{(a_{13}x_1+a_{23}x_2+a_{33}x_3)\\cdot G(x_1,x_2,x_3)-F(x_1,x_2,x_3)\\cdot (b_{13}x_1+b_{23}x_2+b_{33}x_3)}{\\left\\{G(x_1,x_2,x_3)\\right\\}^2}\\\\ \\]\n2次（2階）偏微分 【二階導數】: 函數 \\(F(a_1,a_2,\\cdots,a_n)\\) 對 \\(a_i\\) 取偏微分 \\(\\frac{\\partial F}{\\partial a_i}\\) 時，記作 \\(\\frac{\\partial^2 F}{\\partial a_i^2}\\) ; 取變量 \\(a_j\\) 的偏微分時記作 \\(\\frac{\\partial^2 F}{\\partial a_i\\partial a_j}\\) 或者 \\(\\frac{\\partial^2 F}{\\partial a_j\\partial a_i}\\)。 這些都被稱爲是函數 \\(F\\) 的2次（2階）偏微分。\n","date":1486512000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1486512000,"objectID":"57d8fd4102e8de9c9d9436db7bfd4808","permalink":"https://wangcc.me/post/2017-02-08/","publishdate":"2017-02-08T00:00:00Z","relpermalink":"/post/2017-02-08/","section":"post","summary":"第1章　数学の基礎 P6-10","tags":["learning notes","basic mathematics","linear algebra"],"title":"「統計解析のための線形代数」復習筆記2","type":"post"},{"authors":null,"categories":["statistics"],"content":" 和記號\\(\\sum\\) \\(\\sum\\) 的性質 (1) 下標(添字) \\(x_1 + x_2 + x_3 + \\dots + x_n\\) 記作如下:\\[\\sum_{i=1}^{n}x_i\\] \\(\\sum_{i=1}^{n}x_i\\) 中的\\(i\\) 稱爲dummy index 可以簡略寫爲：\\(\\sum x\\) 或者 \\(\\sum_1 x_i\\), \\(\\sum x_i\\) \\(\\sum\\) 的性質 (2) \\[\\begin{equation} \\sum_{i=1}^{n}(ax_i + by_i)= a\\sum_{i=1}^{n}x_i + b\\sum_{i=1}^{n}y_i \\tag{1} \\end{equation}\\] \\(\\sum_{i=1}^{n}ax_i = a\\sum_{i=1}^{n}x_i\\) 常數(定数)可以提前 \\(\\sum_{i=1}^{n}a = na\\) \\(\\sum_{i=1}^{n}1 = n\\) \\(\\sum_{i=1}^{n}(ax_i+b) = a\\sum_{i=1}^{n}x_i + nb\\) 公式(1)的應用: \\[ \\begin{aligned} \\sum_{i=1}^{n}(ax_i -by_i)^2 \u0026amp;= \\sum_{i=1}^{n}(a^2x_i^2 - 2abx_iy_i + b^2y_i^2) \\\\ \u0026amp;= \\sum_{i=1}^{n}a^2x_i^2 -\\sum_{i=1}^{n}2abx_iy_i + \\sum_{i=1}^{n}b^2y_i^2 \\\\ \u0026amp;= a^2\\sum_{i=1}^{n}x_i^2 - 2ab\\sum_{i=1}^{n}x_iy_i + b^2\\sum_{i=1}^{n}y_i^2 \\end{aligned} \\] 但是，乘法或平方有如下性質，計算方差(分散)或者相關系數時需要注意：\\[\\sum_{i=1}^{n}x_i^2 \\neq (\\sum_{i=1}^{n}x_i)^2\\] 以及 \\[\\sum_{i=1}^{n}x_iy_i \\neq (\\sum_{i=1}^{n}x_i)(\\sum_{i=1}^{n}y_i)\\] 自然數的冪運算之和(冪[べき]乗の和)的公式: \\[ \\begin{aligned} 1+2+3+\\dots+n \u0026amp;= \\sum_{t=1}^{n}t = \\frac{n(n+1)}{2}\\\\ 1^2+2^2+3^2+\\dots+n^2 \u0026amp;= \\sum_{t=1}^{n}t^2 = \\frac{n(n+1)(2n+1)}{6} \\\\ 1^3+2^3+3^3+\\dots+n^3 \u0026amp;= \\sum_{t=1}^{n}t^3 = {\\frac{n(n+1)}{2}}^2 \\\\ 1^4+2^4+3^4+\\dots+n^4 \u0026amp;= \\sum_{t=1}^{n}t^4 = \\frac{n(n+1)(2n+1)(3n^2+3n-1)}{30} \\end{aligned} \\] 上面的公式將會應用在時間序列分析，斯皮尔曼等级相关系数(スピアマンの順位相関係数)的定義公式的推導。 \\(\\sum\\)式子變形成普通計算式： \\[\\sum_{i=1}^{n}f_{ij} = f_{i1} + f_{i2} + f_{i3} + \\dots + f_{in}\\] 此式子也常寫作\\(f_{i+}\\), 或者\\(f_{i\\cdot}\\) \\[\\sum_{i=1}^{m}f_{ij} = f_{1i} + f_{2i} + f_{3i} + \\dots + f_{mj}\\] 此式子也常寫作\\(f_{+j}\\), 或者\\(f_{\\cdot j}\\) \\[\\sum_{i=1}^{2}\\sum_{j=1}^{3}x_{ij} = \\sum_{i=1}^{2}(\\sum_{i=j}^{3}x_{ij}) = \\sum_{i=1}^{2}(x_{i1} + x_{i2} + x_{i3}) = (x_{11} + x_{12} + x_{13}) + (x_{21} + x_{22} + x_{23})\\] 此式子也可以寫作\\(x_{++}\\), 或者\\(x_{\\cdot\\cdot}\\)。另外，中間的式子如果是\\(\\sum_{j=1}^{3}(\\sum_{i=1}^{2}x_{ij})\\)也可以成立，過程如下：\\[\\sum_{j=1}^{3}(\\sum_{i=1}^{2}x_{ij})=\\sum_{j=1}^{3}(x_{1j} + x_{2j}) = (x_{11} + x_{21}) + (x_{12} + x_{22}) + (x_{13} + x_{23})\\] \\[ \\begin{aligned} \\sum_{i=1}^2\\sum_{j=1}^2a_{ij}x_ix_j \u0026amp;= \\sum_{i=1}^2(\\sum_{j=1}^2a_{ij}x_ix_j) \\\\ \u0026amp;= \\sum_{i=1}^2({\\sum_{j=1}^2a_{ij}x_j)x_i} \\\\ \u0026amp;= \\sum_{i=1}^2(a_{i1}x_1 + a_{i2}x_2)x_i \\\\ \u0026amp;= (a_{11}x_1 + a_{12}x_2)x_1 + (a_{21}x_1 + a_{22}x_2)x_2 \\\\ \u0026amp;= a_{11}x_1^2 + (a_{12} + a_{21})x_1x_2 + a_{22}x_2^2 \\end{aligned} \\] \\[ \\begin{aligned} \\sum_{k=1}^3\\left\\{(\\sum_{i=1}^2b_ix_{ik})(\\sum_{j=1}^2b_jx_{jk})\\right\\} \u0026amp;= \\sum_{k=1}^3\\left\\{(b_1x_{1k} + b_2x_{2k})(b_1x_{1k} + b_2x_{2k})\\right\\} \\\\ \u0026amp;= \\sum_{k=1}^3(b_1x_{1k} + b_2x_{2k})^2 \\\\ \u0026amp;= (b_1x_{11} + b_2x_{21})^2 + (b_1x_{12} + b_2x_{22})^2 + (b_1x_{13} + b_2x_{23})^2 \\end{aligned} \\] \\(\\mathop{\\sum\\limits^3\\sum\\limits^3}\\limits_{i\u0026lt;j}e_{ij}\\) 會變成怎樣的式子呢？ 滿足 \\(i\u0026lt;j (i = 1,2,3; j = 1,2,3)\\) 條件的 \\(i,j\\), 有且僅有 \\((1,2),(1,3),(2,3)\\),故 \\(\\mathop{\\sum\\limits^3\\sum\\limits^3}\\limits_{i\u0026lt;j}e_{ij} = e_{12} + e_{13} + e_{23}\\) 那麼\\(\\mathop{\\sum\\limits^3\\sum\\limits^3}\\limits_{i\\neq j}e_{ij}\\)又會變成怎樣的式子呢？ 滿足 \\(i\\neq j (i = 1,2,3; j = 1,2,3)\\) 的 \\((i,j)\\) 有6種組合:\\((1,2),(2,1),(1,3),(3,1),(2,3),(3,2)\\), 故\\(\\mathop{\\sum\\limits^3\\sum\\limits^3}\\limits_{i\\neq j}e_{ij} = e_{12} + e_{21} + e_{13} + e_{31} + e_{23} + e_{32}\\) 加法算式變形爲\\(\\sum\\) 在多元變量分析(多変量解析)中，與前項相比，加法算式變形成爲\\(\\sum\\)式子更加重要。也就是說，以前項計算爲例的話，作爲答案的計算式如果放在題幹，反向求解\\(\\sum\\)式的過程更加常用。簡單練習一下吧：\n將計算式\\(a_1x_1^2 + a_2x_2^2 + a_3x_3^2 + a_4x_4^2 + a_5x_5^2\\)改寫成\\(\\sum\\)式： 先寫下：\\(\\sum\\) 再寫各個單元的共通部分\\((a,x^2)\\)： \\(\\sum ax^2\\) 各單元不同的僅爲下標： \\(\\sum a_ix_i^2\\) 注意到下標的變化規律爲\\(1\\)到\\(5\\)之間的整數，故在\\(\\sum\\)符號的上部寫上\\(5\\)，下部寫上\\(i=1\\): \\(\\sum\\limits_{i=1}^5a_ix_i^2\\) (答) 將計算式\\(f_2(x_2 - \\bar{x})^2 + f_3(x_3 - \\bar{x})^2 + f_4(x_4 - \\bar{x})^2 + f_5(x_5 - \\bar{x})^2\\)改寫成\\(\\sum\\)式： \\(\\sum\\) \\(\\sum f(x-\\bar{x})^2\\) \\(\\sum f_i(x_i - \\bar{x})^2\\) \\(\\sum\\limits_{i=2}^5f_i(x_i - \\bar{x})^2\\) (答) 提問： 我們現在了解了可以使用簡單的\\(\\sum\\)符號來表達復雜有規律的加法算式。請問有沒有相應的記號來代表乘法？ 當然有。用希臘字母\\(\\pi\\)的大寫\\(\\Pi\\)來表示。例如： \\[x_1x_2x_3x_4x_5 = \\prod_{i=1}^5x_i\\] \\[x_1x_2x_3\\dots x_n = \\prod_{i=1}^nx_i\\] 乘法記號可以證明下面的等式成立： \\[\\prod_{i=1}^nx_i^2 = (\\prod_{i=1}^nx_i)^2\\] \\[\\log(\\prod_{i=1}^nx_i) = \\prod_{i=1}^n\\log x_i\\] ","date":1486339200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1486339200,"objectID":"b5a8aa31274abb7200780705bc558b6d","permalink":"https://wangcc.me/post/2017-02-06/","publishdate":"2017-02-06T00:00:00Z","relpermalink":"/post/2017-02-06/","section":"post","summary":"第1章　数学の基礎 P1-5","tags":["learning notes","basic mathematics","linear algebra"],"title":"「統計解析のための線形代数」復習筆記1","type":"post"},{"authors":null,"categories":["diary"],"content":"今年1月7日周六，我在日本名古屋市參加了人生第一次，但願是唯一一次的雅思考試。 就是如下圖的這個叫做TKP名古屋駅前カンファレンスセンター，這建築物名字翻譯成人話其實就是“名古屋車站前TKP會議中心”，之前接待臺灣友人時被問道：“爲什麼日本人有時候用漢字，有時候又用片假名或者平假名。”答曰：“任性。”\n回到雅思考試話題上來。爲什麼一把年紀了還要考雅思？其實說實話，我肯定不是那日考雅思中最老的，看到一個大叔估計50+，我也才快30而已。不老不老。回想當年剛畢業時好多人吭哧吭哧考雅思或者託福，我也跟風（裸）考了一次託福（好像不到80分，iBT，慘不忍睹），當然我自己精心準備的考試是日本語能力測試的2級與1級。如今已經在日本獲得博士學位，留校工作快2年了，爲啥又要去靠雅思捏？答案很簡單，又想出國了。記得之前有一次有點小尷尬的經歷，好像是去愛知縣政府的時候，坐八谷老師的車，一邊聊天，他問道：“哎呀，小王，你沒有考慮以後留學去嗎？” 我楞了一下，過了十幾秒，他好似也意識到了什麼，又說道：“對哦，現在已經在留學了。” 此事後來常被我拿來在酒桌上開玩笑，看我的日語還是很地道的嘛，老師都忘記我是留學生了。\n所以上文我說又想出國，出的是日本國，有點終於要“衝出亞洲，走向世界”的豪情萬丈了。其實我這次追求的是，從一個島國，去另一個島國。試問東瀛到英吉利的距離有多遠？答曰：相隔一次雅思考試的時間，需要一份個人陳述的鋪墊，還要有兩封推薦信的含情脈脈，最終也還不一定能夠買到這張出發的船票。所以我的第一件事就是，花1個月的時間，從早到晚幾乎每日10個小時以上的精力投入，1小時菲律賓外教的口語模考課，3篇劍橋閱讀，1次真題聽力，大小作文每周各練習2-3篇，然後重頭戲我覺得是堅持每天一片聽寫，目標是雅思成績overall大於等於7。終於等到1月7日這天，檢驗我的魔鬼訓練成果的日子。\n日本的雅思考點很少，聽說大陸光上海就6個考點，而且今年的考試費用是1980RMB，真是那啥人傻錢多劍橋速來，考點每年一個的增加，費用也是每年100的上漲。我在的名古屋考點，把名古屋周圍地區的考生都包括了（愛知縣，三重縣，岐阜縣，靜岡縣，說不定還有其他縣市的趕過來），我考試那天也才80個人左右，我還看到有人拖着行李箱從外地趕來考試。全日本考點不知道有沒有超過10個。考試全程感覺周圍的人都好牛逼，不知道是他們提前放棄了還是真的寫的比我還快啊。早晨考完聽力閱讀作文以後是12點半，其實1點開始就有考口語的，只是考官人數有限，口語考試我被安排在了最後一個，輪到我時已經是晚上6點。\n我還記得考完作文的時候長舒一口氣，因爲尿憋了好久了，結果接下來的5個小時漫長等待才是最摧殘人的。考完口語之後我把被問的話題回憶了一下:\nPart 1 Work or study ? Why the work ? Future work you like to do ? Describe a celebrity you know in your country. Do you like to read about celebrities? Do you like to take photos ? What do you do with the photos? Part 2: Describe a very busy time. Part 3: Do you think children in your country, I mean China, are under great stress? Do you think stress in the future will be more or less? Give me some examples about modern ways to deal with stress. 考口語的時候，我的感覺十分緊張，而且在Part3有部分語無倫次。不過結束以後我還跟考官握了握手說我很享受跟你的對話。(I really enjoyed our talk, have a good night!)\n2周以後的1月21日是出分的日子，從中午開始我就不停的上網站刷新，結果一直都提示沒有可顯示的成績。後來到了晚上7點半，刷出了成績：\nOverall 8分的成績還是挺滿意的，1個多月非人的生活終於可以結束了。結果信心滿滿的去登錄London School of Hygiene and Tropical Medicine的報名網站的時候，悲催的是人家關於英文能力的問題只有一個：你的雅思成績總分是否大於7，請回答是或者否。。。。。。。。。。。。。\n所以其實語言考試這玩意兒根本不能說明任何問題。它只提供了本人可以進行英文交流的能力證明，沒有哪個大學會因爲你的英語成績雅思或者託福滿分，就錄取你的。\n","date":1486166400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1486166400,"objectID":"0c3e4564609e192421a58776c851255e","permalink":"https://wangcc.me/post/2017-01-07-ielts-test/","publishdate":"2017-02-04T00:00:00Z","relpermalink":"/post/2017-01-07-ielts-test/","section":"post","summary":"寫在考完雅思（IELTS）一個月以後","tags":["English Learning","IELTs test","Listening","writing","experience","reading","speaking"],"title":"IELTs test 雅思考試感受與我的成績","type":"post"},{"authors":null,"categories":null,"content":"You may find more information/pictures from the homepage of the Aichi Worker\u0026rsquo;s Cohort Study: http://koei-nagoya.blogspot.jp/\n","date":1485993600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1485993600,"objectID":"bebcee91cd8827178739dd0aa3bddce6","permalink":"https://wangcc.me/project/aichi/","publishdate":"2017-02-02T00:00:00Z","relpermalink":"/project/aichi/","section":"project","summary":"A cohort study of cardiovascular diseases in a worksite in central Japan","tags":["Aichi"],"title":"Aichi Workers' Cohort Study","type":"project"},{"authors":null,"categories":null,"content":"You may find more information/pictures from the homepage of the department of Public Health and Health System at the Nagoya University\n","date":1485993600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1485993600,"objectID":"82d4335468812d89b36c157901d60c06","permalink":"https://wangcc.me/project/palau/","publishdate":"2017-02-02T00:00:00Z","relpermalink":"/project/palau/","section":"project","summary":"You may find more information/pictures from the homepage of the department of Public Health and Health System at the Nagoya University","tags":["Palau"],"title":"Health researches I joined in Palau","type":"project"},{"authors":null,"categories":null,"content":"You may find more information from the home page of the JACC project: http://publichealth.med.hokudai.ac.jp/jacc/index.html\n","date":1485993600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1485993600,"objectID":"8047bc17781ae11667616e3ce12b00b3","permalink":"https://wangcc.me/project/jacc/","publishdate":"2017-02-02T00:00:00Z","relpermalink":"/project/jacc/","section":"project","summary":"the Japan Collaborative Cohort (JACC) Study, JACC Studyは日本人の生活習慣ががんとどのように関連しているかを明らかにすることを目的としています","tags":["JACC"],"title":"the Japan Collaborative Cohort (JACC) Study","type":"project"},{"authors":["Norimasa Kikuchi","Takeshi Nishiyama","Takayuki Sawada","Chaochen Wang","Yingsong Lin","Yoshiyuki Watanabe","Akiko Tamakoshi","Shogo Kikuchi"],"categories":null,"content":"Abstract Colorectal cancer is the third most common cancer worldwide, and many risk factors for colorectal cancer have been established. However, it remains uncertain whether psychological stress contributes to the onset of colorectal cancer. Therefore, we conducted a large-scale prospective cohort study to confirm the association between perceived stress and colorectal cancer incidence. We identified 680 cases of colon cancer and 330 cases of rectal cancer during a maximum of 21-year follow-up of 61,563 Japanese men and women. Cox regression analysis adjusted for potential confounders revealed a significant association of perceived stress with rectal cancer incidence but not with colon cancer incidence. This finding is partly consistent with that from only one previous study that addressed an association between perceived stress and the risk of colorectal cancer. However, studies on this topic are sparse and warrant further exploration.\n","date":1484524800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1484524800,"objectID":"5df4285b3c1f276e55d2f1ae12f247a9","permalink":"https://wangcc.me/publication/journal-article/jaccstress_colorectal/","publishdate":"2017-01-16T00:00:00Z","relpermalink":"/publication/journal-article/jaccstress_colorectal/","section":"publication","summary":"Abstract Colorectal cancer is the third most common cancer worldwide, and many risk factors for colorectal cancer have been established. However, it remains uncertain whether psychological stress contributes to the onset of colorectal cancer. Therefore, we conducted a large-scale prospective cohort study to confirm the association between perceived stress and colorectal cancer incidence. We identified 680 cases of colon cancer and 330 cases of rectal cancer during a maximum of 21-year follow-up of 61,563 Japanese men and women.","tags":null,"title":"Perceived Stress and Colorectal Cancer Incidence: The Japan Collaborative Cohort Study","type":"publication"},{"authors":["Esayas Haregot Hilawe","Chifa Chiang","Hiroshi Yatsuya","Chaochen Wang","Edolem Ikerdeu","Kaori Honjo","Takashi Mita","Renzhe Cui","Yoshihisa Hirakawa","Sherilynn Madraisau","Gregorio Ngirmang","Hiroyasu Iso","Atsuko Aoyama"],"categories":null,"content":"Abstract We aimed to investigate the prevalence and predictors of diabetes and prediabetes among adults in Palau. We used data of 1915 adults, aged 25 to 64 years, who participated in the World Health Organization’s (WHO) STEPwise Approach to Risk Factor Surveillance (STEPS) study in Palau. Information on behavioral risk factors of NCDs and physical and biochemical measurements were obtained using standard methods of the WHO. The diagnosis of diabetes and prediabetes was based on the recent American Diabetes Association criteria. Predictors of the prevalence of diabetes and prediabetes were identified using multinomial logistic regression analysis. The overall age-standardized prevalence of prediabetes and diabetes were 40.4% (43.6% for men, 37.4% for women) and 17.7% (18.6% for men, 17% for women), respectively. Old age, overall obesity (high BMI), central obesity (large waist circumference or waist-hip ratio), hypertension and hypertriglyceridemia were significant predictors of prediabetes and/or diabetes. Diabetes occurred at a younger age in “obese” individuals than that of their “non-obese” counterparts. We confirmed that prediabetes and diabetes are highly prevalent in Palau affecting 40% and 18% adults, respectively. Introducing public health interventions to reduce and prevent obesity as early as possible could prove useful to curb the problem.\u0026quot;\n","date":1484524800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1484524800,"objectID":"bd9addba613caa2b025f78e7846c13a5","permalink":"https://wangcc.me/publication/journal-article/steppalau/","publishdate":"2017-01-16T00:00:00Z","relpermalink":"/publication/journal-article/steppalau/","section":"publication","summary":"Abstract We aimed to investigate the prevalence and predictors of diabetes and prediabetes among adults in Palau. We used data of 1915 adults, aged 25 to 64 years, who participated in the World Health Organization’s (WHO) STEPwise Approach to Risk Factor Surveillance (STEPS) study in Palau. Information on behavioral risk factors of NCDs and physical and biochemical measurements were obtained using standard methods of the WHO. The diagnosis of diabetes and prediabetes was based on the recent American Diabetes Association criteria.","tags":null,"title":"Prevalence and predictors of prediabetes and diabetes among adults in Palau: population-based national STEPS survey","type":"publication"},{"authors":null,"categories":["dictation"],"content":"听写于：2017-1-5 9:35\t用时：26:08 正确率：88%\t错词：26个\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nBut we start with a literary star , - Paul Beatty, the first American ever to win the coveted Man Booker Prize, which is worth of £50,000 and a place on the best-seller list. The Sellout is a comic novel which is also unusual for a Man Booker winner . - I laughed out loud reading it . - but the subject method matter, which is a set arrow satire of race relations in contemporary America, also struck me as highly appropriate for 2016 in the wake of Trump's election victory, in and the Black Lives Matter campaign. Our sinerator's After the narrator's father was killed in a police shootout and his hometown is wiped off the map. , he decides to right the wrong by taking on the a slave and reintroducing segregation, actions that land him in a the Supreme Court. This may be hard to believe, coming from a black man. , but I have I've never stolen anything. , never cheated on my taxes or at cards, never snuck into the movies or fail failed to give back the extra change to a drug store drugstore cashierand different , indifferent to the ways of mercantilism and the minimum wage expectations. I've never burgled a houseor , held up a book liquor store, never boarded a crowded bus or subway car, sat the in a seat reserved for the elderly. But here I am, in a cavendish chamber the cavernous chambers of the Supreme Court of the United States of America. ; my car illegally and somewhat ironically parked on Consitution Constitution Avenue. ; my hands cuffed and crossed cross behind my back. ; my right to remain silent long since waved waived and said goodbye to as I sit in the thickly patted a thickly-padded chair that, much like this country, isn't quite as comfortable as it looks. Words worth to be remembered: covet:v. 贪求，觊觎 satire:n. 讽刺；讽刺文学，讽刺作品 mercantilism:n. 重商主义；商人本性；商业主义 burgle:vt. 偷窃，破门盗窃 vi. 偷窃，破门盗窃 cavernous:adj. 似巨穴的 chamber:n. 会客室，议事厅；议院；房间；卧室；腔 v. 限制，把…关在室内；装填 waive:vt. 放弃；搁置 padded:adj. 有装填垫料的；脚底有厚肉的 v. 填补（pad的过去分词形式） land sb in： 使某人陷入 Revealing confidential information to a rival company could land you in serious trouble with your boss. 给竞争公司泄露机密信息很可能会让老板使你陷入麻烦 My rights remain silent 米兰达权利： 美国刑事诉讼中的miranda rights——米兰达权利，也就是犯罪嫌疑人保持沉默的权利，是个具有特殊意义的法律制度。“你有权保持沉默。如果你不保持沉默，那么你所说的一切都能够用作为你的呈堂证供。你有权在受审时请一位律师。如果你付不起律师费的话，我们可以给你请一位。你是否完全了解你的上述权利？”这句话就是著名的“米兰达警告”，也称“米兰达告诫”，即犯罪嫌疑人、被告人在被讯问时，有保持沉默和拒绝回答的权利。 譯文 先来介绍的是文学明星保罗·比蒂（Paul Beatty）。他是今年布克文学奖的获得者，也是第一位获此殊奖的美国作家。除了能获得5万英镑奖金外，他的获奖作品也会在畅销书榜占据一席之地。《出卖》（The Sellout，又译《背叛》）是一本幽默小说，这在布克奖获奖历史中并不常见。这本书常使我捧腹大笑。而它以讽刺当代美国的种族关系为题材，我觉得这非常适合2016年这个年份——特朗普赢得大选胜利，及“黑人的命也是命”（Black Lives Matter）的抗议运动都在2016年发生。 在经历了父亲被警察击毙，家乡城市在地图上消失的事件后，小说的叙述者决定通过雇佣奴隶及施行种族隔离制度来纠正这个错误。这些行为最终使他被送上最高法院的法庭。 “这些话从一个黑人口中说出可能难以让人信服。但是我从来没有偷过东西，从来没有偷税漏税，没有在玩牌时出老千，没有偷偷溜进电影院看电影。我每次都把多找的零钱还给药店收银员。我对重商主义的手段并不在意，对最低薪资标准也没有什么期许。我从未入室盗窃，或是抢劫卖酒的商店。我从没在拥挤的公车或地铁上占用老人专用座位。但我还是落得如此下场，在大而“幽深”的美国最高法庭接受审判。我的车非法地、有些讽刺地停在了宪法大道。我的手被手铐铐住，反剪在背后。当我来到法庭，我不再拥有“保持缄默”的权利。我坐的椅子上垫着厚厚的坐垫，但它跟这个国家一样，都只是看着舒服罢了。\n","date":1483574400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483574400,"objectID":"896ab2be7f95609584023a146ca96975","permalink":"https://wangcc.me/post/2017-1-5/","publishdate":"2017-01-05T00:00:00Z","relpermalink":"/post/2017-1-5/","section":"post","summary":"Man Booker Prize","tags":["dictation","English Learning","Listening","BBC"],"title":"布克獎","type":"post"},{"authors":null,"categories":["dictation"],"content":"听写于：2017-1-4 10:8\t用时：19:19 正确率：91%\t错词：17个\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nOpen any company's annual report and chances are , there will there'll be something about corporate social responsibility. Because we don't just expect the company companies we buy from to be profitable; we expect them to be good corporate citizens, too. , which might explains explain recent news reports that Amazon has quietly appointed a new director of social responsibility. The appointment comes as the online retailer faces mounting criticism over its business practices. A recent article in the New York Times criticized criticised the company for the way it treated its employees. It spoke of a bruising culture and turner back-stabbingthe internal , something Amazon's boss has since insisted it's is not an accurate portrayal. Whatever the case, Christine Bader, Corporate Social Responsibility Campaigner and the former adviser to the UN Secretary General's Special Representative for Business and Human Rights, is now Amazon's Director of Social Responsibility , or CSR. Here , she's speaking to me on our sister programme , Business Matters last year, when I asked her , \" what is does CSR actually meant? \"\nmean. I think the part of the problem with the way that the for quote-unquote corporate responsibility felt field has evolved is that no one knows what it means. It could can mean whatever the CEO wants it to mean. It can mean employing employee volunteer programmes. It can mean reducing your greenhouse gas emissions. Words worth to be remembered bruising /ˈbruːzɪŋ/: a.十分激烈的 例： The administration hopes to avoid another bruising battle over civil rights. 政府希望避免因民权问题再次发生激烈冲突。 back-stabbing: n.伤人暗箭 例句： We weren't really trying to tell anyone that this was Shakespeare, but it is about back-stabbing. 我们真的没打算做一出莎士比亚戏剧，但这确实是关于背后一刀的故事。 portrayal:n. 描绘，描写；画像 譯文 如果浏览任何一家公司的年报，你都很有可能看到一些关于企业社会责任的内容。这是因为我们不仅仅要求我们所投资的公司盈利能力出众，我们还希望他们是优秀的企业公民。这也可能解释了近期的新闻报道有关亚马逊暗自任命了新高管来负责社会责任。这起任命源于之前这家网络零售商所遭遇的大量有关公司行为的负面批评。纽约时报近期文章批判了该公司对待员工的方式。文章讲道亚马逊的冲突文化和内部中伤，但亚马逊老板坚称这些描述都不确凿。无论真相如何，Christine Bader已经成为现任亚马逊社会责任的负责人，她也是一位企业社会责任支持者，曾任联合国秘书长商业与人权特别代表的顾问。她曾在这里接受我在我们姐妹节目Business Matters的采访，那时我问她企业社会责任到底是什么含义。\n我想主要原因是呼吁的核心企业责任领域已经变化，现在没人能说清楚它表示什么。它可以表示CEO所赋予的任何意义。它可以意味着员工志愿项目。它也可以包括削减温室气体排放量。\n","date":1483488000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483488000,"objectID":"107ef572c835ae5762bd57790c7d8f15","permalink":"https://wangcc.me/post/2017-1-4/","publishdate":"2017-01-04T00:00:00Z","relpermalink":"/post/2017-1-4/","section":"post","summary":"corporate social responsibility","tags":["dictation","English Learning","Listening","BBC"],"title":"企業的社會責任","type":"post"},{"authors":null,"categories":["dictation"],"content":"听写于：2017-1-3 11:52\t用时：16:56 正确率：94%\t错词：12个\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nThe rate of Chinese urbanization urbanisation over the last few decades has been astonishing. In 1990, a quarter of China's population lived in urban areas. By last year, over half lived in cities . and the rate isn't slowing. The governments government's building cities across the country, hoping to turn farmers into workers and consumers to drive economic growth. Ten years ago, our China editor Carrie Gracie began visiting a rural community , a thousand 1,000 miles southwest of Beijing. She'd She's seen White Horse Village undergo an extraordinary transformation into neon-lit Wuxi New Town. , and that means \" extra goodies for some\" . The tiny 9-year-old nine-year-old and I have been staring at stuff stuffed toys and superheroes for a very long time. \" Peipei, \" I saidsay, \" you've got 300 yuan, . You can get whatever you want. Now , what is it that you want? \" He just keeps smiling up at me helplessly, popping hopping from one spiral sparrow leg to the other. It's already been a long day. We've , which started at 5 a. m. , when the BBC's White Horse Village crew stumbled from our hotel beds and struggled straggled across the new city's city square to get ready for a live interview into the News Night Newsnight programme. We are We're not the earliest people on the square , though, . The pavement on the main road is thickening with farmers from the surrounding hill villages, laying out their vegetables for keen inspection by the city people who were themselves farmers only five minutes ago. In the old days, and the old days here are only 10 ten years old, there was only farming in this valley. Words worth to be remembered: hop:vt. 搭乘 v. 单足跳跃〔跳行〕 vi. 双足或齐足跳行 n. 蹦跳,跳跃；跳舞；一次飞行的距离 sparrow:n. 麻雀 straggle:vi. 迷路；落伍，掉队；四散，蔓延 n. 散乱 譯文 中国的城市化在过去几十年发展速度惊人。1990年，1/4的中国人居住在城市。去年，中国已经有超过一半的人是城市居民，而且其增长速度还没有放缓的趋势。为响应政策号召，中国各地纷纷建起城市，此举的目的是促使农民转为工人和消费者，以推动经济的增长。十年前，我们的中国事务主编嘉莉·格雷西走访了位于北京西南1000英里处的一个农村。她目睹白马村经历了非同寻常的转变，现在它已经是一座霓虹灯闪烁的巫溪新城，而这对一些人来说意味着“尽善尽美”。\n我和九岁的小佩佩一块盯着毛绒做的玩具和超级英雄，盯了很长的时间。\n“佩佩，”我说，“你有300元，可以买任何想要的东西。现在你想要什么？”\n他只是无奈地看着我笑，两条小腿交替跳跃着。今天是漫长的一天，早上五点BBC的白马村摄制组就出发，匆忙来到新城市广场，为新闻之夜的现场采访做准备。然而我们并不是最早出现在广场上的人 。来自周围山村的农民早就在主要道路的路面上铺好了他们种的蔬菜，等待着城里人的细心检验，而这些城里人自己不久前也是农民。仅仅十年前，这个山谷唯一仰赖的只有农业。\n","date":1483401600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483401600,"objectID":"7eda87575a31bc6897300c6ce696fceb","permalink":"https://wangcc.me/post/2017-1-3/","publishdate":"2017-01-03T00:00:00Z","relpermalink":"/post/2017-1-3/","section":"post","summary":"the urbanisation in China","tags":["dictation","English Learning","Listening","BBC"],"title":"蛻變","type":"post"},{"authors":null,"categories":["dictation"],"content":"听写于：2017-1-2 10:01\t用时：21:24 正确率：93%\t错词：14个\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nFor many people, the Christmas season is the a time for merriment, for eating, for drinking, and for doing everything to excess. If you were you're religious, it might seem to go against what this time of year is all about. , and certainly , there is there's no shortage of church leaders who rail against the commercialization commercialisation of Christmas. Yet a lot of these this conspicuous feasting does have a religious origin. In a moment, we'll hear how many religious holidays were festivals. But first, the link between religion and consumption hasn't gone unnoticed by food companies and retailers. In fact, increasingly we're seeing religious food stuffs, marked as kosher or halal for instance, going mainstream. You can find those marks on a whole range of products now sold in many Western supermarkets. So how did that happen? How did kosher and halal become as mainstream as, say, vegetarianism in the food industry? Elizabeth Hotson went to find out , here in London. There was a time when “if you want wanted a bagelyou get , you'd go to a Jew's Jewish bakery. \" or a halal butcher for land slaughtered lamb slaughted according to Islamic law. Nowadays supermarket supermarkets stock what were was once considered specialist and rather exotic items. , and savi savvy businesses are taking advantagesadvantage. On a rainy day in London, I took refuge in Gifto . – a Pakistani restaurant to the west of the city. Words worth to be remembered: merriment:n. 欢喜,嬉戏 conspicuous:adj. 显眼的，明显的，引人注目的 kosher:adj. 合适的；符合犹太教教规的，干净的 halal:n. 伊斯兰教律法的合法食物 v. 按伊斯兰教律法屠宰牲畜 savvy:adj. 有见识的；具有实际知识的 n. 悟性；头脑 v. 理解；懂, 精明 rail/reɪl/: v. 强烈抗议 例句： It also likes to rail against U.S. support for authoritarian regimes in the Islamic world. 它差点就要责骂美国在伊斯兰世界里支持独裁政体。 take refuge: 避难；躲避 例句： He is my loving God and my fortress, my stronghold and my deliverer, my shield, in whom I take refuge, who subdues peoples under me. 他是我慈爱的主，我的山寨，我的高台，我的救主，我的盾牌，是我所投靠的。 他使我的百姓服在我以下。 譯文 对大多数人来讲，圣诞季意味着欢乐、大餐、畅饮，总之凡事都要多多益善。如果你信教，可能就与当下的气氛背道而驰了，当然，总不乏宗教领袖们在抵制圣诞节商业化。但这些夸张的盛宴确实有其宗教起源。稍后，我们将听到有多少宗教节日是欢庆的节日。但首先，宗教与消费的二者关系早已被食品公司和零售商们所发现。事实上，我们能看到越来越多的宗教食物，比如标注为犹太或伊斯兰食品，成为主流。在很多西方超市里，都能看到一大批食品包装上印有那些标识。那么这一切是如何演变的？犹太和伊斯兰食品是如何成为食品行业的主流素食主义的？伊丽莎白·霍森在伦敦为我们找寻答案。\n曾经有段时间，“如果你想吃贝果，你就去犹太面包房”或想吃清真羔羊肉就去找伊斯兰屠夫。当今，很多超市都贮存曾经那些看似专业性的或异国的商品，精明的商业从中挖掘利益。伦敦的一个阴雨天，我来到城市西边的一家巴基斯坦餐馆Gifto躲雨。\n","date":1483315200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483315200,"objectID":"9e53ecb86e4a4519a160b39a89f3bb53","permalink":"https://wangcc.me/post/2017-1-2/","publishdate":"2017-01-02T00:00:00Z","relpermalink":"/post/2017-1-2/","section":"post","summary":"Religious Food","tags":["dictation","English Learning","Listening","BBC"],"title":"宗教與食品","type":"post"},{"authors":null,"categories":["dictation"],"content":"听写于：2017-1-1 10:23\t用时：14:03 正确率：93%\t错词：16个\n概述：许多父母都对自己的孩子寄予厚望，愿意把宝宝的小动作解读为过人的天赋，但是有多少望子女成龙凤的父母能如愿以偿呢？也要有点“舍得孩子才能套着狼”的精神吧。新年BBC第一弹，祝大家元气满满！\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nIn 1996, Dominique Moceanu made headlines as the youngest-ever member of the US Women\u0026rsquo;s gymnastic Gymnastics team to win a gold medal at the Olympic Games. Her parents were from Romania and inspired by the popular Romanian gymnast Nadia Comăneci, they had high ambition ambitions for their daughter. My parents had it in their hearts that when they had their first child, I would be a gymnast. And when they immigrated to the United States in 1981. , that desire stayed within their hearts . and so after I was born about 6 six months, they did a test of my strength just for fun. And my parents didn\u0026rsquo;t have a washer or dryer so they hanged hung me from a cloth clothes line to see how long I could hold on to the cloth clothes line. My goodness. I know. And so my father dad used to always say I it was such enthusiasm: \u0026quot; You held on until the cloth clothes line broke. \u0026quot; But of course , they were there to catch me. , and they said that was really a sign to them that I was gonna be a great real gymnast. And then eventually , I would\u0026rsquo;ve started gymnastic would start gymnastics by the age of 3 three in the United States. , and I was put in a tennis class and a gymnastic gymnastics class. And really tennis was great, but I didn\u0026rsquo;t have the love that I immediately had for gymnastics, and really the rest was history from that moment on.\nWords worth to be remembered: make headlines: 出现在头条新闻位置；受到宣扬。 例句： But other software, developments, and trends are sure to make headlines and seduce developers in 2010. 但是其他软件、开发和趋势等在2010年必定会成为开发人员所关注的头条新闻。 譯文 1996年，美国女子体操队史上最年轻的运动员多米尼克·莫西阿努荣获了奥运金牌。她的父母来自罗马尼亚，受到罗马尼亚知名体操运动员纳迪亚·科马内奇的启发，他们对女儿也抱着很高的期望。\n我父母打从心里希望自己第一个孩子，也就是我，能成为一名体操运动员。1981年他们移民来美国时，这个愿望就扎根在他们心里。在我6个月大的时候，他们以测试我的力量为乐趣。我家没有洗衣机和烘干机，于是他们把我挂在晾衣绳上，想看看我能在上面坚持多久。\n我的天呐。\n没错。我父亲过去老说这实在是太令人惊叹了：“你居然坚持到晾衣绳都断了”。当然，他们在那里接着我呢。他们觉得这就在向他们暗示我将来肯定能成为一个真正的体操运动员。而我从三岁起就在美国进行体操训练。我同时学习网球和体操。网球运动确实很棒，但是我不像立刻爱上体操那样喜爱网球，而且从那一刻起，其余都是历史了。\n","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"f01b64f58399411861ace06d59a2d872","permalink":"https://wangcc.me/post/2017-1-1/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/post/2017-1-1/","section":"post","summary":"许多父母都对自己的孩子寄予厚望，愿意把宝宝的小动作解读为过人的天赋，但是有多少望子女成龙凤的父母能如愿以偿呢？也要有点“舍得孩子才能套着狼”的精神吧。新年BBC第一弹，祝大家元气满满！","tags":["dictation","English Learning","Listening","BBC"],"title":"望子成龍","type":"post"},{"authors":null,"categories":["dictation"],"content":"Documentary follows autistic boy obsessed with Disney\n听写于：2016-12-31 10:44\t用时：19:00 正确率：95%\t错词：12个\n概括：《蓬勃的生活》根据Ron Suskind同名小说改编，是一部穿插动画与真实影像的纪录片，体现了自闭症儿童的家庭在照顾特殊孩子上所遭遇的困难。Suskind的儿子Owen三岁时不再说话，Suskind夫妇和Owen两岁的弟弟一直在与自闭症作斗争，想要维系他们与Owen的感情，而此时Owen唯一的爱好就是迪士尼动画电影。\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nThe documentary , Life, Animated illustrates the story not just with clips from the films, but also its own animation. And it travels from Owen's childhood through adolescence into adulthood, and even moving from home. Director , Roger Ross Williams had already begun to film with the family before Ron's book, which is also called Life, Animated, was published. He knew his documentary had to differ from, even go beyond the book, and give voice to a new character in the family drama - Owen's mother. Cornelia had never really talked about it. , so, and because Cornel Cornelia is so private. , so for her, every time she would start, I would've told them went to help tell and then have her held tell the story. She would burst into tears. So it was hard to keep her from crying. Cornelia, as a mother, was determined to connect with Owen at any cost. And even when the doctor said, \" do not encourage this obsession with Disney. , lock up the television. \" , and they did lock up the television for a point in the book. And she said, \" no, any way to connect with my son, I'm going to use. \" And so, because of that, really, they've sort of started a revolution , in which they called it call affinity therapy. , a new way of connecting with people living with autism through their passions, finding a pathway through their passions. Well , I suppose that that's, the key point is that at it, through the their passions, it doesn't have to be Disney. , does it? It does not have to be Disney. It can be anything. One kid, his affinity was carnivorous plants. Words worth to be remembered: burst into:闯入；情绪的突然发作 carnivorous:adj. 食肉的 ","date":1483142400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483142400,"objectID":"f8af5dbf2a0b0c68d3c1bcb702252a11","permalink":"https://wangcc.me/post/2016-12-31/","publishdate":"2016-12-31T00:00:00Z","relpermalink":"/post/2016-12-31/","section":"post","summary":"Documentary follows autistic boy obsessed with Disney","tags":["dictation","English Learning","Listening","BBC"],"title":"蓬勃的生活 Life, Animated","type":"post"},{"authors":null,"categories":["dictation"],"content":"听写于：2016-12-30 16:37\t用时：21:04 正确率：95%\t错词：13个\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nReally briefly, Khalida, what is Diabulimia? A Diabulimia is an umbrella term , where people with Type 1 Diabetes deliberately reduce or admit omit the insulin that they need to be taking relative to the sugars, the blood sugars that they have and to the carbohydrates that they are eating with the aim of keeping their weight down or losing weight. When did you first become aware of it? Well, I've been working as a psychiatrist with people with diabetes for nearly 15 years at Kings College Hospital. , and I've been seeing it ever since then. So it's not new, people have always done it? I think it's, I think that's right . That's that there's probably been an element of manipulating the insulin in people who have with Type 1 Diabetes ever since it's probably been invented. And I think what's happening is that we are now getting increasing awareness of it. Well, we'll talk about Emma's experiments own experience in a moment. , but Libby, from your point of view as an organization. organisation, just tell us a little bit about the impact of insulin on the body, . What does it do? Insulin regulates your blood glucose levels, . Glucose is another word for sugar. So people who don't have Type 1 Diabetes, their pancreas produces the right amount of insulin for the food they eat. For The people who do have Type 1 Diabetes, their bodies aren't making that insulin. , so they have to inject the right amount. So , and they have to work out , exactly how much insulin they need to take for the food that they are eating. Now, different people will react to insulin in different ways, will they? Different people have different doses of insulin and take it at different times. Words worth to be remembered: omit:v. 遗漏；疏忽；省略；删除 umbrella term: 涵盖性术语 eg： In prior presentations, this has sometimes been referred to under the umbrella term of 'Semantic Web'. 在先前的介绍中，它有时会在“语义网”这个术语中被提及。 deliberately:adv.故意地 Diabulimia:n. （糖尿病患者的）不规律饮食 英英解释： An eating disorder in which a diabetic person attempts to lose weight by regularly omitting insulin injections. 譯文 简单点说，卡莉达，Diabulimia是什么？\nDiabulimia是一个涵盖性术语，1型糖尿病患者需要注射与他们的血糖和他们所吃的碳水化合物成一定比例的胰岛素，而为了控制或者减少体重，他们会故意减少胰岛素的注入，或者根本不注射胰岛素。\n你第一次注意到这个问题是在什么时候？\n嗯，我在国王学院附属医院当精神病医生，我的病人中就有15年糖尿病史的患者，自从那时我就不断看到这种现象。\n所以这不是新鲜事，糖尿病患者一直都这么做吗？\n可能自从胰岛素被发明出来的时候，就有一个因子在操纵着1型糖尿病患者体内的胰岛素，我觉得这种说法是正确的。我认为我们对它越来越了解了。\n那么，过会儿我们将讨论艾玛的亲身经历，但是莉比，作为一个组织，从你的角度，你能给我们稍微讲讲胰岛素对人体的作用吗。它是怎么作用的？\n胰岛素调节人体血糖含量。葡萄糖是糖的另一种说法。没有患上1型糖尿病的人们，他们的胰腺会产生与他们所吃食物相当的胰岛素。而1型糖尿病患者，他们的身体不会产生胰岛素，所以他们需要注射等剂量的胰岛素，他们需要正确计算出与他们所吃食物相当的胰岛素剂量。\n现在，不同人对胰岛素有不同反应，是吗？\n不同人对胰岛素剂量的需求不同，注射次数也不同。\n","date":1483056000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483056000,"objectID":"2c00e157140aecff63b8e2e72098225f","permalink":"https://wangcc.me/post/2016-12-30/","publishdate":"2016-12-30T00:00:00Z","relpermalink":"/post/2016-12-30/","section":"post","summary":"what is Diabulimia?","tags":["dictation","English Learning","Listening","BBC"],"title":"1型糖尿病患者减肥要小心","type":"post"},{"authors":null,"categories":["dictation"],"content":"听写于：2016-12-29 10:51\t用时：24:29 正确率：90%\t错词：22个\n影片《后裔》以度假胜地夏威夷为背景，讲述中年男子处理突如其来的家庭危机，乔治克鲁尼凭借该片获得第69届金球奖最佳男主角。\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nHello, . As family drama , the Descendants collects at the box office. , Director Alexander Payne describes the power of putting George Clooney in a Hawaiian shirt. The Gotham city for zombies and batman White batmen, why Glasgows smiles better. ? A cult film in more than one sense, Martha Marcy May Marlene, Director/Writer Sean Durkin, on his drewlim-like dream-like study of a girl on the run from the a controlling sect. And as baffed on as BAFTA honours John Hurt, he reflects on a the recent role that got away. But first, the film that opened strongly in cinemas last weekend, starring George Clooney as a family guy, a father and husbandbut , not perhaps obvious casting, but then the Descendants isn't based on an average family setup. The setting is Hawaii and Clooney's character Matt King, is head of a family trust that owns valuable lands rightfully developmentland, rightful of redevelopment. But he is he's currently dealing with a more immediate crisis. His wife, the mother of their two daughters, aged 17 and 10, is lying in a coma after a boating accident. And all of these are against the backdrop of relentless beauty . - blue sea and waving palms. \" My friends on the mainland think just because I live in Hawaii, I am live in paradise, . Like a permanent vacation. We are , we're all just out here, sipping my timesMai Tais, checking shaking our hips, and catching weightswaves. Are they insane? Do they think we are immune to life? How can they possibly think our families are less screwed up, ? Our cancers less fatal, ? Our heartaches less painful? How life had Hell, I haven't been on a surfboard in 15 years. Words worth to be remembered: box office:phr. 票房，票房收入；售票处 Glasgows:格拉斯哥(英国苏格兰西南部港市) BAFTA:abbr. （英）电影和电视艺术学院（British Academy of Film and Television Arts） dream-like:adj. 梦一般的；朦胧的 sect:宗派; 邪教 rightful:adj. 合法的；正当的；公正的；正直的 redevelopment:再发展,再显影,照相加厚,二次显影 backdrop:n. 背景幕；背景；交流声 Mai Tai:迈代鸡尾酒 surfboard:n. 冲浪板 v. 以冲浪板滑水 譯文 大家好。随着家庭类影片《後裔》在影院上映，导演亚历山大•佩恩向我们展示了穿上夏威夷花衬衫的乔治•克鲁尼的巨大魅力。哥谭镇（指代纽约）的僵尸和蝙蝠侠赫赫有名，可是为什么格拉斯哥（苏格兰西南的城市）更能吸引电影制作人的眼球？ 在各个方面都很邪典（非主流）的影片《双面玛莎》的导演和编剧肖恩•德金向我们讲述了如梦似幻的故事\u0026ndash;一个女孩躲避邪教组织魔爪的逃亡之旅。最后，获得英国电影学院杰出贡献奖的老戏骨约翰•赫特回想他最近失之交臂的角色。\n不过首先，我们要介绍上周末在影院强势推出的电影《後裔》。主演乔治•克鲁尼以人夫、人父的居家形象示人。虽然演员阵容不是很惊艳，不过影片中的家庭可不一般。影片的背景设在夏威夷，克鲁尼饰演的麦特•金是家庭信托基金的负责人，有大把有待重新开发的地皮。\n但是他却被另外一件棘手的事情弄得焦头烂额。他的妻子，也就是他17岁、10岁女儿的母亲坐船时遇到事故，至今昏迷不醒。所有这些令人揪心的事情竟然发生在这美不胜收地方—大海碧蓝澄澈和棕榈树影婆娑。\n“内陆的朋友们觉得因为我住在夏威夷，所以我住在天堂。我们好像永远在度假，整天喝着鸡尾酒、跳着草裙舞、去海滩冲浪。他们难道秀逗了吗？他们觉得我们百毒不侵？他们怎么可以觉得我们家的麻烦就比他们的少？我们就算得了绝症也没他们的那么要人命？我们的心就不像他们这么痛？ 苍天啊，我已经有十五年没碰过冲浪板啦。”\n","date":1482969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1482969600,"objectID":"136ba1b8036706ea8ba4b9adaffa38c5","permalink":"https://wangcc.me/post/2016-12-29/","publishdate":"2016-12-29T00:00:00Z","relpermalink":"/post/2016-12-29/","section":"post","summary":"影片《后裔》以度假胜地夏威夷为背景，讲述中年男子处理突如其来的家庭危机，乔治克鲁尼凭借该片获得第69届金球奖最佳男主角。","tags":["dictation","English Learning","Listening","BBC"],"title":"夏威夷的天堂生活","type":"post"},{"authors":null,"categories":["dictation"],"content":"听写于：2016-12-28 11:45\t用时：22:35 正确率：88%\t错词：31个\n概述：跟踪与骚扰的区别，前者对受害人的精神伤害更大\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nStalking steals lives. It stops people from having the normal life sort lifestyle that you and I would've expected. You can would expect, we could go to work, take your kids to school, go about back your, sort of, daily business. It's usually a campaign by somebody who's fix stated fixated and obsessive. , and will, in many casesit stopped , stop at nothing to really shut down the freedom freedoms of the victim. And So it can affect anybody . - we believe it's approximately one in five women and one in ten men. , but we also believe that the figures are underestimated, because quite often, people don't know that they are they're being stalked. When people call us , at the National Stalking Advocacy Service, they want to know what you've called you would call it. Yeah, well, let's, I want to put some examples to you. Let's say, I get intensive incessant phone calls, or I have somebody turning up at on my place of work or I get letters I don't want, I get gifts I don't want. Is this the kind of things thing that happens? It can be. We talked intense talk in terms of harassment in and stalking . and very often harassments would harassment will be more, sort of, ongoing uses and nuisance behaviour, which can have a significant impact one on somebody's life itself. lifestyle and their mental health. , but really, the difference between stalking and harassment is stalking is more about having a fear of violence . and a significant fear for the victim that means that their freedoms are so significantly changed. , they may have to think about moving house, changing jobs. They may suffer from significant mental health problems, such as PTSDor , depression. , a number of our clients were talked will talk to us about those, sort of, ongoing issues and how they deal with it. Words worth to be remembered: fixated:adj. 念念不忘的;迷恋的 He seems to be fixated on this idea of travelling around the world. 他似乎对周游世界这一想法十分迷恋。 incessant:adj. 不間斷的, 連續的 nuisance: n.討厭的人或者事情; 非法妨害 譯文 “跟踪骚扰（stalking）”会剥夺人们原有的生活。它使人们无法继续从前正常的生活方式，如上班、送孩子上学、处理日常事务。跟踪骚扰者异常痴迷且迷恋某人或某事，通常会采取一切手段剥夺受害人的自由。任何人都可能受到跟踪骚扰的危险。我们认为大约每五位女性或十位男性中就有一人遭受到跟踪骚扰，而实际情况可能比这还要严重，因为很多时候人们并不知道自己被跟踪了。当人们电话联系National Stalking Advocacy Service时，他们想要知道你怎么定义那些行为（他们需要你来判断他们是否受到跟踪骚扰）。\n这样吧，我说些例子给你。比如说，有人不停给我打电话，有人经常出没在我上班的地方，或者是我经常收到不想要的信件、礼物。跟踪骚扰就是指这类事情吗？\n可能是跟踪骚扰（stalking），或是骚扰（harassment）。通常，骚扰（harassment）是持续进行的滋扰行为，会严重影响受害者的生活方式和精神健康。跟踪骚扰和骚扰的区别在于：跟踪骚扰更偏向于造成人对暴力的严重畏惧感，受害者的自由权利受到严重威胁，他们会不得不考虑搬家、换工作。受害者可能患上严重的精神疾病，如创伤后应激障碍（PTSD）或抑郁症。许多客户会告诉我们他们不断面临的问题，以及他们是如何处理的。\n","date":1482883200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1482883200,"objectID":"5114430834000eb818cee211d9474d05","permalink":"https://wangcc.me/post/2016-12-28/","publishdate":"2016-12-28T00:00:00Z","relpermalink":"/post/2016-12-28/","section":"post","summary":"跟踪与骚扰的区别，前者对受害人的精神伤害更大","tags":["dictation","English Learning","Listening","BBC"],"title":"跟蹤和騷擾","type":"post"},{"authors":null,"categories":["dictation"],"content":"听写于：2016-12-27 22:40\t用时：17:55 正确率：92%\t错词：20个\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nBut why did Mayor Bloomberg say \" even numbers and zero\" ? Was it necessary to single zero out? It seemed a little odd to us. So we talked to Dr James Grime of the Millennium Maths Project at Cambridge University in the UK. And he says that the a similar issue came up in France in the 1970s. \" In 1977, the smog in Paris was so bad that we they had to restrict car use. So they used the same method that they use they're using in New York now. , alternating days for odd and even license plate licence plates' numbers. And the police did not know whether to stop , the zero-numbered licence plates. Actually, and so they just let them pass because they didn't know whether that it was odd or even. \"\n\" So it seems not everyone is sure . but do mathematicians agree over of whether zero is odd or even? \"\n\" The debate is settled among mathematicians. \"\n\" Mathematicians are agreed that zero is an even number? \"\n\" Zero is an even number, and it will pass every test and every definition of an even number. \"\n\" Well, what are the usual tests or definitions for even numbers? \"\n\" If you take a number and you double it, the result will be an even number. So if you take the number 3 three, and you double it, then you will you'll get 6 six. And 6 six is an even number. So that would will be the definition of even numbers. And zero will pass that test. So if you double zero, you will you'll get zero. But it will pass all the other tests for even numbers as well. If you alternate the even and odd numbers, then zero is between two odd numbers. It's between 1 one and -1. minus one. That's another example of the a test that it would pass. \" Words worth to be remembered: single out : 挑选出 例句： We all did well, but the teacher singled him out for praise. 我们都做得很好，但老师就表扬了他一个人。 譯文 但是，为什么布隆伯格市长要说“偶数和零”？有必要把零单独拿出来说吗？在我们看来这有点奇怪。于是，我们询问了加入了英国剑桥大学千年数学工程的詹姆斯·格兰姆博士。他说1970年代也有过类似的事例。\n“1977年，巴黎弥漫着大量烟雾，空气质量十分糟糕，政府不得不采取措施限制汽车的使用。他们的方法和现在纽约市采用的方法一样，即单双号车辆隔日出行。警察不知道零是单数还是双数，也不知道该不该拦下车牌尾号为零的车辆，只好让这些车开走。”\n“看来并不是所有人都确定零是奇数还是偶数，不过，数学家在这个问题上意见统一吗？”\n“数学家的看法是一致的。”\n“他们认为零是偶数？”\n“零是偶数，它通过了所有测试，符合偶数的所有定义。”\n“对偶数的测试和定义都有哪些？”\n“把一个数乘以2可以得到一个偶数。如果你把3乘以2，结果是6。6就是一个偶数。这就是对偶数的定义。零也符合这条定义。如果你把零乘以2，结果是零。它还通过了其它测试。如果把奇数和偶数间隔排列，零位于两个奇数1和-1之间，它也通过了这项测试。”\n","date":1482796800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1482796800,"objectID":"37a90fa2084fe2349b9cb25fd20532b2","permalink":"https://wangcc.me/post/2016-12-27/","publishdate":"2016-12-27T00:00:00Z","relpermalink":"/post/2016-12-27/","section":"post","summary":"Is zero even or odd?","tags":["dictation","English Learning","Listening","BBC"],"title":"零是偶数还是奇数","type":"post"},{"authors":null,"categories":["dictation"],"content":"听写于：2016-12-26 11:48\t用时：20:02 正确率：91%\t错词：16个\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nMy guest today, Russell Foster, has the grand title of Professor of Circadian Neuroscience at Oxford University, which is the science speak way of saying he is he's obsessed with clocks . - not the ticking type, but rather our biological clocks and the rhythm rhythms of life. More specifically, he studies the way light controls our internal clocks and circadian rhythms , and works to understand how light and dark affect our well-being. , everything from jetlag jet lag to serious mental health problems. Russell had to fight against strong oppositions opposition from eye experts when he discovered a new way in which animals can detect light. He's written a number of books explaining how these biological clocks control our and other animals' daily lives. Russell, welcome to the Life Scientific. Delighted to be here, Jim. Now, does our biological makeup mean it's difficult for us to cope with our 24/7 society? Well, yes, . We carry with a set us sort of three and a half 3. 5 billion years of evolutionary baggage and buried it embedded within our genome our are the instructions of an the internal clock. It's actually , essentially providing us with a the representation of a day within. And every aspect of our physiology is being fine-tuned to the varying demands of activity and rest. And we can't easily work against that embedded programme. Words worth to be remembered: circadian: adj. 昼夜节奏的，生理节奏的 jet lag:phr. 飞行时差综合症；时差反应 makeup:n. 化妆品，组成，体格，性格，【美】补考，【印】排版 fine-tuned:v. 调整，微调；使有规则 譯文 我今天的嘉宾罗素·福斯特拥有牛津大学生理节律神经科学教授的头衔，这是一种科学上的说法，即指他对钟着迷——可并非滴答走时的钟，而是我们的生物钟以及生命节奏。更确切地说，他研究光对我们体内的时钟和昼夜节律的控制方式，并努力理解明暗是如何对我们的身心健康产生影响的，从时差反应到严重的心理健康疾病，无所不包。当罗素发现一种动物察觉亮光的新方式时，他不得不对抗来自眼科专家的强烈反对。他已有大量著作解释这些生物时钟是如何控制我们和动物们的日常生活的。罗素，欢迎来到生命科学节目。\n很高兴加入你，吉姆。\n我们的生物构造是否意味着要应付全天候不停运转的社会很困难？\n嗯，是的。我们可以说是身负35亿年的进化重担，而且已把这内部时钟的所有指令根植于我们的基因组中，这就像是我们体内有一个一天的模型。我们生理机能上的各个方面都随着不断变化的活动和休息的需要而做出微调。要与这植根体内的程序作对可不容易。\n","date":1482710400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1482710400,"objectID":"54ed0fa586b27819878c2cd79795fd66","permalink":"https://wangcc.me/post/2016-12-26/","publishdate":"2016-12-26T00:00:00Z","relpermalink":"/post/2016-12-26/","section":"post","summary":"Biological Clock","tags":["dictation","English Learning","Listening","BBC"],"title":"生物鍾","type":"post"},{"authors":null,"categories":["dictation"],"content":"听写于：2016-12-25 13:1\t用时：24:53 正确率：88%\t错词：23个\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nWe've undertaken this trip to interrogate the phenomenon of ostalgie, a nostalgia for the old GDR , which gripped grips some in modern Germany as well as my traveling travelling companion. We are We're on a the train now, heading pretty much directly to Zwickau. The poor little Trabbi's finally died at its death. And we've headed had to skip out our appointment appointments in Leipzig, which is a shame, because we are gonna were going to visit the Runden Ecke Museum . and see a lot more evidence of the darker aspects of the old East German regime. And Irmtraut Hollitzer at the Runden Ecke Museum sent us this email for through an intermediary. We've all had a little chuckle at the Trabant's failure. , and then apologize apologise, being GemanGerman, of course, very polite. , but it also said -- this is for Irmtraut Hollitzer-- she is , she's very keen -- that ostalgie doesn't win. It's a sucsin succinct and surprising inside insight into the passions that lie underneath the surface of a parent apparent Germanic immunity equanimity towards their recent past. Here we areMichael, Michael. We are in Zwickau. It's here and only here in Zwickau that 3 million Trabants were built. Intended as a socialist car for all, its design was more or less unchanged for 30 years. And East Germans had to wait a decade to get one. Zwickau remains a big moto motor manufacturing town with carble cobbled streets along with which steepgable , gabled buildings stand side by side with bruteness brutalist concrete reminders of the towns town's GDR past. Words worth to be remembered: grip:n. 紧握；对…的影响力；理解；把手 v. 紧握；吸引注意力；对…产生强有力的影响 Leipzig:莱比锡(民主德国城市) succint:adj. 简洁的 equanimity:n. 平静，镇定 cobbled:adj. 用鹅卵石砌/铺成的 gabled:adj. 有山形墙的,人字板制作的 brutalist:n. 野兽派(信奉美术、建筑或文学上野兽主义的人) GDR: abbr. German Democratic Republic (East Germany) 德意志民主共和国（东德） 譯文 我们正舟车劳顿，准备去了解东德情结这种现象。对前东德的怀念之情仍然影响着一些现代德国人，还有我的旅伴。\n我们现在在火车上，基本上是直奔茨维考。我们那辆可怜的卫星牌汽车最终还是抛锚了。很遗憾的是，我们不得不略过计划中的莱比锡市。我们本来打算去参观斯塔西博物馆，去看更多关于前东德政权黑暗面的证据。斯塔西博物馆的Irmtraut Hollitzer通过一位中间人给我们发了邮件邀请。车坏了我们还因此嬉笑了一通，然后身为德国人又礼貌地道歉，还说东德情结没有胜出。说这话的是Irmtraut Hollitzer，她很关注这一点。\n这是对隐藏在表面之下的情感的一种简明而惊人的洞悉，德国的现代史并不像表面那样平静。\n我们到了，迈克。我们到茨维考了。\n就是在这里，茨维考，300万辆卫星汽车曾在这里被生产出来。作为一款社会主义的普及汽车，它的设计30年来都没有什么变化。然而东德人民要等10年才能买到一辆。茨维考如今仍然是个很大的汽车生产基地。鹅卵石街道两边是挺拔的三角屋顶建筑，这些野兽派风格的混凝土建筑承载了茨维考过去的东德意志。\n","date":1482624000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1482624000,"objectID":"a2c04349072c186c7d843701100fadeb","permalink":"https://wangcc.me/post/2016-12-25/","publishdate":"2016-12-25T00:00:00Z","relpermalink":"/post/2016-12-25/","section":"post","summary":"ostalgie","tags":["dictation","English Learning","Listening","BBC"],"title":"東德情節","type":"post"},{"authors":null,"categories":["dictation"],"content":"听写于：2016-12-22 11:40\t用时：19:38 正确率：91%\t错词：17个\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nA film from China now, A Touch Of Sin, subvert subverts expectations. Firstly , it's largely set outside the big cities in bleak rural towns and villages, sometimes dominated by a vast industrial complex. Then the its subject is sudden acts of violence, sometimes of serial surreal almost Tarantino intensity . on the part of ordinary systems citizens variously horrified, tracked trapped or exploited in China's relentless drive to economy economic success. So for example, in a hollowed-out mining town, a man arms himself like a warrior to track down the corrupted corrupt lead who who's skimming off profits. Elsewhere, the receptionist in the a massage parlor parlour demonstrates graphically that there is there's a line that she will not cross. If the set-up seems setups seem archetypalthan , then the source , it's is all too real. Director and screenwriter, Jia Zhangke took away the writing Price prize at last year's Cannes. , but he didn't invent these stories, . He found them through social media in China. \" I heard about these incidences incidents on Chinese version of Twitter called Weibo. I started using Weibo about 3 three years ago , and that's where I learn about a lot of things that are going on all over China. , even in very remote areas. It's through Weibo that these incidents get discussed and a lot of people post comments. So the news of these events gets spread wider. \" Words worth to be remembered: subvert:v. 颠覆；破坏; ...an alleged plot to subvert the state. …一个被指控颠覆国家的阴谋。 industrial complex:[工经] 工业联合企业；大工业中心 surreal:adj. 超现实的，离奇的 relentless:adj. 无情的，残酷的；不间断的；坚韧的 hollow out:vt. 挖空（挖洞；开凿） skim off:phr. 撇取，撇去；提出精华;捞取; 偷走 (钱) parlour:adj. 客厅的 n. 客厅,会客室,特别室 graphically:adv. 生动地；活灵活现地；用图表表示；轮廓分明地 setup:n. 体制；机构；组织；调整；计划；安装，设置；陷害 archetypal:adj. 原型的; ...the archetypal American middle-class family living in the suburbs. …住在郊区的典型美国中产阶级家庭。 譯文 现在，我们来看看一部中国电影，《天注定》，它颠覆了人们的期待。首先，它主要把故事设定在远离大城市的偏远、荒凉的城镇和乡村。这些地区有时被大型工业联合企业占据。其次，电影的主题是突发性暴力事件。导演以超现实主义塔伦蒂诺式的风格讲述了在中国坚持不懈地的追求经济发展的同时，一部分的普通老百姓被恐吓、陷入困境、被剥削。例如，在一个被过度开采的采矿镇，一个男人拿起武器，像一个士兵一样去追踪在利润中捞钱的腐败领导。在别处，一个按摩院的前台用自己的实际行动表明自己不会越过底线。如果你觉得这些设定看上去很典型，其实它全取材于真实生活。去年的戛纳电影节上，导演兼编剧贾樟柯凭此片获得了最佳编剧奖，但这些故事并不是原创的，而是他通过中国的社交媒体发现的。\n“我在微博（中国版推特）上听说了这些事件。大约三年前我开始用微博，我通过这一渠道了解了在中国、甚至在偏远地区发生的许多事。人们在微博上议论这些事情，提出自己的见解，使这些事件流传得更广。”\n","date":1482364800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1482364800,"objectID":"acea2331f25499ed1794aa70aa297230","permalink":"https://wangcc.me/post/2016-12-22/","publishdate":"2016-12-22T00:00:00Z","relpermalink":"/post/2016-12-22/","section":"post","summary":"A touch of sin","tags":["dictation","English Learning","Listening","BBC"],"title":"天注定","type":"post"},{"authors":null,"categories":["dictation"],"content":"\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-12-21 11:56\t用时：18:57 正确率：92%\t错词：17个\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nBut first, how did it used use to be ? and how did we end up here? In 1973, journalist Hunter Davies was allowed into the inner sanctum of Tottenham Hotspur. Davies spent the a whole season with the team, training with them, visiting the players' homes , and witnessing the dressing room confrontations. In the modern era of painstaking media management and tight security, no sports writer will ever again be granted such unprecedented access. Not one of the 19 players in the first team pool had an agent, a lawyer, an accountant, . They had no manager looking after them. It was It's all just a word out of mouthsmouth. They got roughly £200 a week. They lived in £2000 20,000-pound houses . It's since the 1970s . and Spurs at the time, like Arsenal, had no advertising in the programprogramme. You can't believe it, can you? And they had no advertising in the ground on billboards. billboard because they thought felt they were above that, . They were above nasty commercial , merchandising and promotion. That was really the end of the an era they've that's gone on for over 100 years. £200 per week translates into £2,181 in today's money, which is quite a bit. , but no in nearly where near the amount the top players earn today. In 2004, Hunter Davies was invited to write Wayne Rooney's autobiography. Rooney was 19 at the time , and this was set said to be the first in the a series of 5 five to cover the rest of his footballing life. Words worth to be remembered: sanctum: n. 圣所,密室,私室 Tottenham Hotspur: 托特纳姆热刺足球俱乐部，简称热刺，是英格兰超级联赛的球队之一。它是二十世纪首支成为联赛及英格兰足总杯双料冠军的球队。在1963年夺得欧洲优胜者杯宝座，是英国首支取得欧洲赛事锦标的队伍。 a word of mouth 口碑: Few things offer better word of mouth than a rocketing stock price. 最有助于打造良好口碑的莫过于飙升的股价。 billboard:n. 广告牌；布告板 v. 宣传 譯文 但首先，球员过去的收入是怎么样的?我们是如何结束这种观念的呢? 1973年，新闻记者Hunter Davies被允许进入托特纳姆热刺足球俱乐部进行内部随访。Davies花了一个季度的时间和球队呆在一起，他和他们一起训练，一起去球员家做客，亲眼目睹更衣室里球员间的混战打闹。在当今无比谨慎的媒体管理和周密严格的安保措施之下，没有任何体育记者可以再次被准许拥有这种前所未有的近距离接触行为。 起初，在一个19人组成的团队中，没有一个球员有专门的代理人、律师和会计师，也没有球队经理来关照他们。这只是一个口碑的问题。每周他们可以拿到大概200英镑。自20世纪70年代起他们就住在20,000英镑房租的房子里，像阿森纳，在节目中也没有他的广告代言。简直令你难以置信，不是么？ billboard排行榜中都没有有关他们广告的一席之地的原因在于，他们认为自身的价值远不止于这些。他们的价值立于那些不入流的广告营销之上。抱有这种想法的球员时代已经整整结束了超过100年的时间了。 每周两百英镑放到现在就是2181英镑的价值，可以说是相当大的一笔钱，但这笔钱和现在的顶级球员所能赚的相比还够不着边儿。2004年，Hunter Davies受邀编写韦恩鲁尼的自传，那时鲁尼19岁，他的自传被认为是五个球员自传系列中的第一个，以至于其价值盖过了他剩下的的足球生涯。\n","date":1482278400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1482278400,"objectID":"c69eab7ed7c351c5222d221642fbc1da","permalink":"https://wangcc.me/post/2016-12-21/","publishdate":"2016-12-21T00:00:00Z","relpermalink":"/post/2016-12-21/","section":"post","summary":"球员过去的收入是怎么样的?我们是如何结束这种观念的呢?","tags":["dictation","English Learning","Listening","BBC"],"title":"足球運動員掙多少錢?","type":"post"},{"authors":null,"categories":["dictation"],"content":"\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-12-20 11:55\t用时：19:49 正确率：94%\t错词：8个\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nWell , it's been another tough week on global stock market. It's markets - big falls again on Thursday, concerns of a over central bank policy in the US. , but also , as ever , the state of China's economy. One question is being asked: \" how stable is China's currency nowadays? \" For months, the challenges China's been facing had have been a talking point, growth down, stock markets sliding. But if there were to be a real crisis in the country, some reckon the first signs of it might be a run on the country's currency. When are they We're not there yet? , but it does seem as though , wealthier Chinese citizens are increasingly hedging their bets and shipping billions of dollars worth of yuan or renminbi abroad. , switching it into safer foreign investments. , this despite partial government restrictions on the exchange of foreign currency. The BBC's Danny Vincent stepped out of his office in Beijing to visit one area where the black market trade supposedly takes place. I've come to the back streets surrounding Beijing Railway Station. , and I've tried to buy some fake invoices from one of the many street vendors waiting here for customers. Across the road from the station, there are there're crowds of travelers travellers returning to the capital after Chinese New Year. And there is there's a small army of sales people salespeople stopping them to offer their goods and services. Words worth to be remembered: talking point: 话题；论据；讨论的焦点 例： It's bound to be the main talking point during discussions between the prime minister and the president. 这必定会成为首相和总统会谈的主要议题。 hedge one\u0026rsquo;s bets: 为防止损失两面下注，两面讨好 例句： 1861-1865: U.S. Civil War -- British hedge bets on both Union and Confederacy. 1861~1865 美国内战期间，英国佬对北部联邦和南部邦联两头下重注。 supposedly: adv. 据信地，可能地 partial: a.偏袒的 例： I might be accused of being partial. 我可能会被人指责是偏袒的。 譯文 从全球股票市场来看，这又是艰难的一周——周四的又一次大跌，对美国中央银行政策的担忧，当然还有中国的经济状况。有一个问题一直被提及，中国现行的货币有多稳定？几个月来，中国所面临的各种挑战已经成为了谈论的焦点，下降的增长率，下滑的股市。但假设中国真的发生一场危机，一些人认为最初的迹象可能是国家货币的挤兑。目前还没发展到那种地步，但似乎富有些的中国公民逐渐将资本在多方下注，将价值数十亿美元的人民币运往国外，并换成更保险的外国投资产品，即便中国政府在人民币外汇交易上有各种偏袒和限制。BBC记者丹尼·文森特走出北京的办公室，到人们比较熟知的黑市交易地点进行走访。 我来到了位于北京站周围的小路上，想要从其中一个商贩手中买一些假发票，这些商贩在街上招呼着顾客。在北京站对面的路上能看到大波的旅客在新年后返回首都，还有一小撮商贩会打断他们的步伐，兜售一些商品和服务。\n","date":1482192000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1482192000,"objectID":"b9132ce3eee4992b715ca7519db7a6e0","permalink":"https://wangcc.me/post/2016-12-20/","publishdate":"2016-12-20T00:00:00Z","relpermalink":"/post/2016-12-20/","section":"post","summary":"你覺得人民幣可靠嗎？","tags":["dictation","English Learning","Listening","BBC"],"title":"人民幣可靠麼?","type":"post"},{"authors":null,"categories":["dictation"],"content":"\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-12-19 12:00\t用时：19:55 正确率：91%\t错词：17个\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nHow is Donald Trump going to avoid damaging conflict conflicts of interest once he takes up his position as America's next president? A planned press conference was scheduled for this week on the matteris . It's now being canceledcancelled. Mr . Trump would only say via his tweeter Twitter feed that no new deal will deals would be done while he was he's in the White House. That adds to an earlier pledge he made, that he will would step down from the day-to-day management of his far-flung global business empire. Still, many say those measures don't go far enough. So , what are the precedents for how presidents deal with their business affairs once in office? A question for Norman Eisen, who is was the Chief White House Ethics Lawyer for the Obama administration. Ever since the passage of the Ethics in Government Act in 1978 that American presidents have utilised utilized what we refer to as a qualified blind trust or the equivalent. That means that they take their business interest interests, and they give them to the trustee to dispose of, convert it into cash. , and then reinvest behind the wallet a wall of blindness. So the president doesn't know what his investments are, in order to avoid conflicts. As long as the president has investments known to him and othersyou , you'll always have the risk that his judgement judgment will be distorted . or others will reach out in an and attempt to use his investments to influence him. Words worth to be remembered: pledge:n. 抵押；典当品；许诺；誓言；信物 v. 保证，发誓；用……抵押 far-flung:adj. 广布的,广泛的,蔓延的,遥远的 trustee:n. 受托人；理事；信托公司 v. 移交…给受托人 dispose:phr. 处理，解决；转让，卖掉；吃光；除掉 Judgment vs. judgement: In American English, judgement is generally considered a misspelling of judgment for all uses of the word, notwithstanding individual preferences. In British popular usage, judgment was traditionally the preferred form, but judgement has gained ground over the last couple of centuries and is now nearly as common as judgment. blind trust: 保密信托 例： Yang transferred the shares into a blind trust earlier this week. 这个星期的早些时候杨把股票转成了保密信托。 譯文 当唐纳德·特朗普真正宣誓就职美国下任总统时，他如何避免利益冲突带来的损害？本周原定关于该主题的一次记者招待会被取消了。特朗普本人只在他的推特上发文称其在白宫期间不会达成新的交易事项。这也呼应了其前不久的誓言，即他将退出对其遍布全球的商业帝国的实时管控。但仍有很多人认为这一举措不可能持久。那么以前那些成为总统的人是如何处理他们的经济事项的呢？我们请来了奥巴马政府的白宫首席道德律师，诺尔曼·艾森。 自从1978年美国政府道德法案得以通过，美国总统可以充分利用我们所说的合乎标准的保密信托或类似手段。这意味着他们将剥离这些商业利益，并置于信托人处理，转换为现金，再背对背地进行二次投资。所以美国总统们不会知道他们投资于何处，就是为了避免冲突。只要总统们或其他人知悉这些投资，总统的判断就很有可能被左右，或者其他人将利用这些投资游说总统意图影响他。\n","date":1482105600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1482105600,"objectID":"e8ec521974b1d00911f684e838b21d4e","permalink":"https://wangcc.me/post/2016-12-19/","publishdate":"2016-12-19T00:00:00Z","relpermalink":"/post/2016-12-19/","section":"post","summary":"当唐纳德·特朗普真正宣誓就职美国下任总统时，他如何避免利益冲突带来的损害？","tags":["dictation","English Learning","Listening","BBC"],"title":"總統的COI","type":"post"},{"authors":null,"categories":["dictation"],"content":"概述：从小我们就被教育要拾金不昧，但是现实生活中有便宜可占的时候，你会怎么做呢？澳大利亚有位“lucky dog”，把银行当钱包，自称“年轻无知，糊里糊涂”透支了2百万澳元。。。做人还是要诚实啊\nAustralian man spent $1.5m over two years due to bank glitch and somehow got away with it\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-12-18 16:17\t用时：20:03 正确率：91%\t错词：25个\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nNow , what would you do , if you realized realise that you could keep withdrawing money from the bank way beyond what you've actually naturally paid into your account? A young Australian , Luke Moore found himself in that position. In March2010, 2010 he set up an account at a branch of St. George Bank . and an administrative error meant that whenever he asked for a loan, it was automatically granted. Over the next two years or so , he withdrew more than 2 million Australian dollars, about 1. 5 one and a half million U. S. US dollars. He's been telling me when it dawned on him , that he had access to limitless funds. It happened slowly over time. And originally , I just had a normal everyday banking account. My home rentloan, my health insurance, things like that would come in and were coming out of it. And , the first thought sort of wakeweek, I was like , oh, when \" While there was no money to pay the mortgage, what was I gonna do? \" But then , it went through, and then I was like \" OK\" , Okay. and then the next 400 dollarsdollar, so there was then another next 500 dollars 400-dollar payment of my mortgage went through, . And that went on and on like that for and after about 12 months . and all what that was happening was , my account was becoming overdrawn. So the bank said nothing to you , but you didn't think to say anything to the bank , to query what was happening? I am I'm not sure exactly what I thought at the timeall. It was a confusing time for me. , and I was a young and foolish , 22-year-old , I who didn't really, I didn't know about the banking system or anything like that. Words worth to be remembered: administrative:adj. 管理的；行政的 dawn on:渐渐为…所明白，开始为…所理解; 例句： Slowly, it begins to dawn on me: I have been having a nightmare. 渐渐地，我明白了：我一直在做噩梦。 overdrawn:v. 透支；夸张；拉过度 have access to: 使用；接近；可以利用。 例句： Plug-ins can have access to the file system and Internet once they are installed. 插件一旦安装就可以访问文件系统和Internet。 譯文 如果你发现你能从银行不断地取钱，而且所取数目早已超出你存进账户的金额，你会怎么做？2010年3月，澳大利亚的年轻人卢克·穆尔碰巧遇到这样的事。他在圣·乔治银行分行开设了一个账户。一个管理方面的错误导致他不论何时申请贷款，都能被自动批准。接下来两年时间里，他取走了200多万澳元，约合150万美元。他告诉我他是什么时候发现他能无限制地取钱。\n事情就那么慢慢发生了。起初我只有一个日常的银行账户，用来支付我的房贷、医保等等这类东西。第一周左右，我正在发愁“没钱还贷款，我该怎么办？”但是居然成功受理了。于是我就想“那好吧”。接着下一个月我又成功得到了一笔400块的贷款，然后又一笔。这一状态一直持续了12个月，而这一切都是在我账户透支的情况下发生的。\n那么银行没找过你、而你也没觉得应该找银行问问发生了什么吗？\n我不知道自己怎么想的。这事困扰了我一段时间。那会儿我是个年少无知的22岁小伙子，哪懂什么银行系统之类的东西。\n","date":1482019200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1482019200,"objectID":"b7c737a6efb30f6c5ca9900389ebc01d","permalink":"https://wangcc.me/post/2016-12-18/","publishdate":"2016-12-18T00:00:00Z","relpermalink":"/post/2016-12-18/","section":"post","summary":"Australian man spent $1.5m over two years due to bank glitch and somehow got away with it","tags":["dictation","English Learning","Listening","BBC"],"title":"兩百萬","type":"post"},{"authors":null,"categories":["dictation"],"content":"\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-12-17 12:07\t用时：22:37 正确率：91%\t错词：15个\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nThe senior Taliban are surprisingly mild-mannered, . They don't seem to get angry or happy. They believe absolutely that what they are they're doing is right. , but they are they're often naive when it comes to the media. In some ways, Afghanistan is an easy patch. There are no spin doctors here. Afghan men themselves are extremely courteous. Being permitted to carry bags and open doors are my main problems. Unlike any other countries I have country I've visited, I face no sexual harassment. For me, the hardest thing to understand is how Afghans afford have fought such a brutal war over so many years. Politeness, hospitality, decency , are the main traits I have I've encountered. An insight into this paradox came on a visit to the front linesfrontlines. The fighter fighters showed off one of their rocket launches launchers set up in the middle of a ruined village. The group was a mixture of veterans with 20 years ' experience . and youth youths who are had yet to grow beards, but who nonetheless swaggered. After the artillery demonstration, they gave me tea . and then presented me with a bunch of flowers, beautiful , blonde colored bronze-coloured chrysanthemums. It turned out , they've they'd grown the flowers themselves . inside the bunker on which the artillery piece stood. These fighters, who just a couple of months earlier had slid slit throats in vicious hand to hand hand-to-hand fighting, loved growing house plantshouseplants. I wondered if they'd offer you tea before they slid slit your throat. Words worth to be remembered: mild-mannered:adj. 温和的，温柔的 In some ways, Afghanistan is an easy patch. There are no spin doctors here. Afghan men themselves are extremely courteous. 从某些方面来说，阿富汗是一片安逸之地，这里没有专家喉舌，这里的男人也彬彬有礼。 spin doctors: 起导向作用者，定调者；(尤指)助选的高级顾问，谋士；公关专家[亦作 spindoctor] courteous: adj. 有礼貌的，谦恭的 decency:n. 得体，体面；正派 nonetheless:adv. 尽管如此，但是 swaggered: adj. 时髦的 n. 大摇大摆，趾高气扬；吹嘘，自大；威吓 v. 大摇大摆，趾高气扬；夸耀；吹牛；吓唬 artillery: n. 火炮，大炮；炮队；炮术 bunch:n. 群；串；突出物 vt. 使成一串；使打褶 vi. 隆起；打褶；形成一串 n. (Bunch)人名；(英)邦奇 chrysanthemums:n. 菊花 slit:n. 切口，裂缝；投币口 v. 切开，撕开 hand-to-hand:adj. 极接近的；白刃战的；传递到手的 譯文 令我惊讶的是，塔利班高级官员举止非常温和。他们似乎不会生气也不会很开心，对自己所做的事情也深信不疑，但面对媒体时，却显得很天真。从某些方面来说，阿富汗是一片安逸之地，这里没有专家喉舌，这里的男人也彬彬有礼。允许带包以及大门敞开是我遇到的主要问题。但与我去过的其它任何国家不同的是，在这里我从未受到过性骚扰。\n我很难想象阿富汗人民经历过这么多年的残酷战争，因为在这里，我感受到的只有礼貌、热情和文明。\n在一次前线采访中，我对这一悖论有了深入的观察。战士们向我展示了位于废墟村庄中部的导弹发射装置。塔利班组织成员老少不一，有经验丰富的老兵，也有羽翼未丰的青年，但他们依然气宇轩昂。\n火炮展示完毕之后，他们给我递了一杯茶，还送我一束古铜色的菊花。花居然是他们自己在地堡里种的，那片地还残存弹片。几个月前，这些战士还在肉搏战中撕裂别人的喉咙，而他们竟然爱好在室内种植植物。我很好奇，他们在撕裂别人喉咙之前，会不会也递上一杯茶。\n","date":1481932800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1481932800,"objectID":"d907c82eac3ba7865682c5a08cbd226e","permalink":"https://wangcc.me/post/2016-12-17/","publishdate":"2016-12-17T00:00:00Z","relpermalink":"/post/2016-12-17/","section":"post","summary":"Taliban","tags":["dictation","English Learning","Listening","BBC"],"title":"澤國江山入戰圖","type":"post"},{"authors":null,"categories":["dictation"],"content":"\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-12-16 12:31\t用时：28:24 正确率：90%\t错词：23个\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nFor Buddhists, god's waiting room is a stepping stone to another life. And to learn more about that, I've come here to the Amaravati Monastery in its peaceful rural setting , near the town of Berkhamsted , north of London. And I gain I'm going to meet the abbot , Ajahn Amaro. \" Well, from the Buddhist perspective, the things thing that causes the most trouble for us, while we'ar we're alive, or we're dying , is a the habit that of what we all will call clinging or grasping attachment. If you were you're hanging onto on to your possessions, or you're hanging onto on to the regrets that you have . and thinking in terms of the customary Buddist Buddhist pattern that we are of understanding . There is that there's a pattern of past lives and future lives that we are we're all experiencing, so the present life is one of a life large sequence. And so the basic process of Buddhist practice and the underlying current of it is trying to try to end rebirth altogether. \"\n\" I think this is the most fascinating concept. It's very ancient. It emerges within Indian tradition, Buddhist tradition follows it, sick Sikh tradition generally follows it. But what's going on with it? What's the driver? , the driver of reincarnation , as the ideas it developed, was the one that of merit. But this idea of merit of also drives the Jewish , Christian , Islamic world , which is not a reincarnation world. , but it is a world of life, death, judgment, judgement and an afterlife. When in the West, we pick up this concept of reincarnation, we picked pick it up rather positively and joyfully. In actual fact, it's something you just gotta go through and through and through and through , until of course , you gain release. \" Words worth to be remembered: stepping stone: 垫脚石；跳板；进身之阶 例： The next rock in your path may be a stepping stone. 也许人生路上的下一个绊脚石就是垫脚石。 hang on to: 紧紧抓住；紧握 例： For most nations, it is a great achievement to hang on to what you already have. 对于大多数国家来说，为了已经拥有的东西而坚持不懈是伟大的成就。 clinging:adj. 执着的；有黏性的；紧靠着的 v. cling的现在分词；抓紧 grasping: adj. 贪婪的，贪心的 v. grasp的现在分词；抓住 customary: adj. 习惯的；通常的 underlying current: 其潜在的趋势 altogether: adv. 完全地；总共；总而言之 reincarnation: n. 再赋与肉体，化身，再生，转世，轮回 merit: n. 优点，价值；功绩；功过 vt. 值得 vi. 应受报答 afterlife:n. 来世;晚年 joyfully:adv. 欢喜地;高兴地;快乐地 in actual fact: 事實上 譯文 对佛教徒来说，夕阳红的晚年是通向往生的进身之阶。为了深入了解这一点，我来到了伦敦北部靠近巴克汉姆斯特镇的一个宁静偏远的景点——阿玛拉瓦第佛法道场。我将拜见其住持方丈Ajahn Amaro。\n“从佛教的视角来看，引起最大麻烦的东西是我们依赖于附属物的习惯，无论我们是生还是死。如果你紧握住你的财富不放，或是放不下过去的悔恨，那么以传统的佛教模式思考，或者说理解我们都在经历着的前世和来世，今生只是一长串序列中的一部分。所以佛教修行的基本过程及其潜在的趋势是终止轮回转世。”\n“我认为这是最令人着迷的一个概念。它非常古老，起源于印度教传统，佛教继承了它，锡克教大致上也沿袭了它。然而之后发生了什么？随着理念的发展，行善所得的功德成为了投胎转世的驱动者。这种功德的理念同样推动了不存在投胎再生的犹太教、基督教、伊斯兰教世界，形成了生死、审判以及来世的世界。在西方，我们习得了轮回转世的理念，我们相当正面且愉快地接受了它。事实上，它就是你不断地经历磨难直到最后得到解脱的过程。”\n","date":1481846400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1481846400,"objectID":"88f2bcce6067332dfa0e3d2b222b771e","permalink":"https://wangcc.me/post/2016-12-16/","publishdate":"2016-12-16T00:00:00Z","relpermalink":"/post/2016-12-16/","section":"post","summary":"Buddism","tags":["dictation","English Learning","Listening","BBC"],"title":"輪回轉世","type":"post"},{"authors":null,"categories":["dictation"],"content":"\n摩洛哥刑法规定强奸犯与受害者结婚可免除刑责，16岁少女自杀一石激起千层浪. Morocco protest against rape-marriage law\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-12-15 12:8\t用时：22:26 正确率：92%\t错词：20个\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nWomen's right campaign is rights campaigners in Morocco , it have been demonstrated demonstrating in the capital Rabat against the country's rape law laws which allows allow the rapist to marry his victim in order to avoid prosecution. The protesters have been outraged by the suicide of 16-year-old Amina Filali last week after she was severely beaten when she was forced to marry her rapist. Her mother described what happened. \" He raped my daughter, . He mistreated my daughter. She complained to me that she was mistreated . that she didn't get fed, that he threaten threatened to kill her if she stayed there. I told her to not to be afraid, to be patient, to be very patient. \"\nThe protesters have asked the government to remove a specific article in the penal code , which forces rape victims to marry their rapists , has as a way of defending the families' honorfamily's honour. I called My colleague Julian Marshall and got more details . And my from our correspondent in Rabat , Nora Fakim. \" But the Moroccan's Moroccans women's association are protesting today , to ask the government to remove the penal code 475 . and to help change mentalities , because for them, it's a waste of life and there could be future Amina Filalis if the government doesn't do anything. \"\n\" And how much supporters support is there for this pressure to remove this article from the penal code. ? \"\n\" Well, the women's activists are getting a lot of pressure from Moroccans in general. But, but I spoke to them in this morning , and they have been disappointed by the government's reactions reaction so far. , because only 2 two ministers responded . and they are particularly upset at the minister of justice who is defending the judge who forces forced the 15-year-old girl to get married to her rapist. \" Words worth to be remembered: campainger: n. 竞选者，活动家；从军者，出征者；老兵 penal code: 刑法典 譯文 摩洛哥的女权运动者在首都拉巴特示威，抗议该国强奸法案。该法律规定，强奸犯可与被害者结婚，以免受刑责。16岁少女阿米娜•费拉利被迫嫁给强奸犯后，于上周遭毒打后自杀身亡，此事件也点燃了抗议者的怒火。阿米娜的妈妈讲述了事情的经过。\n“他强奸我女儿，还虐待她。我女儿向我诉苦说他虐待她，不给她东西吃，还威胁说如果她留在家里就杀了她。我告诉她不要害怕，要忍耐，一定要忍下去。”\n摩洛哥刑法强迫受害者与强奸犯结婚，以保护家族名誉，抗议者则要求政府废除这项条例。我的同事朱利安•马歇尔与拉巴特记者诺拉•法基姆连线，获知了更多细节。\n“摩洛哥妇女联盟今天抗议，要求政府废除刑法475条，并帮助受害者恢复心理状态，因为如果政府不改变现状，对受害者来说就是浪费生命，今后也可能会发生更多类似阿米娜•费拉利的事件。”\n“那么这次要求废除刑法相关条例的抗议得到了多少支持？”\n“一般来说，妇女运动会受到很多来自摩洛哥人的压力，但我今早与他们交谈，发现他们对政府目前的反应感到失望，因为仅有两名大臣做出回应。而之前法官曾强迫15岁少女嫁给强奸犯，司法部长对此做出的支持也令他们尤为愤慨。”\n","date":1481760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1481760000,"objectID":"0e63f67fc44fc38ea19b982867691b44","permalink":"https://wangcc.me/post/2016-12-15/","publishdate":"2016-12-15T00:00:00Z","relpermalink":"/post/2016-12-15/","section":"post","summary":"摩洛哥刑法规定强奸犯与受害者结婚可免除刑责，16岁少女自杀一石激起千层浪. Morocco protest against rape-marriage law","tags":["dictation","English Learning","Listening","BBC"],"title":"強奸不犯法 喪盡天良的摩洛哥法律","type":"post"},{"authors":null,"categories":["dictation"],"content":"Yemen Starving\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-12-14 14:14\t用时：16:00 正确率：92%\t错词：16个\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nBut first , the UN has issued a stock stark warning that an increasing number of children in Yemen face death from hunger because of a the lack of international help. The Wolrd World Health Organisation says more than half of the country's hospital hospitals are not functioning , due to the conflict that's lasted more than 20 months. Houthi rebels and forces ally shut allied to the former President Ali Abdullah Saleh are fighting the internationally recognized recognised government , which is backed by Saudi Arabia. Since the war began, the number of children suffering from malnutrition has risen 3-foldthreefold. Our special correspondent Fergal Keane has traveled travelled to one of the worst-affected parts of the country. \" It is becoming one of the sounds that defines our time. Another desperate infant in the a Middle-Eastern war without end. In this room, there are two babies. , Juda who's to my left and she is 4 four months old. , and on the right, on another bed with her mother is Eliane, who's 9 nine months old. They are both chronically malnourished. And as I'm looking at Juda. , this, this tiny face staring at me vacantly . and hands that are, they are too tiny for a child who is 4 who's four months of age. Too tiny. \" Words worth to be remembered: stark: adj. 荒凉的；光秃秃的；刻板的；僵硬的；朴实的；完全的 adv. 完全地；明显地 ally to: 与...属一类,与...相似,与...有关系, 同盟 vacantly:adv. 空虚地,无表情地 譯文 首先，联合国发出了一则严肃的警告：也门越来越多的儿童因缺乏国际援助而面临饿死的危险。世界卫生组织称，由于长达20个月的战乱，也门境内超过半数的医院已经停止工作。胡西族的反政府武装以及和前总统阿拉布都结盟的武装力量联合起来，正在与得到国际认可、并由沙特阿拉伯支持的也门政府交锋。自从开战以来，患上营养不良的儿童数量翻了三倍。我们的特派记者 Fergo Kee 已经前往也门儿童饥饿情况最严重的地区之一。\n“（这哭声）也是属于我们的时代的其中一种声音。无休无止的战争，又一个深陷绝望的婴儿。在这则视频中，我左手边的是 Juda, 她四个月大；我右手边的，跟母亲躺在同一张床上的是 Allian, 他九个月大。他们两个都已经长时间营养不良。我此刻正看着 Juda, 这张小脸空洞洞地看着我，而她的手，作为一个四个月大的孩子来说也太小了。实在太小了。”\n","date":1481673600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1481673600,"objectID":"e3db60416c2e30759510581e7cb1deb6","permalink":"https://wangcc.me/post/2016-12-14/","publishdate":"2016-12-14T00:00:00Z","relpermalink":"/post/2016-12-14/","section":"post","summary":"Yemen Starving","tags":["dictation","English Learning","Listening","BBC"],"title":"也門飢荒","type":"post"},{"authors":null,"categories":["dictation"],"content":"Ukraine\u0026rsquo;s Tiny \u0026lsquo;Troublemaker\u0026rsquo;\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-12-13 12:04\t用时：23:33 正确率：92%\t错词：16个\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nWe've been hearing from controversial campaigners who've been making waves across Europe. Today , we go to Ukraine and to Kiev , where clashes between government forces and opposition groups have been the at their most violence violent since the unrest began in November last year. 34-year-old Tetiana Chornovol is an anti-government activist who divides opinion across in the country. As an investigative reporter for opposition websites, she's built a reputation for unusual standsstunts. But in December last year, she became the story herselfm herself when she was brutely brutally attacked after her car was forced off the road. Last month, Outlook's Lucy Ash when went to meet Tetiana and her family in Kiev. Wustim has got a scooter with lights on it. , going round and round. He is He's like Tetiana. Oh , yes, their personalities are very much alike. I've just arrived in the home of Tetiana Chornovol, . In the front room of their house on the outskirts of the capital Kiev. , Tetiana's very determined 3-year-old three-year-old son Wustim has rolled up the carpets carpet so he can whiz above about on with his scooter, watched by his grandma, Natalia. I said to myself, 'Tanya, , hold the will wheel straight, . Stay on the road. ' But this car that was reaming ramming me probably cost $40,000. And I realized, realised that if such an expensive car is delivered its mission deliberately smashing into me. , it means that those guys are being paid a lot to kill me. Words worth to be remembered: unrest: n. 动乱，骚乱，不安的状态 stunt: n. 特技表演；噱头 v. 阻碍，遏制；表演特技; 例句： He used to stunt-drive in acrobatic performances. 他过去常在杂技表演中表演驱车特技。 brutally: adv. 残忍地，野蛮地，狠狠地；直截了当地；难以忍受地 ramming:舂实,捣打,打夯,抛砂,夯,压实 deliberatly:adv. 故意地；从容不迫地 smashing:adj. 极好的；轰动的；粉碎性的 v. smash的现在分词；粉碎 divide opinion: 指令（一群）人的意见产生分歧。 例句： Airport expansion plans divide opinion. 众人对机场扩建方案看法不一 scooter[ˈsku:tə(r)]: n. \u0026lt;英\u0026gt;小型摩托车；（儿童）滑板车。 例句： On the scooter we laughed about the performance. 在摩托车上我们为这场表演哈哈大笑。 whiz[wɪz]: vi. 发出飕飕声；\u0026lt;口\u0026gt;高速移动。 例句： They heard bullets continue to whiz over their heads. 他们听到子弹不断在他们头顶上嗖嗖飞过。 【背景资料】 Ukraine（乌克兰） 乌克兰位于欧洲东部，是欧洲除俄罗斯外领土面积最大的国家。原苏联15个加盟共和国之一，是仅次于俄罗斯和 哈萨克斯坦的第三大加盟共和国。1991年苏联解体后，乌克兰独立。乌克兰地理位置重要，是欧洲联盟与独联体特别是与俄罗斯地缘政治的交叉点。 Kiev（基辅） 基辅为乌克兰首都，经济、文化中心。位于第聂伯河中游两岸，及其最大支流普里皮亚季河与杰斯纳河汇合处附近。面积 782平方千米，人口约260万，全市分为10个行政区。 Tetiana Chornovol Tetiana Mykolayivna Chornovol is a Ukrainian journalist and civic activist, one of the leaders in the ongoing Euromaidan protest campaign. She is famous for investigative reports about corruption in Ukraine, as well as for her adventurous direct actions. On 25 December 2013, Chornovol was the victim of a much published and condemned severe beating. 译文 我们听说过那些在欧洲引起轰动而具有争议的活动家们。今天我们将去乌克兰的基辅，那里自去年11月动乱开始，政府势力和反对派组织间的冲突已经达到暴力顶峰。34岁的塔提亚娜·车娜沃尔是一位反政府激进分子，乌克兰人对她看法不一。作为一位反对派网站的调查记者，她以工作中行事出奇冒险而闻名。但是去年12月，塔提亚娜自己成为了冒险故事的主角，她的车被逼驶离了公路，之后遭到残忍的攻击。上个月，Outlook节目的露西·阿什去基辅见到了她和她的家人。\n乌斯提姆得到一个带灯的小滑板车，他一圈一圈的（踩着它）走。他很像塔提亚娜。\n噢，是的，他们性格非常相像。\n我刚到达塔提亚娜在首都基辅的郊外的家。在她家客厅，车娜3岁的儿子乌斯提姆坚决地卷起了地毯，好在房间里踩着滑板车溜来溜去，奶奶纳塔丽娅紧紧盯着他看。\n我告诉自己：“汤娅，笔直开。别离开公路。”但撞过来的这辆车大概价值4万美元。于是我意识到，如果这么贵的一辆车故意撞我，肯定有人花了大价钱买我的命。\n","date":1481587200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1481587200,"objectID":"fd40cac847c119abbe2dd575dd2b4a4a","permalink":"https://wangcc.me/post/2016-12-13/","publishdate":"2016-12-13T00:00:00Z","relpermalink":"/post/2016-12-13/","section":"post","summary":"Ukraine's Tiny 'Troublemaker'","tags":["dictation","English Learning","Listening","BBC"],"title":"Troublemaker","type":"post"},{"authors":null,"categories":["dictation"],"content":"How soldiers deal with the job of killing\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-12-12 14:24\t用时：20:19 正确率：89%\t错词：22个\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nBen says he wanted to kill, . But there are others who are very reluctant. In the Second World War, one of the combatants , S. L. A. Marshall observed that many didn't shoot. He wrote a study , called Men Against Fire , in which he concluded that there was a reluctance to kill. There They were what were called non-firers. Fear of killing rather than fear of being killed was, he said, was the most common cause of battle failure. Marshall's methods of research have since been questioned, but the broad conclusion is still accepted by many. : soldiers often simply won't fire. The referent Reverend Dr Giles Fraser's phrases are Fraser lectures on morality and ethics at the academy of the British Ministry of Defense. \" I think there is there's a deep human reluctance to kill other people. S. L. A. Marshall found that only 15 % to 20% of combatants in history combat infantry were able to fire their weapons on the enemy. And there were 80%, there 80% that were defects or can't change subjects de facto conscientious objectors when they come it came to the point to fire of firing their own weapon. And this I'd say it is extraordinary. At Gettysburg, this is another extraordinary story. There were 27,000 rifles, muskets , left on the field of battle . and 90% of them were loaded. And actually , it was it's because people were not firing their weapons. \" Words worth to be remembered: Reverend: adj. （对牧师的尊称，前面与the连用）尊敬的 combatant: n. 争斗者,战斗员 infantry: n. 步兵；步兵团 de facto: （法）实际上的 conscientious:adj. 认真的；尽责的；本着良心的；小心谨慎的 objector: n. 反对者；提出异议的人 musket: n. 步枪；滑膛枪，毛瑟枪 譯文 Ben说他曾经想要杀戮。但也有一些人是被逼无奈才这么做的。二战期间，作为参战人员的S.L.A. Marshall观察到当时很多人都没有开枪。他做了一个名叫《反对战火的人》的研究，在这份报告里他得出的结论是人们不愿意杀戮。这些人叫做不开枪的人。Marshall说害怕杀戮而非害怕被杀其实是战斗失败的最常见的原因。Marshall研究的方法虽然一直遭到质疑，但是他得出的大结论还是被广泛接受，那就是士兵一般不愿开枪。Giles Frasers博士在英国国防部学院做了以道德为主题的演讲。\n“我认为人类自内心深处就对杀戮有排斥。S.L.A. Marshall发现战斗步兵队伍中，只有15%到20%的人能够朝敌人开枪。而80%的人在面临开枪那一刻实际上是会产生抵触情绪而不愿意这么做的。我认为这个发现很特别。葛底斯堡的战役也很特别。当时有27000把来复枪和步枪散落在战场上，其中90%的枪都上了膛。实际上这是因为没有人开枪。”\n","date":1481500800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1481500800,"objectID":"289794cf8037a0107bbdbe742bd9c864","permalink":"https://wangcc.me/post/2016-12-12/","publishdate":"2016-12-12T00:00:00Z","relpermalink":"/post/2016-12-12/","section":"post","summary":"How soldiers deal with the job of killing","tags":["dictation","English Learning","Listening","BBC"],"title":"人性與戰爭","type":"post"},{"authors":null,"categories":["dictation"],"content":"\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-12-11 17:20\t用时：22:26 正确率：94%\t错词：14个\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nToday, a fascinating insight into what it's is like to live in Turkey at a time of political repression. The country is in turmoil at the moment. There's been a state of emergency for nearly 6 six months since the attempted coup. According to official figures from the Ministry of Justice. , at least 93,000 people are under investigation for their elegy alleged links to the coup plotters and around 34,000 are facing charges. The last time there was this much political chaos in Turkey was in 1980 , when there was an actual military coup. Burhan Sönmez who' who's known as a novelist these days , was a child at the time. It didn't really hit him until he moved to Istanbul a couple of years later, an idealist 17-year-old idealistic about to embark on a law degree. Of course , I knew Istanbul before I saw that place. All songs, all movies, all stories , and novels and poems about Istanbul in Turkey. Even fairy tales alwaysdirect , there were some scenes things about Istanbul. Was it like a fairy tale , when you first arrived? Yes, definitely. I was amazed. We got a great train station in Istanbul. , Haydarpaşa train stationis . It's just by the sea. I can claim that it's the most beautiful train station in the world. I'll buy itUnbiased, of course. Yes. You just come off the train, you see the bus first, Bosphorus and the Europe is just across the sea. It was a wonderful feeling and it was a bit breezy day, . I think it was September. And you were about to embark on your adult life, I thinkbasically. Oh , yes. Not scared, very much excitedand , a bit nervous, too. Because , coz you don't know what is what's expectingin , that big city, good and bad things. Words worth to be remembered: political repression: 政治壓迫,鎮壓 turmoil: n. 混乱；焦虑；骚动 coup: n. 成功之举；政变；妙计 attempted[əˈtemptɪd]: adj. 未遂的，企图的。例句： The attempted coup took place in January. 那场未遂政变发生在1月份。 alleged: adj. 声称的；被断言的；可疑的 v. allege的过去式和过去分词；宣称 plotter: n. 阴谋者,计划者; unbiased: adj. 公正的；无偏见的 Bosphorus: 伊斯坦布爾海峽 breezy: adj. 有微风的；活泼的，明快的，随意的 譯文 我们今天的节目鲜活地反映了政治镇压时期土耳其的生活。那时，国家处于一片混乱。未遂政变发生近六个月以来，国家一直处于非常时刻。司法部官方数字表明，至少93,000人因涉嫌策划政变而接受调查，约34,000人正面临指控。土耳其最近一次政治骚乱发生在上世纪80年代，那是一场真正的军事政变。近来作为小说家而被人熟知的布尔汗·索门茨在那时还是个孩子。直到几年后这个怀揣理想主义的17岁男孩搬到伊斯坦布尔准备去攻读法律学位时，才对动乱的局面真正有所感触。\n当然，来这之前我就知道伊斯坦布尔。关于土耳其伊斯坦布尔的各种歌曲、电影、故事、小说和诗歌。甚至一些关于伊斯坦布尔的神话故事。\n嗯。你第一次来到这里就觉得它像神话吗？\n是的，那当然，令我惊叹。我们来到伊斯坦布尔的一个宏伟的火车站，海达尔帕夏火车站。它就坐落在海边。我敢说它是世界上最美的火车站。\n这评价很客观，当然。\n是的。一下火车就能看见博斯普鲁斯海峡，而海对面就是欧洲。景色壮美，引人入胜，那天还有些许微风感觉非常棒，我记得当时是9月。\n而你正打算开始成年生活。\n是的。我不害怕，反而非常兴奋，还有点紧张，因为你不知道这座大城市有什么在等着你，好事还是坏事。\n","date":1481414400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1481414400,"objectID":"25ffa71d0bbc973faf1191c92d20140f","permalink":"https://wangcc.me/post/2016-12-11/","publishdate":"2016-12-11T00:00:00Z","relpermalink":"/post/2016-12-11/","section":"post","summary":"The country Turkey is in turmoil at the moment.","tags":["dictation","English Learning","Listening","BBC"],"title":"亂局中的伊斯坦布爾(Istanbul)","type":"post"},{"authors":null,"categories":["dictation"],"content":"\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-12-10\t用时：21:39 正确率：94%\t错词：5个\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nThis week in London, the cinema event of the year - if not the century - has been a five and a half-hour five-and-a-half-hour screening of a film made almost 90 years ago. It is It's Abel Gance's Napoleon, . Digitally restored, once neglected. , it was shown last Sunday with a live performance by the Philharmonia Orchestra conducted by the composer , Carl Davis, who's just turned 80 , and who paid for the restoration. We have had to leave something. This would be what we did of significance. It was a kind of, not memorial , 'cause of some sit around. strong–arm, but what's important, what have you donethat's , it's important, you know. That's , it's Napoleon. OK, okay. And the reason we could watch Napoleon in the 21st century was the dedication of a man who's pursued the fragments and trails across six decades, . Kevin Brownlow, film detective. As a school boy, I was a very enthusiastic film collector , on 9. 5, which was a the standard home movie gage gauge before 8 mm 8mm took over. I had one , called Lion and the Mogols , by the pet of the French intellectuals Jean Epstein . and I couldn't stand it and wanted to get rid of it. So I called the library I bought it from . and they said, well, unfortunately, we we've only got one film left. It's called Napoleon Bonaparte in the French Revolution, which I didn't want . because that sounded like a classroom film. Words worth to be remembered: 爱乐乐团（The London Philharmonia Orchestra: 是伦敦的一支管弦乐团。曾几度易名。乐团于1945年在“EMI唱片公司”制作人华尔特·莱格组织下成立，当时只是一支录音专用乐团。从1995年起，皇家节日大厅成了乐团的固定演出场所。 composer: n. 作曲家；著作者 gauge: n. 标准尺寸；测量仪器；规，表，计；大小，程度；范围；容量；（枪管的）口径 v. 判断；估计；量，测 譯文 本周在伦敦，影院的年度重大事件，暂且不说是本世纪，将会是一部制作长达90年的五个半小时电影。那就是阿贝尔·冈斯的《拿破仑》。这部电影曾经被忽视，如今通过数字修复技术于上周日上映，上映时还配有爱乐管弦乐团的现场演出，由即将80岁并支付了电影修复费用的作曲家卡尔•戴维斯指挥。\n我们必须留下一些什么。这就是我们做这部电影的意义。这不是为了纪念，或是因为什么强势的力量让我们促成了这部电影，真正重要的是你做了什么，这才是重要的，你知道吗，他是拿破仑，这就够了。\n我们之所以能在21世纪看到《拿破仑》是因为一个追求了60年片段和轨迹的人的奉献。他就是电影侦探凯文·布朗罗。\n当我还是个在学校的小男孩时，我就是一个非常热衷于收藏9.5mm电影的人，在8mm占领荧屏之前，这是标准的家庭电影规格。我有一部由法国知识分子让 · 爱普斯坦指导的电影《蒙古的雄狮》，然而我不能忍受它，想要摆脱它。 所以我打电话给我当时买这部电影的图书馆，他们说“不过，不幸的是，我们只剩下一部电影了“。 它就是《法国大革命中的拿破仑·波拿巴》，当时我并不想要它，因为那听起来像一部在教室里放的电影。\n","date":1481328000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1481328000,"objectID":"378e776ef98317579ae9608e416f7286","permalink":"https://wangcc.me/post/2016-12-10/","publishdate":"2016-12-10T00:00:00Z","relpermalink":"/post/2016-12-10/","section":"post","summary":"Napoleon Bonaparte in the French Revolution","tags":["dictation","English Learning","Listening","BBC"],"title":"默片《拿破侖》","type":"post"},{"authors":null,"categories":["dictation"],"content":"Speech Writer to the President\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-12-9 12:29\t用时：21:01 正确率：91%\t错词：30个\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nI think that speech writing is one of those professions where not everyone knows that it exists. So , could you please explain to us, Sarada, what the role of a speech writer is? Sure, . And I would I'd be interested to hear how it might be different for Tina and Kosovo. But here for us, the role of a speech writer is to really help a speaker find their voice. For example, figure out what they want to say and how they want to say it. And then, you know, the job really have of a speech, as far as I believe, is to persuade the an audience to believe something, to do something. And so, you know, your job is as a speech writer, and in my view, is not to come up with the ideas for a speaker, but to help them holding hone in and clarify their own thinking . and could develop an argument and then find the most persuasive way to deliver that argument. I happen to agree. For me, it was very important tothrough , sort of, understand the issues that were dealt dear to President Jahjaga, to her vision. Alsoversion, also to weave in personal stories, policy stanceand that's since , in that sense you want the speech speeches to be impactful and memorable. , and true to the acharacter character of the president and the issues that she and her teams team hold dealdear. That's really interesting, this idea of being true to the character of the president or of the person you are writing for. I mean, do you have to know them well to write a good speech? For me, it was very helpful to be by the president's side and hear her talk, and hear her interact with various audiences, her counterparts, or just the ordinary people in cost of allKosovo. When I wrote a speech, I almost felt like I had to give up my own voice and adopt her voice and her vocabulary , so that the speech was closest to her vature natural voice. So yes, you need to match your speech with her to their words or their physical chemistry. Words worth to be remembered: hone in : 磨炼; Improves my ability to hone in on an idea and learn to execute it quickly as well as meticulously. 改善我迅速深入一个创意骨子里的能力，并且学习如何迅速而尽善尽美地体现它; 锁定目标，进一步查明或发现 例句：We can hone in on your problem areas and have your employees maximizing their skills again in no time. 我们会全力研究您公司的问题，让您的职员在最短的时间里再发挥出他们的最大才能。 dear: adj. 关切 例句： This is a subject very dear to the hearts of academics up and down the country. 这个话题举国上下的学者都非常关切。 this idea of being true to the character of the president: 符合总统本身的特点 譯文 演讲写作也是一种职业，我想并不是所有人都知道还有这样的职业存在。那么，萨拉达，你能给我们讲讲演讲撰稿人是做什么的吗？\n当然可以。而且我也很想听听对蒂娜和科索沃来说会有什么不一样的地方。对于我们来说，演讲撰稿人的职责就是真正地帮助演讲者发现自己的心声，弄清楚自己想要表达什么、怎样去表达。我个人觉得，演讲的真正作用就是说服听众去相信某件事、去做某件事。所以在我看来，作为一名演讲撰稿人，你的工作并不是替演讲者出主意，而是帮助他们发现并且弄清楚自己的想法、确立论据，然后找到最有说服力的方法来传达这条论据。\n跟我的想法不谋而合。对我来说，了解亚希亚加总统所关心的问题、了解她的看法是很重要的。如果你想让演讲有影响力、令人难忘、又符合总统本身的特点、还要反应她和她的内阁成员们所关心的问题，那么在演讲里穿插她的个人经历以及她的政治立场也是很重要的。\n这点很有意思，“符合总统本身的特点”或者其他你为之写作的人物的特点。那么为了写出好的演讲稿，你是不是得充分了解他们才行？\n对我来说，能在总统的身边听她谈话、听她跟各种各样的人交谈——跟她身份相当的人交谈，或是跟科索沃的普通民众——对我是很有帮助的。当我在写演讲稿的时候，我会觉得几乎得完全摒弃自己的声音，要用她的语气、她的用词，这样演讲才会最贴近她本身的说话方式。所以，对的，演讲稿是要跟演讲者的谈吐、跟他们本身的化学效应相匹配的。\n","date":1481241600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1481241600,"objectID":"b86b603b9580918d55ce30b5cbe72a50","permalink":"https://wangcc.me/post/2016-12-9/","publishdate":"2016-12-09T00:00:00Z","relpermalink":"/post/2016-12-9/","section":"post","summary":"Speech Writer to the President","tags":["dictation","English Learning","Listening","BBC"],"title":"對話總統演講撰稿人","type":"post"},{"authors":null,"categories":["dictation"],"content":"\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-12-8 11:32\t用时：14:48 正确率：95%\t错词：8个\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nWe start today with the unlikely story of how an American woman came to be working with truckers to combat human trafficking. Kendis Paris lives in Denver, Colorado with her Anglican priest husband and her their two children. She used to spend her days making bread and cheese, home with her the kids. But now she travels the country, training drivers to recognize recognise and report human trafficking. Her organization organisation is called Truckers Against Trafficking, or TAT. She told me what inspired her to set it up. It was around 2007 and I read a book by David Batstone called Not For Sale. And before that , I probably could give you a working definition of human traffickingif , but that\u0026rsquo;s about it. And I picked that up at the direction of my mother . and got to Chapter 3 Three and I was so appalled and outraged I have had to flip to the back of the book to find out how to help. What did that book say , that shocked you so much? It was the level of systematic reap rape for profit . that just floored me. I just could couldn\u0026rsquo;t believe the numbers, . I couldn\u0026rsquo;t believe that modern-day slavery existed on this scale, around the world and right here in the United States.\nWords worth to be remembered: human trafficking: 贩卖人口。 例句： It has deciphered secret communications about murder, drug transactions, illegal gambling and human trafficking. 那些密码涉及到了谋杀、毒品交易、非法赌博和贩卖人口之间的秘密联系 Anglican[ˈæŋglɪkən]: adj. 英国国教会的。 例句： It is now an Anglican church. 它现在是英国圣公会。 appalled[əˈpɔ:ld]: adj. 惊骇的；丧胆的。 例句: He looks appalled, as if I've tried to assault him. 他那样子显得很惊骇，仿佛我试图袭击他。 priest: n. 牧师，神父，教士 British English lists an alternate spelling as recognise, but the main entry is recognize. North Americans (yes, that includes Canada) prefer the ize spelling, and this is accepted everywhere. However, outside North America some prefer the ise spelling and it is not incorrect. This spelling change goes across all derivatives including: recognizability, recognisability, recognizable, recognisable, recognizeably, recognisably, recognizer, and recogniser. Organise and organize are different spellings of the same word. Organize is the preferred spelling in the U.S. and Canada, and organise is more common outside North America. This extends to all the word’s derivatives, including organized/organised, organizing/organising, and organization/organisation. rape for profit: 为了盈利的抢夺/强奸 give a working definition: 理论上/操作型/工作上的定义 direction: n. 方向；指导；趋势；用法说明 floor: n. 地板，地面；基底；议员席；楼层 v. 铺地板；打倒；难倒 modern-day: adj. 现代的 譯文 今天这个故事听起来似乎不大可能：一位美国女人与卡车司机们联手合作，共同打击贩卖人口。肯蒂斯•帕里斯和她的丈夫——一名英国国教牧师——以及两个孩子生活在科罗拉多州丹佛市。她过去经常在家制作面包和奶酪，陪孩子们在一起。但是现如今，她走遍全国各地，教授司机去识别并上报贩卖人口的犯罪行为。她有一个名为“卡车司机打击贩卖人口”的组织，简称TAT。她告诉我是什么启发她建立这个组织的。\n大概2007年的时候，我读了大卫•巴斯顿的《非卖品》。在此之前也许我能够给出贩卖人口的定义，但仅此而已。我在母亲的推荐下阅读了这本书。在读到第三章时，书中的内容令我既惊恐又愤怒。我不得不将书翻到最后去寻找帮助他们的办法。\n那本书里讲了什么让你如此震惊呢？\n里面涉及被拐卖后强迫卖淫的问题，真的让我吃惊极了。我无法相信那些数字。我无法相信在现代社会还有如此规模的奴隶制度依然存在，而且遍及世界各地，甚至就在美国。\n","date":1481155200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1481155200,"objectID":"6830df9d2692b719febd9fb6eadac471","permalink":"https://wangcc.me/post/2016-12-8/","publishdate":"2016-12-08T00:00:00Z","relpermalink":"/post/2016-12-8/","section":"post","summary":"an American woman came to be working with truckers to combat human trafficking.","tags":["dictation","English Learning","Listening","BBC"],"title":"不可能的故事","type":"post"},{"authors":null,"categories":["dictation"],"content":"概述：对朋友撒了一个谎，需要用更多的谎言来圆它。所以，朋友之间还是坦诚一点比较好。\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-12-7 10:33\t用时：28:09 正确率：89%\t错词：33个\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nIn 1976, my friend Glen decided to give up advertising to spend a year teaching English in the Sudan. On his returnreturning, he was supposed to be staying with his brother, but they had a rower row at Heathrow airport a few minutes after their his plane had landed. , and so Glen phoned and asked if he and his new New Zealander girlfriend, Virginia, could stop for a short while in our spare room in the council tower block where we lived. When they arrived from the airport, dragging several huge overstocked overstuffed suitcases behind them, Glen and Virginia were both painfully thin and the their skin was a bright orange hue , due to a tropical parasite they had picked up. Both of them spend the spent their first few days back in London in our lavatory. , terrible noise is noises issuing from the interior. Me and my wifethen were , Linda, was so worried about catching the same parasite parasites that we used the a toilet in the a pub, a bush right bus ride away . over the river in ones wellWandsworth. As the week weeks of their stay in Londonlengthened, the couple hardly ever went out, but remained in the flat all day, arguing with each other and smoking continuously. After a while, me and Linda had had enough and asked Glen and Virgina Virginia to move out. , but rather than being honestlyhonest, we told them untruthfully that our old friend Christine Walker was coming to stay with us indefinitely , and she will would be needing the room. We looked them initiated of agreeing Reluctantly the emaciated couple agreed to move out, but asked if they could leave the their suitcases behind. , since the only place , they could find to rent was a tiny bedsit with no storage space. We said yes, but this presented us with a problem. : one or other of them would be returning regularly to pick stuff up from our a room in which Chris Walker was supposedly staying. Either for I therefore felt , I had to fake Chris day to day presentsChris's day-to-day presence. Words worth to be remembered: row: n. 行，排；街道；划船；吵闹 v. 划船；争吵; My parents often have rows. 我爸爸妈妈经常吵架。 Heathrow:Airport(英国伦敦的)希思罗机场 overstuff: v. 过度填塞 overstock: n. 供给过多,库存过剩 vt. 供给过多,进货过多,过剩 hue: n. 色彩，色调；喊叫声; Add orange paint to get a warmer hue. 加些橙色颜料使之略呈暖色. Wandsworth: 旺兹沃思[英国英格兰东南部城市](在大伦敦郡的西南部) lengthen: vt. 使延长；加长 vi. 延长；变长 untruthfully:adv. 不真实地;不诚实地 emaciate: v. （使）消瘦，衰弱 day-to-day: 日常的 bedsit [ˈbed.sɪt]: n. 起居卧室两用房 He was living alone in a dingy bedsit in London. 他独自一人住在伦敦一间昏暗的客卧两用出租屋里。 譯文 1976年，我朋友格林决定放弃他的广告事业，转而去苏丹当一年英语教师。他回来后本该跟他兄弟一起住。但当飞机降落在希思罗机场后几分钟，他们吵了一架。于是格林打电话给我，询问能否和他来自新西兰的新女友弗吉尼亚在我家的空房间里小住。他们到我家时，身后拖着几个塞得满满的大箱子。俩人非常消瘦，皮肤由于感染热带寄生虫而显浅橘色。\n重回伦敦的头几天，他俩几乎是在我家卫生间度过的，里面时常传来可怕的声音。我和我妻子琳达担心被感染上同样的寄生虫，所以都跑到离家不远的酒吧用卫生间。酒吧在河对岸的旺兹沃思，公交车几站就到了。\n一周又一周过去了，这对情侣极少出门，他们整日待在公寓不断的争吵、抽烟。过了段时间，琳达和我再也无法忍受，便要求格林二人搬走。但是，我们并没有选择坦诚相告，而是骗他们说有一个老朋友克里斯汀将要来家里住段时间，她需要这间屋子，且不清楚到底要住多久。\n消瘦的格林二人不大情愿地同意了。不过他们想要把行李箱留在这里。因为他们唯一能租到的房子非常小，是个卧室兼起居室的两用租间，没有地方放置行李箱。我们同意了，但随之而来也有一个问题——接下来的日子，格林或是弗吉尼亚会定期回这间房里取东西。这样一来，我便不得不制造这间房被克莉斯居住的假象。\n","date":1481068800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1481068800,"objectID":"36a086c25fcf73ec8f3dfc986c083dc1","permalink":"https://wangcc.me/post/2016-12-7/","publishdate":"2016-12-07T00:00:00Z","relpermalink":"/post/2016-12-7/","section":"post","summary":"概述：对朋友撒了一个谎，需要用更多的谎言来圆它。所以，朋友之间还是坦诚一点比较好。","tags":["dictation","English Learning","Listening","BBC"],"title":"撒謊","type":"post"},{"authors":null,"categories":["dictation"],"content":"\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-12-6 12:9\t用时：20:40 正确率：94%\t错词：7个\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nSeveral senior moments have featured in the news of over recent months. At the beginning of February, Ed Balls forgot the name of Bill Thomas, his business ally. A few weeks later, Green party leader Natalie Bennett had a terrible time remembering her party's policies when she was interviewed by Nick Ferrari on LBC. And I bet Ed Miliband hopes that we've forgotten that he forgot to mention the economy in his Labour Party conference speech last autumn. I have every sympathy for these politicians, because I am I'm almost exactly the same age as they are, and I can't remember a damn darn thing. I've always been absent-minded. In my 30s, I I'd regularly park my car in town for the evening and then forget where I I'd left it. It wasn't unusual for me to forget what I've I'd been doing the day before. And I've often had trouble putting a name to a face. But I've also been good at multi-tasking . and I my general knowledge was once good enough for me to captain my college's University's University Challenge team. There was even a time when I could hold my own in a political debate. Not now. Any childhood dream of becoming an MP is now firmly behind me . because I I'd have no hope of remembering faces, statistics , or possibly even policies. Not only do I have galloping nominal aphasia , - the inability to remember names . I am - I'm also alarmed by the erosion of my cognitive power's powers, full stoppedstop. Words worth to be remembered: darn: adj. 可恶的；完全的 adv. 极其 interj. 可恶 n. 补丁 v. 织补 Green Party: n.绿党（关注环保的政党） 例句： There is a Green party but it only scored around about 10 percent in the vote. 虽然有一个绿党，但在选举中该党只得到了大约10%的选票。 aphasia [əˈfeʒə]: n.失语症（Aphasia is a mental condition in which people are often unable to remember simple words or communicate.） 例句： Unfortunately, he suffered from sudden onset of aphasia one week later.不幸的是, 他术后一星期突然出现失语症。 gallop: n. 疾驰，飞奔；飞快 v. （使）疾驰，飞奔；飞速发展；快速做（或说），急速进行；飞速传递 inability: n. 无能，无力，不能 譯文 最近几个月，一些人像得老年痴呆一样忘事的行为见诸报端。二月初，埃德·鲍尔斯忘记他的商业伙伴比尔·托马斯的名字。几个星期后，绿党领袖娜塔莉·贝内特在接受伦敦广播公司尼克·法拉利采访时差点没回忆起绿党政策。嗯，我敢打赌，埃德·米利班德也希望我们早点忘记去年秋天他在工党会议上忘了提及国家经济的囧事。其实我完全能够理解这些政治家，因为我和他们一样，都一把年纪了，总是记不得东西，该死的。\n心不在焉的习惯我早就有了。我30多岁时经常是晚上把车停好白天就忘了停哪。我经常忘记前一天做过的事情，而且还会记不起别人的名字。\n不过我一直擅长多线程工作，我丰富的知识还曾经让我当上大学挑战队的队长。曾经的我也可以在政治辩论里意气风发，崭露头角。\n现在不行了。因为我糟糕的记忆力，儿时成为国会议员的梦想已离我远去，我记不住面孔，统计数字甚至政策，我记不清名字，好像患了失语症一般，我的认知能力也下降了很多，太可怕了。\n","date":1480982400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480982400,"objectID":"777f451e7ea0e44f012d920cbe758614","permalink":"https://wangcc.me/post/2016-12-6/","publishdate":"2016-12-06T00:00:00Z","relpermalink":"/post/2016-12-6/","section":"post","summary":"Dementia?","tags":["dictation","English Learning","Listening","BBC"],"title":"老年性瞬間失憶","type":"post"},{"authors":null,"categories":["dictation"],"content":"\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-12-5 11:56\t用时：18:03 正确率：87%\t错词：27个\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nThe Trans-Pacific Partnership is dead, so says president elected President-elect Donald Trump. He is going to He's gonna kill it off, he says, on his very first day in the White House in January. So is this the first step towards what Trump watches watchers fear , it's is going to be a more protectionist America under his presidency? Wellmaybe, maybe. But yesterday on the BBC's Hard Talk programprogramme, Mohammed Mohamed El-Erian, the former CEO of PIMCO, now Chair president of President Obama's Global Development Council, says that this move alone will not make much of a difference. Very little , why? , because the TPP is was never finalised. Plus , he is he's not talking about what really scared the do we escape markets beforehand. , which was I am going to gonna impose crush crushing tariffs on China and terriftsMexico. We haven't heard that. I am going to gonna dismantle NAFTA. NAFTA is in existence for decades now. We haven't heard that. He has made a point of not talking about them. Well, whatever the reality and despite the imminent death of TPP. , trans-pacific trade generally still have has some way to run. China, for example, has begun a multi-billion-dollar investment program programme into the new Silk Road as it's known, a path of rail and road connection which is connections that's going to run through the Central Asia to help connect its factories with European and the Middle East Eastern markets. And Beijing, it seemsit , is still look into looking to US businesses for help to complete this project. , as Jane O' brien O'brien now reports. Words worth to be remembered: President-elect: n. 总统当选人；已当选而尚未就职的总统 watcher: n. 看守人,守望者,照顾者 PIMCO: abbr. 太平洋投资管理公司（Pacific Investment Management Company） crushing: adj. 决定性的；压倒的；支离破碎的 v. crush的现在分词, 毁灭性的强调; 例:...since their crushing defeat in the local elections.…自从他们在地方选举中遭到毁灭性的挫败以来。 tariff:n. 关税表，关税；价目表；菜单；价格，费 v. 对...征收关税；定...的税率；按税率定...的价格 NAFTA: 【缩写】=North American Free Trade Agreement 北美自由贸易协议 imminent: adj. 即将发生的，逼近的 dismantle: v.逐步废除; 例：Public services of all kinds are being dismantled. 各种公共服务正被逐步废除。 譯文 根据总统当选人唐纳德·特朗普的说法，跨太平洋伙伴关系协定将被终结。他说，一旦一月份入主白宫，首要就是了结TPP。这是否预示着特朗普观察者一直以来担心的事情已经有了苗头，即美国在其统治下将更注重自我保护主义。可能是的。但在昨天BBC的Hard Talk节目中，PIMCO前CEO，现任美国总统奥巴马的全球发展委员会主席Mohamed El-Erian认为仅这个动作对整体格局只是杯水车薪。 原因很简单，TPP还没有完全敲定。并且他也没说事先逃离市场，增加对中国和墨西哥打击性关税。我们还没听说这点。我会逐步废除北美自由贸易协定，而这一协定已经运转数十年。我们也没听说。他显然在避免提及这些。 无论事实如何，即便TPP即将终止，跨太平洋贸易整体上仍能找到出路。以中国为例，该国投资数十亿美元启动了新的丝绸之路，通过中亚的公路和铁路将其工厂与欧洲和中东市场联结。北京方面似乎仍看好由美国帮忙完成该项目。珍·奥布莱恩带来相关报道。\n","date":1480896000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480896000,"objectID":"5defcde35d732ac26a91259bb89d9100","permalink":"https://wangcc.me/post/2016-12-5/","publishdate":"2016-12-05T00:00:00Z","relpermalink":"/post/2016-12-5/","section":"post","summary":"Trans-Pacific Partnership is dead","tags":["dictation","English Learning","Listening","BBC"],"title":"川普閉關鎖國","type":"post"},{"authors":null,"categories":["dictation"],"content":"HOW A TEAM OF \u0026lsquo;UNCLEAN CLEANERS\u0026rsquo; FOUGHT CASTE WITH CRICKET\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-12-4 17:29 用时：23:16 正确率：92% 错词：21个\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nToday's story is from India. There, even though discriminating against people on grounds of their caste has been illegal for years. , people can still find themselves severely disadvantaged. Vimal Kumar is from one of the lowest class castes - the Chuhras. They are sometimes called \" the untouchables\" . And for generations the family have made their living by cleaning out toilet pits. But today Vimal is studying for a PhD. He's managed to rise above the caste system and wants to help the rest of his community to do the same. On the line from BombayMumbai, Vimal has Vimal's been telling me his story, starting with his time at school. It's still very fresh in my mind, at . In the same school my mum mother was sweeper. She used to clean the dry toilets. , the toilets without water. , and my identity in the school was \" the son of the sweeper\" . They just call The teachers called me not by my name. They call call all called me, they called like: \" The son of sweeper, come here. \" And I felt very shamed for shameful in the school. And were you angry about it? Did you get angry? Yeah. Because, because during our class they always the whole school called me: \" Your mother is cleaning our toilet, . You are bad people. You are dirty people. \" They tried to beat us, . They don't want to touch us . and they don't allow us into their families, into their houses. And I did not didn't like to go school due to these things. I always tried try to go, stayed stay at home. So everyday every day my family pushed hard to me to entering enter in the school. And my mother said: \" Don't worry about that, just avoid these things , and just go to school and concentrate on your study. \" Words worth to be remembered: Mumbai: n. 孟买(位于印度西部,原名为Bombay) on grounds of: 以…为理由，以…为借口，根据。 例句： We base this call on grounds of social justice and equity. 我们基于社会正义和公平发出这一呼吁。 castes: n. 种姓（制度），社会等级（制度） pit: n. 深洞；煤矿；麻子；修理加油站；交易场所 v. 使有麻子；使有凹陷 rise above: 克服；升到…之上；超越…；沾沾自喜。 例句： It tells the story of an aspiring young man's attempt to rise above the squalor of the street. 它讲述了一位有志青年试图摆脱贫苦肮脏的街头生活。 譯文 今天的故事发生在印度。多年来，即使因为种姓而歧视人是违法的，那里的人仍然处于极为不利的地位。维姆·库马尔就是来自最底层种姓中的一员——楚哈拉斯家族。他们时常被称为“贱民”，家族里几代人都是靠打扫厕为生。但如今维姆正在攻读博士学位。他设法摆脱了种姓制度，并帮助处于这一阶层中其他人改变命运。远在孟买的维姆在线向我讲述了他的故事，一切始于他的学生时代。\n那些记忆在我脑海里依旧鲜明。我的母亲也在同一所学校做清洁工。她过去经常打扫干厕，也就是没有水的厕所，而我在学校的身份就是清洁工的儿子。老师们不叫我的名字。他们都这样这么跟我说话，“清洁工的儿子，到这来”。在学校我觉得非常丢脸。\n那为这个你会生气吗？你生过气么？\n是的，全班乃至整个学校都对我说“你妈是给我们扫厕所的。你们是烂人，脏人”。他们还打我们。他们不愿意碰我们，不让我们进他们家门。因此我不喜欢去上学，通常待在家里。每天家人都极力劝我去上学。我的母亲说：“别在意那些，抛开它们，你就去上学，把精力集中在学习上”。\n","date":1480809600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480809600,"objectID":"99fccbe2236a88dc253bcd1b7f9e0a63","permalink":"https://wangcc.me/post/2016-12-4/","publishdate":"2016-12-04T00:00:00Z","relpermalink":"/post/2016-12-4/","section":"post","summary":"HOW A TEAM OF 'UNCLEAN CLEANERS' FOUGHT CASTE WITH CRICKET","tags":["dictation","English Learning","Listening","BBC"],"title":"命運不是天注定","type":"post"},{"authors":null,"categories":["dictation"],"content":"\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-12-3 11:49\t用时：23:21 正确率：91%\t错词：21个\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nLet's begin, though, with a break in the voice, just holding back the sobs . because this is a week when crying at the cinema is positively encouraged. Why? A new film, The Light Between Oceans is - it's an adaptation of M. L. Stedman's novel about a couple played by Michael Fassbender and Alicia Vikander. Some baby's at babies that are lost and the ones had one that's found, but not necessarily in a happy the happiest sequence. , all set against the storm and drown over wrong of a remote Australian island surrounded by unpredictable seas. A recent Oxford University's University study suggested that watching emotionally wrenching drama on screen activates the production of endorphins-scientific endorphins - scientific endorsement that a good cry makes you feel better. For many, The Light Between Oceans will be a terrific night out. Some audience members have cried solely solidly for the film's final hour. The adaptation and the direction is by American director Derek Cianfrance, whose two previous features, Blue Valentine and The Place Beyond the Pines certainly moved muted me to tears. And Derek admitted that he decided to adapt The Light Between Oceans partly because it made him cry. I mean, I love the book. I remember I was on the sea city train in New York City , crying. You know, my face was milking melting from tears , reading the end of the this book . and you know, I was embarrassed. , you know, initially, and then I thought to myselfthat , if anyone else was reading what I was reading, they will would be having the same emotional response responds and I just used that as my North Star. Words worth to be remembered: adaptation: n. 适应；改编，改编本 emotionally wrenching: 让人感到情感上痛苦的 endorphin: n. ［生化］内啡肽:指将麻醉传感器联结在一起的任一肽激素群，主要存在大脑中，可缓解痛感并影响情绪 a chemical produced by your body that reduces pain and can make you feel happier my face was melting from tears: 我的脸上淌满了泪水 North Star: 北极星 sequence [\u0026lsquo;siːkw(ə)ns]: n. 有关联的一组事物, 一连串 例句： At each location on the sequence, we can measure all these different attributes of chromatin. 在序列的每个位置上，我们都可以测量染色质的这些不同属性。 譯文 我们开始吧，先让我们抑制一下呜咽，虽然这周值得在影院一哭。为什么呢？因为一部新电影《大洋之间的灯光》。这部电影改编自M·L·斯坦德曼的小说，讲述的是一对夫妻的故事，由迈克尔·法斯宾德和艾丽西亚·维坎德饰演。妻子屡遭流产，而他们这时候却发现了一个婴儿，但幸福并未就此开始。故事就发生在那片被未知海洋包围的遥远澳大利亚岛屿。最近的一项牛津大学研究表明，观看让人情感上痛苦的荧幕电影可以激活内啡肽的产生——科学认证明，痛快哭一场能让你感觉更好。对很多人来说，观看《大洋之间的灯光》将是一个非常棒的夜晚。一些观众在电影最后都哭的无法自已。剧本的改编和拍摄指导都出自国导演德里克·斯安弗朗斯，他先前的两部作品《蓝色情人节》和《松林外》都让我感动落泪。德里克也承认他之所以决定改编《大洋之间的灯光》一部分原因也是因为它让他热泪盈眶。 “我的意思是我爱这本小说。我还记得我是在纽约的城市列车上看哭的。你知道，看到书的结尾时我的脸上淌满了泪水，你知道的，最初我很尴尬，然后我心想，任何人如果正在读这本小说，他们也将有同样的反应。我只是把它当作我的北极星。”\n","date":1480723200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480723200,"objectID":"377ecda4f8060d8db93a8e9b1925e56a","permalink":"https://wangcc.me/post/2016-12-3/","publishdate":"2016-12-03T00:00:00Z","relpermalink":"/post/2016-12-3/","section":"post","summary":"the Light Between Oceans","tags":["dictation","English Learning","Listening","BBC"],"title":"《大洋之光》","type":"post"},{"authors":null,"categories":["dictation"],"content":"\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-12-2 12:37\t用时：26:21 正确率：91%\t错词：24个\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nI've always felt that there has to be some method of controlling the boar. I've got the ability here to inject boar from where when we were worm them and things like that , from about 6 six feet away. But then when I had, heard that there was a firm, who could actually dart the boar, I was interested. And they came along. And as a result of that, they darted on all eight of our boar. That was on the 1st first of April this year. It's been 100% successful. We have We've had new no young pig boars piglets born. I mean, just looking at the boar, they do move very fast. In all reality, it would it be practical to dart them. ? Oh yes, that's it's that easy to dart them. The guns that they use are very sophisticated with telescopic sights. When they fire, they haven't hardly even noticed notice it. Well, to find out more about the dart. , I've come to meet Phil York , from Animal Welfare Solutions. He is specialized specialises in sedating and vaccinating semi semi-feral animals. Now , he is he's here with me, and he's got a dart in his hand. I have to say , it does look like he's it's got a pretty ferocious needle. And Phil, could you talk me through the dart? This is the dart that we used use to deliver the vaccine. It's a specially imported dart from America. It's very very effective. The darting system that we use is accurate out to 60 yards. And you can see here it has a flight which stabilizes stabilises it. The drug is injected by the vet into the vest vessel of the dart, that . The drug actually comes out to of the end of the dart as it does in the a syringe. There is There's a wax bob barb here. , so when the dart goes into the animal, the body heat melts the wax. Words worth to be remembered: boar: n. 公猪，野猪；公猪肉 worm: n. 虫，蠕虫；螺纹；蜗杆；小人物 v. （使）蠕动；（使）缓慢前进；给…除虫 dart: n. 飞镖，标枪；猛冲，飞奔 v. 飞奔，猛冲；投射 piglet: n. 小猪 feral: adj. 野生的,凶猛的 vessel: n. 船；飞船，飞机；容器，器皿；血管 syringe: n. 注射器,洗涤器 vt. 注射,洗涤 barb: adj. 有倒钩刺的 n. 倒钩，倒刺 譯文 我一直觉得一定有什么方法可以控制野猪的数量增长。我们这里在除虫之类的时候可以从六尺以外进行注射。但是当我听说有一家公司可以远程射中野猪的时候，我很感兴趣。然后他们就过来了。结果，他们飞镖注射了我们全部的八只野猪。这是今年四月一号发生的事情，成功率100%，之后再没有小猪出生了。\n可是看看这些野猪，它们移动速度那么快，射中它们真的实际吗？\n是呢，就这么容易射中。他们用的枪很复杂，带有望远镜的。开枪的时候野猪几乎都不会注意到。\n那么，为了了解更多关于远距离注射的情况，我去见了动物福利研究院的菲尔约克。他专门给半野生动物注射镇静剂和疫苗。现在，他就在我旁边，手里拿着注射飞镖。我不得不说这个注射飞镖的针头看上去非常可怕。菲尔，你能跟我讲讲这个飞镖吗？\n这就是我们用来注射疫苗的飞镖。它是专门从美国进口的。它真的非常好用。我们使用的远程注射系统在60码以外也很精确。你可以看到这里有一个用来保持平衡的镖尾。药物是由兽医注入这个飞镖上的容器里的。药物会从飞镖的尾部流出，就像注射器那样。这儿有个蜡制的倒钩，当飞镖进入动物身体后，体温会使蜡融化。\n","date":1480636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480636800,"objectID":"297fa71aab48abc9db4149463e1d3bff","permalink":"https://wangcc.me/post/2016-12-2/","publishdate":"2016-12-02T00:00:00Z","relpermalink":"/post/2016-12-2/","section":"post","summary":"Boar controlling","tags":["dictation","English Learning","Listening","BBC"],"title":"控制野豬新方法","type":"post"},{"authors":null,"categories":["dictation"],"content":"\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-12-1 17:9\t用时：21:30 正确率：90%\t错词：23个\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\n\" Time present and time past / Are both perhaps present in time future . / And time future contained in time past. / If all time is eternally present , / All time is unredeemable. \" The opening lines of TS Eliot's Burnt Norton, the first of his Four Quartets, were written in the 1930s. , but while they may be familiar to many of you listening, I wonder if any of us have really stopped to consider their fault full import. True, Eliot sounds a speculative of know without note with that \" perhaps\" in the second line. , yet his meaning still seems clear enough . - if our perception of time is as moving ever forward like a river , is purely subjective. In , and the whole span of time , - together with all actual events - has already transpired. And , then nothing we will ever do or say can alter the future, let alone the past. The internation inclination is, I think, to relate Eliot's inside insight directly to notions of free willthen , and hence tomorrow to moral responsibility . - the attribution of which is the thing that most preoccupies us in our social existence. However, I don't really want to discuss that , but instead , focus on time itself, and our conception of it. The fourth dimension appears so much slipperier to us than the first three . - so slippery, indeed, that when we attempt to fix it in our minds , it slithers away from our grasp in a thankly faintly nauseating way. Consider this , - if we reject Eliot inside Eliot's insight and cleave to the view of that the past, together with everything in it, no longer exists, while the future is has yet to come into being. , then all of reality must be bounded by what we think of as the present. Words worth to be remembered: I wonder if any of us have really stopped to consider their full import: 但你们也许从来没有思考过它们的意义。 speculative: adj. 推测出的；投机的 subjective: adj. 主观的；个人的；自觉的 unredeemable [\u0026lsquo;ʌnrɪ\u0026rsquo;di:məbl]adj.不能收回的，不能赎回的 例句： They will be unredeemable for two years, and will be redeemed in eight years. 必须保存2年， 8年内偿还。 transpire: v. 使蒸发；蒸发；发散 : 例句： We don't know what will transpire when we have a new boss. 当我们有位新老板时,不知会发生什么。 let alone: 更不必说；听任；不打扰 inclination: n. 倾向；意愿；倾斜度 attribution: n. 归属；属性；归属物；归因判断 preoccupy: v. 使出神；抢先占有 slipperier: adj. 滑的；狡猾的；不稳定的 (slippery的变形) slither: vi. 蜿蜒地滑行 vt. 使滑动 n. 滑动；滑行 nauseating: adj. 令人恶心的；厌恶的 v. 使恶心（nauseate的ing形式） cleave: v. 劈开，使分开；坚守；披荆斩棘地前进；粘住 cleave to: 粘着；坚持 例句： The tribe cleave to their old belief even after the european arrive。即使在欧洲人到来之后,这些部落仍固守著它们古老的信仰. 托马斯‧史特恩‧艾略特(Thomas Stearns Eliot 1888-1965) 是位出版者、剧作家、文学、社会评论者，及二十世纪最受争议的重要英语诗人，生于美国，在1914年搬至英国。他以《J.阿弗雷德•普鲁弗洛克的情歌》(The Love Song of J. Alfred Prufrock)奠定诗名，是现代主义运动的重要作品，写于1910年，在1915年发表；此诗具有很浓的讽刺意味，刻画当时社会背景下，人们对爱情及生活的复杂心理。他重要的诗作包括《Poems》(诗集1919)、《The Waste Land》(荒原1922)、《The Hollowmen》(中空的人1925)、《Ash Wednesday》(圣灰星期三1930)及《Four Quartets》(四个四重奏1945)。他写过七部剧本，其中以《Murder in the Cathedral》(教堂谋杀1935)为着。他在1948年以《Order of Merit》得诺贝尔文学奖。《焦灼的诺顿》（Burnt Norton）是 《四个四重奏》(Four Quartets)中的第一首诗；他书写时间与救赎(Time and Redemption)的特质，他的意旨在强调个人对目前状况及了解宇宙秩序的需要。 譯文 “现在和过去的时光， 也许都存在于未来之中，且未来的时光包含于过往。假如所有时间都永恒存在，所有时间都无法履行。”这是写于20世纪30年代，艾略特《四个四重奏》中《焦灼的诺顿》的开场白。虽然你们可能很熟悉这几句，但你们也许从来没有思考过它们的意义。诚然，因为一个词“也许”，艾略特的语气听起来不是十分确定，但他的意思已经很清晰：如果时间向一条河流一样滚滚向前只是我们的主观臆测（译者注：就是时间没有先后），所有的时间和事件都已经发生，那么我们就根本无力改变未来，更别说过去了（译者注：看过interstellar的人可以联想电影中四维空间的相关镜头）。人会倾向于将艾略特的这个观点和自由意志以及道德责任联系起来，毕竟这是人作为社会动物最为关心的问题。不过，在这里我只想着眼于时间本身，探讨一下人对时间的感知。\n第四维比前三维费解得多，人想抓住它时，它总是能从我们的指尖溜走。想一想，如果我们不同意艾略特的想法，认为过去的已经过去且不再存在，而未来还没有发生，那么所有的现实都会与此时此刻紧密相关。\n","date":1480550400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480550400,"objectID":"cc907bbf207f36adf49bc4e958e47ec4","permalink":"https://wangcc.me/post/2016-12-1/","publishdate":"2016-12-01T00:00:00Z","relpermalink":"/post/2016-12-1/","section":"post","summary":"Time present and time past","tags":["dictation","English Learning","Listening","BBC"],"title":"時間的錯覺","type":"post"},{"authors":null,"categories":["dictation"],"content":"Trevor McDonald on Redemption\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-11-30 11:49\t用时：18:14 正确率：91%\t错词：27个\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nThere are many views about the concept of redemption. It's generally defined as the action of saving or being saved from sin, error , or evil. We take this to mean that someone has committed an act which is sinfull sinful or evil. My guest today did nothing of the thoughtsort. In fact, she was subjected to an active act of terrible violence and brutality, and it happened when she was only thirteen13. Madeleine Black was raped, yet today she can talk of having been redeemed through the active act of forgiveness. Madeleine, how does one do that? How does one seek or gained gain that redemption? It wasn't something that I ever really set out to do. It There was a combination of about three things. I thought that I had worked it , really were when well and I was healed. But, but there was always something lurking underneath . and when my oldest daughter became nearly thirteen 13, and it was while whilst I was having therapy for over three years . that my therapist suggested to me that maybe they weren't born rapists. And, and you know, I just. .. , I just, there was no way I want wanted to forgive them, I just want wanted somebody to kidnap them, type tie them up, rape and torture them for hours on end like they had done to me. , but he planted a seed in my mind . and that seed started to grow . and I, I just really wanted to understand what did it take for them to take that path. , because they weren't much older than me, . They were maybe 17,18. I wanted to know, how could they be so violent toward towards another human being, what had they seen or heard or experienced themselves that could have made make them behave in that way. ? Words worth to be remembered: redemption: n. （尤指基督教的）拯救，赎罪，救赎例句： They visited the Shrine of Our Lady to pray for redemption. 他们参观了圣母玛利亚的神龛，祈祷以期救赎。 be subjected to sth: 有；遭受，承受 例句： Cars are subject to a high domestic tax. 买汽车要交很高的国内税。 brutality /bruːˈtæl.ə.ti/: n. 残酷性，残忍行径 例句： the brutalities of war 战争的残酷 sort: n. 种类；方式；品质 vt. 将…分类；将…排序；挑选出某物 vi. 分类；协调；交往 redeem: vt. 赎回；挽回；兑换；履行；补偿；恢复 whilst: conj. 同时；时时，有时；当…的时候 set out: （怀着特定目的）开始，着手 例句： She set out with the aim of becoming the youngest ever winner of the championship. 她努力的目标就是成为史上最年轻的冠军。 lurk /lɜːk/: v. 潜伏，潜藏 例句： It seems that old prejudices are still lurking beneath the surface. 表象背后似乎依然潜藏着旧有的偏见。 譯文 “救赎（redemption）”一词的概念有多种解读。它通常指挽救过失、罪恶，或从过失、罪恶中得到解放的过程。我们用这一词表明某人曾犯下罪行。而今天我们的嘉宾未曾犯过罪行。事实上，她在13岁时遭到严重的暴力虐待。玛德琳·布莱克曾被强奸。而她如今认为，自己通过“原谅”得到了救赎。\n玛德琳，一个人要怎么寻求或者得到这样的救赎？\n“寻求救赎”并非我本来的打算。这是三样东西的结合。我曾以为我已经从过去的伤痛中走了出来，那些创伤已经被治愈了。但总有东西潜藏着，让我无法真正释怀。在我长女快到13岁时，我已经接受了3年的心理治疗。我的治疗师暗示我那些人可能并非生来就是强奸犯。但我一点也不想去原谅他们。我只希望他们也被绑架、被绑起来、被强奸、被折磨数小时，就像他们曾对我做的那样。然而，心理医师的话却像颗种子被植入我的脑海，并开始生长。我开始想要了解他们为什么会走上“强奸”这条路。他们并没有比我大多少，大概17、18岁。我想知道，他们怎么会对别人那么残酷、暴力，他们曾看到、听到或经历过什么事情，导致他们会做出这样的行为。\n","date":1480464000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480464000,"objectID":"89b51105ba2565fdcdaa129cb7e6a67a","permalink":"https://wangcc.me/post/2016-11-30/","publishdate":"2016-11-30T00:00:00Z","relpermalink":"/post/2016-11-30/","section":"post","summary":"Trevor McDonald on Redemption","tags":["dictation","English Learning","Listening","BBC"],"title":"救贖","type":"post"},{"authors":null,"categories":["dictation"],"content":"Crisis in Syria\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-11-29 11:36\t用时：20:43 正确率：92%\t错词：15个\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nIn the past six months alone, the number of people living under siege in Syria has doubled to almost a million. In the a briefing to the Security Council, the UN head of aid , Stephen O'Brien also warned that conditions in eastern Aleppo have had gone from terrible to terrifying. He said that most of the violence in Syria was being carried out by government forces. \" There is nothing subtle or complicated about the practice of the besiegements. besiegement, civilians have been are being isolated, starved, bombed bound and denied , been attentioned medical attention and humanitarian assistance. , in order to force them to submit or flee. It is a deliberate attack tactic of cruelty to compound the people people's suffering for political, military, and in some cases, it's an economic game gain, to destroy and defeat a civilian population who cannot fight back. \"\nHe repeated the a call for strong action from the Security Council to back its resolutions, calling for an end to attacks on civilians, to improve humanitarian aid access , and to leave lift sieges. The Council has been divided over how to end for the nearly six-year civil war. Syria's ally, Russia, backed by China is protecting President Assad from Security Council action by veto in vetoing several resolutions. Words worth to be remembered: besiegement: n. 围攻;围绕;推进 subtle: adj. 微妙的；精细的；敏感的；狡猾的；稀薄的 submit: vi. 提交；服从 vt. 使服从；主张；呈递 deliberate: adj. 故意的；深思熟虑的；从容的 v. 仔细考虑；商讨 tactic: n. 策略，战略 adj. 按顺序的，依次排列的 compound: n. [化学] 化合物；混合物；复合词 adj. 复合的；混合的 v. 合成；混合；恶化，加重；和解，妥协 譯文 仅仅在过去半年里，被困在叙利亚的难民人数达到将近一百万，已经相比之前翻了一番。在一份呈交给安理会的简报中，联合国人道主义援助事务负责人史蒂芬·奥布莱恩发出警告：阿勒颇东部的难民处境已经从“糟糕”变成了“惨不忍睹”。他声称，在叙利亚发生的大部分暴力事件都是由叙利亚政府军挑起的。\n“难民们被围困是个纯粹的事实，没有什么复杂和费解的地方。他们与世隔绝、忍饥挨饿、出行受限、得不到任何可以帮助他们撑过眼前难关或者是医疗措施、或者可以让他们逃离这里的人道主义援助。甚至有人出于政治、军事、在有些情况下、甚至是经济上的利益而去欺压践踏这群毫无还手之力的难民、加重他们的痛苦，这实在是人为的暴行。“\n他再三呼吁安理会采取强硬行动来支持他的决议，终止这场内战，疏通人道主义援助渠道、强制禁止围困。安理会的成员在如何结束这场将近长达六年的内战问题上意见产生了分歧。叙利亚政府军的盟友、俄罗斯——其背后还有中国的支持，为了保护叙利亚总统阿萨德免受联合国部分措施的制裁，而对几项决议投出了反对票。\n","date":1480377600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480377600,"objectID":"6f4d8716678bbcab82cb3944f61aca95","permalink":"https://wangcc.me/post/2016-11-29/","publishdate":"2016-11-29T00:00:00Z","relpermalink":"/post/2016-11-29/","section":"post","summary":"Crisis in Syria","tags":["dictation","English Learning","Listening","BBC"],"title":"敘利亞難民危機","type":"post"},{"authors":null,"categories":["dictation"],"content":" Your browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-11-28 12:3\t用时：24:42 正确率：89%\t错词：26个\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nWashington has again voiced concern about Beijing's activities in the South China Sea. China is building airbases on the harbors and harbours on reclaimed land there . and has occupied areas which other nations in the region regard as theirs. This week, America's top navy commanders naval commander in the Pacific warns warned that it could all lead to a dangerous arms race. The two main disputed areas are around the Spratly Islands of off the Malaysian and Philippine coasts . and the Paracel Islands which China seized from Vietnam in 1974. Humphrey Hawksley's been to meet some Vietnamese fishermen who now find themselves on the front linefrontline. \" HeyHum, hon, you gotta come to look at thisat the tour of . \" said a tall American tourists tourist with a faded green baseball cap . as he summoned his wife to an exhibit to in the military history museums museum in Hanoi. In the a scrawled handwritten note from April, 1975. , the Vietnamese high command had ordered his its generals to move at lightning speed in the final offensive against the American back to see him American-backed regime in Saigon. \" Can you imagine our kids one day looking at something like this from isolated ISIL in the Middle Eastyast? \" He asked. The shabby dusty museum has models of the military plans that defeated the French in 1954 and the Americans in 1975 when Communist Vietnam represented everything western democracy condemned. There are huge paintings of historic battles against the Chinese, in which many thousands were killed. , but only from the 10th to 15th centuries. , strangely there is there's nothing about Vietnam's more recent border-war with China in 1979 , which it it'd also won. Words worth to be remembered: Harbor vs. harbour: There is no difference in meaning between harbor and harbour. Harbor is the preferred spelling in American English, and harbour is preferred in all other main varieties of English. reclaimed: v. 回收利用；改造（reclaim的过去分词）；开垦土地 adj. 回收的，再生的；翻造的 Paracel Islands: 帕拉塞尔群岛(某些外国人沿用的殖民主义者对我国西沙群岛的称呼) scrawled: n. 潦草的笔迹 v. 潦草地写；乱涂乱画 Saigon: n. 西贡（越南一座城市，现称胡志明市） shabby: adj. 破旧的；卑鄙的；吝啬的；低劣的 dusty: adj. 布满灰尘的，灰尘弥漫的；轻率的回绝，毫无用处的回答 譯文 华盛顿再次声明对北京政府在南中国海的活动表示关注。中国正在南中国海的人工岛上建造军用机场及港口，并占用了其他国家视为本国领土的领地。\n本周，一位美国太平洋区高级海军司令警告说，中国的做法可能令南中国海地区卷入危险的军备竞赛。两个最受争议的地区分别是临近马来西亚和菲律宾的斯普拉特利群岛（中国称为南沙群岛），以及中国在1974年从越南占领的帕拉塞尔群岛（中国称为西沙群岛）。BBC韩福瑞（Humphrey Hawksley）见过一些越南渔民，如今这些渔民发现自己正处于领土争端的风口浪尖。\n“嘿，亲爱的，你得过来看看这个。”说话的是一个带着绿色棒球帽的高个子美国游客，他正在河内市的一处军事历史博物馆招呼他的妻子过去看一件展品。这是一份记录于1975年4月的潦草的手写笔记，记录了越南最高指挥部命令其下所有指挥官以闪电般速度向西贡进攻，对美国所扶持的南越政权发动最后的攻势。“亲爱的，你能想象有一天我们的孩子在中东ISIL的某个博物馆里看到类似这样的么？”他问道。这间破败的布满灰尘的博物馆里陈列了1954年越南战胜法国，以及1975年击退美国人的作战计划模型，那时的越南共产党代表了西方民主社会所谴责的一切。博物馆中还展有描绘越南人抗击中国人的历史性战役的巨幅绘画，数千人在这些战役中被杀害。不过奇怪的是绘画所描述的仅为10世纪到15世纪期间的战役，而没有时间较近的一场发生于1979年的中越边境战争，越南同样也赢得了此场战役的胜利.\n","date":1480291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480291200,"objectID":"9f36f0ce4b829da270f375b59c1dfda1","permalink":"https://wangcc.me/post/2016-11-28/","publishdate":"2016-11-28T00:00:00Z","relpermalink":"/post/2016-11-28/","section":"post","summary":"Vietnam war","tags":["dictation","English Learning","Listening","BBC"],"title":"一春夢雨常飄瓦","type":"post"},{"authors":null,"categories":["dictation"],"content":"The Secret Fraudster in my Family\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-11-27 13:58\t用时：25:37 正确率：88%\t错词：33个\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nToday , we have we've an intriguing tale from Illinois in the US about a stolen identity, missing money , and the a most unlikely for allfraudster. Identity theft is increasingly common these dayssince . It's when someone gets hold of your name and personal information , and uses them to take out a loan or a credit card and so on. , and can rack up huge bills without you your knowing it. What But when it happened to Axton Bets-Hamilton Betz-Hamilton in the early 1990s, it was quite a real rare thing, and it came as a total shock to her. , though it was nothing compared to what she found out later about the culprit. Axton was just 19 when she realized realised something was wrong. I was a sophomore in college and I was excited that I was moving off campus. So I was moving out from of the dorm, dorms. I was getting my very first apartment. And, and I called the electric company to establish service . and , they sent me a letter . and the letter said, : \" We need a hundred dollar 100-dollar deposit to establish service due to your credit score. \"\nAnd basicallyBetz, a your credit score says what state you will find out they, your finances are in, whether or not you your are a safe bet if you are you're gonna basically borrow money from somebody or in current incur some kind of loan. Correct. And there was a number that called to call at the bottom of the letter to get a copy of my credit report. , and I called it out of curiosity. Six weeks later , this large vanilla manila envelope came in the mail . and it was from the credit reporting agency. And I opened up the envelope and realized realised that my credit report was 10 ten pages long with collection agency entries and fraudulent credit card entries that dated back 8 eight years to 1993. Words worth to be remembered: fraudster: n. 骗子；诈骗犯 Realise and realize are different spellings of the same word, and both are used to varying degrees throughout the English-speaking world. Realize is the preferred spelling in American and Canadian English, and realise is preferred outside North America. The spelling distinction extends to all derivatives of the verb, including realised/realized, realising/realizing, and realisation/realization. sophomore: n. 大学二年级生；（美）有二年经验的人 adj. 二年级的；二年级学生的 incur: vt. 招致，引发；蒙受; 例句： The government had also incurred huge debts 政府也已负债累累。 manila: n. 马尼拉麻；马尼拉纸（等于manilla） adj. 马尼拉纸制的；马尼拉麻制的 get hold of: 把握；抓住；得到。例句： He got hold of some money before the banks close today. 他今天在银行关门前取了些钱出来。 rack up 击倒，获胜。 例句： Lower rates mean that firms are more likely to rack up profits in the coming months. 更低的费率意味着各公司更有可能在未来的几个月里获得大量利润。 safe bet 准能赢的打赌（或事情）。 例句： It is a safe bet that the current owners will not sell. 十有八九现在的业主不会出售。 fraudulent[ˈfrɔ:djələnt] adj. 欺骗的，不诚实的；奸诈。 例句： There is no evidence that the broker was in league with the fraudulent vendor没有证据表明该经纪人与进行诈骗的卖主狼狈为奸 譯文 我们今天的故事发生在美国伊利诺斯州，主人公身份被盗用导致钱财尽失，而行骗者竟是她意想不到的人。近年来身份盗用事件日益猖獗。犯罪分子窃取你的姓名、身份信息并在你一无所知的情况下利用它们提取贷款、盗刷信用卡，或是掳走大捆钞票。而这一切发生在上世纪90年代的阿克斯顿·贝茨-汉密尔顿身上就极为罕见了，这对她是个不小的打击，但是和她后来发现真凶是谁相比根本算不了什么。19岁那年，她才意识到出了问题。\n那时我上大二，想到要离开校园我还挺兴奋的。我搬离了宿舍，拥有了自己的第一间公寓。我给电力公司打电话要求建立服务，他们寄给我一封信，上面写着：“基于您的信用评分，我们需要您支付100美元的保证金来建立服务。”\n贝茨，信用评分就意味着，你的财务状况都在这里了，如果你要借钱或者贷款，是否值得信任（借钱给你是不是安全）。\n没错。信末还附着一个电话号码，说打过去可以索要我信用报告的复印件。出于好奇，我打电话过去。6周后，我收到信用报告机构寄来的一个大马尼拉纸信封，打开后发现我的信用报告有10页那么长，里面涵盖了自1993年到当时8年内全部的收账代理商和信用卡欺诈的条目。\n","date":1480204800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480204800,"objectID":"f8c37e4d302ea08738525f1998f3333b","permalink":"https://wangcc.me/post/2016-11-27/","publishdate":"2016-11-27T00:00:00Z","relpermalink":"/post/2016-11-27/","section":"post","summary":"The Secret Fraudster in my Family","tags":["dictation","English Learning","Listening","BBC"],"title":"身份竊賊","type":"post"},{"authors":["Yingsong Lin","Yukari Totsuka","Baoen Shan","Chaocheng Wang","Wenqiang Wei","Youlin Qiao","Shogo Kikuchi","Manami Inoue","Hideo Tanaka","Yutong He"],"categories":null,"content":"Abstract PURPOSE: The extremely high incidence of esophageal cancer in certain rural areas of China has prompted significant intellectual curiosity and research efforts both in China and abroad.\nMETHODS: We summarize the research progress over the past several decades in high-risk areas (Linxian, Cixian, Shexian, and Yanting) based on literature research and our field trip (2012-2013).\nRESULTS: Considerable progress in clarifying the environmental risk factors and pathogenesis of esophageal cancer in high-risk areas has been achieved over the past several decades. Epidemiologic evidence suggests that carcinogen exposure and nutritional deficiency, rather than smoking and drinking, may be the major risk factors for esophageal cancer in the Taihang Mountains region, where the incidence of esophageal cancer is among the highest in the world. Two genome-wide association studies have identified variants in PLCE1 at 10q23 that are significantly associated with esophageal cancer risk. Recent whole-exome studies have revealed a comprehensive mutation pattern, in which the C\u0026gt;T transition is the predominant mutation type.\nCONCLUSIONS: Despite extensive research, the main causative factors that contribute to esophageal cancer in high-risk areas have not yet been elucidated. Challenges in this research area include determining the causative role of nitrosamine, identifying other potential carcinogens, and conducting fruitful international collaborative studies based on a multidisciplinary approach. Increased international collaboration will contribute to a better understanding of the etiology of esophageal cancer.\n","date":1480032000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480032000,"objectID":"4231934882b810e7d0a51dd2c578989e","permalink":"https://wangcc.me/publication/journal-article/esophageal/","publishdate":"2016-11-25T00:00:00Z","relpermalink":"/publication/journal-article/esophageal/","section":"publication","summary":"Abstract PURPOSE: The extremely high incidence of esophageal cancer in certain rural areas of China has prompted significant intellectual curiosity and research efforts both in China and abroad.\nMETHODS: We summarize the research progress over the past several decades in high-risk areas (Linxian, Cixian, Shexian, and Yanting) based on literature research and our field trip (2012-2013).\nRESULTS: Considerable progress in clarifying the environmental risk factors and pathogenesis of esophageal cancer in high-risk areas has been achieved over the past several decades.","tags":null,"title":"Esophageal cancer in high-risk areas of China: research progress and challenges","type":"publication"},{"authors":null,"categories":["dictation"],"content":"Cultural Inclusion and Exclusion\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-11-25 16:1\t用时：29:16 正确率：91%\t错词：23个\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nNow, Pokémon Go, . In case you haven't heard of it, it's it is a worldly wildly popular augmented reality game just take that's taken the world by storm since it's launched. I've persisted resisted playing so farand . I'm worried I might get hooked and ended end up spending hours of my life trying to chase after Pikachu or a Jigglypuff. But it seemsthat I am ain't a , I'm in the minority. With the release of the game in Australia, New Zealand , and the United States on the 6th of July, and subsequently the UK, Canada, several European countries , and in Egypt. , the app apparently has more daily users than Twitter and more downloads than the dating app dating-out Tinder. Politicians are getting involvedtoo, too. US presidential candidateHilary , Hillary Clinton, used Pokémon to encourage Americans to register to vote. Despite its ' successI've , I have read some worrying reports of players, walking off cliffs or straight into crime scenes while playing the game. So, so there are some obvious safety concerns. But what happens when Pokémon Go goes to the museum. ? Many historic and culture cultural institutions have found themselves designated as Poké-stops or Poké-gyms almost overnight. To tell us what the game might mean for a culture our cultural spacesI am , I'm joined by Blaire Moskowitz, a New York-based blogger , and academic, who also works for a company that makes apps for museums. Thank you for joining us, Blaire. Thank you for having me. Now I believe you are part of the original Pokémon generation. Yes, . A lot of Millennials remember the app as kids. What's a Millennial , and do you mind me ask asking how old you are? I am I'm 27 , and a child from the 90s would be a Millennial. When it was originally released in the 90s, we had the playing cards, . We watch watched the cartoons. So now seeing the app come back on our phones, kind of reminds us of our childhood, and it's the first bit of massive mass nostalgia from for Millennials. Words worth to be remembered: augment: n. 增加；增大 vt. 增加；增大 vi. 增加；增大 augmented reality: 增强现实 hooked: adj. 钩状的；吸毒成瘾的；入迷的 v. 用钩固定；捉住（hook的过去分词） Millennial: adj. 一千年的；千禧年的 nostalgia: n. 乡愁；怀旧之情; Her work is pervaded by nostalgia for a past age. 她的作品充满怀旧之情. designate [\u0026lsquo;dezɪgneɪt] v. 指明; 任命; 指出 There are efforts under way to designate the bridge a historic landmark. 人们正在努力将这座桥定为历史遗迹。 譯文 我们来说说Pokémon Go。这是一款超级流行的增强现实游戏，该游戏一经推出便掀起全球热潮。我一直按捺着没敢玩，担心会上瘾，然后就要花大把时间追逐皮卡丘和波波球。不过看起来我是少数派。\nPokémon Go于7月6日在澳洲、新西兰和美国地区上线，随后登陆英国、加拿大、埃及和几个欧洲国家，一经发售便收获众多迷粉。该游戏软件的日常用户数量已超过Twitter，软件下载量也超过约会软件Tinder。政客也会跟Pokémon Go“套套近乎”。例如美总统候选人希拉里就借Pokémon Go之名拉选票，鼓励美国公民注册选民身份。不过，尽管该游戏已取得巨大成功，但也存在隐忧。我看到一些相关事故报道，如玩家过于沉迷游戏从而走下悬崖或闯入犯罪现场。安全隐忧显而易见。\n另外，当游戏扯上博物馆又会怎样？许多历史文化机构发现几乎一到晚上自己就被游戏标为小精灵基站或道场。Pokémon Go对我们的文化景点有什么影响？我们请来从事博物馆软件开发相关工作的纽约博主、学术研究者布莱尔·莫斯凯维茨。\n“布莱尔，欢迎来到我们节目。”\n“谢谢你们的邀请。”\n“我觉得你应该是玩Pokémon初代游戏的那一代人。”\n“是。许多千禧一代的人都记得小时候那款游戏。”\n“什么是千禧一代？我能问问你的年龄么？”\n“我今年27。千禧一代指90年代出生的人。90年代的时候，我们玩过Pokémon对战卡片，看过动画片。如今Pokémon Go这款手机游戏让我们想起了童年时光，引发千禧一代用户的怀旧情绪。”\n","date":1480032000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480032000,"objectID":"b3a40efe21cb913c91596c42cbb6e95a","permalink":"https://wangcc.me/post/2016-11-25/","publishdate":"2016-11-25T00:00:00Z","relpermalink":"/post/2016-11-25/","section":"post","summary":"Cultural Inclusion and Exclusion","tags":["dictation","English Learning","Listening","BBC"],"title":"一弦一柱思華年","type":"post"},{"authors":null,"categories":["dictation"],"content":"Sir Edward Burnett Tylor\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-11-24 11:44\t用时：28:56 正确率：90%\t错词：22个\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nLike many English men Englishmen who suffered from tuberculosis in the 19th century, Sir Edward Burnett Tylor went abroad on medical advice, seeking the dryer drier air of warmer regions. Tylor came from a prosperous Quaker business family. , so he had the resources for a long trip. In 1855 , in his early 20s, he left for the New World . and after befriending Henry Christy, a Quaker archaeologist archeologist he met in his travels, they ended up riding together through the Mexican countryside, visiting Aztec ruins and dusty pueblos. Christy was already an experienced archaeologist, a dunter archeologist and under his tutorage tried to learn tutelage Tylor learned how to work in the field. , and his Mexican surgen filed sergeant fired in him an enthusiasm for the study a thorough way of faraway societiesancients , ancient and modern, that lasted the rest of his life. In 1871, he published his masterwork, Primitive Culture, which can lay claimed claim to being the first work of modern anthropology. Over the decades as his beard moved morphed from a lastrus lustrous Garibaldi to a vast seavery silvery cumulonimbus boost that would have made Gandalf jealous. Tylor added to his knowledge of the worlds of world's peoples through study in the museum and the library. Primitive Culture was, in some respects, a chorus quarrel with another book that had \" culture \" in the title , - Matthew Arnold's Culture and Anarchy. , a collection that had appeared just two years earlier. For Arnold, the poet the and literary critic, culture was the 'pursuit of our total affection perfection by means of getting to knowand , on all the matters which most concern us, the best , which has been thought and said in the world. ' Words worth to be remembered: dryer vs. drier: Drier is a comparative adjective meaning more dry. A dryer is one of many types of electrical appliances used to dry things. The words were once interchangeable. The distinction crept into the language through the 20th century and has only recently solidified. Some dictionaries still list the words as variants of each other, but the words are almost always kept separate in 21st-century publications. archaeology vs. archeology : Archaeology is the standard spelling throughout the English-speaking world, even in American English, where the a is dropped from many words traditionally containing ae (or æ, as it used to be rendered). Archeology is a somewhat common variant. It is about two centuries old and is common enough to have earned its way into many of the major dictionaries, but it has hasn’t gained much traction in edited writing. The traditional spelling remains the safer choice. befriend [bɪ\u0026rsquo;frend]:v. 待人如友, 帮助 I asked a certain comrade to befriend this fellow so that he would not interrupt our talks. 我请一位伙伴向这家伙套近乎以便能让他不来打断我们的谈话。 pueblo [\u0026lsquo;pwebləʊ]: n. 印第安人村庄 Early Amerindians related to the Pueblo Indians is known for skill in making baskets. 和普韦布洛印地安人有联系地早期美国印地安人;以编篮子的技术而闻名。 tutelage: n. 指导，指引；保护，监护，托管 sergeant: n. 警官；军士，中士；高等律师 morph: n. 语素；变种；变 v. 在屏幕上变换图像；改变 lustrous: adj. 有光泽的,光辉的; The moon was above, lustrous and serene. 天上的月亮皎洁肃穆。 cumulonimbus: n. 积雨云 quarrel: n. 吵架；反目；怨言；争吵的原因；方头凿 vi. 吵架；争论；挑剔 literay critic: 文学评论家 anarchy: n. 无政府状态，混乱 Edward Burnett Tylor 爱德华·伯内特·泰勒（Edward Burnett Tylor，1832－1917），英国文化人类学的奠基人、古典进化论的主要代表人物。泰勒出生于伦敦一个富有的工厂主家庭，接受了良好的私人教育。 Matthew Arnold 英国诗人、评论家。拉格比公学校长、托马斯·阿诺德之子。他所倡导的“文化批评”理论在西方文学史及思想史上占有重要的地位。阿诺德自称为“英国文化的倡导者”,后人尊他为“文化使徒”。 譯文 19世纪，同众多感染肺结核的英国人一样，爱德华·伯内特·泰勒遵从医嘱离开英国，前往更温暖、更干燥的地区。泰勒出生于一个富裕的贵格会商人家庭，故而他有足够的资金进行长途旅行。1855年，二十多岁的泰勒动身前去美洲的途中结交了贵格会考古学家亨利·克里斯蒂。最后二人一同奔赴墨西哥乡下，考察阿兹特克遗址和满是灰尘的普埃布洛村落。\n克里斯蒂当时已是经验颇丰的考古学家。在他的指导下，泰勒学会如何在考古现场工作。而受墨西哥当地巡佐的影响，泰勒对这里的人类族群研究——不论是古代的还是近代的——产生了浓厚的热情，这份热情一直燃烧到他生命的最后一刻。1871年，泰勒的代表作《原始文化》出版，该书可以称得上是近代人类学的第一巨作。随后几十年间，当他的胡子从光亮的加里波第式变为让甘道夫都会嫉妒的银白色长髯，泰勒一直通过博物馆和图书馆学习世界人类族群方面的知识。从某些方面来讲，《原始文化》是对另一部书《文化与无政府状态》（马修·阿诺德著）的驳斥。《文化与无政府状态》系列书籍比《原始文化》早两年出版。文化之于阿诺德——这位诗人及文学评论家——是一种对完美的追求，即了解与我们关系最密切的所有事物，领略世界上最好的想法和说法。\n","date":1479945600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1479945600,"objectID":"41512dd45252c9dbb1f23ad8237a7a0b","permalink":"https://wangcc.me/post/2016-11-24/","publishdate":"2016-11-24T00:00:00Z","relpermalink":"/post/2016-11-24/","section":"post","summary":"Sir Edward Burnett Tylor","tags":["dictation","English Learning","Listening","BBC"],"title":"空城澹月華","type":"post"},{"authors":null,"categories":["dictation"],"content":"雙胞胎一起講話有點瘮人\u0026hellip;..\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-11-23 11:53\t用时：18:50 正确率：89%\t错词：27个\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nPaula and Bridgette Powers are identical twins from eastern Australia who do the most extraordinary thing. They choose to speak at the same time. The pair has have spent almost every day of their lives together. They are in their 40s now . and they share a passion for seabirds. They set up the Twinnies Pelican and Seabird Rescue refuge off the Sunshine Coast . and they went into a studio in Perth to tell me about themselves. Here they both are. Well, I'm sad. mum said it was very easy for us to look be looked after . and she said that we are easy lived to amuse one another. , and she said that we had our own twin language . so no one else could understand what we are were talking about. I mean, you talked talk about having a twin language when you were little, but the way you speak now is amazing. , in unison like that. YesYeah, it does happen just happens naturally . and people think that we put it on but we don't. Not rightNo way. Actually , we both got this the flu at the moment . and we are on the same antibioticsanabolics. What? Do you always get the same illness illnesses at the same time? Yes, unfortunately. What? Right back to when you were little? Yes, that's right. I had had my appendix out, and then Bridgette had her appendix out two weeks later. Wow, . So it that was, so that's Paula to have who had the appendix out first. Yes, . That's right. And then I I'll have to suffer for two weeks. Words worth to be remembered: unison: adj. 同音的 n. 调和，和谐，一致，齐唱，齐奏; in unison 齐声；一齐；一致地，和谐地。 例句: Michael and the landlady nodded in unison. 迈克尔和房东太太一起点头。 put it on: 装腔作势；夸大；夸张。 例句： It wasn't as hard as you claimed；you were putting it on． 这并不象你说的那么难，你在夸大其词。 anabiolic: adj. 合成代谢的; Anabolic steroids are a synthetic version of the hormone testosterone, and promote the storage of protein and tissue growth. 合成代谢类固醇相当于人造的睾丸激素，它能促进蛋白质的积累和组织的生长。 譯文 葆拉和布丽奇特·鲍尔斯是澳大利亚东部的同卵双胞胎，她们会做一件非常不寻常的事——能同时说话。这对双胞胎几乎天天生活在一起。目前她们40多岁，并且非常喜欢海鸟。她们在阳光海岸建立了推尼斯鹈鹕和海鸟救助站。她们来到了我们位于佩斯的演播室讲述她们的故事。接下来有请这对双胞胎。\n妈妈说照管我俩非常轻松，我们总能相互逗趣，她说我们有自己的双胞胎语言，其他人都无法听懂我们在说什么。\n你们小时候有自己双胞胎语言，但是你们现在的说话方式很不可思议，完全一致。\n是的，这是自然而然的，大家以为我们是装的，但其实不是，根本不可能。事实上我们同时得了感冒，而且代谢相同。\n真的假的？你们一直都是同时生病、生同样的病吗？\n是的，很不幸。\n真的么？你们从小就这样吗？\n是啊，没错。我做了阑尾切除手术，然后布丽奇特几周后也摘了她的。\n哇，那也就是说是葆拉先摘的阑尾。\n是这样。然后两周后我也不得不经历同样的遭遇。\n","date":1479859200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1479859200,"objectID":"ac1007314c60da9ab0a7478e57fc4858","permalink":"https://wangcc.me/post/2016-11-23/","publishdate":"2016-11-23T00:00:00Z","relpermalink":"/post/2016-11-23/","section":"post","summary":"雙胞胎一起講話有點瘮人.....","tags":["dictation","English Learning","Listening","BBC"],"title":"異口同聲","type":"post"},{"authors":null,"categories":["dictation"],"content":"\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-11-22 22:16\t用时：26:49 正确率：95%\t错词：12个\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nLast month, Global Trade Talks took place in Geneva. AlrightAll right, before you reach for your radio, please do not switch us off, . This is important. Governments were debating, yet again, whether it was time to cut the subsidies that some of them give to their farmers. It's an old saw. Many, especially in the poor world, have been arguing against , since practiced this practice for years. Farmers in poor countries get much less you see than the generous handouts for super-rich farmers in places like Europe, the US , and Japan, distorting the global market markets, some say. But those rich governments are unwilling to change a dacades-old-practicedecades-old practice. And this, in part, is our theme today in Business Daily. We're looking first at one specific example, . Argentina is a major agriculture agricultural producer. The newly-elected center-right centre-right government there has been struggling to change a state-driven policy of handouts that has become bloated and some say unsustainable, both for the economy and the environment. Grace Livingstone reports. I have I've come to the Argentine flatlands, the Pampas, to see what farmers ' ink think of the new government. The Pampas used to be synonymous with cattle and cowboys. , but now most of this area is planted with genetically-modified soya. So I've come to the city of Rosario to talk to the Soya Association, which represents all those who grow or process soya. It's a very smart glass building, giving you an indication of just how much wealth the soya industry has generated. Words worth to be remembered: It\u0026rsquo;s an old saw. saw /sɔː/ n.格言 例句： The old saw \u0026quot;you should learn something new every day\u0026quot; is a good one. 古老的格言“每天你都应该学些新的东西”说的很好 centre-right: 中间偏右 handout/ˈhændaʊt/: n.施舍物; 救济品; 救济金 例：Each family is being given a cash handout of six thousand rupees. 每个家庭都被给予6000卢比的救济金。 synonymous /sɪˈnɒnɪməs/: a.密不可分的 例： Paris has always been synonymous with elegance, luxury and style. 巴黎与优雅、华贵和时尚一直是密不可分的。 譯文 上个月，全球贸易谈判在日内瓦举行。你可能要拿起收音机了，但稍等，不要换台。这很重要。政府之间又在争论，焦点仍然锁定在是否要削减发给农民的补贴。这是老生常谈。尤其在贫困地区，人们对此已经有数年的争议。在贫困国家的农民所得到的补贴远低于那些欧洲、美国和日本等国超级富有的农民所得到的政府的慷慨之助，而一些人认为这样会使全球市场走向失真。但那些有钱的政府并不愿意改变这样一个数十年历史的惯例。而这，某种程度上，就是我们今天每日财经的主题。我们首先来看一个实例。阿根廷是一个主要的农业生产国。新上任的中右翼政府想方设法改变这样一项愈发奢侈的国民政策，一些人认为这在经济和环境方面看都是不可持续的。Grace Livingstone 带来报道。 我来到了位于阿根廷的潘帕斯平原，了解这里的农民如何看待新一届的政府。潘帕斯曾经是牛群和牛仔的天下，但现在这里大部分地区都种植了转基因大豆。我来到了位于罗萨里奥的大豆联盟，它能够代表所有种植或加工大豆的群体。这个建筑外表是智能玻璃的，让你一下子就能感觉到大豆产业带来了丰厚的经济利润。\n","date":1479772800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1479772800,"objectID":"29fdecb0a7ede4e83ec1c29fc4812ca1","permalink":"https://wangcc.me/post/2016-11-22/","publishdate":"2016-11-22T00:00:00Z","relpermalink":"/post/2016-11-22/","section":"post","summary":"全球贸易谈判在争论否要削减发给农民的补贴。","tags":["dictation","English Learning","Listening","BBC"],"title":"大草原上的農業","type":"post"},{"authors":null,"categories":["dictation"],"content":"(俄國人的口音真難懂(⊙o⊙))\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-11-21 11:56\t用时：24:03 正确率：87%\t错词：27个\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nToday, an extraordinary insight into how things have been changing in Russia for gay and transgender people. At the end of the Soviet era in the 1980s, Tapir could wear high heels in the army and no one minded. These days he is he's afraid for his life. Tapir identifies as gender quierqueer, somewhere between male and female. That he was , though he's comfortable was with the masculine pronoun. He works as a bodyguard at events for lesbian, gay, bisexual , and transgender people. , and he has he's been telling me about the kind of trouble that activist activists can face, like at this event in 2014 at a culture center cultural centre in Moscow. The worst of that episode was in October, 2014 when there was a tackle attack of the so-called Russian battles battlers against the Sakharov Centre. What as was going on at the Sakharov Centre? It was prompted by the Coming Out Day , which was a stage staged at the Sakharov Center Centre by the LGBT activists. They were expecting that actually a limited number as usual who would turn up for this action, like 8,10 eight, ten people, the usual sort of arriving actvists. right-wing nationalistic activists, but they didn't expect actually the huge crowd of whom who will turn out up, calling themselves anti-maidan people. They would be constricted from all sorts of fright between nationaly sickright-wing, nationalistic factions. There are some people apparently from the Donbass area, . They were unusually brittle virulent Words worth to be remembered: queer: n. 同性恋者；怪人；伪造的货币 adj. 奇怪的；同性恋的；不舒服的；心智不平衡的 vt. 搞糟；使陷于不利地位; gender queer 同性恋。 例句： From the start, Timothy introduced himself as masculine-of-center gender queer. 从入学一开始，提摩西在自我介绍是说自己是一位偏男性的跨性人。 masculine pronune: 阳性代词；阳性代名词。 right-wing: adj. 右翼的；右派的 natinalistic: adj. 民族主义的；国家的 virulent: adj. 剧毒的,致命的,刻毒的,恶毒的,恶性的,有病毒的,充满敌意的 constrictvt. 收缩；压紧；阻碍；挤压。 例句： Men and women alike have been constricted by traditional sexual roles. 男性和女性同样受到传统性别角色的束缚。 【背景介绍】National Coming Out Day“全国出柜日”是国际上LGBT（包括同性恋、双性恋和跨性别）人群的庆祝出柜和提高社会认识的节日，LGBT社群成员和他们的支持者（常被称为“同盟”）在每年10月11日庆祝这个节日，在英国则是在10月12日来庆祝。（资料来源：http://tieba.baidu.com/p/2645622245） 譯文 目前，俄罗斯针对同性恋和变性人的看法产生巨大的转变。上世纪80年代苏维埃末期，塔皮尔在军队可以穿高跟鞋，而且没人会介意。然而最近他却非常担心自己的生活。塔皮尔确认自己是介于男女之间的同性恋，但他不介意使用男性代词。他在保护女同、男同、双性人、变性人的活动中负责安保工作。他告诉我这些活动者们要面对的各种困难，比如发生在2014年莫斯科文化中心的这次运动。\n最糟糕事件发生在2014年10月，所谓的俄罗斯战士反对萨哈罗夫中心的活动而引发的袭击。\n萨哈罗夫中心发生了什么？\n出柜日那天萨哈罗夫中心聚集了不少LGBT（同性恋、双性恋及变性者）活动者们，于是引发了袭击事件。活动者们本来希望像平常一样只让一部分人参加这次活动，像是8个或者10个人，他们通常都是右翼的国家主义分子。但是活动者们不希望一大群自称“反广场”的人都来参加。他们将会受到来自各种右翼国家主义派的限制。有些人显然来自顿巴斯地区，他们充满恶意又激进好斗。\n","date":1479686400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1479686400,"objectID":"9f831da3bb8414cf555b09d5cfbe9fcf","permalink":"https://wangcc.me/post/2016-11-21/","publishdate":"2016-11-21T00:00:00Z","relpermalink":"/post/2016-11-21/","section":"post","summary":"俄國人的口音真難懂(⊙o⊙)","tags":["dictation","English Learning","Listening","BBC"],"title":"另類保鏢","type":"post"},{"authors":null,"categories":["dictation"],"content":"\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 用时：24:13 正确：93% 奖励： 4 日期：2016-11-19\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nIt's that time of a year again. The Tuesday after the August bank holiday, summer holidays are almost over. Very soon, it would will be back to school. For those of us without children, the first clue is all the advertises for adverts of school uniforms. Getting kitted out for a new term is a lesson in life. There is There's repetition in the cycle of one academic year after another, but also growth in a the fact that clothes that bought just 12 months ago were no longer fit and require replacement. My mum always got around it this by buying uniforms several sizes too big for me , that will would last forever. In yesterday's Guardian, the philosopher Julian Baggini observed that life can appear forward moving and linear when it is in fact sick a little cyclical, too. , and that we we'll do better to appreciate the rhythms of here and now, rather than fast fuss over the distant future. As a Christian, my mind immediately turns to Ecclesiastes, where it says\" , 'For everything, there is a season, and the a time for every matter under heaven. \" ' This passage is popular at funerals partly because it talks about a time to die, but also a time to be born. , reminding us that birth and death, like summer and winter, are not the end, but part of a cycle. It seems to say to the mourners: \" , 'Take comfort, . Life goes on\" . ' Words worth to be remembered: Bank Holiday: 银行假日（英语：bank holiday）指的是英国、部分英联邦国家和部分欧洲国家（如瑞士）以及部分英国前殖民地（如香港）的公共假日。在爱尔兰，口语中也将公共假日称为银行假日 advert: n. 广告 v. 注意；提及 kit out: 穿着: She was kitted out with winter coat, skirts, jumpers. 她全副武装，穿着冬天的外套、裙子、套头衫。 Guardian: 《卫报》（The Guardian）是英国的全国性综合内容日报。与《泰晤士报》、《每日电讯报》被合称为英国三大报。由约翰·爱德华·泰勒创办于1821年5月5日。因总部设于曼彻斯特而称为《曼彻斯特卫报》。1959年8月24日改为现名。 philosopher: n. 哲学家, 哲人 He came to London in 1750 and soon acquired a reputation as a philosopher and man of letters. 1750年他到伦敦,不久获得了哲学家和学者的名声。 cyclical: adj. 循环的；周期的 rhythm: n. 节奏；规则变化 fuss: n. 大惊小怪，抱怨；争吵 v. 忙乱，大惊小怪；（为小事）烦恼 You needn't fuss. There's no disgrace. 你不必大惊小怪。没有什么丢人的事。 Ecclesiastes: n. 传道书 譯文 又逢一年开学时。八月银行假期一过便预示暑假即将结束。再过不久就要开学了。对我们这群没有子女的人来说，开学临近最明显的迹象要数那些校服广告。为新学期准备服装可算是人生一课。学嘛是一年跟着一年的上，个子嘛也是一年跟着一年的长。12个月前的校服现在穿不上了，这不就得买新衣裳了。我妈妈通常给我买大好几码的校服，这样就不用年年买新衣服了。\n哲学家朱利安（Julian Baggini）在昨天的卫报上评论说，生命表现为向前移动的直线，但同时它也是循环往复的。把重心放在当前，而不过分关注遥远的未来，这样我们的生活会更好。\n作为一名基督徒，我立马想到《旧约传道书》上面说的：“凡事都有定期，天下万物皆有定时。”这部分选段经常出现在葬礼上。它讲述的不仅是死亡，还包括新生，提醒我们生或死好比夏季与冬季并非意味终结，而是漫长循回中的一部分。它仿佛在向默哀者低语：“节哀。生命仍在继续。”\n","date":1479513600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1479513600,"objectID":"13ee37c05aca429afdb97a26da048eff","permalink":"https://wangcc.me/post/2016-11-19/","publishdate":"2016-11-19T00:00:00Z","relpermalink":"/post/2016-11-19/","section":"post","summary":"It's that time of a year again.","tags":["dictation","English Learning","Listening","BBC"],"title":"物換星移幾度秋","type":"post"},{"authors":null,"categories":["dictation"],"content":"Italy\u0026rsquo;s devastating earthquake\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-11-18 13:15\t用时：28:52 正确率：94%\t错词：15个\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点 I mean, it is that unfolding. It\u0026rsquo;s very unfamiliar to me. But you think these are all connected somehow. Well, first, earthquakes in Italy turn tend to occur more in groups or sequences than a lot of other places in the world. This is somewhat unique to Italy. It\u0026rsquo;s not a striking difference, but it\u0026rsquo;s a noticeable one. And it\u0026rsquo;s probably because the faults there are relatively relative young. , less than a 1 million years old. Compare that to the San Andreas\u0026rsquo;, which is over 10 million years old. So these are little kind of broken shards of faults that haven\u0026rsquo;t really been organized organised by repeated earthquakes into a long continuous smooth fault. So, that means that if you jostle one, you turn tend to move the others around it. And no one fault is able to rupture for a very long distance and produce a very large earthquake. So we get these little groups or families of moderate size moderate-sized events. But it\u0026rsquo;s interesting that you mentioned L\u0026rsquo;Aquila , that was several years ago. , there was some pre-rumbling that happenhappened, I\u0026rsquo;m not sure what was going on afterward and afterwards, then in August you have once, that\u0026rsquo;s a long separation. , and then a few weeks late later you\u0026rsquo;ve got another one and then another one. You know, the timing , as well as the distance between these events , seems curious to me. I am I\u0026rsquo;m not sure what to make of it. Well, it\u0026rsquo;s interesting. Take a typical earthquake, say a magnitude 6six. It\u0026rsquo;s gonna going to produce aftershocks. Aftershocks have a unique property. : the longer you go by in time, the more spread out they are. But their magnitudes don\u0026rsquo;t get smaller with time.\nWords worth to be remembered: unfolding: v. 展开；开展，发展显露；显露 fault: n. 故障；[地质] 断层；错误；缺点；毛病；（网球等）发球失误 vt. （通常用于疑问句或否定句）挑剔 vi. 弄错；产生断层 San Andrias: 圣安德列斯断层（英语：San Andreas Fault，又译圣安地列斯断层、圣安德烈亚斯断层、圣安德鲁斯断层），是北美洲一处频繁活动的断层。 shard: n. 瓷器的碎片;碎片;翅鞘;外壳 jostle: n. 推撞，拥挤 v. 推挤；争夺，竞争 rupture: n. 破裂；决裂；疝气 v. （使）破裂；发生气 L\u0026rsquo;Aquila (earthquake): 2009年拉奎拉地震 ; 意大利拉奎拉地震 rumbling: n. 隆隆声 v. rumble的现在分词；隆隆响 I\u0026rsquo;m not sure what to make of it: (someone) didn\u0026rsquo;t know what to make of (something) This phrase means that a person didn\u0026rsquo;t understand something or didn\u0026rsquo;t know what it was. In the example above, the speaker didn\u0026rsquo;t know whether the guy was serious or joking, whether he really didn\u0026rsquo;t care about her or whether he was trying to hide his true feelings. Other feelings that you might have when you \u0026ldquo;don\u0026rsquo;t know what to make of\u0026rdquo; something are: You\u0026rsquo;re not sure whether something is good or bad. You can\u0026rsquo;t decide what something is. You don\u0026rsquo;t know why a person said something the way that they said it. 譯文 这种展开方式我不太熟悉。但你觉得它们之间都是有某种联系的？\n嗯，首先，相比于世界上许多其他地方来说，意大利的地震更趋向于多震源或连续发生。意大利在这点上多少有点特别。这个特点不是多么突出，却也是很明显的。这很可能是因为意大利的地质断层形成时间还不够长，还不到一百万年。把它跟圣安地列斯的断层一比，人家已经一千多万年了。所以这些都只是断层碎片，还没有受到地震的频繁推挤而变成那种大范围的连续光滑断层。那么这就意味着，如果动了一个断层，那它周围其他的断层也会跟着移动。而断层中的任何一个都不可能造成大范围地震，震感也不可能很强。所以才会有这些小的中级震群或震族。\n但是你提到了几年前拉奎拉的那次地震很有意思，先是有一些震动——我不确定之后发生了什么——然后在八月份又发生了一次，这中间隔了很长一段时间，结果几周之后又发生了一次，然后又发生了一次。这几次地震之间的时间间隔和距离间隔都让我觉得很奇怪。我不知道该怎么理解这个问题。\n嗯，确实有趣。就一次典型的地震来说，就说六级地震吧。它会产生余震。余震有一种独一无二的特性：过去的时间越长，波及的范围就越广。但是震级并不会随着时间而降低。\n","date":1479427200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1479427200,"objectID":"6690f19ce6c70b4c10ae346e9b24a599","permalink":"https://wangcc.me/post/2016-11-18/","publishdate":"2016-11-18T00:00:00Z","relpermalink":"/post/2016-11-18/","section":"post","summary":"Italy's devastating earthquake","tags":["dictation","English Learning","Listening","BBC"],"title":"意大利地震成因","type":"post"},{"authors":null,"categories":["dictation"],"content":"Fight for life by Sofia Zambuto\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-11-17 11:29\t用时：21:51 正确率：94%\t错词：18个\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nThe silence was terrible. I knew it would hit us at any moment. When it came, the wave was colossal and came crashing crushing down on us. Two of my little ones clung to me. They saw their father go under. He disappeared. We didn't see him resurface. Then we went on the under, too. I was desperate to keep them alive , as and they clung to me as I fall fought to reach the choptop. The deadly phone foam made it almost impossible to breathe. The wreckage was everywhere. I had heard of the horror of these moments . and I knew there would be worse to come. My children were heaving and fighting to breathe. Their father was nowhere to be seen. Most of the others had been massacred by the torturous wave. The air is stunk stank of toxic filmsfumes. The debris of dead bodies, dead children surrounded us. Very soon, the next wave would come for us. I wanted to scream, but I couldn't. My young ones didn't know what had hit them. If we don't move now, it would will be a short the sure death of us. I had to find a way. I had to find a passageway for us. The water had gone down the a good bed bit and we could move now. Those few who had survived were beginning to run. They were scampering. We were all searching for passage. We kept moving. My children scurried after me, not wanting to be left behind. And Then I saw it, . I saw the way. There was a way open to us now. 'Children, , we must move now, . We must leave our home now. We must find a new home. The wave, the flood is coming again. Run with me now as far fast as your legs would will carry you. Follow me now, . Follow me. ! ' The passage passenger was approaching. This was our chance, our only chance to escape , to survive. Words worth to be remembered: colossal: adj. 巨大的；异常的，非常的; 例句： In the centre of the hall stood a colossal wooden statue, decorated in ivory and gold. 大厅中央矗立着一尊用象牙和金子装饰的木质巨型雕像。 heave: v. 上下起伏 例句： As the wind increased, the deck of the ship began to heave beneath his feet. 随着风力增大，轮船甲板开始在他脚下颠簸起来。 cling: vi. 坚持，墨守；紧贴；附着 massacre: vt. 残杀；彻底击败 n. 大屠杀；惨败 resurface: vt. 重铺路面；为…铺设新表面 vi. 重新露面；浮上水面 stank: n. 恶臭；发怒，大吵大闹 v. 发出恶臭；令人讨厌 fumes: n. （强烈而刺激的）气味，气体 名词fume的复数形式 a good bit a fairly large amount of something 大量的 例句： We’ve still got a good bit to do. 我们还有一堆事儿没做。 scamper: n. 蹦跳；奔跑 vi. 蹦蹦跳跳；奔跑，惊惶奔跑 scurry: n. 急跑；短距离赛跑（或赛马） vi. 急赶；急跑 vt. 急赶 譯文 （水面）安静的可怕。我知道洪水可能在任一时刻到来，卷着巨浪猛烈地拍向我们。我的两个小孩子紧紧的抓着我。他们看到他们的父亲沉入水中，消失不见。我们没看到他再次浮出水面。之后我们也被水淹没。我拼命地想让孩子们活下去。他们紧紧抓着我，我奋力去抓住高处。水上大量的泡沫使我们难以呼吸。到处都是碎片残骸。我曾听闻洪水的可怕，我知道更糟的情况将来临。\n我的孩子们上下扑腾着拼命去呼吸。四下都看不到他们父亲的身影。大部分人已经丧命。空气中弥漫着难闻的气味。周围飘浮着尸体的残肢、失去生命的孩童的尸体。我知道很快，很快第二波洪水就要来临。我想尖叫，但我不能。我的孩子们还不知道他们面临的是什么。\n如果现在不离开，等待我们的只有死亡。我必须想办法。我必须为我们找到逃命的出路。水面降下去很多。我们可以移动了。为数不多的幸存者已经准备逃离了。他们四下惊逃。我们都在找出路，不停的找。我的孩子们紧跟着我，不想被落在后面。然后我看到了它，我看到了出路，它向我们敞开。\n“孩子们，我们要走了。我们必须离开我们的家，找一个新的住所。洪水又要来了。跟着我，尽你们所能的紧跟着我。”\n其他人在靠近。这是我们的机会，唯一的逃生的机会。\n","date":1479340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1479340800,"objectID":"d518bd35f827b9fe1e987b3eb1764129","permalink":"https://wangcc.me/post/2016-11-17/","publishdate":"2016-11-17T00:00:00Z","relpermalink":"/post/2016-11-17/","section":"post","summary":"Fight for life by Sofia Zambuto","tags":["dictation","English Learning","Listening","BBC"],"title":"地籟風聲急","type":"post"},{"authors":null,"categories":["dictation"],"content":"The Book that Changed Me Episode 5 of 5\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-11-16 14:46\t用时：25:06 正确率：92%\t错词：15个\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nThe People's War is a book of social history by Angus Calder. It was published in 1969 . and I bought it at Singapore airport when I was travelingtravelling, I suppose now I'd call it backpacking, in South East Southeast Asia. As I traveled travelled through Vietnam, where the American president said presence sat on the country like Goya Goya's Colossus of Chaos. , and through Cambodia, which had yet to learn the benefits of receiving the protection of the USA. , I was learning through The People's War about a far away faraway country, of which I knew little , - my own. 'This is a war of unknown warriors. ' Churchill told the war world in the summer of 1940. The whole of the war in warring nations are engaged, not only soldiers, but the entire population - men, women , and children. In 1940 and the years that followed, the people of Britain were protagonists in their own history in a fashion never known before. , hence the title of this book , '- The People's War. I was born during the war. To talk of the war nowadays is to define oneself as a child of the 40's. 40s, as surely is as printing one's birth certificate. , but at least until recently, you could be confident of sharing a common language. Both near Bosnia, the Gulfof , the Falklands and the Iraq Warhas noticed , have forced us not to take so much for granted. It was the war , that provided the basic grammar for my parents' lives. It was literally and metaphorically , the time of their lives , invoked with mathematical regularity throughout out my childhood. , and their then memories of the period wound up and regulated my emotional clock. Words worth to be remembered: protagonist: n. 主演；主要人物，领导者 The protagonist enters left stage. 男主角从舞台左边出现; Both novels trace the growth and development of the protagonist's character. 这两部小说都描述主人公性格的成长和发展过程。 invoke: v. 祈求, 实行, 恳求 Let us invoke the blessings of peace. 让我们祈求和平之福。 Goya: 弗朗西斯科·何塞·德·戈雅-卢西恩特斯，西班牙浪漫主义画派画家。画风奇异多变，从早期巴洛克式画风到后期类似表现主义的作品，他一生总在改变，虽然他从没有建立自己的门派，但对后世的现实主义画派、浪漫主义画派和印象派都有很大的影响，是一位承前启后的过渡性人物。 https://en.wikipedia.org/wiki/Francisco_Goya Bosnia: 波斯尼亚战争，是原南斯拉夫解体时的内部战争，是波斯尼亚和黑塞哥维那，发生在1992年3月和1995年之间和塞尔维亚之间的武力冲突。。在旧南斯拉夫开始解体时，波斯尼亚黑塞哥维那亦在1992年宣告独立。 Gulf: 海湾战争，是以美国为首的多国部队于1991年1月17日～2月28日在联合国安理会授权下，为恢复科威特领土完整而对伊拉克进行的局部战争，同时也是人类战争史上现代化程度最高、使用新式武器最多、投入军费最多的一场战争。 Falkland: 马尔维纳斯群岛战争，简称马岛战争或福克兰群岛战争（英语：FalklandsWar）或福克兰海战，也有部分媒体简称为福岛战争，是1982年4月到6月间，英国和阿根廷为争夺马岛（阿根廷称“马尔维纳斯群岛”）的主权而爆发的一场战争。 本文的譯文, 滬江的版本實在是太差了. 之前都沒有仔細閱讀他們給的翻譯. 特別是最後一自然段的最後幾句. 所以今天就沒有譯文了. ","date":1479254400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1479254400,"objectID":"f55f30e9a52b4b9fc1ab000190d8c3ab","permalink":"https://wangcc.me/post/2016-11-16/","publishdate":"2016-11-16T00:00:00Z","relpermalink":"/post/2016-11-16/","section":"post","summary":"The Book that Changed Me Episode 5 of 5","tags":["dictation","English Learning","Listening","BBC"],"title":"鐵馬冰河入夢來","type":"post"},{"authors":null,"categories":["dictation"],"content":"\u0026lsquo;Miracle Girl\u0026rsquo;: Aircrash Sole Survivor\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-11-15 11:9\t用时：24:33 正确率：94%\t错词：16个\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nWe are We're starting with a remarkable survival story from the Indian Ocean. In June, 2009, a plane traveling travelling from Yemen to the Comoro Islands plunged into the water just a few miles from its destinationa reported crashed from . A report into the crash suggested that human error may have been one factor. 152 people on board were killed. Only one people person survived, : a 13-year-old French girl called Bahia Bakari. The media adopted dubbed her , \" the Miracle Girl\" . Seven years on, she's been telling me what happened. I was just on holidays with my mommum. We were going to a wedding in my mother's family . and I was a bit anxious because it was a long time that I didn't take a flight. And on the morning of the trip , my uncle and my cousin took us to the airport. Do you remember what the plane itself was like ? because there were reports that it wasn't in the greatest condition. Yes, . That day we took 2 two planes . - the first plane to go to Sana'a in Yemen, and then from Yemen to the Comoros. And yesindeed, indeed the conditions of the plane were not very good. There was a fly in the plane. There was a really bad smell, like a toilet smell. But apart from that the whole flight was normal until the accident. What do you remember of the situation as the plane came into land? So the flight was going normally until quite a long way into the flight. , so everyone was sleeping, and suddenly they announced we are we're gonna land , and everyone had to put their belts on. That But everyone was sleeping, . So people were quite quiet and I was worried. So I looked at people, thinking: why is not everyone more worried and, or shouting? And suddenly we went through a lot of turbulence . and then it's a complete black hole. I don't remember anything, just waking up basically in the water. Words worth to be remembered: plunge into: 投入；跳入；突然或仓促地开始某事。例句： He plunged into the cold water.他跳入冰冷的水中。 dub[dʌb]: vt.（以剑触肩）封…为爵士；授予称号；起绰号。 例句： Today's session has been widely dubbed as a \u0026quot;make or break\u0026quot; meeting. 今天的会议被大众称为“不成则散”的会议。 譯文 我们今天要讲的是发生在印度洋上的精彩逃生故事。2009年6月，一架由也门驶向科摩罗群岛的飞机在距目的地几英里处不幸坠入大海。坠机报告显示事故发生的原因之一是人为失误。这起空难造成152名乘客丧生，只有13岁的法国女孩巴希亚·巴卡里幸存下来，媒体都称她为“奇迹少女”。7年过去了，她告诉我事故发生的情况。\n当时我和妈妈一起去度假。我们去参加妈妈亲戚这边的一个婚礼。我有点担心，因为我很久没坐过飞机了。出发那天早上，我的叔叔和表兄开车送我们去了机场。\n你还记得当时飞机怎么样吗？因为有报告表明它当时状态不是太好。\n是的。那天我们乘坐了两架飞机——先是开往也门的萨那，然后再从也门飞往科摩罗。是的，当时飞机的状态不是非常好。飞机里有只苍蝇，还有非常难闻的、像厕所里的味道。但是除此以外，直到事故发生，整个飞行过程都是正常的。\n那你还记得飞机降落时的情况吗？\n飞机正常飞行了很长的一段时间，大家都睡着了。突然他们通知我们要降落，让所有人都系好安全带。但是大家都在睡觉，人们都非常安静，我很担心。我看着大家，心想：为什么你们都不怎么害怕或者尖叫呢？突然我感觉到飞机穿过大量气流，接着就如同掉进黑洞一般。我不省人事，醒来时发现自己身在水中。\n","date":1479168000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1479168000,"objectID":"a1288a1beb20ace1f44b02850ba14979","permalink":"https://wangcc.me/post/2016-11-15/","publishdate":"2016-11-15T00:00:00Z","relpermalink":"/post/2016-11-15/","section":"post","summary":"'Miracle Girl': Aircrash Sole Survivor","tags":["dictation","English Learning","Listening","BBC"],"title":"唯一生還","type":"post"},{"authors":null,"categories":["dictation"],"content":"Beijing\u0026rsquo;s Property Problem\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-11-14 13:50\t用时：20:04 正确率：88%\t错词：22个\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nThe streets of Beijing are busy and bustling. China has undergone an economic miracle over the past quarter-century . and it's in urbanization seen urbanisation on an unprecedented scale. More than half of the Chinese population now live in cities. The capital is home to 21 million people. That unofficial figure is almost double all the number who lived here of at the tenth turn of Milleniumthe millennium. And that has created many problems including the now sky-high cost of living in a high-rise city. It's causing unhappiness among many of the young and therefore concerned concern among policy-makers. A song about it is regularly being played on people's smartphonessmart phones. And the government has just tightened restrictions to try to get it handled a handle on China's property-buying frenzy. It's pretty lively here around Houhai Lake, . This is one of the fee few parts of Beijing , that still , resembles the city of just half a century ago. There is There's a collection of traditional red and green wallen tooth to rebuildings one- and two-storey buildings in front of the tawest Taoist temple here. Some Beijing-ness Beijingers marvel at the breathtaking changes the city see their city's seeing in recent years. Paddy Petty fields have been concreted over for housing estates. Low-rise buildings demolished for skyscrapers. The area around Houhai Lake is lucky to survive. Words worth to be remembered: quarter-century: 25年 bustle /ˈbʌsəl/ v.奔忙 例： My mother bustled around the kitchen. 我母亲在厨房里忙得团团转。 millennium: n. 千禧年；一千年 high-rise: adj. 多层的 n. 多层高楼 sky-high: adj. 极高的,昂贵的 adv. 极高,粉碎 frenzy: n. 狂热；狂暴；狂怒 v. 使狂怒 Taoist: adj. 道教的 n. 道士, 道教信徒 marvel: n. 令人惊奇的人或事 v. 对…感到惊讶, 大为赞叹 petty: adj. 琐碎的；小气的；小规模的 concreted: adj. 混凝土的；实在的，具体的；有形的 n. 混凝土 v. （使）凝固；用混凝土修筑 low-rise: adj. 不高的 譯文 北京的大街川流不息、熙熙攘攘。过去25年来，中国创造了经济奇迹，城市化范围之广、进程之快前所未有。有一半多的中国人居住在城市。首都北京的常住人口有2100万，这一非官方数字几乎比世纪之交时翻了一番。这也产生了很多问题，比如在高楼林立的城市里生活的高成本。许多年轻人寻找不到幸福感，因此这也成为了众多政策制定者的担忧。人们的智能手机上经常播放一首有关的歌曲。政府为了控制人们购买房产的疯狂举动，刚刚出台了限制的紧缩政策。\n后海这里十分热闹。这里是北京为数不多的仍保留了半个世纪前北京模样的地方。在一座道观前，有一片传统的红绿砖瓦、一两层楼的房屋。一些北京本土人对于北京近些年的巨变仍感叹不已。零星的空地被开发成房地产，拔地而起混凝土建筑。平房被拆除，取而代之的是摩天大楼。后海这片地方得以幸存。\n","date":1479081600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1479081600,"objectID":"5ba7484226db989921760d50767a50cf","permalink":"https://wangcc.me/post/2016-11-14/","publishdate":"2016-11-14T00:00:00Z","relpermalink":"/post/2016-11-14/","section":"post","summary":"Beijing's Property Problem","tags":["dictation","English Learning","Listening","BBC"],"title":"天價北京房","type":"post"},{"authors":null,"categories":["dictation"],"content":"Jacqueline Bisset\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-11-12 10:47\t用时：27:26 正确率：88%\t错词：25个 提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nWe are We're beginning with one of the most popular and affections infectious examples of the Jone genre from 1963. 1973, because it's there's a new Blu-ray release of Francois Truffaut's Day For Night, which takes it's its title from the blue filter used to make daylight look nocturnal . - a whole lot easier than filming in genuine dark. The plot dancers surrounded dances around the director played by Truffaut himself, as he tries to marshal his production despite the various personal dramas of costume cast and crew. His film was is made in France, in French, but his star is English. She arises arrives from Hollywood in the a flurry of paparazzi, young, beautiful and slightly neurotic. The in-joke here is the that star Pamela is played by British actress Jacqueline Bisset, who would indeed be have been arriving with attendance attendant publicity. By the early 70s, she'd already been a Bond girl, Steve McQueen's love interesting interest in Bullitt and an air hostess and Captain Dean Martin's girlfriend in Paro peril in the sky blockbuster Airport. And then, one day in 1972, Jacqueline Bisset was in Paris - not so much for the last tango with as the disco tech , - when an emissary arrived from UAE royalty. It's actually rather a strange story, from my point of view. I was in Paris as I used to go sometimes quite often, and I often used to go to a place and dance. And I was staying that particular evening in a hotel I've I'd never stayed at before in my life. , and about 11 : 00 o'clock in the morning. , a very out-of-breath handsome young man came rushing up the stairs, knocking on my door, sayingthere is , you must, there's a phone call for meyou. I saidthere is , no, there's a mistake, . Nobody knows I am I'm here.\nWords worth to be remembered: genre: adj. 风俗画的 n. 种，类；类型；（文学作品等的）体裁，样式，风俗画: I believe in the story of the genre itself. 我相信这种音乐流派本身的故事。 nocturnal: adj. 夜间的；夜行性的 marshal: n. 元帅；司仪；警察局长；消防队长 v. 排列，整理；引领；集结，编队: If so, you're a leader who will take your company to new levels and marshal all the resources(and personalities) of a team destined for success. 倘是如此，你将是一个引领你的公司走上新台阶、合理配置团队资源（包括人力资源）以驶向成功彼岸的领导者 cast and crew: 演员和工作人员 a flurry of paparazzi: 一连串的狗仔队 flurry: n. 一阵风、雨或雪；疾风；骚动；慌张 v. （使）慌张，激动 Why the flurry? 这么慌张干嘛？ paparazzi: n. 专门追逐名人的摄影记者 in-joke: n. 圈内人(才能领会)的笑话 peril: n. 危险；冒险 v. 危及 blockbuster: n. 重磅炸弹，大片，畅销书: The difference between us and a Hollywood blockbuster is that we have to keep it tied to the science as closely as possible. 我们创作出来的作品与好莱坞大片之间的不同点在于，我们的东西总是尽可能的贴近现实科学。 emissay: n. 使者；间谍；密使 adj. 间谍的；密使的: Now spring coming, a couple, bride and bridegroom, like a spring emissary is standing happily before us with the wedding music playing on. 春天来了，一对春天的使者，踏着婚礼进行曲正含情脉脉立在大家面前。 UAE: abbr. 阿拉伯联合酋长国（United Arab Emirates） out-of-breath: 喘不过气来，上气不接下气 attendant adj. 伴随的；侍候的: Such patients run an increased probability of hospitalization, with all the attendant costs to the patient and to the health care system that hospitalization entails. 此类患者趋于住院治疗的的几率增加，随之而来的患者和医疗保健系统住院治疗所承担的费用也增加。 譯文 我们先来看看这一类电影中最受欢迎且最有感染力的一部作品，导演Francois Truffaut在1973年的作品《日以作夜》。这部电影最近发行了最新的蓝光版本。电影得名于拍摄时采用的蓝色滤镜。滤镜让白天看起来像是夜晚，比在黑暗中拍摄容易得多。电影情节围绕着一位导演展开，讲述他如何克服剧组里的一系列戏剧性事件，只为完成电影的拍摄。Truffaut本人扮演这一角色。导演的法语电影拍摄于法国，但主角是英国人。她年轻貌美，又有点神经质，从好莱坞来到这里，轰动了一众狗仔。这里有个行内人才懂的笑点：片中的女星Pamela由英国演员Jacqueline Bisset扮演，她本人的出现确实会引起广泛关注。在70年代初，她已饰演过邦女郎，导演Steve McQueen的作品《布利特》中的情人，以及风靡一时的电影《国际机场》中机长Dean Martin的空乘女友。随后，1972年的一天，Jacqueline在巴黎（并不是像歌里唱的那样“为了最后一曲探戈”），一位来自阿联酋皇室的信使找到了她。\n在我看来，这真的是一件非常不可思议的事。我那时在巴黎。我以前常去那里，也常去一个地方跳舞。在那个特别的晚上，我住在一间我从未住过的酒店。早上11点左右，一位帅得让人窒息的年轻人急急忙忙跑上楼来，敲着我房间的门，一边说有我的电话。我说，不对，你搞错了，没人知道我住在这里。\n","date":1478908800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1478908800,"objectID":"3bbd57d402e21284ac2d9cf1d319ba60","permalink":"https://wangcc.me/post/2016-11-12/","publishdate":"2016-11-12T00:00:00Z","relpermalink":"/post/2016-11-12/","section":"post","summary":"Jacqueline Bisset","tags":["dictation","English Learning","Listening","BBC"],"title":"日以作夜","type":"post"},{"authors":null,"categories":["dictation"],"content":"Canada and EU sign free trade deal\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-11-11 16:48\t用时：20:05 正确率：90%\t错词：19个\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nAfter 7 seven years of negotiation and 7 seven days of at the bulging Belgian region of Wallonia digging that hills their heels in over approving the deal. , the European Union and Canada have finally signed that their landmark free trade agreement. After a short, but possibly in the circumstances, appropriate delay. , the Canadian Prime Minister Justin Trudeau flew into Brussels , for the signing ceremony. Those applauds There was applause and jubilation ask as ink was finally put to paper. Well in his comments, Mr . Trudeau concentrated on the economic benefits of the deal. First and foremost. , Canadians and Europeans share the understanding that in order for a real and meaningful economic growth . we need to create more good, well-paying jobs for our citizens. For aggressive Progressive trade agreements like the one signed today will do just that. While Well the European Council President Donald Tusk gave it a far broader significance. Todays' Today's decisions demonstrate that the disintegration of the Western community does not need to become a lasting trend. , that we still possess enough strength and determination, at least some of us, to counter the fatalism of the decay of our political worldsworld. In this particular moment in the EU's history , this positive sign means a great deal.\nWords worth to be remembered digging their heels in 顽抗；拒绝让步；坚持自己的立场 例句：Officials dug their heels in on particular points. 在一些特定问题上,官员们拒不让步. jubilation: n. 欢腾，欢庆，庆祝活动 Belgian: adj. 比利时的 n. 比利时人 as ink was finally put to paper 签字的那一刻 progressive: adj. 进步的; 不断前进的; 进行的; n. 改革论者; 进步分子; 例句：The company tries to project an image of being innovative and progressive. 该公司努力以富有创新和进取精神的形象出现。 disintegration: n. 瓦解，崩溃；分解 integration: n. 整合; 一体化; 结合; （不同肤色、种族、宗教信仰等的人的） 混合; 例句：The aim is to promote closer economic integration. 目的是进一步促进经济一体化。 fatalism: n. 宿命论 broader significance: 更深远、更广泛的意义 譯文 经过长达七年的谈判和比利时的瓦隆地区七天的不签约风波，欧盟和加拿大终于签订了历史性的的自由贸易协定。在短暂（又正合时宜）的延误之后，加拿大总理贾斯廷·特鲁多飞往布鲁塞尔，参加签署仪式。签字的那一刻，全场欢呼，掌声雷动。\n特鲁多先生的发言着重强调协定带来的经济效益。\n首先，加拿大和欧洲都深知，为了真实而有意义的经济增长，我们需要为市民们创造更好、待遇更优厚的工作机会。积极的贸易协定(指签署的自贸协定)有助于实现这一目标。\n而欧洲理事主席多纳尔德·图斯克赋予了该项协议更深远的意义。\n今天的决议体现了西方社会的分崩离析并非大势所趋，因为至少我们当中仍有一部分人，有着足够的力量和决心，去反抗政治世界衰败消亡的宿命。在欧盟历史上这个特殊的时刻，这个积极的信号意义非常重大。\n","date":1478822400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1478822400,"objectID":"1572deb72b6ab24db2a33cc4efe6a726","permalink":"https://wangcc.me/post/2016-11-11/","publishdate":"2016-11-11T00:00:00Z","relpermalink":"/post/2016-11-11/","section":"post","summary":"Canada and EU sign free trade deal","tags":["dictation","English Learning","Listening","BBC"],"title":"歐盟加拿大籤署自由貿易協定","type":"post"},{"authors":null,"categories":["dictation"],"content":"爲了準備IELTS雅思英語考試, 此篇文章之後的聽寫計劃盡量加入BBC英音的音頻資料.\nAleppo\u0026rsquo;s underground orphanage\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-11-10 15:38\t用时：21:18 正确率：96%\t错词：13个\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nThe rebel-held part of the city of Aleppo is currently under siege from Russian and Syrian government forces . and, according to the UN Special Envoy, faces total ruined ruin within the next two months. Before the war, Asmar Halabi ran two furniture shops in the city with his father, . These have been destroyed by the shelling. Now he runs an orphanage which is home to 50 children, many of whose parents have been killed in the bombardment. And it's not just the children who've lost relatives. , Asmar's own father and 3 three of his 4 four sisters have also been killed. On the line from Aleppo , he told me about the day in 2014 when an air strike hit the schools school where 2 two of his sisters and the women woman who is now his wife were studying. There was an exhibition for paintings and drawings for children above the age of 13. , and it's it also was the end of the academic here. year and there was some sort of celebration. At that time the school was attacked by regime forces and the whole school was destroyed. I got the news at around 9: 30 in the morning. People called me and told me of the incident. I immediately rushed to the school. I was able to find one of my sisters, killed. The second one, I kept looking for her under the rubbles and ruins for 2 two to 3 three hours until I was able to find any traces of her, and I couldn't. And then eventually they told me that she is in the hospital and we found her there. She was killed in that strike. My wife, we looked for her for days and days . and we couldn't find here her for 4 four days until later on they called us in the hospital and they told us that she was found in the school at that time and she was taken to hospital.\nWords worth to be remembered: Aleppo: 阿勒颇[叙利亚西北部城市] under siege: 被包围；受...困扰的；一再遭到批评的 envoy: n. 使节，外交官；全权公使 shelling: n. 壳；外壳；外形；炮弹 v. 剥壳；剥落 bombardment: n. 炮击，轰炸 regime force: 政府军 譯文 据联合国特使称，叛军控制的阿勒颇市部分地区目前正处于俄罗斯和叙利亚政府军的包围之下，并将在接下来的两个月中变得满目疮痍。战争开始前，阿斯马尔·哈拉比和父亲在市内经营了两家家具店，但是都被炮击摧毁。目前他开了一家孤儿院，收养了50个孩子，他们中很多人的父母都在轰炸中丧生。失去亲人的不光是孩子们，阿斯马尔的父亲和他四姐妹中的三人也都不幸遇难。远在阿勒颇的他向我讲述了2014年学校遭受空袭那天的事，当时他的两个姐妹和他现在的妻子正在那里上学。 那时有一个13岁以上孩子参加的画展，那也是学年结束的日子，所以有一些庆祝活动。当时学校被执政势力袭击，整个校园被毁。我是在早上9点半左右得到的消息。有人打电话告诉我这件事。我立刻赶往学校，找到了其中一个不幸丧生的姐妹。我在废墟和瓦砾中一直寻找我另一个姐妹，找了两三个小时也没找到她的踪迹。后来他们告诉我她在医院，我们在那里找到了她。她已经在袭击中身亡。我们整日地寻找我的妻子，四天过去都没找到，直到后来他们给我们打电话，说她在学校被发现，当时就被送进了医院。\n","date":1478736000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1478736000,"objectID":"a5c9c6a9c314f2d654cecf4eccb847a5","permalink":"https://wangcc.me/post/2016-11-10/","publishdate":"2016-11-10T00:00:00Z","relpermalink":"/post/2016-11-10/","section":"post","summary":"Aleppo's underground orphanage","tags":["dictation","English Learning","Listening","BBC"],"title":"用愛治愈傷痛","type":"post"},{"authors":null,"categories":["dictation"],"content":"Farmed Trout Bred to Fatten Up Fast\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-10-7 18:36\t用时：20:18正确率：91%\t错词：24个\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nHalf of all fish people eat worldwide now come from fish farms. So farms need to do more to keep up with demand. And If we look to the future, at today's per capital capita fish intake around the world . we will would need to double aquaculture production. Ron Hardy is the University of Idaho's Director of Aquaculture Research. He presented his research at a the recent International Symposium on Fish Nutrition and Feeding in Sun Valley, Idaho, which him he also chaired. In the wild, rainbow trout eat insects and other, smaller fish. But Hardy says there aren't enough little fish to feed larger fish in the wild and still meet market demand as the human population increases. So he's used selected selective breeding to create strains of farmed fish that get by on food that's less expensive than little fish . - feed made from soybeanscorns , corn and weedwheat. Some of the farmed fish really thrived. thrive: 16 years ago, Hardy had to wait a year for a one-pound trout. These days, his efforts yield trout up to four times as large in the same amount of time. So, it become like would be kind of like if you are were going to you know breed a, I don't know , box ? dogs. So we've got everything from Rottweilers to , you know , little Scotties or whatever. But the farmed fish aren't are not completely vegetarian. Soybeans don't have skeletons, they don't have bones . and the bones in fish meal are a major source of mineral minerals for the fish, for example. And much like humans, fish diets require omega-3 fatty acids, not found in terrestrial plants. So Hardy has to add a little fish-oil back into the plant-based feed. For those of us without waiterswaders, this kind of aquaculture is our best shot at a fish fry.\nWords worth to be remembered: aquaculture: n. 水产养殖；水产业 get by: phr. 过得去；过活；通过 trout: n. 鳟鱼，鲑鱼 wader: n. 步涉者，涉禽类，钓鱼用的防水长靴 譯文 全世界的人消费的所有鱼类中,有一半现在都来自于渔场.所以渔民们需要更努力工作来保证供给.如果从长计议, 按照当今全世界的人均鱼类消费水平来看,未来水产养殖业的产量需要达到翻倍才可以供应的上.\n这是Ron Hardy, University of Idaho水产养殖研究的负责人.他最近在Idaho 的太阳山谷Sun Valley举行的渔业营养与养殖国际论坛上发表了他的相关研究, 他同时也是这次会议的主席.\n在野生地区, 红鳟鱼吃昆虫或者是其它小鱼.但是Hardy说, 在野生环境中可没有那么多小鱼供应给大一点的鱼, 随着人类数量的增加, 也没有那么多供应给人类. 所以他就选择性的 用比小鱼便宜的食物饲养人工鱼苗——就是用大豆，玉米和小麦做成的鱼食。\n有些鱼苗大量繁殖成功：16年前，Hardy 不得不需要等一年才能得到一个一磅重的鳟鱼。近年来，他的努力所得到的鳟鱼，在同样的养殖时间内在体格上已经达到原来的四倍。所以，比如说你之前饲养的是小狗， 那么我们现在已经有从德国Rottweilers 罗特韦尔犬到小型苏格兰犬等各种品种了。\n但是，渔场的鱼并不是完全素食的。大豆没有骨架，它们没有骨头而鱼食中的骨头是鱼类所需的比如说矿物质的主要来源。\n和人类十分相似的是，鱼的膳食中也需要欧米伽3不饱和脂肪酸，这在陆地植物中是不含有的。所以，Hardy不得不在植物性的鱼食中再添加一点儿鱼油。对于我们那些不是捕鱼高手的人来说，这种水产品可是我们在选择炸鱼食品中最好的选择了吧。\n","date":1476316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1476316800,"objectID":"f8da72f8fa7c246b44ce26071adae27c","permalink":"https://wangcc.me/post/2016-10-13/","publishdate":"2016-10-13T00:00:00Z","relpermalink":"/post/2016-10-13/","section":"post","summary":"Some Malaria Mosquitoes May Prefer Cows to Us","tags":["dictation","English Learning","Listening","60s Science"],"title":"挑食的蚊子","type":"post"},{"authors":null,"categories":["dictation"],"content":"Farmed Trout Bred to Fatten Up Fast\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-10-7 18:36\t用时：20:18正确率：91%\t错词：24个\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nHalf of all fish people eat worldwide now come from fish farms. So farms need to do more to keep up with demand. And If we look to the future, at today's per capital capita fish intake around the world . we will would need to double aquaculture production. Ron Hardy is the University of Idaho's Director of Aquaculture Research. He presented his research at a the recent International Symposium on Fish Nutrition and Feeding in Sun Valley, Idaho, which him he also chaired. In the wild, rainbow trout eat insects and other, smaller fish. But Hardy says there aren't enough little fish to feed larger fish in the wild and still meet market demand as the human population increases. So he's used selected selective breeding to create strains of farmed fish that get by on food that's less expensive than little fish . - feed made from soybeanscorns , corn and weedwheat. Some of the farmed fish really thrived. thrive: 16 years ago, Hardy had to wait a year for a one-pound trout. These days, his efforts yield trout up to four times as large in the same amount of time. So, it become like would be kind of like if you are were going to you know breed a, I don't know , box ? dogs. So we've got everything from Rottweilers to , you know , little Scotties or whatever. But the farmed fish aren't are not completely vegetarian. Soybeans don't have skeletons, they don't have bones . and the bones in fish meal are a major source of mineral minerals for the fish, for example. And much like humans, fish diets require omega-3 fatty acids, not found in terrestrial plants. So Hardy has to add a little fish-oil back into the plant-based feed. For those of us without waiterswaders, this kind of aquaculture is our best shot at a fish fry.\nWords worth to be remembered: aquaculture: n. 水产养殖；水产业 get by: phr. 过得去；过活；通过 trout: n. 鳟鱼，鲑鱼 wader: n. 步涉者，涉禽类，钓鱼用的防水长靴 譯文 全世界的人消费的所有鱼类中,有一半现在都来自于渔场.所以渔民们需要更努力工作来保证供给.如果从长计议, 按照当今全世界的人均鱼类消费水平来看,未来水产养殖业的产量需要达到翻倍才可以供应的上.\n这是Ron Hardy, University of Idaho水产养殖研究的负责人.他最近在Idaho 的太阳山谷Sun Valley举行的渔业营养与养殖国际论坛上发表了他的相关研究, 他同时也是这次会议的主席.\n在野生地区, 红鳟鱼吃昆虫或者是其它小鱼.但是Hardy说, 在野生环境中可没有那么多小鱼供应给大一点的鱼, 随着人类数量的增加, 也没有那么多供应给人类. 所以他就选择性的 用比小鱼便宜的食物饲养人工鱼苗——就是用大豆，玉米和小麦做成的鱼食。\n有些鱼苗大量繁殖成功：16年前，Hardy 不得不需要等一年才能得到一个一磅重的鳟鱼。近年来，他的努力所得到的鳟鱼，在同样的养殖时间内在体格上已经达到原来的四倍。所以，比如说你之前饲养的是小狗， 那么我们现在已经有从德国Rottweilers 罗特韦尔犬到小型苏格兰犬等各种品种了。\n但是，渔场的鱼并不是完全素食的。大豆没有骨架，它们没有骨头而鱼食中的骨头是鱼类所需的比如说矿物质的主要来源。\n和人类十分相似的是，鱼的膳食中也需要欧米伽3不饱和脂肪酸，这在陆地植物中是不含有的。所以，Hardy不得不在植物性的鱼食中再添加一点儿鱼油。对于我们那些不是捕鱼高手的人来说，这种水产品可是我们在选择炸鱼食品中最好的选择了吧。\n","date":1475798400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1475798400,"objectID":"6a652cf1ac196791b8bd7d814445ea0d","permalink":"https://wangcc.me/post/2016-10-7/","publishdate":"2016-10-07T00:00:00Z","relpermalink":"/post/2016-10-7/","section":"post","summary":"Farmed Trout Bred to Fatten Up Fast","tags":["dictation","English Learning","Listening","60s Science"],"title":"吃魚油的魚","type":"post"},{"authors":null,"categories":["dictation"],"content":"Silk Road Transported Goods\u0026ndash;and Disease\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-10-6 12:2\t用时：68:10 正确率：91%\t错词：43个\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点 For thousand thousands of years, what\u0026rsquo;s called the Silk Road was a group of lands land and sea trade routes that connected the Far East with South Asia, Africa, the Middle East , and southern Europe. Of course, when humans travel , they carry their pathogens with them. So scientists and historians have wondered if the Silk Road was a transmission route , not just for goods, but for infectious disease. Now we have the first hard evidence of ancient Silk Road travelers spreading their infections. The find comes from a 2000-year-old 2,000-year-old latrine , that had first been excavated in 1992. The report is in the Journal of Archaeological Science: Reports. So the site is a relate relay station on the Silk Road in northwest China. It\u0026rsquo;s just to the eastern end of the Tarim Basin, which is a large and varied arid area . It\u0026rsquo;s just to the east of the Taklamakan desert, and not far from the Gobi Desert. So this is a dry part of China. Piers Mitchell, a paleopathologist , at the University of Cambridge, and one of the study\u0026rsquo;s authors, along with a his student Ivy Yeh and colleagues in China. In the latrine, archaeologists found used hygiene sticks rap wrapped with clothescloth. These were used for what you think they were used for. This escalation excavation was great . because the clothes were cloth was still preserved in and the fezzes which feces was still adherent to the clothes cloth on some of the sticks. So the archaeologists tagged archaeologist kept these sticks in the museum. And so my Ph. D. student, Ivy Yeh, who is the who\u0026rsquo;s first author of on the paper. , she went out to China took some scrapings from the fezzes feces adherent to the clothes cloth. So we were then able to analyze that down the microscope when she brought it back to Cambridge. Where they found legs eggs from parasites - including one from a liver fluke. And that\u0026rsquo;s the exciting one because that\u0026rsquo;s only found in East eastern and southern China and in Korea. , where they have marched the marshy areas that would have the right snails and the right fish. The fluke needs snails and fish for its lifecycle. , but there were no such snails or fish in this dry region of China. So the unlucky travelers traveler who harbored the parasite had to have transported the disease to that spot. Well , firstly it tells us that people were doing a very long journey, journeys along the long Silk Road . and you might think that\u0026rsquo;s obvious. But no one really knew how long people were traveling. Some people may have been trading, don\u0026rsquo;t need to go only going short distances selling their goods on to the next person. And so the goods might have gone all over the way on along the Silk Road, but people might not. But we know that some people were doing huge distances. Secondly , it shows that you know this was, would be a viable route for the spread of those other infectious diseases like Bubonic plague , and leprosy , and anthrax . that people had previously been suggested might have been spread between East Asia and Europe along the Silk Road. Because bone modern genetic analyses have just shown similarities between the strains of one end and the other. Mitchell says there is there\u0026rsquo;s much more work to be done to better understand the spread of diseases around the world. Perhaps from analyzing skeletons - or various other kinds of remains - to be found along the Silk Road.\nWords worth to be remembered: latrine: n. a public toilet in a military area relay: n. 接力赛；替班；中继设备；转播，传送 v. 传达；转播，传送；（使）接替 arid: adj. 干旱的，干燥的；贫瘠的，不毛的；枯燥无味的 archaeologist: n. 考古学家 excavation: n. 挖掘，发掘；挖，开凿 Taklamakan desert: 塔克拉玛干沙漠 feces: n. 排泄物，渣滓 Gobi Desert: 戈壁沙漠（蒙古和中国西北部） marshy: adj. 多沼地的，湿地的，沼地的 Bubonic plague: n. [医]黑死病,淋巴腺鼠疫 leprosy: n. 麻疯病,腐败 athrax: n. 炭疽热 譯文 几千年前， 有个被称为丝绸之路的地方就是一段陆地和海洋上的贸易路线图，贯穿远东和南亚，中东以及欧洲南部。 当然，那时候人们在旅途中也是会携带者病菌的。所以，科学家和历史学家早就好奇，是不是丝绸之路不仅仅是贸易通商之路，还是传染性疾病的传播之旅阿。\n如今，我们找到第一手资料能够证明古老的丝绸之旅上的商旅们确实也传播了他们感染的传染病。这些证据来自于1992年首次挖掘出的一个具有2000年的公厕。该研究报告已经发表在《考古科学：通报》杂志上。\n所以这个位点是一个中国西北部丝绸之路上的一个驿站。它通往塔里木盆地的东部末端，是通往塔克拉玛干沙漠东部的一片较大的草木荒芜地区， 离戈壁滩不是很远。所以这里是中国的一块非常干燥的土地。\n这是Piers Mitchell,剑桥大学的一位古生物病理学家，也是这项研究的作者之一，其它参与者是他的学生Piers Mitchell,和中国的一些同事。在这个公厕里，考古学家们发现了使用过的包裹着布料的卫生棒。这些东西就是你想象中的需要用到的那些东西。\n这次挖掘的意义重大，因为，织物的保存还是完好的，而粪便仍然还粘附在棒子上的布料上的某些地方。所以考古学家们把这样的一些棍子保存在了博物馆里。并且因此我的博士生Ivy Yeh, 他就是这篇文章的第一作者，她就去中国从那些布料上刮下了一些粪便样品。当她回到剑桥之后，我们才能用显微镜观察分析。他们发现了一些寄生虫的卵——包括一种肝吸虫。\n因为这种东西只在东部和南部的中国以及韩国存在，所以这个发现还是令人兴奋的。那些地区 都有沼泽地区，里面生存着合适的宿主蜗牛和鱼类。这种肝吸虫的生活史中需要蜗牛和鱼类，但是在这些干燥的中国地区是没有这类蜗牛和鱼类的。所以，被寄生的这些不幸的旅行者们不得不把这种疾病带到了这个站点。\n首先，这告诉我们，人们在丝绸之路上行走了相当长的一段距离，并且，你也许认为这还不时很显然的事情吗。但是，没有人真正了解人们具体行走了多长的距离不是吗。一些人也许一直会做着贸易，只走了较短的距离后就把他们的货物卖给下一个人。而这些货物也许被运送了全程，而人们就不一定都走了全程的。但是我们子回到有些人确实走了很远的距离。\n第二点，这表明，这是，也许是传播像 Bubonic plague腺鼠疫，麻风病和炭疽热等其它类传染性疾病的一个可行的途径，这些疾病人们以前曾经假设过也许就是在中亚和欧洲地区沿着丝绸之旅开始流行传播的。因为现代遗传分析已经显示出在这条路一端和另外一端的菌株之间的遗传相似性。\nMitchell 表示，还有很多工作需要做，这样将有助于更好的理解全世界的疾病传播情况。或许是通过分析遗迹残骸骨架——或者其它种类的残留物——未来在丝绸之路上所发现的那些东西，就可以了解了。\n","date":1475712000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1475712000,"objectID":"1a6c03fabd0d24332b9b78949eb6897a","permalink":"https://wangcc.me/post/2016-10-6/","publishdate":"2016-10-06T00:00:00Z","relpermalink":"/post/2016-10-6/","section":"post","summary":"Silk Road Transported Goods--and Disease","tags":["dictation","English Learning","Listening","60s Science"],"title":"絲綢之路上的傳染病","type":"post"},{"authors":["Takayuki Sawada","Takeshi Nishiyama","Norimasa Kikuchi","Chaochen Wang","Yingsong Lin","Yoshiyuki Watanabe","Akiko Tamakoshi","Shogo Kikuchi"],"categories":null,"content":"Abstract Breast cancer is the most common cancer in women. However, it remains unproven whether psychological factors have an influence on breast cancer incidence. In our earlier study, subjects possessing two personality traits, decisiveness and “ikigai” (a Japanese word meaning something that makes one’s life worth living), showed a significantly lower risk of developing breast cancer, although no psychological factors have been convincingly demonstrated to have an influence on breast cancer development in other studies. Therefore, we conducted this follow-up analysis to confirm the association between breast cancer incidence and psychological traits, using the final dataset of a large-scale prospective cohort study in Japan. We identified 209 cases of incident breast cancer out of a maximum 21-year follow-up of 29,098 Japanese women. Cox proportional hazard regression analysis, adjusted for the same potential confounders used in our previous study, did not reveal any significant relationships between breast cancer incidence and four psychological traits: having “ikigai”, decisiveness, ease of anger arousal, and perceived stress. Our finding is consistent with previous studies, and suggests that the psychological traits are unlikely to be an important risk factor for breast cancer.\n","date":1472774400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1472774400,"objectID":"bf677d83ab7787a97c84168b3f8a3fbf","permalink":"https://wangcc.me/publication/journal-article/jaccbreast/","publishdate":"2016-09-02T00:00:00Z","relpermalink":"/publication/journal-article/jaccbreast/","section":"publication","summary":"Abstract Breast cancer is the most common cancer in women. However, it remains unproven whether psychological factors have an influence on breast cancer incidence. In our earlier study, subjects possessing two personality traits, decisiveness and “ikigai” (a Japanese word meaning something that makes one’s life worth living), showed a significantly lower risk of developing breast cancer, although no psychological factors have been convincingly demonstrated to have an influence on breast cancer development in other studies.","tags":null,"title":"The influence of personality and perceived stress on the development of breast cancer: 20-year follow-up of 29,098 Japanese women","type":"publication"},{"authors":null,"categories":["dictation"],"content":" Your browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 Inbred Songbirds Croon out of Tune\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nJust like humans have to learn to talk. Some birds , songbirds aren't born singing . - they have to learn to carry a tune. From So in the beginning , they just bubblebabble. Raissa de Boer, a behavior behavioral ecologist at the Unversity University of Antwerp in Belgium. And they learn from a tweetertutor, so they need an example song in order to learn it. She says that the example song might come from the chick's father. And over time, the baby bird tweets tweaks that tweet, to make it a songits own. And then it takes almost a year until they are they're fully adults adult, and until the next spring, for the final song to come out. De Boer and her colleagues investigated that song-learning process in canaries, using two groups of baby birds. : the first consists consisted of inbred birds, whose parents were siblings, ; the second had the parents that were unrelated. And the researchers found that the songs of inbred birds , and those of the other, outbred birds . Sounds sound pretty similar to the human ear. I cannot tell the difference. But computer analysis revealed that the inbred birds sang notes at slightly different pitches - and with tones that were not quite as pure. So basically the sing they sang out of tune, in comparison to outbred birds. The results appear in the Proceedings of the Royal Society B. Even though our untrained ears have a hard time telling the tones apart. , female canaries seem seemed to notice. They turned tended to lay smaller eggs, and fewer ovum of them, when they meet mated with inbred birds as opposed with to the better songsters. Suggesting that the quality of the songs' gene a songbird's genes may be revealed in its tunes. Words wortsh to be remembered: songbird n. 鸣禽,鸣鸟,女歌手 babble n. 潺潺声；胡言乱语 v. 喋喋不休；含糊不清地说，呀呀学语；作潺潺声；泄露 Antwerp 安特衛普, 比利時城市 tweak n. 拧，捏，扭；苦恼；微调，改进 v. 拧，捏，扭；稍稍调整 canary 金丝雀,告密者,歌女 (复数) inbreed vt. 使同系繁殖,使近亲交配,使在内部生成 inbred adj. 天生的,生来的,同系繁殖的 outbreed vt. 使远系繁殖 pitch n. 投；投球；音高；程度；沥青 v. 投掷；为…定音高；搭帐篷；用沥青涂 songster n. 歌手,作曲者,诗人,鸣鸟 譯文 就像是人类不得不学会說話谈话, 夜莺们也不是生来就会唱歌滴——它们也不得不去找准旋律。 所以在一开始它们也就是那样呀呀学语。这是Raissa de Boer， 一位比利时安特卫普大学的行为生态学家。而它们需要跟随一位导师，为了学会唱歌它们还需要一首练习曲。\n她说,鳥兒們的练习曲也许是来自于它们的老爸。并且随着它们慢慢长大，小鸟会纠正一下自己的鸣叫，找到适合自己的style. 随后，还大约需要一年的时间直到它们完全长大后，直到一年后的春天，最后的练习结果便要出炉。\nDe Boer 和她的同事们调查了金丝雀的音乐学习之路，她们用的是两组幼鸟：第一组是近亲繁殖的后代，它们的父母是有血缘关系的。第二组的不是近亲的后代。随后研究人员们发现，近亲后代的鸟叫和非近亲后代的鸟叫对人类的耳朵来讲，没什么差别。我就分不清谁是谁呢。\n但是，通过电脑分析之后，结果揭示出，近亲的小鸟们有一些不同的音调——并且音色也不是十分纯正。所以它们基本上是唱跑调了，当然是和非近亲的后代相比。 该研究结果已经发表在《皇家协会进展B》杂志上。\n而且即使是我们这些没有经过培训的普通听者，都很难分辨出这些音色上的区别，雌性金丝雀貌似是了解的这一点的。当她们和近亲的鸟类交配时，更倾向于产个头小一些并且，数量也少一些的卵，仿佛这样并不利于产生优良歌声的后代。这提示我们，一个夜莺的基因质量也许从它们的音色上就可以看出来一些呢。\n","date":1470873600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1470873600,"objectID":"badb131ccd6aa4dc53d207bec06400eb","permalink":"https://wangcc.me/post/2016-08-11/","publishdate":"2016-08-11T00:00:00Z","relpermalink":"/post/2016-08-11/","section":"post","summary":"Inbred Songbirds Croon out of Tune","tags":["dictation","English Learning","Listening","60s Science"],"title":"近親繁殖讓鳥兒不會唱歌","type":"post"},{"authors":null,"categories":["dictation"],"content":"Married Couples Pack On More Pounds\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-8-10 12:47\t用时：25:23 正确率：95%\t错词：13个\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点 Bridget Jonesand , in print ant and on screen, called them the \" Smug Marrieds\" . - the happy couples that seem seemed to have it all. But maybe the fictional Jones should have called them the \" Plump Marrieds\" . \" Because along with our friend offering couples unbright unbridled bliss. , marriage can cause them to pack on some extra pounds. That's according to a study in The Journal of Family Issues. Sociologist Jay Teachman, at Western Washington University, examined data from the National Longitudinal Study of Youth. The data included in fell info about more than 3,000 African Americans over a 20-year period. Teachman tracked body mass indexbody-mass-index, BMI. , a measure of obesity, from adolescence to middle age. And he analyzed the relationship between BMI, marital status , and changes in marital status. It turned out that living without a partner usually equated to being thinner and having a lower BMI compared with married people and couples living together. The single folks included the never-marrieds and the divorced. Both men and women gained weight . but when it came to race, black women had the most rapid weight gain, followed by white women and then black and white men. The weight gain was just a few pounds . - but even a slighted slightly higher BMI is associated with weight-related health issues. Several reasons exist for the weight discrepancy between the single and married people. For example, married men and women may be less concerned about their body weight because they are they're no longer actively seeking a mate. Plus, married people have a regular dining partner. , possibly leading to more meals. On the single side. , those who were are widowed , or have gone through a divorce may lose weight due to stress. So , well, while this news may not be good for the \" smug marrieds\" . , it may be welcomed by Bridget Jones. The single, and very weight-conscious, Jones may actually of have had the easier path to staying thin than her married friends.\nWords worth to be remembered: unbridled adj. 无缰辔的,无羁勒的,无拘束的 smug adj. 沾沾自喜的，自以为是的；体面的 n. 自命不凡的人；书呆子 plump adj. 丰满的；充裕的；鼓起的 adv. 沉重地；突然地 n. 扑通声 v. （使）鼓起；变丰满，使丰满；（使）突然沉重地落下 譯文 Bridget Jones, 在印刷出版物或者荧屏上的动态形象上,都会称他们是得瑟的一对——那些欢喜冤家看上去十分美满。但是也许故事中的Jones本应该称他们为富态的一对。 因为，除了提供给夫妻双方无拘束的幸福生活以外，婚姻还会造成他们的体重直线上升。这是根据一项发表在《家庭报》杂志上的一项研究得出的结论。\n西华盛顿大学的社会学家Jay Teachman，调研了全国青年纵向调查得到的数据。这些数据包括了对超过3000名的非洲裔美国人进行了长达20年的调查的所有信息。\nTeachman调查了身体质量指数 BMI， 一种从青年到中年都是用(適用)的，衡量肥胖症的重要指标。然后他分析了BMI与体能(婚姻)状态以及体能(婚姻)状态改变之间的关系。结果显示，和已婚并且住在一起的夫妇相比较，没有伴侣的人通常较为苗条并且有比较低的BMI。单身人士中包括了一直未婚和已经离婚的人。\n夫妻双方的体重都会增加，但是从种族的角度看，黑人妇女们的体重增长的最快，其次是白人妇女，然后才是黑人以及白人男士。说体重增加其实也不过就是几磅而已——可是BMI就算是少量的增加，也是密切联系着与增重相关的健康问题(密切相關)。\n造成单身和已婚人士之间的体重的差异有很多原因。比如说，已婚男士和女士也许因为不在需要主动寻找合适的配偶所以减少了对自己体重的重视。此外，已婚后夫妻双方一般都会一起吃晚餐，这也可能使得他们(比單身者日常飲食次數更多)的饭量增加吧。从单身的角度看，寡居或者正在离婚的人士也许会因为压力而造成体重下降。\n所以尽管这则消息对得瑟的夫妇们来讲或许并(不)算好消息，但对Bridget Jones来讲倒是还不错。这个单身的，十分关注体重的Jones, 也许真的有比她那些已婚朋友们更简便的保持苗条的方法。\n","date":1470787200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1470787200,"objectID":"eb42cf81c35341277d16f36073bc9120","permalink":"https://wangcc.me/post/2016-8-10/","publishdate":"2016-08-10T00:00:00Z","relpermalink":"/post/2016-8-10/","section":"post","summary":"Married Couples Pack On More Pounds","tags":["dictation","English Learning","Listening","60s Science"],"title":"婚后易发胖","type":"post"},{"authors":null,"categories":["dictation"],"content":"Evolution Ed Defenders Make Rapids Progress in Grand Canyon\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-8-5 15:29\t用时：23:29 正确率：89%\t错词：23个\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点\nSo what is does rafting down the Grand Canyon have to do with science education? Ann Reid, the former researcher research biologist and current executive director of the National Center for Science Education. She spoke to me July 7th at the Fern Glen Canyon campsite . the morning of our penultimate date day rafting down the Colorado River in the Grand Canyon. Well, the for NCSE is it's one of the most powerful places on the Earth to show the difference differences between religious thinking and scientific thinking. Because for a very small minority of Christians who believe the Earth have been is 6,000 years old, the Grand Canyon is the best evidence , they have of Noah's flood. Creation is to Creationists run multiple raft trips down the Colorado each year. A Web site touting sums, some says the trips will encourage your faith as we reveal the truth of God's creation found in this gigantic reminint remnant of the Biblical Flood. Which creation is believed creationists believe occurred some 4,400 years ago. And of course , scientists of many, many different subdisciplines have figured out that the canyon is much older than that. There is, there's the rocksthere is , there's the biologythere is , there's the hydrology. , it's just a fantastic place to learn how science explains scientists explain the world around us. The annual NCSE Colorado River trip holds about two dozen guests and features talks by a geologist and an evolutionary biologist. This year, two public school teachers received all-expense paid scholarship trips . so they could bring their experience experiences and learning , back to the classroom. And I record recorded lots more audio, which will I'll be posted in posting as Scientific American Science Talk podcasts in the coming weeks.\nWords worth to be remembered: Grand Canyon pla.大峡谷{GCN,美国} penultimate adj. 由字尾倒数第二的 campsite n. 露营地（为游客所设），野营地，度假营地，营地 Creationist n. 神灵论者，上帝论者 tout n. 兜售者，招揽员 v. 兜售；招徕；拉选票；标榜，吹捧，吹嘘 remnant adj. 剩余的，残余的 n. 剩余；残余 hydrology n. 水文学 譯文: 所以划下Grand Canyond大峡谷和科学教育有神马关系呢？\n这是Ann Reid，之前是生物学研究员，现在是国家科学教育中心（NCSE）的执行主席。她在７月７日 Fern Glen Canyon大峡谷的露营地，和我进行了一次谈话，就在我们要用木筏划下Grand Canyon大峡谷的科罗拉多河的前一天早上。\n对于NCSE来讲，这里是地球上最强大的地方之一，可以显示出宗教和科学思想之间的差异。因为对于很小一部分相信地球存在已经有６０００年的基督教信徒来讲，Grand Canyon大峡谷是他们认为诺亚曾发生洪水的最好证据。\n神创论者们每年都会行驶多功能木筏划过科罗拉多。一个宣传网站上提及这种旅行是“将鼓励你的信仰，因为我们在圣经记录的洪水中遗留下来的庞大产物中揭示出了上帝创造一切的真相。\n当然，所有科学家，许许多多不同分支学科的科学家们已经指出，这个大峡谷的存在时间比记录中的年代要更久远，那里有岩石，那里也有生物，那里有水文地理，这里是了解科学家们如何解释我们生存的世界的极好的一个地方。\n每年举行的NCSE科罗拉多河漂流之旅可以吸引到２４名观众，以及由一位地理学家和一位进化生物学家进行解说的记录短片。今年，两个公立学校的老师收到了全奖支持的旅行，所以他们可以将他们的这次经历和学到的知识带回课堂。并且，我已经收录了许多相关音频，　之后我会在今后几周陆续放到我们科学美国人科学谈话的播客节目里。\n","date":1470355200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1470355200,"objectID":"04faf10b31a5f32e09bb6a1a4d71a4b8","permalink":"https://wangcc.me/post/2016-8-5/","publishdate":"2016-08-05T00:00:00Z","relpermalink":"/post/2016-8-5/","section":"post","summary":"Evolution Ed Defenders Make Rapids Progress in Grand Canyon","tags":["dictation","English Learning","Listening","60s Science"],"title":"科學與神學的融合之旅","type":"post"},{"authors":null,"categories":["dictation"],"content":"Powerball Lottery Winning Made Inevitable (If Not Easy)\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-8-4 13:10\t用时：44:49 正确率：90%\t错词：43个\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点 There is There\u0026rsquo;s a story in the book. , a story about people who took advantage of the law of inevitability to win the lottery. That\u0026rsquo;s mathematician David Hand, talking about his 2014 book , The Improbability Principle: Why Coincidences, Miracles and Rare Events Happen Every Day. The law of inevitability comes into play in the lottery drawings . - some set of the numbers will be drawn, so a potential winning combination is inevitable. The key word being \u0026quot; potential\u0026quot; , as nobody has yet won the multistate Powerball lottery. Which means that the jackpot for the next drawing, the night of January 13th, is up to some $1. 3 billion. You could buy a ticket and hope, . Or, as you may have amusedmused, you could buy every possible set of numbers to inevitably win. In 1992, Virginia State Lottery. , the Virginia State Lottery is a six/44 lottery . - you have to choose six numbers out of 44 , - which means it\u0026rsquo;s a one in seven million chance that a ticket, particular ticket , to will be the Jackpot winning jackpot-winning ticket. Seven million. So if you bought all the tickets, it will would only cost you $7 million. Stay So they waited until the roll of rollover jackpot to had built up to, haven\u0026rsquo;t hadn\u0026rsquo;t been won, to so it built up over several weeks to $27 million. If you managed manage to spend $7 milllion million and buy all the seven million tickets , you are guaranteed to hold the jackpot winning ticket. But there is there\u0026rsquo;s a lot of organizations organization involved in this. In fact, what happened , was they put together a consortium of 2 and a half thousand 2,500 people. , each of whom pays paid $3000, so there are about3,000 or thereabouts, so they have had $7 million. And then in a the few days\u0026rsquo; window , they had available they ran around buying , - trying to buy all the seven million tickets. \u0026quot; Trying \u0026quot; to buy, . Because the consortium only managed to buy five million tickets. So winning was not inevitable, their chances were only about 5 five out of 7seven. As it happened, however, they did have the winning ticket. , so they were guaranteed winning the jackpot. The organization beforehand, the logistics of running around trying to buy these ticketsthey may , the nail-biting looking through the tickets, it\u0026rsquo;s easier . if you just to get a job. So if you are you\u0026rsquo;re thinking of getting together a consortium for the Powerball. , keep in mind that for this lottery there is there\u0026rsquo;s only a one in 292 million chance of winning. And tickets are 2 two bucks a pop. So you and your buddies are gonna going to have to come up with almost $600 million to buy every combo and take advantage of the law of inevitability. And if others pick the same winning numbers and you have to split the winnings . you could basically break even or even lose money. To put the frenzy in perspective, I like to recall the wisdom from of statistician Michael Orkin, author of the book , What Are the Odds? Chance in Everyday Life. Back to in 2001 , the Powerball jackpot had reached ￥$295 million . and the odds back then were better, only 175 million to wonone. Orkin told me, if you have to drive 10 miles to buy a Powerball ticket, you are you\u0026rsquo;re 16 times more likely to get killed in a car crash on your way than you are to win. So if you death you\u0026rsquo;re dead set on buying a lottery ticket, at least walk.\nWords worth to be remembered: jackpot n. 头奖；累积赌注金；最大成功 muse n. 沉思；冥想；缪斯女神 v. 沉思，冥想；沉思地说 thereabout adv. 在那附近;大约那时 nail-biting n. 咬指甲癖性,没有办法的状态,束手无策 combo n. 联合体；结合物；小型爵士乐队 frenzy n. 狂热；狂暴；狂怒 v. 使狂怒 dead set n. 决定性攻击，猛烈攻击 adj. 堅定不移的 譯文： “()”中的是我認爲正確的翻譯 黑體字是我認爲翻譯得好的部分 书里有这样一个故事,关于那些利用必然规律赢得彩票的人们. 这是数学家David Hand, 他在谈论2014年他自己出版的一本书, 名为Improbability Principle: Why Coincidences, Miracles and Rare Events Happen Every Day.“不可能之规律: 为什么每天都会发生巧合, 奇迹以及及其少见的一些事儿.”必然规律被积极应用到了彩票抽奖当中——某些组数字会被抽中，所以猜中数字组合是一种必然性事件。 关键词是将来有可能，因为有人已经赢得多重美国谈力球彩票大奖过了。也就是说，下次抽奖的奖金，1月13日晚的那次抽奖，已经达到13亿美金。你也可以买一张试试看。或许，正如你曾默默思考过的结果那样，你也许应该把所有可能的数字组合都买下哦。 1992年，Virginia State州的那次彩票，当时是6/44型的彩票——你必须从44个数字中选出六个——也就是说这是个7百万分之一的可能性事件，如果某一个组合的彩票猜中大奖。700万種组合哦。所以如果你买下所有可能的组合，也只会花掉7百万美元而已。 所以他们等待转动奖池积累剩余奖金，所以，这就积累了好几周，最后达到2千7百万美金。 如果你打算花7百万买下所有可能中的彩票，你也必然会最终中奖的。 但是，有好多不同组织参与运作。 实际上，当时的结果是，他们组成了一个2500人的联合机构，每人花差不多3000美金，所以他们最终共同出资7百万美金。 然后，在之后几天里，他们轮流去买彩票——尽可能的买下所有可能出现的中奖组合。 只是尽量去买。因为，这个联合机构之打算买下(最终也只成功买到)5百万张彩票而已。所以获奖概率就不是必然的了，他们的获奖概率就是5/7。 然而事实是，他们最终还是获得大奖，所以他们确实获得了积累的所有奖金。这个联合组织预先，组织大家轮流买这些可能获奖的彩票，紧张的计算彩票获奖概率，比一般工作可难多了。 所以如果你正考虑召集这样一个Powerball彩票联合购买机构，一定要记住这种彩票的获大奖概率只有2亿9千2百万分之一。而彩票是两美金一个号码。所以你和你的伙伴们将需要告到6亿美金才能买到每一个可能的组合，并且利用到这种必然事件规律。如果有其它类似机构购买了同样有可能获奖的号码组合，你们就不得不分享最终大奖，那么你有可能最后只是收支平衡或者甚至还要亏本呢。 为了正确激发你的热情，(不是給你的熱情澆冷水，)我不禁回想起统计学家Michael Orkin的智慧，他也是《可能性是多少？每天的生活中都有机会。》这本书的作者。回看一下2001年Powerball的奖池已经积累到了2亿9千5百万美金并且那时的概率也搞，只有一亿7千5百万分之一。Orkin告诉我说，如果你需要驱车10英里去买Powerball彩票的话，你在行车途中出车祸死亡的概率是你能赢得大奖概率的16倍以上。所以，如果你十分热切的(依然意志坚定地)要买张彩票的话，至少走路去啦。\n","date":1470268800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1470268800,"objectID":"4383f52ecbb3c3851e6bb331bb759f90","permalink":"https://wangcc.me/post/2016-8-4-powerball/","publishdate":"2016-08-04T00:00:00Z","relpermalink":"/post/2016-8-4-powerball/","section":"post","summary":"Powerball Lottery Winning Made Inevitable (If Not Easy)","tags":["dictation","English Learning","Listening","60s Science"],"title":"彩票中獎概率之謎","type":"post"},{"authors":null,"categories":["dictation"],"content":"Fat Gets Gut Bacteria Working against the Waistline\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-8-3 13:55\t用时：26:08 正确率：93%\t错词：20个\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点 Think it\u0026rsquo;s your inability to resist cheesecake that\u0026rsquo;s making it tough to fit into your skinny genesjeans? Well, your bacteria may share some of the blame. Because a new study in mice shows that the responsive response of intestinal microbes to a high-fat diet ends up triggering the release of a hormone that makes mammals feel hungry, causing them to eat even more. The finding is served up in the journal Nature. Previous work had has shown that the types of bacteria in the gut in diabetic or obese individuals are different from the bacteria in healthy people. But does these bacteria make a this bacterial makeup contribute to these disorders, ? Or is it just a side effect? To unravel this mystery, researchers put mice on a high-fat diet. The animals experienced to built a buildup of a chemical called acetate, particularly in the large intestine. That location points to gut bacteria, which can produce acetate, as a possible culprit. So the researchers wiped out the microbes using antibiotics or a simple saline wash. And acetate levels plummeted. Okay, so the gut bacteria in fat-fed mice make acetate. What does acetate do? Well, it gets the involuntary part of the nerve nervous system, the parasympathetic nerve nervous system. , to put out the call to produce more insulin. Unfortunately, in this case, acetate also gets the parasympathetic nerve nervous system to stimulate the production of a hunger hormone called ghrelin. And the more fat fats an animal consumes, the more acetate it makes . - which means the more ghrelin it produces , and, of course, the more it eats. And bacteria make the whole sequence happen. The researchers are now investigating , whether the same biochemical events happen in humans. If they do, it\u0026rsquo;s possible that obtaining a better assortment assortments of gut bacteria could help us control our weight. Of course, the best way to get those good bacteria is from a fecal transplant . - in which bacteria-rich feces come out of one person and into you. The very thought of which could help curb your appetite.\nWords worth to be remembered: serve up phr. 端上，提供，提出 buildup n. 组织,组成,增强 acetate n. 醋酸纤维（素） plummet n. 铅锤；坠子 v. 垂直落下；骤然下跌 assortment n. 花色品种；混合物；分类 curb n. 限制；路缘，路边 v. 控制，抑制 ghrelinn n. 胃饥饿素（一种胃肠道激素） saline adj. 含鹽的，鹽的 n. 鹽湖；鹽溶液 譯文： “()”中的是我認爲正確的翻譯 黑體字是我認爲翻譯得好的部分 认为是你自己难以控制的吃奶酪蛋糕才导致穿不进去紧身牛仔裤？好吧，你体内的细菌也许也要付一些责任的。因为一项最新研究显示，在小鼠的实验中显示肠道内的微生物对高脂肪的膳食做出的响应就是结束开始释放一种可以使哺乳动物感到饥饿的荷尔蒙(激素)，导致他们吃的更多了。该研究已经发表在《自然》杂志上。\n之前的研究工作已经显示出糖尿病或者肥胖的个体肠道内的细菌种类与健康人肠道内的细菌种类是不同的。但是，这种细菌组合确实(會造成)有助于这些(疾病，指的糖尿病或者肥胖)体内代谢紊乱的形成吗？或者这其实只是代谢紊乱后的(疾病的)副作用？为了揭开这个谜团，研究人员们喂养实验小鼠高脂肪的膳食。这些小动物们呈现出对一种(类)名为乙酸(鹽)的化学物质的积累，特别是在大肠内。\n定位显示它们属于肠道细菌，可以生成乙酸(鹽)， 所以它们既有可能就是始作俑者呢。因此研究人员们用抗生素或者一种简单的鹽水清洗方法来清除掉可能存在的微生物。 结果发现乙酸(鹽)的含量显著下降。好吧，所以就是吃脂肪的小老鼠们体内的肠道细菌制造出的乙酸。那么这些乙酸(鹽)是用来干神马的？额，它可以到达神经系统的非自主部分，就是副交感神经系统，用来熄灭要求制造更多胰岛素的信号。不幸的是，这种情况下，乙酸(鹽)还作用于副交感神经系统来刺激制造一种饥饿激素又名生长激素释放肽。而一只动物所消耗(攝入)的脂肪越多，它产生出的乙酸(鹽)就越多——也就是说有更多的饥饿激素被制造出来，当然，它也会因此吃更多的食物。而细菌便是这整个连锁反应中的起始环节呢。\n研究人员现在正在研究是否在人体内也有类似的生化反应事件的发生。如果答案是肯定的，那么(人類很有可能通過獲得較優質的腸內菌羣來控制體重。)人们将有可能获得一种更好的分类肠道细菌的方法，并将有助于我们控制自己的体重。当然，最好的方法是从移植(含有優質腸內細菌的糞便)实验中找到那些有益的细菌——也就是从一个人体内排出的富含细菌的粪便，移植到你的体内。理想情况下这种方法可以有助于抑制你的食欲。(這畫面光想一想就能抑制你的食欲啊！！)\n","date":1470182400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1470182400,"objectID":"03cb55cd33831b5cd8844979dbf7c8c9","permalink":"https://wangcc.me/post/2016-08-03-gutbacteria/","publishdate":"2016-08-03T00:00:00Z","relpermalink":"/post/2016-08-03-gutbacteria/","section":"post","summary":"Fat Gets Gut Bacteria Working against the Waistline","tags":["dictation","English Learning","Listening","60s Science"],"title":"肠道菌减肥计划","type":"post"},{"authors":null,"categories":["dictation"],"content":"正確率：87%\nYour browser does not support the audio element. Bees Rank Pollen by Taste\n提示：红色：错误单词，绿色：补上正确单词，\u0026lt;spanclass=\u0026ldquo;diff_alert\u0026rdquo;\u0026gt;黄色：纠正大小写与标点 Walk through Time Times Square , - you\u0026rsquo;re bumbled bombarded with advertising. And it turns out, a bumblebee , might have a similar feeling, buzzing through a field of flowers. So these flowers , are these build balls of billboards, they\u0026rsquo;re advertising are good. It\u0026rsquo;s a good, this delicious nectar to the worldreward, and bees are very picky shoppers. Anne Leonard, a pollination biologist at the University of Nevada, Reno. She describes the a flower field as a sort of pollination marketplace. And one way , bees choose where to visit. ? Bees are nectar experts. They are really good at assessing even really small differences , in like sugar concentration of nectar. They also scope out the shape and size of flowers and their color and scent. And now Leonard and her colleagues have discovered that bumblebees are pollen aficionados, too. They found out that out by lacing batches of cherry pollen with either table sugar or bitter quinine. And they to display the pollen to bees. , we got really into it , - we set up a started 3-D printing flowers in our lab. And for the answer, anther - the male flower part, which presents the pollen - pipe cleaners. So we\u0026rsquo;ve we bought out Michael\u0026rsquo;s craft store\u0026rsquo;s store supplies of these pipe cleaners and used them in our experiments. It Turns out that bees will would return again and again to the same colored color flower that dispenses dispensed sweet pollen, and spent spend more time collecting there. But when confronted with the bitter pollen? They sought a different color colored flower for the their very next stop. All of which suggest suggests that, in addition to savor in savoring nectar, bees taste pollen too - and judge flowers by it. The results are in the journal Biology Letters. The finding means that plants have to find a happy medium. : So can you make your pollen attractive enough , that the bees will collect it, but distasteful stuff enough that they won\u0026rsquo;t collect too much of it. ? And that balancing act, of carefully collaborative calibrated chemistry is - it\u0026rsquo;s just one of the many transactions that plays out in a the buzzing pollination marketmarketplace. Well, Where the objective object is to make a sweet profit.\nWords worth to be remembered: bombard n. 射石炮 v. 炮击，轰炸；连珠炮似的质问（或批评） bumblebee n. 大黄蜂 billboard n. 广告牌；布告板 v. 宣传 nectar n. 神酒，甜美饮料，甘露，花蜜 pollination n. 授粉 pollen n. 花粉 v. 传授花粉给 quinine n. 奎宁 savor n. 滋味；气味；食欲 v. 品尝；尽情享受；意味着，带有…的性质；有…的滋味，加调味品于 distasteful adj. 味道差的,不愉快的,讨厌的 calibrate v. 测定；校准，调整 anther n. 雄蕊的花粉囊,花药 buy out phr. 全部買下 譯文： “()”中的是我認爲正確的翻譯 黑體字是我認爲翻譯得好的部分 穿过时代广场——你会被各种广告震撼(轟炸)的。可是有研究结果显示，一只大黄蜂飞过一片花海的话，和你也会有同样的感觉呢。所以这些花儿就像是这些广告牌，它们也是在宣传一件商品，那就是这美味的花蜜了，并且蜜蜂其实是很挑剔的顾客哦。 这是Anne Leonard， 里诺市的内华达大学中的一位研究授粉的生物学家。\n她把一片花海比作为一种花粉市场。那么蜜蜂会选那个方向开始工作呢？ 蜜蜂们可是花蜜研究专家呢。它们 真的很善于对甚至是花蜜中的糖含量这样的细微差别做出评判。它们也会打量一下花的外形和大小，颜色以及气味。现在， Leonard和她的同事们已经发现，大黄蜂也是花粉爱好者呢。\n他们是通过在樱桃花粉中掺杂糖粉或者苦奎宁粉的一组实验得出的结论。并且把这些花粉提供给蜜蜂后进行演示，我们真的很的对此着了魔——我们已经在实验室里用三(3)D打印机打印出花朵。而花药部分——花的雄性器官，也就是生出花粉的地方——像(用的)是烟斗通条(清理管)的管状结构。我们购买的是(買光了)Michael手工艺商品店中的这些烟斗通条(清理管)并且用在了我们的实验中。\n结果显示，蜜蜂会一次又一次的返回同样颜色的会散布甜味花粉的花朵，并且在此处花费更多的采集时间。但是当它们碰到了苦味的花粉后会怎样的？它们会选择一个不同颜色的花朵作为下一站。所有结果显示，除了喜欢花蜜，蜜蜂也会尝花粉哦——并且用这个方法来鉴定花朵的可用性。该研究结果已经发表在~~《生物学报》~~(Biology Letters不知道有沒有官方中文譯名還是保留原名的好)杂志上。\n该研究结果意味着，植物们有个愉快的合作伙伴：那么你能不能让你的花粉更有吸引力，以便蜜蜂会采集它？而不是让花粉尝起来很糟糕以至于它们都不愿采太多？ 而这种权衡行为(的方法)，也就是仔细的调整化学合成——只是在它们繁忙的花粉交易市场中达成的众多交易中的一种。在那里，唯一的目的就是要收获甜蜜。\n","date":1470096000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1470096000,"objectID":"91fcb311093bda0a3f4f0aba308c2ee2","permalink":"https://wangcc.me/post/2016-08-02-honey/","publishdate":"2016-08-02T00:00:00Z","relpermalink":"/post/2016-08-02-honey/","section":"post","summary":"Bees Rank Pollen by Taste","tags":["dictation","English Learning","Listening","60s Science"],"title":"花蜜專家","type":"post"},{"authors":null,"categories":["R techniques"],"content":"今日仰視slidify作者Ramnath Vaidyanathan的github頁面發現超酷的rMaps包，抄過來在自己電腦上實驗一下。\n下載 require(devtools) install_github('ramnathv/rCharts@dev') install_github('ramnathv/rMaps') 繪制美國2010年的各州的犯罪率 Crime Rates (per 100,000) by State at 2010 利用以下的R編碼在Rstudio裏實現上圖效果\nlibrary(rMaps) map \u0026lt;- ichoropleth(Crime ~ State, data = subset(violent_crime, Year == 2010)) map map$publish(\u0026quot;Crime Rates (per 100, 000) by State across Years\u0026quot;) # \u0026lt;- 引號中的將會是生成的動態地圖的網頁名稱 ## Loading required package: httr ## Please enter your github username: ***** # \u0026lt;- 在此處輸入你自己的github用戶名 ## Please enter your github password: ***** # \u0026lt;- 在此輸入你自己的github用戶密碼 ## Your gist has been published ## View chart at http://rcharts.github.io/viewer/***** # \u0026lt;- 會出現生成的網頁版可互動地圖鏈接 你也可以用下面的編碼將網頁保存爲獨立的html文件(注意：無網絡連接時可能無法正常顯示)\nmap$save(\u0026quot;mymap.html\u0026quot;, cdn = TRUE) # \u0026lt;- 引號中爲保存的目標文件名稱 繪制1960-2010年每年的各州犯罪率 Crime Rates (per 100, 000) by State from 1960-2010 帶滑動條的可互動地圖/Animated Choropleth 編碼:\nichoropleth(Crime ~ State, data = violent_crime, animate = \u0026quot;Year\u0026quot;) 效果:(你可以拖動左上角的滑動條顯示每年的各州犯罪率，越來越高，今年老川上臺估計全線飄高。。。)\n上圖的自動播放版/Animated Choropleth(注意左上角出現播放按鈕) 編碼:\nichoropleth(Crime ~ State, data = violent_crime, animate = \u0026quot;Year\u0026quot;, play = TRUE) 效果:\n以下为2016年6月2日更新 普通青年用的(伦敦市区)地图： 代码:\nmap \u0026lt;- Leaflet$new() map$setView(c(51.505, -0.09), zoom = 13) map$tileLayer(provider = 'Stamen.Watercolor') # \u0026lt;- 地图颜色为水彩效果 # map$tileLayer(provider = \u0026quot;OpenStreetMap\u0026quot;) # \u0026lt;- 无水彩效果地图 map$marker( c(51.5, -0.09), bindPopup = 'Hi. I am a popup' ) map library(rMaps) library(leaflet) map \u0026lt;- Leaflet$new() map$setView(c(51.505, -0.09), zoom = 13) #map$tileLayer(provider = 'Stamen.Watercolor') # \u0026lt;- 地图颜色为水彩效果 map$tileLayer(provider = \u0026quot;OpenStreetMap\u0026quot;) # \u0026lt;- 无水彩效果地图 map$marker( c(51.5209, -0.1303), bindPopup = 'Hi. I am in LSHTM' ) map map$save('mychart.html', cdn = TRUE) 黑白色圖:\n一般色彩圖: 以下爲自娛自樂 編碼:\nL2 \u0026lt;- Leaflet$new() L2$setView(c(35.175776, 137.040663), 13) L2$tileLayer(provider = \u0026quot;OpenStreetMap\u0026quot;) L2$marker( c(35.191379, 137.047885), bindPopup = 'Hi. I am here. | 快来打我啊！' ) L2 效果:\n","date":1464652800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1464652800,"objectID":"c5b06d8466edef1cc104b3eb3ed920da","permalink":"https://wangcc.me/post/2016-05-31-rmaps/","publishdate":"2016-05-31T00:00:00Z","relpermalink":"/post/2016-05-31-rmaps/","section":"post","summary":"想知道別人家都是怎麼畫出絢麗可互動的地圖的嗎？","tags":["R","rMaps","interactive","leaflet"],"title":"rMaps: 超級酷的地圖","type":"post"},{"authors":null,"categories":["test"],"content":" Dairy Consumption and Risk of Stroke Dairy Consumption and Risk of Stroke Dairy Consumption and Risk of Stroke Dairy Consumption and Risk of Stroke Dairy Consumption and Risk of Stroke list 1\\(^2\\) list 2 ","date":1464307200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1464307200,"objectID":"6192987367137d9f5ba64a01c6c9d7e1","permalink":"https://wangcc.me/post/2016-05-27-hello-world/","publishdate":"2016-05-27T00:00:00Z","relpermalink":"/post/2016-05-27-hello-world/","section":"post","summary":"test page","tags":["test","mathjax"],"title":"Hello World","type":"post"},{"authors":["Chaochen Wang","Hiroshi Yatsuya","Yuanying Li","Atsuhiko Ota","Koji Tamakoshi","Yoshihisa Fujino","Haruo Mikami","Hiroyasu Iso","Akiko Tamakoshi","JACC Study Group"],"categories":null,"content":"Abstract Excess intake of iodine is a suspected risk factor for thyroid cancer. Previous epidemiological research from Japan reported that daily intake of seaweed was associated with a four-fold higher risk in postmenopausal women, whereas others reported a null association. A major source of iodine intake in Japan is from edible seaweeds, and it is reported to be among the highest in the world. We examined the association between seaweed intake frequency and the risk of thyroid cancer in women in the Japan Collaborative Cohort Study followed from 1988 to 2009. Seaweed intake, together with other lifestyle-related information was collected using a self-administered questionnaire at baseline. Seaweed intake frequency was categorized as follows: 1–2 times/week or less, 3–4 times/week, and almost daily. Hazard ratios and the 95% confidence intervals of thyroid cancer incidence according to seaweed intake frequency were estimated using Cox proportional hazards regression. During 447 876 person-years of follow-up (n=35 687), 94 new cases of thyroid cancer were identified. The crude incidence rate was 20.9 per 100 000 person-years. The hazard ratio of thyroid cancer in women who consumed seaweed daily compared with women who ate it 1–2 times/week or less was 1.15 (95% confidence interval: 0.69–1.90, P for trend=0.59). Further analyses did not indicate any association between seaweed intake and the risk of thyroid cancer on statistically adjusting for potential confounding variables as well as on stratification by menopausal status. The present study did not find an association between seaweed intake and thyroid cancer incidence in premenopausal or in postmenopausal women.\n","date":1462060800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1462060800,"objectID":"0205673195a6115e989e5c74e2042efd","permalink":"https://wangcc.me/publication/journal-article/jaccseaweed/","publishdate":"2016-05-01T00:00:00Z","relpermalink":"/publication/journal-article/jaccseaweed/","section":"publication","summary":"Abstract Excess intake of iodine is a suspected risk factor for thyroid cancer. Previous epidemiological research from Japan reported that daily intake of seaweed was associated with a four-fold higher risk in postmenopausal women, whereas others reported a null association. A major source of iodine intake in Japan is from edible seaweeds, and it is reported to be among the highest in the world. We examined the association between seaweed","tags":null,"title":"Prospective study of seaweed consumption and thyroid cancer incidence in women: the Japan Collaborative Cohort Study.","type":"publication"},{"authors":null,"categories":["dictation"],"content":"Gut Microbes Lessen Mice Malarial Malaise\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-3-8 | 正确率：89%\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点 A malaria infection begins when the a mosquito injects the Plasmodium parasite into the blood. But getting sick is not a certain outcome. The vast majority of people really only develop either mild malaria or even a symptomatic asymptomatic infections. Nathan Schmidt, a cellular immunologist at the University of Louisville. It\u0026rsquo;s a very small subset of the hundreds of millions of cases that progress to sever severe malaria. Now , some of the variation in owner\u0026rsquo;s illness severity is genetic, . Or whether the patient is partially immune. , thanks to past exposures. But Schmidt and his colleagues had have found another factor that could influence the disease. : the hosts\u0026rsquo; host\u0026rsquo;s microbiome. The first clue came during an experiment in lab mice. : because even though the mice were almost identical genetically. , mice that have had been bought from different venders vendors showed variability in their response to infection by the malaria parasite. Turns out, that the mice have had different microbiomes. So the researchers did more tests . - they transplanted the gut bugs of both the resistant and the susceptible animals into other mice that had no gut bacteria. And again, mice that now had the resistant microbiomics, respell microbial mix were spared the worse worst of the a malaria infection . - possibly through some sort of \u0026rsquo;booster effectunder \u0026rsquo; on their immune system , thanks to the microbes. The study appears at in the Proceedings of the National Academy of Sciences. As for optimizing our microbimes. microbiomes? I think that we are we\u0026rsquo;re pretty far away from , you know , this having any kind of real therapeutic potential for humans. Yogurt alone, for example, didn\u0026rsquo;t much help the mice. But if and when we do find the right recipe for the anti-malaria anti-malarial microbiome, the researchers say, it could lessen the parasite\u0026rsquo;s effects, . And perhaps , save thousands of lives.\nWords worth to be remembered: asymptomatic adj. 无症状的 plasmodium 疟原虫【植】变形 microbiome 微生物菌羣 vendor n. 摊贩；卖主；供应商；自动售货机 譯文： “()”中的是我認爲正確的翻譯 黑體字是我認爲翻譯得好的部分 当一只蚊子把Plasmodium 寄生虫注入到血中, 你便感染了疟疾. 但是也不是感染了就一定会发病.大多数被感染的人只是轻微疟疾或者连感染的症状都没有. 这是Nathan Schmidt, Louisville大学的细胞免疫学家.成百上千的感染病例中只有一小部分会转为急性疟疾症状.\n这些疾病严重程度上的变化是遗传因素造成的.或者说,是否病人是属于对此疾病部分免疫,还要感谢之前的感染.但是Schmidt和他的同事已经发现还有一个因素会影响到疾病的程度: 那就是宿主的微生物群.\n在实验室的小鼠中进行的试验首先显示出证据: 即使小鼠几乎在遗传特征上是等同的,从不同的来源所购买到的小鼠在它们对感染性疟疾的反应也是不同的.结果显示,正是由于小鼠们有不同的微生物群.\n因此研究人员们进行了更多的测试——他们将有抗性和易感性的动物中采集到的肠道细菌移植给没有任何肠道细菌的洁净小鼠。然后再次对它们进行疟疾感染实验，结果现在已经被移植了有抗性微生物混合物的小鼠们再次免于遭受疟疾的严重迫害——或许多亏了它们这些微生物通过某种促进效应，对它们的免疫系统有积极作用。该研究已发表在《美国国家科学研究进展》杂志上。\n至于说要优化这些微生物群？我认为我们现在距离做到能够此类对人类有治疗效果的手段还很远。就拿酸奶举个例子吧，对小鼠来讲没有任何促进作用。但是如果真能做到的话，当我们真的找到调治抗疟疾的微生物群配方时，这些研究人员们表示，还真能够用于减弱寄生虫的致病作用。并且或许能拯救更多的生命呢。\n","date":1457395200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1457395200,"objectID":"f25b2d1386c6c92f986ad5ee8cf7adba","permalink":"https://wangcc.me/post/2016-3-8/","publishdate":"2016-03-08T00:00:00Z","relpermalink":"/post/2016-3-8/","section":"post","summary":"Gut Microbes Lessen Mice Malarial Malaise","tags":["dictation","English Learning","Listening","60s Science"],"title":"肠道菌群的抗病展望","type":"post"},{"authors":null,"categories":["dictation"],"content":"Opioid Epidemic Gets Treatment Prescription\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-3-4 | 正确率：90%\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点 We\u0026rsquo;ve been dealing with an epidemic in the United States related to opioids. Wilson Compton, deputy director of the National Institute on Drug Abuse of the National Institutes of Health. He spoke February 12th at the annual meeting of the American Association for the Advancement of Science in Washington, D. C. What we\u0026rsquo;ve seen is that the increase increased number of prescription prescriptions are driving , the nonmedical use of prescription drugs. Drugs that are prescribed to treat pain. So there is there\u0026rsquo;s tremendous availability of prescriptions. There is are some 260 million prescriptions written in each year for opioids. That\u0026rsquo;s not tablets, that\u0026rsquo;s prescriptions. So its it\u0026rsquo;s millions and millions of these , and some of them are available for diversion and used use inappropriately. Some of the pain patients may become hooked, . Or their meds may find their ways way to friends or relatives who take them recreationally. Or , a prescription opioid users user may transition to heroin. Heroin is just another opioid drug. , so the brain doesn\u0026rsquo;t distinguish , whether it comes from a pharmacy or it comes from a street drug dealer. What\u0026rsquo;s being been drawing the most public health attention , is the overdose death rates related to prescription of opioids and heroin, which have increased markedly over the last 15 years. Right now , we see drug overdose is overdoses killing nearly 50,000 persons in a given year in the United States. What are we doing about this? We are We\u0026rsquo;re focusing in three areas. We need to think about prevention in terms of reducing access to prescriptions as the ultimate upstream driver of this epidemic. We need to be thinking in terms to of saving lives acutely by providing greater access to the reversing drugs, drug - this is nalaoxone the naloxone - that blocks , the impactsimpact. So if somebody is overdose somebody\u0026rsquo;s overdosed and there are no they\u0026rsquo;re not breathing , and I can get this medication to them quickly. , I can resuscitate them. I guess there is That gives us a chance to help them turn their lives around with a the third aspect, which is greater access to effective treatment, through medication assistant medication-assisted treatment. But we also think we need to come up with better approaches for treating pain. Whether What are non-medical approaches , that might use brain stimulation techniques, for example, to treat pain. Whether ? What are other medications that don\u0026rsquo;t include the opioid existence system that might be useful. ? These are the kinds of advances that science can bring us so that we can turn around this overdose epidemic . They are that right now shows tremendous increase increases each year.\nWords worth to be remembered: hooked adj. 钩状的；吸毒成瘾的；入迷的 resuscitate v. （使）复苏，复兴 譯文： “()”中的是我認爲正確的翻譯 黑體字是我認爲翻譯得好的部分 在美国，我们已经在处理和鸦片有关的流行病了。\nWilson Compton，美国国家健康研究会国家药物滥用研究会副主席。这是他2月12日在华盛顿举行的美国科学进步协会年度会议上的讲话\n我们已经看到越来越多的处方被用作非医疗的用途。这些药物是用于治疗疼痛的。\n所以很多处方都可以被利用。每年含有鸦片的处方大约有2.6亿张。这些可不是药片，这些是处方哦。所以有更多的药片其中大部分都被用于消遣娱乐以及其它非正常用途。\n有些疼痛患者也许就上瘾了。或者他们的药流向了他们的朋友或者亲属被用作娱乐。再或者处方药鸦片使用者也许转而使用了海洛因。\n海洛因只是鸦片类药物的另一种形式，所以大脑并不会刻意区分是否它是来自于药房还是街头小贩。\n已经正在引起公众健康关注的是过量使用处方类鸦片和海洛因后的死亡率，最近15年来已经显著增加。目前，我们在美国，一年因过量使用此类药物而死亡的人数就接近5万人。\n我们该做点儿神马呢？ 我们已经着重于以下三方面。我们需要去阻止，减少处方药的获得机会，因为最终是开出的处方药导致这种流行病的出现。我们需要思考，通过更有利的使用应对药物及时拯救生命——比如纳洛酮，烯丙羟吗啡酮（一种吗啡拮抗药）——用于阻断鸦片的药性。所以，如果有人用药过量后呼吸停止了，我们能及时应用这种药物去治疗他们，我们就能救活他们。这就给我们机会去帮助他们，由此可以引出我们可以做的第三个方面，那就是通过药物辅助治疗的方法实施更有效率的治疗。\n但是我们还认为我们需要想出更好的治疗疼痛的方法。非药物的方法，比如用刺激大脑的技术，去治疗疼痛? 采用其它不不含有鸦片相关化合物的药物？这些科学进步所能带给我们的方法使我们能逆转现在每年大量增长的因药物过度使用造成的流行病。\n","date":1457049600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1457049600,"objectID":"f18830fefc1e382adedb1375aff96235","permalink":"https://wangcc.me/post/2016-3-4/","publishdate":"2016-03-04T00:00:00Z","relpermalink":"/post/2016-3-4/","section":"post","summary":"Opioid Epidemic Gets Treatment Prescription","tags":["dictation","English Learning","Listening","60s Science"],"title":"磕药是种流行病","type":"post"},{"authors":null,"categories":["dictation"],"content":"Super Bowl Sunday\u0026rsquo;s Food Needs Work\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-3-1 | 正确率：88%\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点 It\u0026rsquo;s nearly game day , and, if you are you\u0026rsquo;re a fan, you\u0026rsquo;ve already set aside your roomy sweat pants, roomiest sweatpants and your own personal Family-size bag of Nacho Cheese Doritos. But if you are you\u0026rsquo;re at all concern concerned about over indulging, overindulging - which, if you live in America. , you probably should be . - you might take a tip from a public health advocate from at the New York City Food Policy Center. Charlse Charles Platkin says that one way to avoid over doing overdoing it , is to consider how much you\u0026rsquo;ve you\u0026rsquo;d have to exercise to work off what you consume. So , to prepare for Super Bowl Sunday , - the second biggest day for food consumption in the U. S. - Platkin crunched the numbers for some of our favorite couch-side snacks. And he has he\u0026rsquo;s helpfully converted them to football-themed into exercise equivalenceequivalents. So, for example, two slices of Domino\u0026rsquo;s ultimate pepperoni hand-tossed crust pizza would require running nearly 11,000 yards . - that\u0026rsquo;s 109 football fields - at a speed of 5 five miles per hour. Two KFC original drum sticks, drumsticks? Just do the wave , 1,561 times. To pay for a single potato chip loaded with French onion dip you\u0026rsquo;d have to sing along with Coldplay and Beyonce for 30 minutes during the half timehalftime. And even five pretzels , - yes, five pure puny little pretzels out of the a bag - would take six-and-a-half minutes of jumping up and down whenever your team scores a touch downtouchdown. Which means that if you want to avoid post-bowl paunch, your team better bring it. Either that or just a dip of stick with the celery sticks. And pass on the dip, if you want to maintain the current size of your N end zone.\nWords worth to be remembered: roomy adj. 广阔的，宽敞的，宽大的 sweatpant n. 【美】宽松长运动裤 family-size adj. 适合(足够)全家用的 overindulge v. [I] 溺爱，过分放任，过分沉溺（与in连用） [T] 过度沉溺，放纵，过分纵情（于）（与in连用） advocate n. 拥护者，提倡者；辩护律师；谋利益者 v. 拥护，提倡 work off phr. 宣泄，释放，排解 drumstick n. 下段鸡腿肉；鼓槌 pretzel 椒盐脆饼干 puny adj. 微小的,弱小的,微不足道的 touchdown n. 触地,触地得分,着地,降落 譯文： “()”中的是我認爲正確的翻譯 黑體字是我認爲翻譯得好的部分 这就算是个全民游戏日了,如果你是它的粉丝的话,恐怕已经准备好居家服和家庭装芝士玉米立体脆.但是,如果你还担心放纵过渡的话——如果你住在美国，你或许应该——应该从纽约城市食品政策研究中心的公众健康宣讲会上找点儿建议。Charles Platkin建议，有一种方法来避免过度饮食就是先考虑一下你吃这些东西后需要做多少运动来消耗掉所含的热量。\n所以，为了准备周日的超级杯——美国食品消费第二大高峰日——Charles Platkin尝试了大量的人们喜爱的零食小吃。并且，他将其所含热量转换为足球运动背景下的一些等量的运动项目。\n所以，比如，两角多米诺旗下的全手工意大利辣肠口味的皮萨所含热量大概需要跑11000吗——也就是大概109个足球场 ——速度需要保持每小时5英里。\n那么两块肯德基原味鸡块呐? 就随机摆动1561次吧。\n为了消耗一片沾有法式洋葱酱料的薯片，你必须要在中场休息的时候跟随酷玩乐队和比昂斯唱满30分钟。\n而且仅仅5片椒盐卷饼饼干——是的，一袋里就那么很小的饼干——也需要你蹦蹦跳跳6..5分钟，不管你支持的队伍是否得分晋级。\n也就是说吧，如果你想避免看过比赛之后胖三斤，你最好接受这些建议。或者就用芹菜棒代替那些零食好啦。而且如果你想保持当前的各种身体指数，那就必须还不能有任何酱料哦。\n","date":1456790400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1456790400,"objectID":"e608d0418bde0be125ddc3309c1a707d","permalink":"https://wangcc.me/post/2016-3-1/","publishdate":"2016-03-01T00:00:00Z","relpermalink":"/post/2016-3-1/","section":"post","summary":"Super Bowl Sunday's Food Needs Work","tags":["dictation","English Learning","Listening","60s Science"],"title":"每逢佳节胖三斤","type":"post"},{"authors":null,"categories":["dictation"],"content":"Beet Juice Could Help Body Beat Altitude\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-2-25 | 正确率：90%\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点 At 14,505 feet, Mount Whitney towers over California\u0026rsquo;s Sierra Nevada range. It\u0026rsquo;s the tallest peak in the lower 48 , - which means that it tracks attracts thousands of weekend warriors every year . - or make that weekend weakened warriors. You see them stabbing staggering along in the narrowrocket , rocky trail, huffing for air, dizzy and exhausted from the low oxygen levels at high elevation. And once they are they\u0026rsquo;re sick. ? The best medicine is to go down. That\u0026rsquo;s the actually the only cure that works. Svein Gaustad, a physiologist at the Norwegian University of Science and Technology. He says previous studies suggested that suggest blood vessels turned tend to contracted contract at high altitudes. Possibly because they need oxygen to relax . - exactly what\u0026rsquo;s in short supply on mountaintops. But there might may be a dietary way to get more oxygen to your blood vessels. : in the form of beet juice. The juice contains nitrate, which the body converts to nitric oxide, the compound that keeps arteries limber. Gaustad and his colleagues tested that theory during a trek in Nepal, at 12,000 feet. 8 Eight volunteers alternately drinks drank shots of regular beet juice, and another day, beet juice with a nitrate the nitrates stripped out. A few hours later, the researchers measured blood flow and artery diameters with ultrasound. And they found that the regular beet juice did indeed restore blood vessels back to their low-elevation flexibility, whereas the nitrate-stripped juice did not. The result is results are in the journal of Nitric Oxide. Gaustad says better vascular function has the potential to deliver more blood , - and therefore more oxygen - to tired muscles. But they still don\u0026rsquo;t know if that translates to better performance at altitude. And, he saysthat beet , beets won\u0026rsquo;t hurt, but there are they\u0026rsquo;re no substitute for proper acclimatization. If I had buffalo a bottle of beets around I will would take it for sure. But that won\u0026rsquo;t bringing bring you to Mount Everest just by drinking beetroot.\nWords worth to be remembered: stagger adj. 蹒跚的；令人惊愕的；难以置信的 huff v. 深呼吸，气喘吁吁，上气不接下气 contract v. 收縮 beet n. 甜菜，甜菜根，糖萝卜 nitrate n. 硝酸盐;硝酸钾;硝酸钠 acclimatization n. 服水土,顺应,适应环境 Mout Everest n. 埃佛勒斯峰(喜马拉雅山主峰之一,中国称珠穆朗玛峰) 譯文： “()”中的是我認爲正確的翻譯 黑體字是我認爲翻譯得好的部分 高达14505英尺，Mount Whitney 高耸在加州内华达山脉中。它可是较低的48 座山峰中最高的一座——也就是说每天它能吸引到成百上千的周末勇士来攀登——或者把这些勇士给搞垮。你可以看到他们沿着狭窄的岩石小路上蹒跚而行，上气不接下气，头昏眼花的在高海拔地区的缺氧环境下精疲力尽的抗争着。而一旦他们生病了怎么办呐？\n最好的办法就是撤下去啦。这真的是有效的治疗办法。Svein Gaustad是，挪威科技大学的一位生理学家。他说以前的研究表明血管在高海拔地区会趋于收缩。或许因为她们需要氧气来放松——这在山顶上实在是个短缺的物资呢。但或许可以靠膳食给你的血管补充氧气哦： 就就来一杯甜菜汁吧。这里面含有硝酸盐，我们的身体可以将之转化为一氧化氮，这种化合物能保持血管的柔软灵活性。\nGaustad 和他的同事们在尼泊尔的徒步旅行中测试了上述理论，该地区大概有12000英尺。八名志愿者或者饮用一杯普通压榨的甜菜汁，而另一天，甜菜汁是脱硝酸盐的。几小时后，研究人员们用超声波测量了他们的血液流速和动脉的直径。他们发现，饮用普通甜菜汁的人们确实恢复了在低海拔地区时候的血管弹性，而饮用脱硝酸盐的甜菜汁的人们的血管并没有恢复。改研究结果已经发表在《一氧化氮》杂志上。\nGaustad表示，较好地血管功能就有输送更多血液的潜力——并且因此你的身体会获得更多的氧气——到疲劳的肌肉组织。但是他们还不清楚这种转化作用会根据海拔表现的更好。而且，他说，甜菜也没什么害处，但是这些物质还是不能取代合适的环境适应能力。如果我有一瓶甜菜汁随身携带，我一定会食用的。但是光靠喝甜菜汁应该不会支撑你爬上喜马拉雅山的吧。\n","date":1456358400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1456358400,"objectID":"2342a3638ed4a01166002744a7f9065d","permalink":"https://wangcc.me/post/2016-2-25/","publishdate":"2016-02-25T00:00:00Z","relpermalink":"/post/2016-2-25/","section":"post","summary":"Beet Juice Could Help Body Beat Altitude","tags":["dictation","English Learning","Listening","60s Science"],"title":"甜菜汁的好处","type":"post"},{"authors":null,"categories":["dictation"],"content":"Suicide Differences by Region Related to Gun Availability\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-2-22 | 正确率：91%\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点 One of the things we know for sure , in the United States , is that a gun in a the home increases the likelihood that someone in the home will die , in a violent death , - from gun accidents, from a women woman being murdered by a man or in an intimated intimate partner violent violence situation , and particularly , by suicide. David Hemenway, he is . He\u0026rsquo;s the director of the Harvard Injury Control Research Center, he is . He\u0026rsquo;s also a professor of Health Policy and Management at the Harvard T. H. Chan School of Public Health. The gun violence discussion often seems to give short shift shrift to suicide. , even though more than 60% of the approximately 22000 32,000 annual U. S. firearms deaths are suicides. Hemenway spoke at January 6th 26th at the a Harvard School of Public Health forum on gun violence as a public health issue. The evidence is overwhelming, from case control studies to and ecological studies. For example, why do we have very different suicide rates across cities, across states, across regions in the United States, . To explain the differences in suicide rates across states. , turns out it is it\u0026rsquo;s not well explained at all by differences in mental health, it is it\u0026rsquo;s not well explained at all by differences in numbers the number of psychiatrists, it is it\u0026rsquo;s not even explained by differences in suicide radiation ideation among the population or even suicide attempts. What really explains the difference in the United States across the populations is the number of guns. Because of it\u0026rsquo;s gun suicides suicide which is so different. And someone who commits suicide with a gun very likely would not have either attempted or succeeded if the gun were not available. For example, a 2013 Swiss study , tracked men after the size of their army was cut in half, effectively when removing guns from half of that group, . The overall suicide rate went down, and the researcher researchers estimated that only 22% of all the men who would\u0026rsquo;ve would have killed themselves with a gun if it had been available wound up committing the act by other means. The presence of guns the gun just make makes it significantly easier to take their your own lives life impulsively. The entire hour-long forum featuring Hemenway and other researchers discussing gun violence as a public health issue is archived on lineonline. Just google \u0026quot; Harvard public health forum\u0026quot; .\nWords worth to be remembered: ideation n. 观念构成 譯文： “()”中的是我認爲正確的翻譯 黑體字是我認爲翻譯得好的部分 有件事儿大家都很清楚，在美国，私人拥有枪支的合法增加了家庭成员因暴力死亡的可能性——原因可能是枪支走火，也可能是三角关系引发血案，特别还有可能是自杀。\n这是David Hemenway，哈佛人身伤害控制研究中心的负责人。他也是哈佛公共健康T. H. Chan学院的一名健康政策与管理学教授。枪支暴力谈论通常因自杀而草草收场，即便在美国每年大约有32000起火拼死亡案件中有60%都是自杀行为。Hemenway在1月26日举行的哈夫公共健康学院的研讨会上指出枪支暴力犯罪属于公共健康问题。\n来自于案例对照研究和生态研究得到的证据是非常有说服力的。比如，为什么不同城市，不同的州，以及美国境内不同区域的自杀率十分的不同？为了解释不同地区的自杀率之间的差异原因，研究结果显示这些原因根本不能用精神疾病的发病率不同来解释，根本不能用精神病医师的数量多少来解释，甚至也根本不能用人群中的自杀倾向或者自杀企图的比例上的差异来解释。真正在美国人口中出现的这种差异的普遍原因就是枪支的数量。因为是用枪支自杀的数量上有差异。\n并且，如果没有枪，用枪支自杀的某人很可能本来就不用出现用枪自杀的动机以及后来用枪自杀成功的这种事儿。比如， 在2013年瑞士，有一项研究追踪调查了军火大小被减半后，就有效的把枪从作案凶器中的出现频率降低了一半。整个自杀率下降，而研究人员们估计，所有自杀的研究对象中，有22% 的人，如果可以选择枪这种工具，那么他们才会自杀，用其它工具他们就不进行自杀行为了。 枪支的出现只是使得你结束生命的冲动出现后更加容易的被满足。\n含有Hemenway和其它研究人员关于枪支暴力犯罪属于公共健康问题的全部论坛内容已经可以在线收听。只需要用谷歌搜索“Harvard public health forum”选择相应内容即可。\n","date":1456099200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1456099200,"objectID":"0e98b7a9b801b07b42f4dcf754f0e2e8","permalink":"https://wangcc.me/post/2016-2-22/","publishdate":"2016-02-22T00:00:00Z","relpermalink":"/post/2016-2-22/","section":"post","summary":"Suicide Differences by Region Related to Gun Availability","tags":["dictation","English Learning","Listening","60s Science"],"title":"枪支自杀与公共健康","type":"post"},{"authors":null,"categories":["dictation"],"content":"Antioxidant Use Still Small Mixed Bag\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-2-18 | 正确率：89%\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点 Are you gobbling up antioxidants as part of your diet and nutrition regimentregimen? The benefits seems maybemay be, well. It seems surprising, but even after several decades , we don\u0026rsquo;t have a clear answer. There is , there\u0026rsquo;s not, if there is were really cross the wall across-the-board powerful benefits we would\u0026rsquo;ve would have seen it. , and that\u0026rsquo;s not the situation. Walter Willett, . He chairs the Department of Nutrition at the Harvard T. H. Chan School of Public Health. He spoke at a January 15th forum on Cancer and Diet , that wound up touching on diet and health in general. The studies are, so far, randomized trials that have been done, don\u0026rsquo;t show much benefit. There was actually a surprising . increase in lung cancer with beta-carotene, one of the antioxidants, in people who smoke or smoked and were heavy drinkersalthough, although there is was no increase in risk in people if you who were generally pretty healthy . to start with. So even the randomized trials give different answers. , I think, that antioxidants are not a magic solution to cancer or other diseases. , but they are there probably are some benefits. One example is , that in the a physicians health study , randomized trial over 12 yearsare, at the end of that period of time , those taking beta-carotene had better cognitive function than people on placebo - – a really interesting and potential potentially important finding. So antioxidants may provide some benefits to some people. But even if there are, that\u0026rsquo;s only a small part of the changes . that we need to make them in diet and lifestyle to reduce our risk of cancer. cancers, there is are so many other things that are really quite well documented. The entire hour-long forum featured featuring Willett and other researchers discussing diet and health is archived on lineonline. Just google \u0026quot; Harvard public health forum\u0026quot; .\nWords worth to be remembered: regimen n. 生活规则；养生法；政体 across-the-board adj. 全面的 adv. 全面地 譯文： “()”中的是我認爲正確的翻譯 黑體字是我認爲翻譯得好的部分 你每天是不是都要吞很多抗氧化的产品作为营养健康补充？或许有用。。。吧。\n很令人吃惊，但即便经过了几十年的研究，我们还不是很清楚它的作用，还没有个定论吧，如果真有什么可见成效，我们也早就该发现了，而不是现在这种状况。\nWalter Willett，他是哈佛 T. H. Chan公共健康 学院 营养学系 的主任 。他 在 1月 15日 巨型的 健康与癌症 研讨会上 做出了挑衅一般饮食与健康之间关系的 演讲 。\n目前的相关 研究 已经进行了 随机试验 ，发现并没有什么 可见的显著益处。过去曾发现实际上，其中一种抗氧化物质，β-胡萝卜素 在 吸烟和酗酒的人群中与肺癌的患病率增高有关，尽管在一开始就 很 健康的人中并没有发现有 相关风险的增加。所以 即使随机试验给出了不同的 结论，我认为， 抗氧化物也并不是对癌症或者其它 疾病有很神奇效果的一种物质，但是或许它们 确实有促进健康的一些效果。有一个内科医生的健康研究进行了超过12年的随机试验中的例子显示，在研究期间内， 那些服用了β-胡萝卜素的 人群的认知能力比服用安慰剂的人群提高了不少—— 这个研究发现还挺有趣也挺重要的。\n所以，抗氧化物也许对某些人来讲能提供一些促健康的益处 。但即便是有好处，对我们饮食和生活方式的影响以至于减少患癌症的风险这方面的作用也是微乎其微，还有许多其它因素都被证明确实会对疾病的发生能起到重要作用。\n完整的研讨会包括Willett 和其它研究人员关于健康和饮食关系方面的讨论内容已经在线发布。只需要在 谷歌上搜索“Harvard public health forum”即可。\n","date":1455753600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1455753600,"objectID":"f540c3cc84130f919a24a8574651b3d5","permalink":"https://wangcc.me/post/2016-2-18/","publishdate":"2016-02-18T00:00:00Z","relpermalink":"/post/2016-2-18/","section":"post","summary":"Antioxidant Use Still Small Mixed Bag","tags":["dictation","English Learning","Listening","60s Science"],"title":"抗氧化物的实际功效还不明","type":"post"},{"authors":null,"categories":["dictation"],"content":"Espresso Machines Brew a Microbiome of Their Own\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-2-2 | 正确率：91%\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点 What\u0026rsquo;s the lively as liveliest part of your kitchen, in terms of harboring bacteria? Is it the cutting board, ? The dish bunch, sponge? Or maybe your coffee maker. coffeemaker? Because even though caffeine has antibacterial effects, it turns out that espresso machines can harbor a whole menagerie of bacteria , - including some pathogenic species , more commonly associated with the toilet. Researchers sampled ten Nespresso brand espresso machines. , zeroing in on an the drip trays, which catch those last drops of bround brown gold after a brew. They found that 9 nine of 10 the ten machines harbored residues rich in Enterococcus bacteria, a typical marker of human feagle fecal contamination. And another typical common resident was Pseudomonas , - which have has both benign and pathogenic strains. Pseudomonas appears to thrives thrive in the presents presence of caffeine, and even breaks it down. Which suggest suggests the bugs might be put to work , decaffeinating coffee, or cleaning caffeine residues from our water wayswaterways. The findings appear in the journal of Scientific Reports. As for your next espresso shot, ? Don\u0026rsquo;t worry too much. The researchers did not find any bacteria in the coffee pots pods themselves . - so they say our fingertips might be the to blame for spreading the single soles than datasingle-celled invaders. And they write that it\u0026rsquo;s \u0026quot; absolutely not the case\u0026quot; that espresso Nespresso machines are dangerous for human health. Just wash the drip tray more often with soap and water. , just as you would any other food-contaminated surface. So that the only thing that is brewing in your espresso machine , is your coffee.\nWords worth to be remembered: menagerie n. 动物园,动物展览 pathogenic adj. 使生病的,成为病原的,病原性的 zero in 瞄准具校正，调整归零 residue n. 剩余物；残留物；残余，残渣 enterococcus 肠道球菌; 肠球菌 fecal adj. 排泄物的,渣滓的 pseudomonas 假单胞菌属; 假单胞细菌属 pod n. 豆荚，蚕茧 v. 从豆荚中剥出；结豆荚 譯文： “()”中的是我認爲正確的翻譯 黑體字是我認爲翻譯得好的部分 你厨房里最有生机的地方是哪儿啊，我是说附着的细菌呐？切菜板？刷碗的海绵？ 或许，你的咖啡机？ 因为即便咖啡因有抗细菌的效果，有研究显示浓缩咖啡机能附着一群细菌——包括一些和厕所马桶有关的致病菌相类似的致病菌呢。\n研究人员们对10台Nespresso牌的浓缩咖啡机进行采样，主要范围集中在水盘，那里在煮咖啡之后有些残留暗金色的液滴。他们发现，10台机器中有9台，残留物里含有Enterococcus肠球菌属的细菌，一种人类粪便污染物被细菌感染的标志。另一种Pseudomonas假单胞菌属细菌——则有的是无害的，有些是致病的菌种被发现残留。假单胞菌在有咖啡因存在的情况下会大量繁殖，然后分解咖啡因。这种特性提示我们也许可用它进行去咖啡因咖啡的制作，或者从我们的排水口中清除咖啡因。该研究结果发表在《科学报告》杂志上。\n关乎你的下一次煮咖啡嘛？别太担心了。研究人员在咖啡包里没有发现任何种类细菌——因此他们表示我们指尖也许对扩散此类单细胞生物有促进作用。然后他们写道，这绝对不是说浓缩咖啡剂对人类的健康有害。只需要像担心你的其它食物表面会被污染一样，经常用肥皂水清洗一下滴水盘。 所以之后，你的咖啡机里就只有一种你想要煮的，咖啡啦。\n","date":1455494400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1455494400,"objectID":"4357c199cb0e4264028d6f0d8f9b9b59","permalink":"https://wangcc.me/post/2016-2-2/","publishdate":"2016-02-15T00:00:00Z","relpermalink":"/post/2016-2-2/","section":"post","summary":"Espresso Machines Brew a Microbiome of Their Own","tags":["dictation","English Learning","Listening","60s Science"],"title":"咖啡机的隐患","type":"post"},{"authors":null,"categories":["dictation"],"content":"Healthful Diet Switch Helps Even Late in Life\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2016-2-15 | 正确率：90%\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点 Let\u0026rsquo;s say your diet , hasn\u0026rsquo;t been so great. Maybe too much read red meat, especially processed meat. Maybe , too many sugary soft drinks. And maybe , you\u0026rsquo;ve been eating like that for decades. So , what\u0026rsquo;s the point of trying to make some helpful healthful changes now, after the damage has been presumably been done? It is impressive that changes even very late in life. , such as even being older and having a heart attack. , a dietary change came can within a matter of a few months drop by our risk greatly of reoccurred a recurrent heart attack or death. Walter Willett, . He chairs the Department of Nutrition of at the Harvard T. H. Chan School of Public Health. He spoke at a January 15th forum on Cancer and Diet that wound up touching on diet and health in general. So , it\u0026rsquo;s never too late to make important changes. For diabetes also, that if we change our diet almost immediately our risk of diabetes goes down. But that\u0026rsquo;s not to say you just should just wait till you are `til you\u0026rsquo;re old , just live to start living a healthy lifewith. We have seen We\u0026rsquo;re seeing in some studies now that what women ate as adolescents, especially if they ate a lot more red meat, that affected breast cancer risk later in their life. So it\u0026rsquo;s definitely important if you want the healthiest overall life is to start a healthy lifestyle early. But if you you\u0026rsquo;ve sort of ignored things , it it\u0026rsquo;s never too late , and you can to still get some benefit. The entire hour-long forum featuring Willett and other researchers discussing diet and health is archived onlineon line. Just google \u0026quot; “Harvard public health forum\u0026quot; .\n譯文： “()”中的是我認爲正確的翻譯 黑體字是我認爲翻譯得好的部分 要说你的饮食已经很好了吧。也许有很多红肉，特别是加工肉类。或许还有很多有糖饮料。并且你或许已经这样吃了几十年了。那么现在突然要尝试更健康的饮食究竟是为何呢，要是以前不健康的话，也都已经深受其害很久了不是吗？\n晚年的生活会有很大不同，比如说衰老加速或者心脏变差，饮食改变能在即使几个月内就减少心脏病复发或者死亡几率。\nWalter Willett是哈佛公共健康T. H. Chan 学校的营养系主任。他在1 月15日关于癌症和饮食的论坛上作出的有关饮食和健康与死亡的关系的言论。\n所以，不要说做出重要的改变已经太迟了。对糖尿病也是，如果我们改变我们的饮食，几乎立刻就能减少罹患糖尿病的几率。但这也并没有说，你只需要等，直到你老了以后再开始你的健康生活。现在我们正在进行的一些实验显示，女士在青少年时期的饮食，特别是如果她们吃过很多红肉，这会在她们今后的生命中增加他们罹患乳腺疾病的几率。 所以如果你想要你的生活健康，尽可能早的选择一种健康的生活方式是绝对重要的。 如果你总是忽略这些事，那起码现在开始做到也没有太迟。\n全部的几小时论坛上都是Willett和其它研究人员们在线讨论饮食和健康相关研究所得到的研究结论。尽请搜索 谷歌网站上的 Harvard public health forum。\n","date":1455494400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1455494400,"objectID":"619636d2aa5eafba1825b7d3f46d0216","permalink":"https://wangcc.me/post/2016-2-15/","publishdate":"2016-02-15T00:00:00Z","relpermalink":"/post/2016-2-15/","section":"post","summary":"Healthful Diet Switch Helps Even Late in Life","tags":["dictation","English Learning","Listening","60s Science"],"title":"調整飲食習慣永遠都不晚","type":"post"},{"authors":["Chaochen Wang","Hiroshi Yatsuya","Hideaki Toyoshima","Keiko Wada","Yuanying Li","Esayas Haregot Hilawe","Mayu Uemura","Chifa Chiang","Yan Zhang","Rei Otsuka","Atsuhiko Ota","Yoshihisa Hirakawa","Atsuko Aoyama"],"categories":null,"content":"Abstract Objective: To investigate differences in the association of parental history of diabetes with the risk of type 2 diabetes mellitus (T2DM) in the offspring according to the sex of the parent and the offspring\u0026rsquo;s body weight.\nMethods: A prospective cohort study of 4446 middle-aged non-diabetic Japanese men and women were followed in Aichi Prefecture, central Japan, from 2002 to 2011. Subjects were categorized by their self-reported parental history of diabetes (\u0026ldquo;no parental history,\u0026rdquo; \u0026ldquo;father only,\u0026rdquo; \u0026ldquo;mother only,\u0026rdquo; and \u0026ldquo;both\u0026rdquo;). The association of parental history of diabetes and incidence in the offspring was examined according to overweight status adjusted for age, sex, birth weight, smoking, alcohol consumption, physical activity, total energy intake, body mass index, and number of metabolic syndrome components.\nResults: During follow-up (median 8.9 years), 277 subjects developed T2DM. Parental history of diabetes was positively associated with T2DM incidence. However, stratified analysis by overweight status revealed that only maternal history was associated with increased T2DM incidence in non-overweight subjects (hazard ratio=2.35, 95% confidence interval: 1.41-3.91). While in overweight subjects, paternal history was significantly associated with higher T2DM incidence (hazard ratio=1.98, 95% confidence interval: 1.19-3.28).\nConclusions: Our results suggest that parental history of diabetes mellitus is associated with the incidence of T2DM in offspring differently according to the sex of the affected parent and the offspring\u0026rsquo;s body weight.\n","date":1451520000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451520000,"objectID":"d9f4decf78b8ab30e616216e83e5d19f","permalink":"https://wangcc.me/publication/journal-article/fhdm2/","publishdate":"2015-12-31T00:00:00Z","relpermalink":"/publication/journal-article/fhdm2/","section":"publication","summary":"Abstract Objective: To investigate differences in the association of parental history of diabetes with the risk of type 2 diabetes mellitus (T2DM) in the offspring according to the sex of the parent and the offspring\u0026rsquo;s body weight.\nMethods: A prospective cohort study of 4446 middle-aged non-diabetic Japanese men and women were followed in Aichi Prefecture, central Japan, from 2002 to 2011. Subjects were categorized by their self-reported parental history of diabetes (\u0026ldquo;no parental history,\u0026rdquo; \u0026ldquo;father only,\u0026rdquo; \u0026ldquo;mother only,\u0026rdquo; and \u0026ldquo;both\u0026rdquo;).","tags":null,"title":"Association between parental history of diabetes and the incidence of type 2 diabetes mellitus differs according to the sex of the parent and offspring’s body weight: A finding from a Japanese worksite-based cohort study","type":"publication"},{"authors":null,"categories":["dictation"],"content":"Overeating Due to Stress?\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2015-9-18 | 正确率：96%\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点 Stress can make some people (me included) lose our appetite. And other folks find comfort in food. But such behaviors may actually even out in the long term, . Because researchers find that people who change eating patterns when stressed out may actually make up for those not-so-healthy impulses during easier times. So finds in a study in the journal Psychological Science. Volunteers for the study self-identified as either \u0026quot; munchers\u0026quot; or \u0026quot; skippers\u0026quot; . Each person will have had to interact with another person via video chat. , with the intention of meeting them later. After each video interaction participants receive received a message either stating that their partner had decided not to meet them, or that they were excited to meet them. As a control, some participants were told that the study had just been canceled. Then , the researchers offered ice cream to everyone , - as much as they wanted. The munchers who got rejected ate more ice cream than did those in the control groups. group, and the skippers who were rejected ate less. All as you\u0026rsquo;d expectedexpect. But here is here\u0026rsquo;s the twist, : Among the participants who received positive feed backfeedback, the munchers actually ate less than the control group, . And the skippers ate more. So even stress-eaters are sometimes less-eaters. Unless they are they\u0026rsquo;re always stressed out.\nWords worth to be remembered: munch v. 用力咀嚼，大声咀嚼 twist n. 扭，绞，搓，缠，编；窍门；曲解；扭伤 v. 扭转，扭弯，旋转，绞；缠绕，盘绕；扭伤；歪曲；扭动 譯文： “()”中的是我認爲正確的翻譯 黑體字是我認爲翻譯得好的部分 压力可以使某些人（也包括我啦）失去胃口。还有些人会从食物中找到慰籍。但是这样的行为也许会在长时间后得到调和。因为研究人员们发现，因压力而改变饮食模式的人，在没有压力的时候出现的不那么健康的饮食冲动实际上会得到弥补。 这是在《心理学科学》杂志上发表的一项研究得出的结论。\n该研究中的志愿者都是自我标榜为“吃货一枚”或者“吃不吃看情况”的人。每隔人都不得不通过视频聊天和另外一个人进行一次互动，目的就是为了之后和他们进行正式会面。每个视频互动过后的参与者会受到一个留言信息，内容是他们的同伴决定不和他们见面了，或者是他们的同伴十分期待和他们会面。对照组中，参与人员们会被告知这次的研究实验活动被取消了。然后，研究人员们为每人准备了冰淇凌——要多少就给多少哦。\n被告知会面被拒绝的吃货们比对照组的人吃掉更多的冰淇凌，而被拒绝的 三餐不固定的人则比对照组的人吃的冰淇凌要少。所有的结果都在意料之中。\n可是也有个小纠结：在收到积极反馈的参与者们，标榜为吃货的 人比对照组的人吃的冰淇淋少。而三餐不固定的人却吃得比对照组的人多。\n所以即便是因压力而吃东西的人有时候也会吃得少一些的。除非他们总是被压力困扰着。\n","date":1442534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1442534400,"objectID":"a7669a36c706621ee0bb5f956f2faeab","permalink":"https://wangcc.me/post/2015-9-18/","publishdate":"2015-09-18T00:00:00Z","relpermalink":"/post/2015-9-18/","section":"post","summary":"Overeating Due to Stress?","tags":["dictation","English Learning","Listening","60s Science"],"title":"暴饮暴食有原因","type":"post"},{"authors":null,"categories":["dictation"],"content":"Cosmetic Ads\u0026rsquo; Science Claims Lack Foundation\nYour browser does not support the audio element. 你的瀏覽器不支持音頻播放。請使用chrome科學上網。 听写于：2015-9-16 | 正确率：84%\n提示：红色：错误单词，绿色：补上正确单词，黄色：纠正大小写与标点 Clinically Proven. Break through Breakthrough Technology. Ten Years of Genetic Research. These are phrases you might expect to find in the pages of Scientific American. But these descriptions also show on up in commercials and print ads for cause medicscosmetics. Now one a study found finds that some or - well, make that \u0026quot; a lot \u0026quot; - of those science-sounding claims are simply not true. Researchers looked at nearly 300 ads in magazines such as Vogue. They analyzed claims in the ads and ranked them on a scale ranging from acceptable to out right outright lie. And they found that just 18% of the both boasts that the researchers looked at were true, . 23% were out right outright lies, . And 42% were too vague to even classifiedclassify. The study is in the Journal of Global Fashion Marketing. The Food and Drug Administration regulates will what goes into your cosmetics and what goes on the label. If the a claim is blatenly blatantly untrue, the FDA can take action. Vague languages language on labels maybe may be a way to keep the FDA at bay. Meanwhile, ads are regulated by the Federal Trade Commission. Just last year they charged L\u0026rsquo;Oreal for deceptive advertising of it\u0026rsquo;s its Génifique products, which the company said were clinical clinically proven the to boost genes\u0026rsquo; activity that will would lead to the production of protein proteins causing visibly younger skin in just 7 seven days. A settlement agreement forced L\u0026rsquo;Oreal to back off on the claimclaims. So take those closemetic cosmetic ads with the a grain of salt scrub - after all, if scientists have had really come up with a product that burst reversed your wrinkles or grew your eye ashes eyelashes, it will would sell itself.\nWords worth to be remembered: cosmetic adj. 表面的；美容的 n. 化妆品，装饰品 outright adj. 完全的，彻底的；直率的 adv. 完全的，彻底的；即刻；率直地 blatantly adv. 喧闹地；公然地；露骨地；极为，完全 keep sth. at bay v. 牵制,不使逼近 譯文： “()”中的是我認爲正確的翻譯 黑體字是我認爲翻譯得好的部分 临床实验证实..突破性技术..十年遗传学研究表明..这些词你也许本想在科学美国人的杂志上看到.但是这些描述你也能在化妆品广告和印刷宣传册上见到.\n现在有一项研究发现,有些——好吧，有很多——这些坚实的科学声明基本就不是真的。\n研究人员们察看了300多份杂志上的广告，比如时尚杂志。他们分析了在广告上的这些声明并把它们按真实程度从可以接受到完全是谎言这样的顺序排名。然后他们发现，宣称该产品被科研人员们检查过后的产品中有18%是真的。由23% 就是谎言。还有42% 说的模棱两可而不好被归类。该研究已发表在《全球时尚营销》杂志上。\n食品药品管理局规定什么成分被用在你的化妆品里以及产品说明上应该写什么。如果一项宣称是明显不真实的，那么FDA是可以采取行动处理的。标签上的用词模糊也许是一种阻止 FDA 的一种方法。\n同时，广告是被联邦商贸委员会管理的。就在去年，欧莱雅刚刚因为其产品Génifique的虚假广告被罚款，广告中该公司说道，临床证明其商品促基因表达的活性，从而导致引起可以只需要7天便可以使你的皮肤年轻的一些蛋白质被表达出来。一项达成一致的和解迫使欧莱雅撤回这项声明。\n所以对化妆品广告可别太当真了——毕竟，如果科学家们已经想到如何退却你的皱纹或者是增长你的睫毛，还能轮得上这些公司？\n","date":1442361600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1442361600,"objectID":"f8afbb6d01668fd32464dcb02110022b","permalink":"https://wangcc.me/post/2015-9-16/","publishdate":"2015-09-16T00:00:00Z","relpermalink":"/post/2015-9-16/","section":"post","summary":"Cosmetic Ads' Science Claims Lack Foundation","tags":["dictation","English Learning","Listening","60s Science"],"title":"化妆品广告的科学依据","type":"post"},{"authors":["Mayu Uemura","Hiroshi Yatsuya","Esayas Haregot Hilawe","Yuanying Li","Chaochen Wang","Chifa Chiang","Rei Otsuka","Hideaki Toyoshima","Koji Tamakoshi","Atsuko Aoyama"],"categories":null,"content":"Abstract Background: Skipping breakfast has been suspected as a risk factor for type 2 diabetes (T2DM), but the associations are not entirely consistent across ethnicities or sexes, and the issue has not been adequately addressed in the Japanese population.\nMethods: We followed 4631 participants (3600 men and 1031 women) in a work-site cohort of participants aged 35–66 years in 2002 through 2011 for T2DM development. Frequency of eating breakfast was self-reported and was subsequently dichotomized to breakfast skippers, who eat breakfast 3–5 times/week or less, and to eaters. Cox proportional hazards models were used to adjust for potential confounding factors, including dietary factors, smoking and other lifestyles, body mass index (BMI), and fasting blood glucose (FBG) at baseline.\nResults: During 8.9 years of follow-up, 285 T2DM cases (231 men and 54 women) developed. Compared to participants who reported eating breakfast every day, maximally-adjusted hazard ratios and 95% confidence intervals (CI) of those with the frequency of almost every day and 3–5, 1–2, and 0 days/week were: 1.06 (95% CI, 0.73–1.53), 2.07 (95% CI, 1.20–3.56), 1.37 (95% CI, 0.82–2.29), and 2.12 (95% CI, 1.19–3.76), respectively. In a dichotomized analysis, breakfast skipping was positively associated with T2DM incidence (maximally-adjusted hazard ratio 1.73; 95% CI, 1.24–2.42). The positive associations were found in both men and women, current and non-current smokers, normal weight and overweight (BMI ≥25 kg/m2), and normal glycemic status and impaired fasting glycemic status (FBG 110 to \u0026lt;126 mg/dL) individuals at baseline (Ps for interaction all \u0026gt;0.05).\nConclusions: The present study in middle-aged Japanese men and women suggests that skipping breakfast may increase the risk of T2DM independent of lifestyles and baseline levels of BMI and FBG.\n","date":1430784000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1430784000,"objectID":"01040b46ef34401a3d3f658d11b1dbd8","permalink":"https://wangcc.me/publication/journal-article/bfskip/","publishdate":"2015-05-05T00:00:00Z","relpermalink":"/publication/journal-article/bfskip/","section":"publication","summary":"Abstract Background: Skipping breakfast has been suspected as a risk factor for type 2 diabetes (T2DM), but the associations are not entirely consistent across ethnicities or sexes, and the issue has not been adequately addressed in the Japanese population.\nMethods: We followed 4631 participants (3600 men and 1031 women) in a work-site cohort of participants aged 35–66 years in 2002 through 2011 for T2DM development. Frequency of eating breakfast was self-reported and was subsequently dichotomized to breakfast skippers, who eat breakfast 3–5 times/week or less, and to eaters.","tags":null,"title":"Breakfast Skipping is Positively Associated With Incidence of Type 2 Diabetes Mellitus: Evidence From the Aichi Workers’ Cohort Study","type":"publication"},{"authors":["Esayas Haregot Hilawe","Hiroshi Yatsuya","Yuanying Li","Mayu Uemura","Chaochen Wang","Chifa Chiang","Hideaki Toyoshima","Koji Tamakoshi","Yan Zhang","Nobuo Kawazoe","Atsuko Aoyama"],"categories":null,"content":"Abstract Background: Although the association between cigarette smoking and risk of type 2 diabetes is well established, its mechanisms are yet to be clarified. This study examined the possible mediating effects of adiponectin, leptin, and C-reactive protein (CRP) concentrations on the smoking-diabetes association.\nMethods: Between 2002 and 2011, we followed 3338 Japanese workers, aged 35–66 years, who were enrolled in the second Aichi workers’ cohort study. We used multivariable-adjusted Cox regression models to determine the hazard ratios and respective 95% confidence intervals (CIs) of the association between smoking status and risk of diabetes. A multiple mediation model with bootstrapping was used to estimate the magnitude and the respective bias-corrected (BC) 95% CIs of the indirect effects of smoking on diabetes through the three biomarkers.\nResults: Relative to never smokers, the risk of diabetes was significantly elevated in current (hazard ratio 1.75, 95% CI 1.25–2.46) and ex-smokers (hazard ratio 1.54, 95% CI 1.07–2.22). The indirect effects of smoking on diabetes through adiponectin levels were statistically significant among light (point estimate 0.033, BC 95% CI 0.005–0.082), moderate (point estimate 0.044, BC 95% CI 0.010–0.094), and heavy smokers (point estimate 0.054, BC 95% CI 0.013–0.113). In contrast, neither the indirect effects of smoking on diabetes through leptin nor CRP levels were significant, as the corresponding BC 95% CIs included zero.\nConclusions: In our analysis, adiponectin concentration appeared to partially mediate the effect of smoking on diabetes, while leptin and CRP levels did not.\n","date":1423094400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1423094400,"objectID":"44c8300eb80dd61fc762fc4f986552d3","permalink":"https://wangcc.me/publication/journal-article/smmd/","publishdate":"2015-02-05T00:00:00Z","relpermalink":"/publication/journal-article/smmd/","section":"publication","summary":"Abstract Background: Although the association between cigarette smoking and risk of type 2 diabetes is well established, its mechanisms are yet to be clarified. This study examined the possible mediating effects of adiponectin, leptin, and C-reactive protein (CRP) concentrations on the smoking-diabetes association. Methods: Between 2002 and 2011, we followed 3338 Japanese workers, aged 35–66 years, who were enrolled in the second Aichi wor","tags":null,"title":"Smoking and Diabetes: Is the Association Mediated by Adiponectin, Leptin, or C-reactive Protein?","type":"publication"},{"authors":["Chaochen Wang (王 超辰)","Hiroshi Yatsuya (八谷 寛)","Koji Tamakoshi (玉腰 浩司)","Hiroyasu Iso (磯 博康)","Akiko Tamakoshi (玉腰 暁子)"],"categories":null,"content":"Abstract BACKGROUND: Findings regarding the association between milk consumption and all-cause mortality reported by studies carried out in Western populations have been inconsistent. However, no studies have been conducted in Japan on this issue. The present study aimed to investigate the association of milk drinking with all-cause, cardiovascular, and cancer mortality in Japan.\nMETHODS: The data were obtained from the Japan Collaborative Cohort (JACC) study. A total of 94 980 Japanese adults aged 40-79 years who had no history of cancer, stroke, or chronic cardiovascular diseases were followed between 1988 and 2009. Multivariable-adjusted hazard ratios (HRs) and 95% confidence intervals (CIs) of mortalities were assessed using a Cox proportional hazard regression model and taking the lowest milk consumption group as the reference.\nRESULTS: During a median of 19 years of follow-up, there were 21 775 deaths (28.8% and 35.3% from cardiovascular diseases and cancer, respectively). Drinking milk 1-2 times a month was associated with lower all-cause mortality in men compared to those who never drank milk (multivariable-adjusted HR 0.92; 95% CI, 0.85-0.99). In women, those who drank 3-4 times a week also had a lower mortality risk compared with those who never drank milk (HR 0.91; 95% CI 0.85-0.98). Inverse associations between drinking milk and mortality from cardiovascular diseases and cancer were found only in men.\nCONCLUSIONS: Drinking milk at least 1-2 times a month was associated with lower all-cause mortality in men compared to never drinking milk. An inverse association was also found between drinking milk and mortality from both cardiovascular diseases and cancer. However, lower all-cause mortality in women was found only in those who drank milk 3-4 times/week.\n","date":1420416000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420416000,"objectID":"367082628fd5a1aeeb4e92cee30cd288","permalink":"https://wangcc.me/publication/journal-article/milk/","publishdate":"2015-01-05T00:00:00Z","relpermalink":"/publication/journal-article/milk/","section":"publication","summary":"Abstract BACKGROUND: Findings regarding the association between milk consumption and all-cause mortality reported by studies carried out in Western populations have been inconsistent. However, no studies have been conducted in Japan on this issue. The present study aimed to investigate the association of milk drinking with all-cause, cardiovascular, and cancer mortality in Japan. METHODS: The data were obtained from the Japan Collaborative Cohort (JACC) study. A total of 94 980","tags":null,"title":"Milk drinking and mortality: findings from the Japan Collaborative Cohort Study","type":"publication"},{"authors":["Hiroshi Yatsuya","Yuanying Li","Esayas Haregot Hilawe","Atsuhiko Ota","**Chaochen Wang**","Chifa Chiang","Yan Zhang","Mayu Uemura","Ayako Osako","Yukio Ozaki","Atsuko Aoyama"],"categories":null,"content":"Abstract Although the global prevalence of both the overweight and obese is on the rise, there are variations among regions or countries, and sexes. Approximately half or more than half of the population are overweight/obese defined as body mass index $\\geqslant$ 25 $kg/m^2$in the Americas (61.1%), Europe (54.8%), and Eastern Mediterranean (46.0%) according to the World Health Organization, while a much lower prevalence is observed in Africa (26.9%), South-East Asia (13.7%), and the Western Pacific (25.4%). Females are more likely to be overweight/obese in the Eastern Mediterranean, Africa, South-East Asia and the majority of countries in the Americas and Western Pacific but not in the most of the countries in Europe. These region-sex-ethnicity differences in prevalence may be a clue to the causes of the obesity epidemic. Epidemiological studies done in the USA, Europe, and Asia found that higher BMI was significantly associated with increased incidence of coronary artery disease (CAD) and ischemic stroke, but the association with hemorrhagic stroke incidence was not always consistent. The association of BMI with CAD and ischemic stroke was generally independent of known mediators, which would indicate the importance of controlling or preventing overweight/obesity for the prevention of cardiovascular disease.\n","date":1415664000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1415664000,"objectID":"2315ca95433089d80491da581aa92055","permalink":"https://wangcc.me/publication/journal-article/globalobesity/","publishdate":"2014-11-11T00:00:00Z","relpermalink":"/publication/journal-article/globalobesity/","section":"publication","summary":"Abstract Although the global prevalence of both the overweight and obese is on the rise, there are variations among regions or countries, and sexes. Approximately half or more than half of the population are overweight/obese defined as body mass index $\\geqslant$ 25 $kg/m^2$in the Americas (61.1%), Europe (54.8%), and Eastern Mediterranean (46.0%) according to the World Health Organization, while a much lower prevalence is observed in Africa (26.9%), South-East Asia (13.","tags":null,"title":"Global Trend in Overweight and Obesity and Its Association With Cardiovascular Disease Incidence","type":"publication"},{"authors":["Hiroshi Yatsuya","Takashi Nihashi","Yuanying Li","Yo Hotta","Kunihiro Matsushita","Takashi Muramatsu","Rei Otsuka","Masaaki Matsunaga","Kentaro Yamashita","Chaochen Wang","Mayu Uemura","Akiko Harada","Hiroshi Fukatsu","Hideaki Toyoshima","Atsuko Aoyama","Koji Tamakoshi"],"categories":null,"content":"Summary Background To examine the association of intrahepatic fat with homeostasis model assessment-insulin resistance (HOMA-IR), a marker of insulin resistance, in Japanese adults, and whether intrahepatic fat is associated with insulin resistance independent of waist circumference and other measures of obesity.\nMethods Fifty-three individuals aged 37–69 were studied. Spectrum obtained using a 3-T magnetic resonance imager was analysed with LCModel to quantify intrahepatic fat. Blood levels of insulin, glucose and other biochemical markers were obtained after 8 h or more fasting. Percent body fat was estimated by a bioelectrical impedance analyzer. HOMA-IR and intrahepatic fat content were log-transformed in the analysis.\nResults We found a positive correlation between intrahepatic fat and HOMA-IR, which was independent of the anthropometric measures of obesity. In contrast, significant and positive correlations of body mass index, percent body fat, and waist circumference with HOMA-IR were largely explained by their associations with intrahepatic fat. Intrahepatic fat was positively associated with alanine transaminase and triglycerides even after adjustment for HOMA-IR.\nConclusion Intrahepatic fat was associated with insulin resistance independent of age, sex, and measures of obesity in Japanese adults. Hypertriglyceridemia and liver injury may directly occur subsequent to intrahepatic fat accumulation.\n","date":1404172800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1404172800,"objectID":"58e7aff6cdbd3ec1ed8cbd3a5f088e33","permalink":"https://wangcc.me/publication/journal-article/liverfat/","publishdate":"2014-07-01T00:00:00Z","relpermalink":"/publication/journal-article/liverfat/","section":"publication","summary":"Summary Background To examine the association of intrahepatic fat with homeostasis model assessment-insulin resistance (HOMA-IR), a marker of insulin resistance, in Japanese adults, and whether intrahepatic fat is associated with insulin resistance independent of waist circumference and other measures of obesity.\nMethods Fifty-three individuals aged 37–69 were studied. Spectrum obtained using a 3-T magnetic resonance imager was analysed with LCModel to quantify intrahepatic fat. Blood levels of insulin, glucose and other biochemical markers were obtained after 8 h or more fasting.","tags":null,"title":"Independent association of liver fat accumulation with insulin resistance.","type":"publication"},{"authors":["Mayu Uemura","Hiroshi Yatsuya","Koji Tamakoshi","**Chaochen Wang**","Chifa Chiang","Rei Otsuka","Hideaki Toyoshima","Satoshi Sasaki","Atsuko Aoyama"],"categories":null,"content":"要 約 目的: アメリカ心臓協会(AHA)は、心血管疾患による死亡率を 10 年間で 20%減少させることを目標として、心血管健康度(AHA 指標)を 2010 年に定めた。本研究の目的は、日本人の一集団において、AHA 指標の中の食事・栄養素摂取に関する 5 項目と健診成績との関連を調べることである。\n方法および結果: 2002 年に 35-66 歳の愛知県内自治体職員 3,446 人に、簡易型自記式食事歴法質問票を用いて食事調査を実施した。AHA 指標の中で食事・栄養素摂取に関して定められた野菜・果物、魚介類、食物繊維の豊富な全粒穀物、食塩、清涼飲料水の 5 項目の各摂取量に基づいて対象者を三群に分けた。野菜・果物、魚介類は摂取量の最も多い第 3 三分位、食塩、清涼飲料水は最も少ない第 1 三分位を「良好な摂取状況」とみなした。食物繊維の豊富な全粒穀物は玄米・胚芽米・麦・雑穀を「いつも食べる」を「良好な摂取状況」とみなした。これら 5 項目について「良好な摂取状況」の達成項目数別の人数(割合)は、0 項目が 537 人(15.6%)、1 項目が1,453 人(42.2%)、2 項目が 1,029 人(29.9%)、3 項目が 365 人(10.6%)、4 項目以上が 62 人(1.8%)であった。この 5 項目の三分位による摂取量と健診成績との関連性を、性、年齢、喫煙歴、1 週間の運動日数、肉体労働の有無、アルコール摂取量、食事・栄養素摂取項目を調整して検討した。その結果、野菜・果物摂取量は body mass index(BMI)と負の関連、魚介類摂取量は中性脂肪(TG)と負の関連、HDL コレステロール(HDLC)、血糖(PG)と正の関連、食塩摂取量は TG と有意な正の関連、清涼飲料水摂取量は BMI、拡張期血圧(DBP)、総コレステロール(TC)、LDL コレステロール、TG、PG と有意な正の関連を示した。しかし、BMI を調整すると、魚介類と TG、HDLC、PG 以外の上記関連は統計学的有意性が消失した。食物繊維の豊富な全粒穀物と検査値との間には有意な関連は認められなかった。食事・栄養素摂取に関する指標の達成項目数と検査値との関連については、達成項目数が多い群ほど、BMI、TG が有意に低く、HDLCが有意に高くなる量・反応関係が認められたが、TG や HDLC との関連は BMI で調整すると有意性が消失した。\n結論: 本研究は一勤労者集団を対象としたものであるが、我が国においても、食事・栄養素摂取に関する AHA 指標と血圧及び脂質代謝異常との間に有意な関連がみられた。特に魚介類と清涼飲料水の摂取は複数の検査項目の異常と有意な関連を認めたが、そのうち清涼飲料水の摂取との関連は肥満を介することが示唆された。今後、我が国の生活習慣病予防対策を立案する際には、魚介類の摂取不足および清涼飲料水の摂取過多に関連する病態にも着目する必要があると考えられた。\nキーワード:アメリカ心臓協会,心血管疾患,食事,健診,清涼飲料水,魚介類 ","date":1391212800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1391212800,"objectID":"74a4074098a0b0c18d57937ca1cf3c3f","permalink":"https://wangcc.me/publication/journal-article/uemuradietary/","publishdate":"2014-02-01T00:00:00Z","relpermalink":"/publication/journal-article/uemuradietary/","section":"publication","summary":"要 約 目的: アメリカ心臓協会(AHA)は、心血管疾患による死亡率を 10 年間で 20%減少させることを目標として、心血管健康度(AHA 指標)を 2010 年に","tags":null,"title":"Association between five dietary health metrics of the American Heart Association and results of health checkups in a cross-sectional study of Japanese workers.","type":"publication"},{"authors":["Chaochen Wang","Hiroshi Yatsuya","Koji Tamakoshi","Mayu Uemura","Yuanying Li","Keiko Wada","Kentaro Yamashita","Leo Kawaguchi","Hideaki Toyoshima","Atsuko Aoyama"],"categories":null,"content":"Abstract Background: Elevated high-sensitivity C-reactive protein (hs-CRP), a marker of low-grade systemic inflammation, may be involved in the etiology of type 2 diabetes mellitus (T2DM). However, whether inflammation precedes development of T2DM independent of cigarette smoking and obesity remains to be confirmed.\nMethods: We studied 4213 civil servants in a local government in Japan aged 35-66 years at baseline in 2002, who donated blood samples and were followed 6 years. Hazard ratios (HR) of T2DM according to the hs-CRP quartiles [range Q1: 0.02-0.18 (reference), Q2: 0.18-0.33, Q3: 0.33-0.67 and Q4: 0.67-9.62 mg/L) were estimated by Cox proportional hazards model adjusted for gender, age, body mass index, alcohol intake, smoking status (current, past and never), number of cigarettes per day, physical activity, family history of diabetes (Model 1) and variables in Model 1 + glucose (Model 2).\nResults: The geometric mean [95% confidence interval (CI)] of hs-CRP was 0.36 mg/L (0.34-0.37). During the follow-up, 156 new T2DM cases were confirmed. In total sample, Model 2 HRs (95% CIs) for hs-CRP quartiles Q2-Q4 compared with Q1 were 0.69 (0.36-1.26), 1.47 (0.91-2.39) and 1.78 (1.10-2.88), respectively (p for linear trend = 0.014). Stratified analysis revealed that a statistically significant association was observed only in normal weight non-current smokers with Model 2 HRs (CIs) being 0.79 (0.29-2.17), 2.63 (1.25-5.56) and 3.19 (1.49-6.86) for Q2-Q4 compared with Q1, respectively (p for linear trend = 0.0006). The relationship did not change materially after further adjusting for log-homeostasis model assessment or exclusion of past smokers.\nConclusion: These findings imply that higher hs-CRP itself or existence of chronic systemic inflammation precedes onset of T2DM independent of obesity and smoking.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"71fb4097db9de266302e5b2b2fb1f29e","permalink":"https://wangcc.me/publication/journal-article/crpdm2/","publishdate":"2013-07-01T00:00:00Z","relpermalink":"/publication/journal-article/crpdm2/","section":"publication","summary":"Abstract Background: Elevated high-sensitivity C-reactive protein (hs-CRP), a marker of low-grade systemic inflammation, may be involved in the etiology of type 2 diabetes mellitus (T2DM). However, whether inflammation precedes development of T2DM independent of cigarette smoking and obesity remains to be confirmed.\nMethods: We studied 4213 civil servants in a local government in Japan aged 35-66 years at baseline in 2002, who donated blood samples and were followed 6 years. Hazard ratios (HR) of T2DM according to the hs-CRP quartiles [range Q1: 0.","tags":null,"title":"Positive association between high‐sensitivity C‐reactive protein and incidence of type 2 diabetes mellitus in Japanese workers: 6‐year follow‐up","type":"publication"},{"authors":["Chaochen Wang","Hiroshi Yatsuya","Takashi Nihashi","Takashi Muramatsu","Koji Tamakoshi","Yuanying Li","Kunihiro Matsushita","Yo Hotta","Hiroshi Fukatsu","Hideaki Toyoshima"],"categories":null,"content":"Abstract Objective: We aimed to determine whether complement 3 (C3) is positively related to the degree of liver fat content in healthy Japanese adults.\nMethods: Middle-aged male subjects who drank less than 46 g/day alcohol (n=40) were studied. H1 magnetic resonance spectroscopy was used to quantify liver fat content, (3T, Siemens MR, TE=30 ms). Volume of interest was selected in the right liver lobe with a size of 4×4×4 cm. The spectrogram was analyzed by LCModel.\nResults: Mean age and body mass index were 62.5 years and 24.5 kg/m2 . The geometric mean and 95% confidence interval of C3 (mg/dl) in the lowest, medium, and highest liver fat tertile were 96.5 (92.8, 100.5), 111.1 (104.6, 115.6), 112.2 (106.7, 117.9), respectively after adjustment for age, body mass index, smoking status, alcohol intake and blood FFA level (one-way ANCOVA p=0.008, linear p=0.010).\nConclusion: We found a linear and positive relationship between C3 and liver fat content. Present findings may implicate hepatocyte injury already present with accumulation of triacylglycerol.\n","date":1304208000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1304208000,"objectID":"89433a03aca0402cc0375f41370b60e7","permalink":"https://wangcc.me/publication/journal-article/blood-c3_liverfat/","publishdate":"2011-05-01T00:00:00Z","relpermalink":"/publication/journal-article/blood-c3_liverfat/","section":"publication","summary":"Abstract Objective: We aimed to determine whether complement 3 (C3) is positively related to the degree of liver fat content in healthy Japanese adults.\nMethods: Middle-aged male subjects who drank less than 46 g/day alcohol (n=40) were studied. H1 magnetic resonance spectroscopy was used to quantify liver fat content, (3T, Siemens MR, TE=30 ms). Volume of interest was selected in the right liver lobe with a size of 4×4×4 cm. The spectrogram was analyzed by LCModel.","tags":null,"title":"Positive Association between blood C3 Level and Liver Fat Content Quantified by 1H Magnetic Resonance Spectroscopy in Japanese Men","type":"publication"}]