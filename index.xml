<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Be ambitious on Be ambitious</title>
    <link>https://winterwang.github.io/</link>
    <description>Recent content in Be ambitious on Be ambitious</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017-2019 Chaochen Wang | 王超辰</copyright>
    <lastBuildDate>Sat, 29 Jun 2019 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>地鎮祭</title>
      <link>https://winterwang.github.io/post/jichinsai/</link>
      <pubDate>Sat, 29 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/jichinsai/</guid>
      <description>&lt;p&gt;今日在即將開工建房的土地上，建築公司請來神社的神職人，花了大約一個小時的時間，幫助我們完成了
地鎮祭的儀式。&lt;/p&gt;

&lt;p&gt;地鎮祭儀式主要是為了祈福接下來的建築過程以及主人家人居住之後的平安與幸福的祈禱儀式。聽說中國農村如果自己土地上蓋房子，也會有相似的儀式來祈求神靈守護。&lt;/p&gt;

&lt;p&gt;本來儀式本身我們並不擔心會有什麼意外的事情發生。是這一週的天氣預報總是雷雨和陰雨不斷。於是家人都很擔心週六的地鎮祭儀式時的天氣，從週一開始每天都在手機查閱天氣情況。還好到了地鎮祭舉行的週六這天，天公作美在早上11點儀式開始前雨停下。我們抵達土地時，神職人也為了以防萬一搭好了臨時擋風遮雨的帳篷。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://winterwang.github.io/post/2019-06-30-jichinsai_files/IMG_20190629_104451.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;可以看到神職人已經為我們準備好了供奉的水果蔬菜，清酒，驅邪的食鹽和潔淨的大米：
&lt;img src=&#34;https://winterwang.github.io/post/2019-06-30-jichinsai_files/IMG_20190629_104416.jpg&#34; alt=&#34;準備就緒&#34; /&gt;&lt;/p&gt;

&lt;p&gt;之後儀式順利舉行，神職人帶著我們在土地建房的東南西北各個方向上把祭拜後的食鹽和淨米，還有清酒灑在地面上。&lt;/p&gt;

&lt;p&gt;兒子也很聽話地參與了祭拜和祈禱的整個過程：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://winterwang.github.io/post/2019-06-30-jichinsai_files/mmexport1561797204704.jpg&#34; alt=&#34;兒子&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://winterwang.github.io/post/2019-06-30-jichinsai_files/mmexport1561797168564.jpg&#34; alt=&#34;還是兒子&#34; /&gt;&lt;/p&gt;

&lt;p&gt;全家一起在未開工的新家地址上第一張合照產生啦：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://winterwang.github.io/post/2019-06-30-jichinsai_files/mmexport1561797153590.jpg&#34; alt=&#34;合照&#34; /&gt;&lt;/p&gt;

&lt;p&gt;感謝上蒼，我們順利在無風無雨中結束儀式，建築現場負責的工人安排說明了接下來直至我們能夠搬新家之前的流程之後，天又一下子陰沈下來，轉眼又大雨傾盆。&lt;/p&gt;

&lt;p&gt;祝願我們的新家可以平安無事地建成，建成後，也保佑我們會幸福美滿地在這裏繼續我們的生活。封面是從神職人手裏領受的守護。聽說會被一直埋在地基下面，從今天起，保佑這片土地。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>菜鳥的Mac筆記本折騰之路</title>
      <link>https://winterwang.github.io/post/2019-06-28-macbook/</link>
      <pubDate>Fri, 28 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/2019-06-28-macbook/</guid>
      <description>&lt;p&gt;這些年習慣了折騰Ubuntu之後，突然切換回高大上的 Macbook Pro，真心不太適應。&lt;/p&gt;

&lt;p&gt;首先吐槽花了兩天時間才搞定家裏和學校的打印機聯接設定。真是不曉得昨天早上和晚上為啥死都連不上的佳能打印機驅動，今天重新打開電腦之後就突然能接上了。果然&amp;rdquo;重新啟動&amp;rdquo;永遠是最有有效的招數。其實完全不知道未重啟之前到底電腦的哪根筋沒有接好。&lt;/p&gt;

&lt;p&gt;接下來讓人不可思議的是 MacOS 沒有OpenBUGS。經過放狗搜索之後找到了&lt;a href=&#34;https://oliviergimenez.github.io/post/run_openbugs_on_mac/&#34; target=&#34;_blank&#34;&gt;在蘋果電腦上安裝 OpenBUGS 的方法&lt;/a&gt;。簡單總結就是下載 Wine 之後用 wine 來跑OpenBUGES，介面風格像80年代的筆電一樣：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://winterwang.github.io/post/2019-06-28-testpost_files/wineOpenBUGS.png&#34; alt=&#34;wineOpenBUGS&#34; /&gt;&lt;/p&gt;

&lt;p&gt;即使最終能夠使用OpenBUGS的介面進行運算之後，在Ubuntu和Windows都能自動識別並連結OpenBUGS引擎的 &lt;a href=&#34;https://cran.r-project.org/web/packages/BRugs/index.html&#34; target=&#34;_blank&#34;&gt;Brugs&lt;/a&gt; 包竟然還是無法下載識別MCMC引擎。所以下載了勉強能夠使用 R2OpenBUGS 包計算，但是還需要用下面繁瑣的設定：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;WINE=&amp;quot;/usr/local/bin/wine&amp;quot;
WINEPATH=&amp;quot;/usr/local/bin/winepath&amp;quot;
OpenBUGS.pgm=&amp;quot;/Users/{user.name}/Wine\ Files/drive_c/Program\ Files/OpenBUGS/OpenBUGS323/OpenBUGS.exe&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;於是，我的學習筆記大概率估計是只能在Ubuntu上更新了。&lt;/p&gt;

&lt;p&gt;還有要吐槽的一個事情就是，StataIC 版本在Ubuntu下可以使用命令行直接在終端呼叫Stata引擎進行統計分析並且輸出結果，十分方便。而且設定了路徑之後可以直接在Rstudio裡面加入Stata引擎，使得Rmarkdown的代碼部分可以自由地在R和Stata之間切換。但是Mac版本的StataIC竟然沒有命令行的安裝方式。繼續放狗查詢才知道，必須是StataSE或者StataMP版本才可以。這是另一條讓我滾回Ubuntu的理由嗎？&lt;/p&gt;

&lt;p&gt;Mac電腦現在在我眼裡已經爛得只剩下顏值了。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://winterwang.github.io/post/2019-06-28-testpost_files/IMG_20190627_102846.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;這兩天折騰安裝各種軟件之時，發現Texlive 2019的Mac版本安裝完畢之後佔據將近6個G的硬盤空間。於是狠狠心刪除了之後換上輕巧簡潔的 &lt;a href=&#34;https://yihui.name/tinytex/&#34; target=&#34;_blank&#34;&gt;TinyTex&lt;/a&gt;，發現整個世界都純淨了。&lt;/p&gt;

&lt;p&gt;只需要在Rmd文檔的YAML部分加入下面這段代碼，一份日語（中文）英文混合的PDF文檔即可在Rstudio中編譯完成，從安裝到完成第一個日語文檔的編譯，基本上僅僅耗時十分鐘左右。好評！&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;---
title: &amp;quot;日本語&amp;quot;
header-includes:
  - \usepackage{xltxtra}
  - \usepackage{zxjatype}
  - \usepackage[ipaex]{zxjafont}
output: 
  pdf_document: 
    latex_engine: xelatex 
---
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://winterwang.github.io/post/2019-06-28-testpost_files/TinyTexJap.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>誰還未覺醒 | Do you hear people sing</title>
      <link>https://winterwang.github.io/post/do-you-hear-people-sing/</link>
      <pubDate>Wed, 12 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/do-you-hear-people-sing/</guid>
      <description>&lt;video width=auto height=auto controls allowfullscreen&gt;
  &lt;source src=&#34;https://winterwang.github.io/post/2019-06-12-do-you-hear-people-sing_files/peoplesing.mp4&#34; type=&#34;video/mp4&#34;&gt;
  &lt;source src=&#34;movie.ogg&#34; type=&#34;video/ogg&#34;&gt;
  Your browser does not support the video tag.
&lt;/video&gt;

&lt;blockquote&gt;
&lt;p&gt;試問誰還未發聲&lt;br /&gt;
都捨我其誰衛我城&lt;br /&gt;
天生有權還有心可作主&lt;br /&gt;
誰要認命噤聲&lt;br /&gt;
試問誰能未覺醒&lt;br /&gt;
聽真那自由在奏鳴&lt;br /&gt;
激起再難違背的那份良知和應&lt;/p&gt;

&lt;p&gt;為何美夢仍是個夢&lt;br /&gt;
還想等恩賜泡影&lt;br /&gt;
為這黑與白這非與是&lt;br /&gt;
真與偽來做證&lt;br /&gt;
為這世代有未來&lt;br /&gt;
要及時擦亮眼睛&lt;/p&gt;

&lt;p&gt;無人有權沉默&lt;br /&gt;
看著萬家燈火變了色&lt;br /&gt;
問我心再用我手&lt;br /&gt;
去為選我命途力拼&lt;br /&gt;
人既是人&lt;br /&gt;
有責任有自由決定遠景&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;這是一個值得銘記的日子。這是一個需要我們所有人都認識到自己所處的世代，要面臨劇變的時刻。&lt;/p&gt;

&lt;p&gt;在那個廣場上，你要麼是王維林，要麼就是那臺坦克。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://winterwang.github.io/post/2019-06-12-do-you-hear-people-sing_files/IMG_20190609_180641.jpg&#34; alt=&#34;lie&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>君與長江|傍晚穿過廣場</title>
      <link>https://winterwang.github.io/post/2019-06-04/</link>
      <pubDate>Tue, 04 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/2019-06-04/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://chinadigitaltimes.net/chinese/2019/06/%E5%90%9B%E4%B8%8E%E9%95%BF%E6%B1%9F-%E5%82%8D%E6%99%9A%E7%A9%BF%E8%BF%87%E5%B9%BF%E5%9C%BA/&#34; target=&#34;_blank&#34;&gt;原文鏈接&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://winterwang.github.io/post/2019-06-06_files/toprotest0.png&#34; alt=&#34;go to protest!&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;我不知道一個過去年代的廣場&lt;br /&gt;
從何而始，從何而終&lt;br /&gt;
有的人用一小時穿過廣場&lt;br /&gt;
有的人用一生 &amp;mdash;&amp;mdash;&lt;br /&gt;
早晨是孩子，傍晚已是垂暮之人&lt;br /&gt;
我不知道還要在陽光中走出多遠&lt;br /&gt;
才能停住腳步？&lt;/p&gt;

&lt;p&gt;還要在陽光中眺望多久才能&lt;br /&gt;
閉上眼睛？&lt;br /&gt;
當高速行駛的汽車打開刺目的車燈&lt;br /&gt;
哪些曾在一個明媚早晨穿過廣場的人&lt;br /&gt;
我從汽車的後視鏡看見過他們一閃即逝&lt;br /&gt;
的面孔&lt;br /&gt;
傍晚他們乘車離去&lt;/p&gt;

&lt;p&gt;一個無人離去的地方不是廣場&lt;br /&gt;
一個無人倒下的地方也不是&lt;br /&gt;
離去的重新歸來&lt;br /&gt;
倒下的卻永遠倒下了&lt;br /&gt;
一種叫做石頭的東西&lt;br /&gt;
迅速地堆積，屹立&lt;br /&gt;
不像骨頭的生長需要一百年的時間&lt;br /&gt;
也不像骨頭那麼軟弱&lt;/p&gt;

&lt;p&gt;每個廣場都有一個用石頭壘起來的&lt;br /&gt;
腦袋，使兩手空空的人們感到生存的&lt;br /&gt;
分量。以巨大的石頭腦袋去思考和仰望&lt;br /&gt;
對任何人都不是一件輕鬆的事&lt;br /&gt;
石頭的重量&lt;br /&gt;
減輕了人們肩上的責任，愛情和犧牲&lt;/p&gt;

&lt;p&gt;或許人們會在一個明媚的早晨穿過廣場&lt;br /&gt;
張開手臂在四面來風中柔情地擁抱&lt;br /&gt;
但當黑夜降臨&lt;br /&gt;
雙手就變得沉重&lt;br /&gt;
唯一的發光體是腦袋裏的石頭&lt;br /&gt;
唯一刺向石頭的利劍悄然墜地&lt;/p&gt;

&lt;p&gt;黑暗和寒冷在上升&lt;br /&gt;
廣場周圍的高層建築穿上了瓷和玻璃的時裝&lt;br /&gt;
一切變得矮小了。石頭的世界&lt;br /&gt;
在玻璃反射出來的世界中輕輕浮起&lt;br /&gt;
像是塗在孩子們作業本上的&lt;br /&gt;
一個隨時會被撕下來揉成一團的陰沉念頭&lt;/p&gt;

&lt;p&gt;汽車疾駛而過，把流水的速度&lt;br /&gt;
傾瀉到有着鋼鐵筋骨的龐大混凝土制度中&lt;br /&gt;
賦予寂靜以喇叭的形狀&lt;br /&gt;
一個過去年代的廣場從後視鏡消失了&lt;/p&gt;

&lt;p&gt;永遠消失了 &amp;mdash;&amp;mdash;&lt;br /&gt;
一個青春期的，初戀的，佈滿粉刺的廣場&lt;br /&gt;
一個從未在賬單和死亡通知書上出現的廣場&lt;br /&gt;
一個路初胸膛，挽起衣袖，紮緊腰帶&lt;br /&gt;
一個雙手使勁搓洗的帶補丁的廣場&lt;/p&gt;

&lt;p&gt;一個通過年輕的血液流到身體之外&lt;br /&gt;
用舌頭去舔，用前額去下磕，用旗幟去掩蓋&lt;br /&gt;
的廣場&lt;/p&gt;

&lt;p&gt;空想的，消失的，不復存在的廣場&lt;br /&gt;
像下了一夜的大雪在早晨停住&lt;br /&gt;
一種純潔而神祕的融化&lt;br /&gt;
在良心和眼睛裏交替閃爍&lt;br /&gt;
一部分成爲叫做淚水的東西&lt;br /&gt;
零一部分在叫做石頭的東西裏變得堅硬起來&lt;/p&gt;

&lt;p&gt;石頭的世界崩潰了&lt;br /&gt;
一個軟組織的世界爬到高處&lt;br /&gt;
整個過程就像泉水從吸管離開礦物&lt;br /&gt;
進入密閉的，蒸餾過的，有着精美包裝的空間&lt;br /&gt;
我乘坐高速電梯在雨天的傘柄裏上升&lt;/p&gt;

&lt;p&gt;回到地面時，我看到雨傘一樣張開的&lt;br /&gt;
一座圓形餐廳在城市上空旋轉&lt;br /&gt;
像一頂從魔法變出來的帽子&lt;br /&gt;
它的尺寸並不適合&lt;br /&gt;
用石頭壘起來的巨人的腦袋&lt;/p&gt;

&lt;p&gt;那些曾托起廣場的手臂放了下來&lt;br /&gt;
如今巨人僅靠一柄短劍來支撐&lt;br /&gt;
它會不會刺破什麼呢？比如，一場曾經有過的&lt;br /&gt;
從紙上掀起，在牆上張貼的脆弱革命？&lt;/p&gt;

&lt;p&gt;從來沒有一種力量&lt;br /&gt;
能把兩個不同世界長久地粘在一起&lt;br /&gt;
一個反覆張貼的腦袋最終將被撕去&lt;br /&gt;
反覆粉刷的牆壁&lt;br /&gt;
被露出大腿的混血女郎佔據了一半&lt;br /&gt;
另一半時頭髮再生，假肢安裝之類的誘人廣告&lt;/p&gt;

&lt;p&gt;一輛嬰兒車靜靜地停在傍晚的廣場上&lt;br /&gt;
靜靜地，和這個快要發瘋的世界沒有關係&lt;br /&gt;
我猜嬰兒和落日之間的距離有一百年之遙&lt;br /&gt;
這是近乎無限的尺度，足以測量&lt;br /&gt;
穿過廣場所要經歷的一個幽閉時代有多麼漫長&lt;/p&gt;

&lt;p&gt;對幽閉的普遍恐懼，使人們從各自的棲居&lt;br /&gt;
雲集廣場，把一生中的孤獨時刻變成熱烈的節日&lt;br /&gt;
但在棲居深處，在愛與死的默默的注目禮中&lt;br /&gt;
一個空無人際的影子廣場被珍藏着&lt;br /&gt;
像緊閉的懺悔室只屬於內心的祕密&lt;/p&gt;

&lt;p&gt;是否穿越廣場之前必須穿越內心的黑暗&lt;br /&gt;
現在黑暗中最黑的兩個世界合爲一體&lt;br /&gt;
堅硬的石頭腦袋被劈開&lt;br /&gt;
利劍在黑暗中閃閃發光&lt;/p&gt;

&lt;p&gt;如果我能用被劈成兩半的神祕黑夜&lt;br /&gt;
去解釋一個雙腳踏在大地上的明媚早晨 &amp;mdash;&amp;mdash;&lt;br /&gt;
如果我能沿着灑滿晨曦的臺階&lt;br /&gt;
去登上虛無之巔的巨人的肩膀&lt;br /&gt;
不是爲了升起，而是爲了隕落 &amp;mdash;&amp;mdash;&lt;br /&gt;
如果黃金鐫刻的銘文不是爲了被傳頌&lt;br /&gt;
而是爲了被抹去，被遺忘，被踐踏 &amp;mdash;&amp;mdash;&lt;/p&gt;

&lt;p&gt;正如一個被踐踏的廣場遲早要落到踐踏者頭上&lt;br /&gt;
哪些曾在一個明媚早晨穿過廣場的人&lt;br /&gt;
他們的黑色皮鞋也遲早要落到利劍之上&lt;br /&gt;
像必將落下的棺蓋落到棺材上那麼沉重&lt;br /&gt;
躺在裏面的不是我，也不是&lt;br /&gt;
行走在劍刃上的人&lt;/p&gt;

&lt;p&gt;我沒想到這麼多人會在一個明媚的早晨&lt;br /&gt;
穿過廣場，避開孤獨和永生&lt;br /&gt;
他們時幽閉時代的倖存者&lt;br /&gt;
我沒想到他們會在傍晚時離去或倒下&lt;/p&gt;

&lt;p&gt;一個無人倒下的地方不是廣場&lt;br /&gt;
一個無人站立的地方也不是&lt;br /&gt;
我曾時站着的嗎？還要站立多久？&lt;br /&gt;
畢竟我和哪些倒下去的人一樣&lt;br /&gt;
從來不是一個永生者&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;https://winterwang.github.io/post/2019-06-06_files/toprotest1.png&#34; alt=&#34;go to tiananmen is my duty&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Berksonian Bias</title>
      <link>https://winterwang.github.io/post/berksonian-bias/</link>
      <pubDate>Sat, 09 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/berksonian-bias/</guid>
      <description>&lt;p&gt;在幫別人做論文質量評價的時候，也是個自我學習的過程。在下不才，目前無償爲幾個雜誌做審稿的工作。&lt;/p&gt;

&lt;p&gt;今天發來的稿件是某財大氣粗，曾經因爲篡改患者治療數據而臭名昭著的製藥公司員工寫的文章。那文章讀起來不長，卻實在是狗屎一堆。他們對歐洲高血壓患者中，直腸息肉的發生和直腸癌的發病率進行了數據庫分析。其中作者提到它們爲了避免&lt;a href=&#34;http://influentialpoints.com/Training/berksons_bias.htm&#34; target=&#34;_blank&#34;&gt;伯克森偏倚(Berksonian Bias)&lt;/a&gt;，還對未執行直腸指檢篩查的高血壓患者的數據進行了分析。聽說高血壓患者中，直腸癌發病率也高（不知是真是假）。該文章在背景中大言不慚說別人的研究都搞不定到底是因爲治療高血壓的藥物導致了直腸癌，還是高血壓本身導致了直腸癌。我讀到這裏不禁已經覺得此文下半身應該已經涼涼。忍住不笑看完它們華麗麗地對每個risk factor單獨進行了一個回歸模型之後我的下巴已經掉了一半。心中暗想，他家就這水平還生產全世界最著名的高血壓藥物真的沒關係嗎？怪不得要去篡改患者的數據了。&lt;/p&gt;

&lt;p&gt;在鄙人的觀察下，該論文估計是沒戲了，但是這個偏倚的類型卻是我這個流行病學博士僧未曾聽說過的。算是這篇屎一樣的論文中唯一的發光點。&lt;/p&gt;

&lt;p&gt;原來這種偏倚最常見於樣本均採集自醫院樣本的病例對照研究。&lt;/p&gt;

&lt;p&gt;如果我們使用均來自於醫院的樣本作爲病例對照研究的對象，那麼我們需要假設，我們研究A疾病患者被收治到醫院內的概率，不會被該疾病可能的 risk factor 影響。但是，這種前提很多時候是無法被滿足的。特別是當A疾病的 risk factor 本身是另一種疾病的時候。理由很簡單，一名同時患有兩種疾病的患者去醫院報道的概率，顯然是要高於只有一種疾病的患者的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;When we take the sample we have to assume that the chance of admission to hospital for the disease is not affected by the presence or absence of the risk factor for that disease. This may not be the case, especially if the risk factor is another disease. This is because people are more likely to be hospitalized if they have two diseases, rather than only one.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如本文開頭使用的圖片中所提示的那樣，Sacket (1979)&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:Sackett-D-L-1979&#34;&gt;&lt;a href=&#34;#fn:Sackett-D-L-1979&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; 當年從一般社區人羣中採集了2784名樣本進行調查，看這些研究對象中呼吸道系統疾病，和身體機能嚴重下降(locomotor disease)之間是否存在相關性。然後，他又對同一樣本中，過去半年內住院過的患者進行了相同的分析。結果顯示，如果只看住院的患者數據，作者會作出&amp;rdquo;患有呼吸道疾病，有較高概率導致身體機能嚴重下降。&amp;rdquo;這樣的結論。也就是兩種疾病之間存在相關性。但是如果看右半側全體社區人羣數據的話，兩種疾病之間並無顯著的關係。顯然後者才是正確的結論。前者之所以會推導出錯誤的結論，純粹是因爲同時患有兩種疾病的人，比起只有呼吸系統疾病，或者只有身體機能下降的患者有更高的概率&lt;strong&gt;住院&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;回頭想起我們曾經做過那麼多院內病例對照研究，不禁感到細思極恐。。。。。。&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:Sackett-D-L-1979&#34;&gt;Sackett, D.L. (1979). Bias in analytic research. Journal of Chronic Diseases 32, 51-63. &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:Sackett-D-L-1979&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Rstan Wonderful R-(5)</title>
      <link>https://winterwang.github.io/post/logistic-rstan2/</link>
      <pubDate>Sun, 03 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/logistic-rstan2/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a&gt;另一種形式的貝葉斯邏輯回歸&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a&gt;分析的目的&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a&gt;確認數據分佈&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a&gt;思考數據模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#-stan-&#34;&gt;寫下 Stan 模型代碼&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a&gt;檢查模型參數的收斂情況&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a&gt;檢查模型的擬合情況&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div class=&#34;section level1&#34;&gt;
&lt;h1&gt;另一種形式的貝葉斯邏輯回歸&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://wangcc.me/post/logistic-rstan/&#34;&gt;前面一節&lt;/a&gt;使用的數據是以學生爲單位，將每名學生的實際課時數和實際出勤數進行了彙總之後的總結性數據，本章我們來看看相同數據的另一種形式。由於分析中有人建議說，天氣狀況對出勤率也是有較大的影響的，所以希望在&lt;a href=&#34;https://wangcc.me/post/logistic-rstan/&#34;&gt;前一節&lt;/a&gt;已有的邏輯回歸模型中增加對天氣狀況的調整。那麼這時候需要使用的就是彙總之前的數據，也就是要是用實際記錄了每名學生每一次課時的出勤與否的原始數據。值得注意的是，這時候&lt;a href=&#34;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap05/input/data-attendance-3.txt&#34;&gt;原始數據&lt;/a&gt;中每名學生的記錄有許多行，因爲每行記錄的是該名學生每次上課時的天氣狀況和他/她是否出勤(0,1)的結果。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d &amp;lt;- read.table(&amp;quot;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap05/input/data-attendance-3.txt&amp;quot;, 
                     sep = &amp;quot;,&amp;quot;, header = T)
head(d, 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    PersonID A Score Weather Y
## 1         1 0    69       B 1
## 2         1 0    69       A 1
## 3         1 0    69       C 1
## 4         1 0    69       A 1
## 5         1 0    69       B 1
## 6         1 0    69       B 1
## 7         1 0    69       C 0
## 8         1 0    69       B 1
## 9         1 0    69       A 1
## 10        1 0    69       A 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Weather\)&lt;/span&gt;，天氣數據 (&lt;code&gt;A&lt;/code&gt; = 晴天，&lt;code&gt;B&lt;/code&gt; = 多雲，&lt;code&gt;C&lt;/code&gt; = 下雨)；&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;，該次課時學生是否出勤 (&lt;code&gt;0&lt;/code&gt; = 缺勤，&lt;code&gt;1&lt;/code&gt; = 出勤)。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其他的數據和前一節中使用的數據相同。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level1&#34;&gt;
&lt;h1&gt;分析的目的&lt;/h1&gt;
&lt;p&gt;本次數據分析的目的依然是瞭解幾個預測變量，天氣，是否喜歡打工，是否熱愛學習，對學生出勤率的影響。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level1&#34;&gt;
&lt;h1&gt;確認數據分佈&lt;/h1&gt;
&lt;p&gt;你可以用先進的 &lt;code&gt;tidyverse&lt;/code&gt; 進行簡單的數據彙總，看看天氣狀況不同時實際出勤率是否有差別:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
d %&amp;gt;% 
  group_by(Weather, Y) %&amp;gt;% 
  summarise (n= n()) %&amp;gt;%
  mutate(rel.freq = paste0(round(100 * n/sum(n), 2), &amp;quot;%&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 4
## # Groups:   Weather [3]
##   Weather     Y     n rel.freq
##   &amp;lt;fct&amp;gt;   &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;   
## 1 A           0   306 24.31%  
## 2 A           1   953 75.69%  
## 3 B           0   230 31.51%  
## 4 B           1   500 68.49%  
## 5 C           0   138 33.91%  
## 6 C           1   269 66.09%&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果你不想學習 &lt;code&gt;tidyverse&lt;/code&gt;，也可以用下面的方法獲得類似的效果，&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;aggregate(Y ~ Weather, data = d, FUN = table)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Weather Y.0 Y.1
## 1       A 306 953
## 2       B 230 500
## 3       C 138 269&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;無論是哪種方法，我們都能大概猜出，天氣是晴天的時候 (&lt;code&gt;Weather = A&lt;/code&gt;)，出勤率相對較高。&lt;/p&gt;
&lt;p&gt;在作者的原著中，&lt;a href=&#34;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap05/run-model5-5.R&#34;&gt;使用的是給分類型變量強制賦予連續值的方法&lt;/a&gt;，這點確實有點噁心，爲了正常的模型，我們需要把天氣轉換成爲更加常見的啞變量 (dummy variable) 如下:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d &amp;lt;- fastDummies::dummy_cols(d, select_columns = &amp;quot;Weather&amp;quot;)
head(d)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   PersonID A Score Weather Y Weather_B Weather_A Weather_C
## 1        1 0    69       B 1         1         0         0
## 2        1 0    69       A 1         0         1         0
## 3        1 0    69       C 1         0         0         1
## 4        1 0    69       A 1         0         1         0
## 5        1 0    69       B 1         1         0         0
## 6        1 0    69       B 1         1         0         0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&#34;section level1&#34;&gt;
&lt;h1&gt;思考數據模型&lt;/h1&gt;
&lt;p&gt;我們設想的數學模型應該是這樣子的:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\text{logit}(q[i]) &amp;amp; = b_{1} + b_{2}A_{i} + b_{3}\text{Score}_{i} + b_{4}\text{WeatherB} + b_{5}\text{WeatherC} \\ 
\text{where} &amp;amp; \\ 
&amp;amp; \text{ WeatherB} = 0, \text{ WeatherC} = 0 \text{ indicates weather = A} \\ 
&amp;amp; \text{ WeatherB} = 1, \text{ WeatherC} = 0 \text{ indicates weather = B} \\ 
&amp;amp; \text{ WeatherB} = 0, \text{ WeatherC} = 1 \text{ indicates weather = C} \\
Y[i] &amp;amp;\sim \text{Bernulli}(q[i])
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;-stan-&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;寫下 Stan 模型代碼&lt;/h1&gt;
&lt;p&gt;下面是相應的 Stan 模型:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data {
  int I;
  int&amp;lt;lower=0, upper=1&amp;gt; A[I];
  real&amp;lt;lower=0, upper=1&amp;gt; Score[I];
  int&amp;lt;lower=0, upper=1&amp;gt; W_B[I];
  int&amp;lt;lower=0, upper=1&amp;gt; W_C[I];
  int&amp;lt;lower=0, upper=1&amp;gt; Y[I];
}

// The parameters accepted by the model. 
parameters {
  real b[5];
}

// The model to be estimated. 
model {
   for (i in 1:I)
    Y[i] ~ bernoulli_logit(b[1] + b[2]*A[i] + b[3]*Score[i] + b[4]*W_B[i] + b[5]*W_C[i]);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;和跑它們的 R 代碼&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rstan)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: StanHeaders&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## rstan (Version 2.18.2, GitRev: 2e1f913d3ca3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## For execution on a local, multicore CPU with excess RAM we recommend calling
## options(mc.cores = parallel::detectCores()).
## To avoid recompilation of unchanged Stan programs, we recommend calling
## rstan_options(auto_write = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;rstan&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:tidyr&amp;#39;:
## 
##     extract&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data &amp;lt;- list(I=nrow(d), A=d$A, Score=d$Score/200, 
             W_A=d$Weather_A, W_B = d$Weather_B, W_C = d$Weather_C, 
             Y=d$Y)
fit1 &amp;lt;- stan(file=&amp;#39;stanfiles/myex4.stan&amp;#39;, data=data, pars=c(&amp;#39;b&amp;#39;, &amp;quot;OR1&amp;quot;, &amp;quot;OR2&amp;quot;, &amp;quot;OR3&amp;quot;, &amp;quot;OR4&amp;quot;, &amp;quot;q&amp;quot;), seed=1234)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &amp;#39;myex4&amp;#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0.000825 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 8.25 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 9.00202 seconds (Warm-up)
## Chain 1:                11.9006 seconds (Sampling)
## Chain 1:                20.9026 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &amp;#39;myex4&amp;#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 0.000453 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 4.53 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 10.2069 seconds (Warm-up)
## Chain 2:                10.8169 seconds (Sampling)
## Chain 2:                21.0238 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &amp;#39;myex4&amp;#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 0.00047 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 4.7 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 9.80458 seconds (Warm-up)
## Chain 3:                10.5184 seconds (Sampling)
## Chain 3:                20.3229 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &amp;#39;myex4&amp;#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 0.000455 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 4.55 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 9.61086 seconds (Warm-up)
## Chain 4:                9.89098 seconds (Sampling)
## Chain 4:                19.5018 seconds (Total)
## Chain 4:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Stan model: myex4.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##             mean se_mean   sd     2.5%      25%      50%      75%    97.5%
## b[1]        0.27    0.00 0.23    -0.18     0.11     0.27     0.42     0.73
## b[2]       -0.63    0.00 0.09    -0.81    -0.69    -0.63    -0.56    -0.45
## b[3]        1.96    0.01 0.36     1.27     1.72     1.96     2.21     2.65
## b[4]       -0.38    0.00 0.11    -0.59    -0.45    -0.38    -0.31    -0.17
## b[5]       -0.50    0.00 0.12    -0.73    -0.58    -0.50    -0.41    -0.25
## OR1         0.54    0.00 0.05     0.45     0.50     0.54     0.57     0.64
## OR2         7.60    0.06 2.79     3.55     5.58     7.12     9.13    14.12
## OR3         0.69    0.00 0.07     0.55     0.64     0.69     0.74     0.85
## OR4         0.61    0.00 0.08     0.48     0.56     0.61     0.66     0.78
## q[1]        0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[2]        0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[3]        0.61    0.00 0.03     0.54     0.59     0.61     0.63     0.68
## q[4]        0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[5]        0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[6]        0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[7]        0.61    0.00 0.03     0.54     0.59     0.61     0.63     0.68
## q[8]        0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[9]        0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[10]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[11]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[12]       0.61    0.00 0.03     0.54     0.59     0.61     0.63     0.68
## q[13]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[14]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[15]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[16]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[17]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[18]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[19]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[20]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[21]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[22]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[23]       0.61    0.00 0.03     0.54     0.59     0.61     0.63     0.68
## q[24]       0.61    0.00 0.03     0.54     0.59     0.61     0.63     0.68
## q[25]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[26]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[27]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[28]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[29]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[30]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[31]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[32]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[33]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[34]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[35]       0.61    0.00 0.03     0.54     0.59     0.61     0.63     0.68
## q[36]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[37]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[38]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[39]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[40]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[41]       0.72    0.00 0.02     0.67     0.70     0.72     0.74     0.77
## q[42]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[43]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[44]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[45]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[46]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[47]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[48]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[49]       0.67    0.00 0.02     0.62     0.65     0.66     0.68     0.71
## q[50]       0.67    0.00 0.02     0.62     0.65     0.66     0.68     0.71
## q[51]       0.67    0.00 0.02     0.62     0.65     0.66     0.68     0.71
## q[52]       0.67    0.00 0.02     0.62     0.65     0.66     0.68     0.71
## q[53]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[54]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[55]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[56]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[57]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[58]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[59]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[60]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[61]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[62]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[63]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[64]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[65]       0.67    0.00 0.02     0.62     0.65     0.66     0.68     0.71
## q[66]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[67]       0.67    0.00 0.02     0.62     0.65     0.66     0.68     0.71
## q[68]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[69]       0.67    0.00 0.02     0.62     0.65     0.66     0.68     0.71
## q[70]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[71]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[72]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[73]       0.67    0.00 0.02     0.62     0.65     0.66     0.68     0.71
## q[74]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[75]       0.67    0.00 0.02     0.62     0.65     0.66     0.68     0.71
## q[76]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[77]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[78]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[79]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[80]       0.67    0.00 0.02     0.62     0.65     0.66     0.68     0.71
## q[81]       0.67    0.00 0.02     0.62     0.65     0.66     0.68     0.71
## q[82]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[83]       0.67    0.00 0.02     0.62     0.65     0.66     0.68     0.71
## q[84]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[85]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[86]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[87]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[88]       0.67    0.00 0.02     0.62     0.65     0.66     0.68     0.71
## q[89]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[90]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[91]       0.67    0.00 0.02     0.62     0.65     0.66     0.68     0.71
## q[92]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[93]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[94]       0.67    0.00 0.02     0.62     0.65     0.66     0.68     0.71
## q[95]       0.67    0.00 0.02     0.62     0.65     0.66     0.68     0.71
## q[96]       0.67    0.00 0.02     0.62     0.65     0.66     0.68     0.71
## q[97]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[98]       0.74    0.00 0.02     0.71     0.73     0.74     0.76     0.78
## q[99]       0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.69
## q[100]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[101]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[102]      0.73    0.00 0.02     0.68     0.72     0.73     0.74     0.77
## q[103]      0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[104]      0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[105]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[106]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[107]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[108]      0.73    0.00 0.02     0.68     0.72     0.73     0.74     0.77
## q[109]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[110]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[111]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[112]      0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[113]      0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[114]      0.73    0.00 0.02     0.68     0.72     0.73     0.74     0.77
## q[115]      0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[116]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[117]      0.73    0.00 0.02     0.68     0.72     0.73     0.74     0.77
## q[118]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[119]      0.73    0.00 0.02     0.68     0.72     0.73     0.74     0.77
## q[120]      0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[121]      0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[122]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[123]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[124]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[125]      0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[126]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[127]      0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[128]      0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[129]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[130]      0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[131]      0.73    0.00 0.02     0.68     0.72     0.73     0.74     0.77
## q[132]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[133]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[134]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[135]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[136]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[137]      0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[138]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[139]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[140]      0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[141]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[142]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[143]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[144]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[145]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[146]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[147]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[148]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[149]      0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[150]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[151]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[152]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[153]      0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[154]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[155]      0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[156]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[157]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[158]      0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[159]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[160]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[161]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[162]      0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[163]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[164]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[165]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[166]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[167]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[168]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[169]      0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[170]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[171]      0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[172]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[173]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[174]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[175]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[176]      0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[177]      0.67    0.00 0.03     0.61     0.65     0.67     0.69     0.72
## q[178]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[179]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[180]      0.67    0.00 0.03     0.61     0.65     0.67     0.69     0.72
## q[181]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[182]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[183]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[184]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[185]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[186]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[187]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[188]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[189]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[190]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[191]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[192]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[193]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[194]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[195]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[196]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[197]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[198]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[199]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[200]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[201]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[202]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[203]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[204]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[205]      0.67    0.00 0.03     0.61     0.65     0.67     0.69     0.72
## q[206]      0.67    0.00 0.03     0.61     0.65     0.67     0.69     0.72
## q[207]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[208]      0.69    0.00 0.02     0.64     0.68     0.69     0.71     0.74
## q[209]      0.77    0.00 0.02     0.73     0.75     0.77     0.78     0.80
## q[210]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[211]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[212]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[213]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[214]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[215]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[216]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[217]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[218]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[219]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[220]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[221]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[222]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[223]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[224]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[225]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[226]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[227]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[228]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[229]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[230]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[231]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[232]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[233]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[234]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[235]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[236]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[237]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[238]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[239]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[240]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[241]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[242]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[243]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[244]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[245]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[246]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[247]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[248]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[249]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[250]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[251]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[252]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[253]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[254]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[255]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[256]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[257]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[258]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[259]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[260]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[261]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[262]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[263]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[264]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[265]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[266]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[267]      0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[268]      0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[269]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[270]      0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[271]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[272]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[273]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[274]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[275]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[276]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[277]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[278]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[279]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[280]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[281]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[282]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[283]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[284]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[285]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[286]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[287]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[288]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[289]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[290]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[291]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[292]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[293]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[294]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[295]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[296]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[297]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[298]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[299]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[300]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[301]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[302]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[303]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[304]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[305]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[306]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[307]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[308]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[309]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[310]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[311]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[312]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[313]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[314]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[315]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[316]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[317]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[318]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[319]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[320]      0.67    0.00 0.02     0.62     0.65     0.67     0.69     0.71
## q[321]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[322]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[323]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[324]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[325]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[326]      0.67    0.00 0.02     0.62     0.65     0.67     0.69     0.71
## q[327]      0.67    0.00 0.02     0.62     0.65     0.67     0.69     0.71
## q[328]      0.67    0.00 0.02     0.62     0.65     0.67     0.69     0.71
## q[329]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[330]      0.67    0.00 0.02     0.62     0.65     0.67     0.69     0.71
## q[331]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[332]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[333]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[334]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[335]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[336]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[337]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[338]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[339]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[340]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[341]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[342]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[343]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[344]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[345]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[346]      0.67    0.00 0.02     0.62     0.65     0.67     0.69     0.71
## q[347]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[348]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[349]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[350]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[351]      0.67    0.00 0.02     0.62     0.65     0.67     0.69     0.71
## q[352]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[353]      0.67    0.00 0.02     0.62     0.65     0.67     0.69     0.71
## q[354]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[355]      0.67    0.00 0.02     0.62     0.65     0.67     0.69     0.71
## q[356]      0.67    0.00 0.02     0.62     0.65     0.67     0.69     0.71
## q[357]      0.67    0.00 0.02     0.62     0.65     0.67     0.69     0.71
## q[358]      0.67    0.00 0.02     0.62     0.65     0.67     0.69     0.71
## q[359]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[360]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[361]      0.67    0.00 0.02     0.62     0.65     0.67     0.69     0.71
## q[362]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[363]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[364]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[365]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[366]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[367]      0.67    0.00 0.02     0.62     0.65     0.67     0.69     0.71
## q[368]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[369]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[370]      0.67    0.00 0.02     0.62     0.65     0.67     0.69     0.71
## q[371]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[372]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[373]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[374]      0.67    0.00 0.02     0.62     0.65     0.67     0.69     0.71
## q[375]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[376]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[377]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[378]      0.67    0.00 0.02     0.62     0.65     0.67     0.69     0.71
## q[379]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[380]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[381]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[382]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[383]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[384]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[385]      0.67    0.00 0.02     0.62     0.65     0.67     0.69     0.71
## q[386]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[387]      0.67    0.00 0.02     0.62     0.65     0.67     0.69     0.71
## q[388]      0.67    0.00 0.02     0.62     0.65     0.67     0.69     0.71
## q[389]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[390]      0.67    0.00 0.02     0.62     0.65     0.67     0.69     0.71
## q[391]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[392]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[393]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[394]      0.64    0.00 0.03     0.59     0.62     0.64     0.66     0.70
## q[395]      0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[396]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[397]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[398]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[399]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[400]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[401]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[402]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[403]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[404]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[405]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[406]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[407]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[408]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[409]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[410]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[411]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[412]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[413]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[414]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[415]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[416]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[417]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[418]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[419]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[420]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[421]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[422]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[423]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[424]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[425]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[426]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[427]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[428]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[429]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[430]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[431]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[432]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[433]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[434]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[435]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[436]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[437]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[438]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[439]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[440]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[441]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[442]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[443]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[444]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[445]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[446]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[447]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[448]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[449]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[450]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[451]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[452]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[453]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[454]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[455]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[456]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[457]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[458]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[459]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[460]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[461]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[462]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[463]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[464]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[465]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[466]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[467]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[468]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[469]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[470]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[471]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[472]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[473]      0.85    0.00 0.01     0.82     0.84     0.85     0.85     0.87
## q[474]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[475]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[476]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[477]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[478]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[479]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[480]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[481]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[482]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[483]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[484]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[485]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[486]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[487]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[488]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[489]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[490]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[491]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[492]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[493]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[494]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[495]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[496]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[497]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[498]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[499]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[500]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[501]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[502]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[503]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[504]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[505]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[506]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[507]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[508]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[509]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[510]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[511]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[512]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[513]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[514]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[515]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[516]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[517]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[518]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[519]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[520]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[521]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[522]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[523]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[524]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[525]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[526]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[527]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[528]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[529]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[530]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[531]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[532]      0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.82
## q[533]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[534]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[535]      0.84    0.00 0.01     0.82     0.84     0.84     0.85     0.87
## q[536]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[537]      0.77    0.00 0.02     0.72     0.75     0.77     0.78     0.81
## q[538]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[539]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[540]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[541]      0.66    0.00 0.02     0.61     0.64     0.66     0.67     0.70
## q[542]      0.66    0.00 0.02     0.61     0.64     0.66     0.67     0.70
## q[543]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[544]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[545]      0.63    0.00 0.03     0.57     0.61     0.63     0.65     0.68
## q[546]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[547]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[548]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[549]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[550]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[551]      0.66    0.00 0.02     0.61     0.64     0.66     0.67     0.70
## q[552]      0.63    0.00 0.03     0.57     0.61     0.63     0.65     0.68
## q[553]      0.66    0.00 0.02     0.61     0.64     0.66     0.67     0.70
## q[554]      0.66    0.00 0.02     0.61     0.64     0.66     0.67     0.70
## q[555]      0.63    0.00 0.03     0.57     0.61     0.63     0.65     0.68
## q[556]      0.63    0.00 0.03     0.57     0.61     0.63     0.65     0.68
## q[557]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[558]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[559]      0.63    0.00 0.03     0.57     0.61     0.63     0.65     0.68
## q[560]      0.66    0.00 0.02     0.61     0.64     0.66     0.67     0.70
## q[561]      0.63    0.00 0.03     0.57     0.61     0.63     0.65     0.68
## q[562]      0.66    0.00 0.02     0.61     0.64     0.66     0.67     0.70
## q[563]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[564]      0.66    0.00 0.02     0.61     0.64     0.66     0.67     0.70
## q[565]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[566]      0.63    0.00 0.03     0.57     0.61     0.63     0.65     0.68
## q[567]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[568]      0.66    0.00 0.02     0.61     0.64     0.66     0.67     0.70
## q[569]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[570]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[571]      0.74    0.00 0.02     0.70     0.72     0.74     0.75     0.77
## q[572]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[573]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[574]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[575]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[576]      0.75    0.00 0.02     0.71     0.74     0.75     0.77     0.80
## q[577]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[578]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[579]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[580]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[581]      0.75    0.00 0.02     0.71     0.74     0.75     0.77     0.80
## q[582]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[583]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[584]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[585]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[586]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[587]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[588]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[589]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[590]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[591]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[592]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[593]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[594]      0.75    0.00 0.02     0.71     0.74     0.75     0.77     0.80
## q[595]      0.75    0.00 0.02     0.71     0.74     0.75     0.77     0.80
## q[596]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[597]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[598]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[599]      0.75    0.00 0.02     0.71     0.74     0.75     0.77     0.80
## q[600]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[601]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[602]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[603]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[604]      0.75    0.00 0.02     0.71     0.74     0.75     0.77     0.80
## q[605]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[606]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[607]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[608]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[609]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[610]      0.75    0.00 0.02     0.71     0.74     0.75     0.77     0.80
## q[611]      0.77    0.00 0.02     0.74     0.76     0.77     0.79     0.81
## q[612]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[613]      0.83    0.00 0.01     0.81     0.83     0.83     0.84     0.86
## q[614]      0.75    0.00 0.02     0.71     0.74     0.75     0.77     0.80
## q[615]      0.75    0.00 0.02     0.71     0.74     0.75     0.77     0.80
## q[616]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[617]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[618]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[619]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[620]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[621]      0.58    0.00 0.03     0.52     0.56     0.58     0.59     0.63
## q[622]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[623]      0.58    0.00 0.03     0.52     0.56     0.58     0.59     0.63
## q[624]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[625]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[626]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[627]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[628]      0.58    0.00 0.03     0.52     0.56     0.58     0.59     0.63
## q[629]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[630]      0.58    0.00 0.03     0.52     0.56     0.58     0.59     0.63
## q[631]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[632]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[633]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[634]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[635]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[636]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[637]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[638]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[639]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[640]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[641]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[642]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[643]      0.58    0.00 0.03     0.52     0.56     0.58     0.59     0.63
## q[644]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[645]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[646]      0.58    0.00 0.03     0.52     0.56     0.58     0.59     0.63
## q[647]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[648]      0.58    0.00 0.03     0.52     0.56     0.58     0.59     0.63
## q[649]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[650]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[651]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[652]      0.58    0.00 0.03     0.52     0.56     0.58     0.59     0.63
## q[653]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[654]      0.58    0.00 0.03     0.52     0.56     0.58     0.59     0.63
## q[655]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[656]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[657]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[658]      0.58    0.00 0.03     0.52     0.56     0.58     0.59     0.63
## q[659]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[660]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[661]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[662]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[663]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[664]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[665]      0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.65
## q[666]      0.69    0.00 0.02     0.66     0.68     0.69     0.70     0.72
## q[667]      0.58    0.00 0.03     0.52     0.56     0.58     0.59     0.63
## q[668]      0.58    0.00 0.03     0.52     0.56     0.58     0.59     0.63
## q[669]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[670]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[671]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[672]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[673]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[674]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[675]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[676]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[677]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[678]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[679]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[680]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[681]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[682]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[683]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[684]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[685]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[686]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[687]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[688]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[689]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[690]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[691]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[692]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[693]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[694]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[695]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[696]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[697]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[698]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[699]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[700]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[701]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[702]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[703]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[704]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[705]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[706]      0.73    0.00 0.02     0.69     0.71     0.73     0.74     0.76
## q[707]      0.80    0.00 0.01     0.77     0.79     0.80     0.80     0.82
## q[708]      0.70    0.00 0.02     0.65     0.69     0.70     0.72     0.75
## q[709]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[710]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[711]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[712]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[713]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[714]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[715]      0.70    0.00 0.02     0.65     0.68     0.70     0.71     0.75
## q[716]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[717]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[718]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[719]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[720]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[721]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[722]      0.70    0.00 0.02     0.65     0.68     0.70     0.71     0.75
## q[723]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[724]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[725]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[726]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[727]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[728]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[729]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[730]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[731]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[732]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[733]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[734]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[735]      0.70    0.00 0.02     0.65     0.68     0.70     0.71     0.75
## q[736]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[737]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[738]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[739]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[740]      0.70    0.00 0.02     0.65     0.68     0.70     0.71     0.75
## q[741]      0.70    0.00 0.02     0.65     0.68     0.70     0.71     0.75
## q[742]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[743]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[744]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[745]      0.70    0.00 0.02     0.65     0.68     0.70     0.71     0.75
## q[746]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[747]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[748]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[749]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[750]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[751]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[752]      0.70    0.00 0.02     0.65     0.68     0.70     0.71     0.75
## q[753]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[754]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[755]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[756]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[757]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[758]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[759]      0.70    0.00 0.02     0.65     0.68     0.70     0.71     0.75
## q[760]      0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.76
## q[761]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[762]      0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.82
## q[763]      0.70    0.00 0.02     0.65     0.68     0.70     0.71     0.75
## q[764]      0.70    0.00 0.02     0.65     0.68     0.70     0.71     0.75
## q[765]      0.56    0.00 0.02     0.51     0.54     0.56     0.58     0.61
## q[766]      0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[767]      0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[768]      0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[769]      0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[770]      0.56    0.00 0.02     0.51     0.54     0.56     0.58     0.61
## q[771]      0.56    0.00 0.02     0.51     0.54     0.56     0.58     0.61
## q[772]      0.56    0.00 0.02     0.51     0.54     0.56     0.58     0.61
## q[773]      0.56    0.00 0.02     0.51     0.54     0.56     0.58     0.61
## q[774]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[775]      0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[776]      0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[777]      0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[778]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[779]      0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[780]      0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[781]      0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[782]      0.56    0.00 0.02     0.51     0.54     0.56     0.58     0.61
## q[783]      0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[784]      0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[785]      0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[786]      0.56    0.00 0.02     0.51     0.54     0.56     0.58     0.61
## q[787]      0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[788]      0.56    0.00 0.02     0.51     0.54     0.56     0.58     0.61
## q[789]      0.56    0.00 0.02     0.51     0.54     0.56     0.58     0.61
## q[790]      0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[791]      0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[792]      0.56    0.00 0.02     0.51     0.54     0.56     0.58     0.61
## q[793]      0.56    0.00 0.02     0.51     0.54     0.56     0.58     0.61
## q[794]      0.56    0.00 0.02     0.51     0.54     0.56     0.58     0.61
## q[795]      0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[796]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[797]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[798]      0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[799]      0.56    0.00 0.02     0.51     0.54     0.56     0.58     0.61
## q[800]      0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[801]      0.56    0.00 0.02     0.51     0.54     0.56     0.58     0.61
## q[802]      0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[803]      0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[804]      0.56    0.00 0.02     0.51     0.54     0.56     0.58     0.61
## q[805]      0.56    0.00 0.02     0.51     0.54     0.56     0.58     0.61
## q[806]      0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[807]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[808]      0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[809]      0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[810]      0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[811]      0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[812]      0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[813]      0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[814]      0.56    0.00 0.02     0.51     0.54     0.56     0.58     0.61
## q[815]      0.56    0.00 0.02     0.51     0.54     0.56     0.58     0.61
## q[816]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[817]      0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[818]      0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[819]      0.56    0.00 0.02     0.51     0.54     0.56     0.58     0.61
## q[820]      0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[821]      0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[822]      0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[823]      0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[824]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[825]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[826]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[827]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[828]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[829]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[830]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[831]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[832]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[833]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[834]      0.71    0.00 0.02     0.66     0.69     0.71     0.72     0.76
## q[835]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[836]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[837]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[838]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[839]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[840]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[841]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[842]      0.71    0.00 0.02     0.66     0.69     0.71     0.72     0.76
## q[843]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[844]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[845]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[846]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[847]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[848]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[849]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[850]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[851]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[852]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[853]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[854]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[855]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[856]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[857]      0.71    0.00 0.02     0.66     0.69     0.71     0.72     0.76
## q[858]      0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.77
## q[859]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[860]      0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.83
## q[861]      0.71    0.00 0.02     0.66     0.69     0.71     0.72     0.76
## q[862]      0.67    0.00 0.03     0.61     0.65     0.67     0.68     0.71
## q[863]      0.74    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[864]      0.74    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[865]      0.74    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[866]      0.74    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[867]      0.74    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[868]      0.67    0.00 0.03     0.61     0.65     0.67     0.68     0.71
## q[869]      0.67    0.00 0.03     0.61     0.65     0.67     0.68     0.71
## q[870]      0.67    0.00 0.03     0.61     0.65     0.67     0.68     0.71
## q[871]      0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.70
## q[872]      0.74    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[873]      0.74    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[874]      0.74    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[875]      0.74    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[876]      0.74    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[877]      0.74    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[878]      0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.70
## q[879]      0.74    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[880]      0.74    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[881]      0.74    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[882]      0.67    0.00 0.03     0.61     0.65     0.67     0.68     0.71
## q[883]      0.74    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[884]      0.74    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[885]      0.67    0.00 0.03     0.61     0.65     0.67     0.68     0.71
## q[886]      0.67    0.00 0.03     0.61     0.65     0.67     0.68     0.71
## q[887]      0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.70
## q[888]      0.74    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[889]      0.67    0.00 0.03     0.61     0.65     0.67     0.68     0.71
## q[890]      0.74    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[891]      0.67    0.00 0.03     0.61     0.65     0.67     0.68     0.71
## q[892]      0.74    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[893]      0.74    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[894]      0.74    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[895]      0.74    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[896]      0.74    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[897]      0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.70
## q[898]      0.74    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[899]      0.67    0.00 0.03     0.61     0.65     0.67     0.68     0.71
## q[900]      0.74    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[901]      0.74    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[902]      0.67    0.00 0.03     0.61     0.65     0.67     0.68     0.71
## q[903]      0.74    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[904]      0.74    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[905]      0.64    0.00 0.03     0.58     0.62     0.64     0.66     0.70
## q[906]      0.74    0.00 0.02     0.70     0.73     0.75     0.76     0.78
## q[907]      0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.91
## q[908]      0.83    0.00 0.02     0.78     0.81     0.83     0.85     0.87
## q[909]      0.85    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[910]      0.85    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[911]      0.83    0.00 0.02     0.78     0.81     0.83     0.85     0.87
## q[912]      0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.91
## q[913]      0.85    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[914]      0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.91
## q[915]      0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.91
## q[916]      0.85    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[917]      0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.91
## q[918]      0.85    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[919]      0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.91
## q[920]      0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.91
## q[921]      0.85    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[922]      0.85    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[923]      0.83    0.00 0.02     0.78     0.81     0.83     0.85     0.87
## q[924]      0.85    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[925]      0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.91
## q[926]      0.85    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[927]      0.85    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[928]      0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.91
## q[929]      0.85    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[930]      0.83    0.00 0.02     0.78     0.81     0.83     0.85     0.87
## q[931]      0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.91
## q[932]      0.83    0.00 0.02     0.78     0.81     0.83     0.85     0.87
## q[933]      0.85    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[934]      0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.91
## q[935]      0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.91
## q[936]      0.85    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[937]      0.85    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[938]      0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.91
## q[939]      0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.91
## q[940]      0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.91
## q[941]      0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.91
## q[942]      0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.91
## q[943]      0.85    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[944]      0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.91
## q[945]      0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.91
## q[946]      0.83    0.00 0.02     0.78     0.81     0.83     0.85     0.87
## q[947]      0.83    0.00 0.02     0.78     0.81     0.83     0.85     0.87
## q[948]      0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.91
## q[949]      0.85    0.00 0.02     0.80     0.83     0.85     0.86     0.88
## q[950]      0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.91
## q[951]      0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.91
## q[952]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[953]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[954]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[955]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[956]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[957]      0.69    0.00 0.02     0.64     0.67     0.69     0.71     0.73
## q[958]      0.69    0.00 0.02     0.64     0.67     0.69     0.71     0.73
## q[959]      0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[960]      0.69    0.00 0.02     0.64     0.67     0.69     0.71     0.73
## q[961]      0.69    0.00 0.02     0.64     0.67     0.69     0.71     0.73
## q[962]      0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[963]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[964]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[965]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[966]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[967]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[968]      0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[969]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[970]      0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[971]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[972]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[973]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[974]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[975]      0.69    0.00 0.02     0.64     0.67     0.69     0.71     0.73
## q[976]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[977]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[978]      0.69    0.00 0.02     0.64     0.67     0.69     0.71     0.73
## q[979]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[980]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[981]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[982]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[983]      0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[984]      0.69    0.00 0.02     0.64     0.67     0.69     0.71     0.73
## q[985]      0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[986]      0.69    0.00 0.02     0.64     0.67     0.69     0.71     0.73
## q[987]      0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[988]      0.69    0.00 0.02     0.64     0.67     0.69     0.71     0.73
## q[989]      0.69    0.00 0.02     0.64     0.67     0.69     0.71     0.73
## q[990]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[991]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[992]      0.69    0.00 0.02     0.64     0.67     0.69     0.71     0.73
## q[993]      0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[994]      0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[995]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[996]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[997]      0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[998]      0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[999]      0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1000]     0.69    0.00 0.02     0.64     0.67     0.69     0.71     0.73
## q[1001]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1002]     0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[1003]     0.69    0.00 0.02     0.64     0.67     0.69     0.71     0.73
## q[1004]     0.69    0.00 0.02     0.64     0.67     0.69     0.71     0.73
## q[1005]     0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[1006]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1007]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1008]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1009]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1010]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1011]     0.69    0.00 0.02     0.64     0.67     0.69     0.71     0.73
## q[1012]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1013]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1014]     0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[1015]     0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[1016]     0.69    0.00 0.02     0.64     0.67     0.69     0.71     0.73
## q[1017]     0.69    0.00 0.02     0.64     0.67     0.69     0.71     0.73
## q[1018]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1019]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1020]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1021]     0.69    0.00 0.02     0.64     0.67     0.69     0.71     0.73
## q[1022]     0.69    0.00 0.02     0.64     0.67     0.69     0.71     0.73
## q[1023]     0.69    0.00 0.02     0.64     0.67     0.69     0.71     0.73
## q[1024]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1025]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1026]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1027]     0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[1028]     0.66    0.00 0.03     0.60     0.64     0.66     0.68     0.72
## q[1029]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1030]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1031]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1032]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1033]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1034]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1035]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1036]     0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[1037]     0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[1038]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1039]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1040]     0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[1041]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1042]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1043]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1044]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1045]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1046]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1047]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1048]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1049]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1050]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1051]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1052]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1053]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1054]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1055]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1056]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1057]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1058]     0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[1059]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1060]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1061]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1062]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1063]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1064]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1065]     0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[1066]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1067]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1068]     0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[1069]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1070]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1071]     0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[1072]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1073]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1074]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1075]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1076]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1077]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1078]     0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[1079]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1080]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1081]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1082]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1083]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1084]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1085]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1086]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1087]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1088]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1089]     0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[1090]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1091]     0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[1092]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.58
## q[1093]     0.50    0.00 0.03     0.43     0.48     0.50     0.52     0.56
## q[1094]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1095]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1096]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1097]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1098]     0.62    0.00 0.02     0.57     0.60     0.62     0.64     0.67
## q[1099]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.71
## q[1100]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.71
## q[1101]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.71
## q[1102]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.71
## q[1103]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.71
## q[1104]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.62
## q[1105]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1106]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.62
## q[1107]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.62
## q[1108]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.62
## q[1109]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.62
## q[1110]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1111]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.71
## q[1112]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.71
## q[1113]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.71
## q[1114]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1115]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.71
## q[1116]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.71
## q[1117]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.62
## q[1118]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.71
## q[1119]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.71
## q[1120]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.71
## q[1121]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.62
## q[1122]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.71
## q[1123]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.62
## q[1124]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.71
## q[1125]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.71
## q[1126]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.62
## q[1127]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.71
## q[1128]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.71
## q[1129]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.71
## q[1130]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.62
## q[1131]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.62
## q[1132]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.71
## q[1133]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1134]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.62
## q[1135]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1136]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.62
## q[1137]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.62
## q[1138]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.71
## q[1139]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.71
## q[1140]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.62
## q[1141]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1142]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.71
## q[1143]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1144]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.71
## q[1145]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.62
## q[1146]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.71
## q[1147]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.62
## q[1148]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1149]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.62
## q[1150]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.71
## q[1151]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1152]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.71
## q[1153]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.71
## q[1154]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.71
## q[1155]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.71
## q[1156]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.71
## q[1157]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.71
## q[1158]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.62
## q[1159]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.71
## q[1160]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.62
## q[1161]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1162]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1163]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.71
## q[1164]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1165]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.62
## q[1166]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.71
## q[1167]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.71
## q[1168]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.62
## q[1169]     0.58    0.00 0.02     0.53     0.56     0.58     0.60     0.62
## q[1170]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1171]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.71
## q[1172]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.71
## q[1173]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.71
## q[1174]     0.67    0.00 0.02     0.63     0.66     0.67     0.68     0.71
## q[1175]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.61
## q[1176]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1177]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1178]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1179]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.60
## q[1180]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1181]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.60
## q[1182]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1183]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.60
## q[1184]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1185]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1186]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1187]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1188]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1189]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1190]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1191]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1192]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1193]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1194]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1195]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1196]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1197]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1198]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1199]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1200]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1201]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1202]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1203]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1204]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.60
## q[1205]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1206]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1207]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1208]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1209]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1210]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1211]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1212]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.60
## q[1213]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.60
## q[1214]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1215]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.60
## q[1216]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1217]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.60
## q[1218]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1219]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1220]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1221]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1222]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1223]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1224]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.60
## q[1225]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1226]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1227]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1228]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1229]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1230]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.60
## q[1231]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1232]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1233]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1234]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.60
## q[1235]     0.58    0.00 0.02     0.53     0.56     0.58     0.59     0.62
## q[1236]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1237]     0.66    0.00 0.02     0.63     0.65     0.66     0.68     0.70
## q[1238]     0.55    0.00 0.03     0.49     0.53     0.55     0.57     0.60
## q[1239]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1240]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1241]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1242]     0.63    0.00 0.03     0.58     0.62     0.63     0.65     0.69
## q[1243]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1244]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1245]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1246]     0.66    0.00 0.02     0.61     0.65     0.66     0.68     0.70
## q[1247]     0.66    0.00 0.02     0.61     0.65     0.66     0.68     0.70
## q[1248]     0.66    0.00 0.02     0.61     0.65     0.66     0.68     0.70
## q[1249]     0.66    0.00 0.02     0.61     0.65     0.66     0.68     0.70
## q[1250]     0.66    0.00 0.02     0.61     0.65     0.66     0.68     0.70
## q[1251]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1252]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1253]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1254]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1255]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1256]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1257]     0.66    0.00 0.02     0.61     0.65     0.66     0.68     0.70
## q[1258]     0.63    0.00 0.03     0.58     0.62     0.63     0.65     0.69
## q[1259]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1260]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1261]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1262]     0.66    0.00 0.02     0.61     0.65     0.66     0.68     0.70
## q[1263]     0.66    0.00 0.02     0.61     0.65     0.66     0.68     0.70
## q[1264]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1265]     0.66    0.00 0.02     0.61     0.65     0.66     0.68     0.70
## q[1266]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1267]     0.66    0.00 0.02     0.61     0.65     0.66     0.68     0.70
## q[1268]     0.63    0.00 0.03     0.58     0.62     0.63     0.65     0.69
## q[1269]     0.66    0.00 0.02     0.61     0.65     0.66     0.68     0.70
## q[1270]     0.66    0.00 0.02     0.61     0.65     0.66     0.68     0.70
## q[1271]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1272]     0.66    0.00 0.02     0.61     0.65     0.66     0.68     0.70
## q[1273]     0.63    0.00 0.03     0.58     0.62     0.63     0.65     0.69
## q[1274]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1275]     0.66    0.00 0.02     0.61     0.65     0.66     0.68     0.70
## q[1276]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1277]     0.66    0.00 0.02     0.61     0.65     0.66     0.68     0.70
## q[1278]     0.63    0.00 0.03     0.58     0.62     0.63     0.65     0.69
## q[1279]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1280]     0.63    0.00 0.03     0.58     0.62     0.63     0.65     0.69
## q[1281]     0.66    0.00 0.02     0.61     0.65     0.66     0.68     0.70
## q[1282]     0.66    0.00 0.02     0.61     0.65     0.66     0.68     0.70
## q[1283]     0.66    0.00 0.02     0.61     0.65     0.66     0.68     0.70
## q[1284]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1285]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1286]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1287]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1288]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1289]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1290]     0.66    0.00 0.02     0.61     0.65     0.66     0.68     0.70
## q[1291]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1292]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1293]     0.63    0.00 0.03     0.58     0.62     0.63     0.65     0.69
## q[1294]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1295]     0.66    0.00 0.02     0.61     0.65     0.66     0.68     0.70
## q[1296]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1297]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1298]     0.63    0.00 0.03     0.58     0.62     0.63     0.65     0.69
## q[1299]     0.66    0.00 0.02     0.61     0.65     0.66     0.68     0.70
## q[1300]     0.66    0.00 0.02     0.61     0.65     0.66     0.68     0.70
## q[1301]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1302]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1303]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1304]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1305]     0.63    0.00 0.03     0.58     0.62     0.63     0.65     0.69
## q[1306]     0.74    0.00 0.02     0.70     0.73     0.74     0.75     0.77
## q[1307]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1308]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1309]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1310]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1311]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1312]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1313]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1314]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1315]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1316]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1317]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1318]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1319]     0.57    0.00 0.03     0.52     0.55     0.57     0.59     0.63
## q[1320]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1321]     0.57    0.00 0.03     0.52     0.55     0.57     0.59     0.63
## q[1322]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1323]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1324]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1325]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1326]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1327]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1328]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1329]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1330]     0.57    0.00 0.03     0.52     0.55     0.57     0.59     0.63
## q[1331]     0.57    0.00 0.03     0.52     0.55     0.57     0.59     0.63
## q[1332]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1333]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1334]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1335]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1336]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1337]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1338]     0.57    0.00 0.03     0.52     0.55     0.57     0.59     0.63
## q[1339]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1340]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1341]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1342]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1343]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1344]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1345]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1346]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1347]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1348]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1349]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1350]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1351]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1352]     0.57    0.00 0.03     0.52     0.55     0.57     0.59     0.63
## q[1353]     0.60    0.00 0.02     0.56     0.59     0.60     0.62     0.64
## q[1354]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1355]     0.69    0.00 0.02     0.65     0.68     0.69     0.70     0.72
## q[1356]     0.57    0.00 0.03     0.52     0.55     0.57     0.59     0.63
## q[1357]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1358]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1359]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1360]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1361]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1362]     0.63    0.00 0.02     0.58     0.61     0.63     0.64     0.67
## q[1363]     0.60    0.00 0.03     0.54     0.58     0.60     0.62     0.65
## q[1364]     0.63    0.00 0.02     0.58     0.61     0.63     0.64     0.67
## q[1365]     0.63    0.00 0.02     0.58     0.61     0.63     0.64     0.67
## q[1366]     0.60    0.00 0.03     0.54     0.58     0.60     0.62     0.65
## q[1367]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1368]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1369]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1370]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1371]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1372]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1373]     0.63    0.00 0.02     0.58     0.61     0.63     0.64     0.67
## q[1374]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1375]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1376]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1377]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1378]     0.63    0.00 0.02     0.58     0.61     0.63     0.64     0.67
## q[1379]     0.60    0.00 0.03     0.54     0.58     0.60     0.62     0.65
## q[1380]     0.63    0.00 0.02     0.58     0.61     0.63     0.64     0.67
## q[1381]     0.63    0.00 0.02     0.58     0.61     0.63     0.64     0.67
## q[1382]     0.63    0.00 0.02     0.58     0.61     0.63     0.64     0.67
## q[1383]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1384]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1385]     0.63    0.00 0.02     0.58     0.61     0.63     0.64     0.67
## q[1386]     0.60    0.00 0.03     0.54     0.58     0.60     0.62     0.65
## q[1387]     0.60    0.00 0.03     0.54     0.58     0.60     0.62     0.65
## q[1388]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1389]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1390]     0.63    0.00 0.02     0.58     0.61     0.63     0.64     0.67
## q[1391]     0.60    0.00 0.03     0.54     0.58     0.60     0.62     0.65
## q[1392]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1393]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1394]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1395]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1396]     0.60    0.00 0.03     0.54     0.58     0.60     0.62     0.65
## q[1397]     0.60    0.00 0.03     0.54     0.58     0.60     0.62     0.65
## q[1398]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1399]     0.60    0.00 0.03     0.54     0.58     0.60     0.62     0.65
## q[1400]     0.63    0.00 0.02     0.58     0.61     0.63     0.64     0.67
## q[1401]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1402]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1403]     0.63    0.00 0.02     0.58     0.61     0.63     0.64     0.67
## q[1404]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1405]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1406]     0.71    0.00 0.02     0.68     0.70     0.71     0.72     0.74
## q[1407]     0.60    0.00 0.03     0.54     0.58     0.60     0.62     0.65
## q[1408]     0.72    0.00 0.02     0.67     0.70     0.72     0.73     0.76
## q[1409]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1410]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1411]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[1412]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[1413]     0.72    0.00 0.02     0.67     0.70     0.72     0.73     0.76
## q[1414]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1415]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1416]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1417]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1418]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1419]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[1420]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[1421]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[1422]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1423]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[1424]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[1425]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1426]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1427]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1428]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1429]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1430]     0.72    0.00 0.02     0.67     0.70     0.72     0.73     0.76
## q[1431]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1432]     0.72    0.00 0.02     0.67     0.70     0.72     0.73     0.76
## q[1433]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1434]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[1435]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1436]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1437]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1438]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[1439]     0.72    0.00 0.02     0.67     0.70     0.72     0.73     0.76
## q[1440]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1441]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[1442]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1443]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[1444]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1445]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[1446]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1447]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1448]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[1449]     0.72    0.00 0.02     0.67     0.70     0.72     0.73     0.76
## q[1450]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[1451]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1452]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[1453]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[1454]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[1455]     0.72    0.00 0.02     0.67     0.70     0.72     0.73     0.76
## q[1456]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1457]     0.72    0.00 0.02     0.67     0.70     0.72     0.73     0.76
## q[1458]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1459]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1460]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[1461]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[1462]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1463]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1464]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1465]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1466]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1467]     0.74    0.00 0.02     0.71     0.73     0.74     0.75     0.78
## q[1468]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1469]     0.72    0.00 0.02     0.67     0.70     0.72     0.73     0.76
## q[1470]     0.72    0.00 0.02     0.67     0.70     0.72     0.73     0.76
## q[1471]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1472]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1473]     0.81    0.00 0.01     0.78     0.80     0.81     0.82     0.83
## q[1474]     0.86    0.00 0.01     0.84     0.86     0.86     0.87     0.89
## q[1475]     0.81    0.00 0.02     0.77     0.80     0.81     0.83     0.85
## q[1476]     0.79    0.00 0.02     0.75     0.78     0.79     0.81     0.83
## q[1477]     0.86    0.00 0.01     0.84     0.86     0.86     0.87     0.89
## q[1478]     0.86    0.00 0.01     0.84     0.86     0.86     0.87     0.89
## q[1479]     0.81    0.00 0.02     0.77     0.80     0.81     0.83     0.85
## q[1480]     0.86    0.00 0.01     0.84     0.86     0.86     0.87     0.89
## q[1481]     0.86    0.00 0.01     0.84     0.86     0.86     0.87     0.89
## q[1482]     0.86    0.00 0.01     0.84     0.86     0.86     0.87     0.89
## q[1483]     0.79    0.00 0.02     0.75     0.78     0.79     0.81     0.83
## q[1484]     0.79    0.00 0.02     0.75     0.78     0.79     0.81     0.83
## q[1485]     0.81    0.00 0.02     0.77     0.80     0.81     0.83     0.85
## q[1486]     0.79    0.00 0.02     0.75     0.78     0.79     0.81     0.83
## q[1487]     0.79    0.00 0.02     0.75     0.78     0.79     0.81     0.83
## q[1488]     0.81    0.00 0.02     0.77     0.80     0.81     0.83     0.85
## q[1489]     0.86    0.00 0.01     0.84     0.86     0.86     0.87     0.89
## q[1490]     0.81    0.00 0.02     0.77     0.80     0.81     0.83     0.85
## q[1491]     0.79    0.00 0.02     0.75     0.78     0.79     0.81     0.83
## q[1492]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1493]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1494]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1495]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1496]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1497]     0.71    0.00 0.02     0.66     0.69     0.71     0.72     0.75
## q[1498]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1499]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1500]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1501]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1502]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1503]     0.71    0.00 0.02     0.66     0.69     0.71     0.72     0.75
## q[1504]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1505]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1506]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1507]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1508]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1509]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1510]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1511]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1512]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1513]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1514]     0.71    0.00 0.02     0.66     0.69     0.71     0.72     0.75
## q[1515]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1516]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1517]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1518]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1519]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1520]     0.71    0.00 0.02     0.66     0.69     0.71     0.72     0.75
## q[1521]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1522]     0.71    0.00 0.02     0.66     0.69     0.71     0.72     0.75
## q[1523]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1524]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1525]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1526]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1527]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1528]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1529]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1530]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1531]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1532]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1533]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1534]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1535]     0.71    0.00 0.02     0.66     0.69     0.71     0.72     0.75
## q[1536]     0.73    0.00 0.02     0.69     0.72     0.73     0.74     0.77
## q[1537]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1538]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1539]     0.80    0.00 0.01     0.77     0.79     0.80     0.81     0.82
## q[1540]     0.71    0.00 0.02     0.66     0.69     0.71     0.72     0.75
## q[1541]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.82
## q[1542]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.82
## q[1543]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.82
## q[1544]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1545]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.76
## q[1546]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.76
## q[1547]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1548]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.76
## q[1549]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.76
## q[1550]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1551]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.82
## q[1552]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.82
## q[1553]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.82
## q[1554]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.76
## q[1555]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1556]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.82
## q[1557]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.82
## q[1558]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.82
## q[1559]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.82
## q[1560]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.76
## q[1561]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.82
## q[1562]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.76
## q[1563]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.82
## q[1564]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.76
## q[1565]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.82
## q[1566]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.82
## q[1567]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.76
## q[1568]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.82
## q[1569]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.76
## q[1570]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.82
## q[1571]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1572]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.76
## q[1573]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.82
## q[1574]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.76
## q[1575]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.76
## q[1576]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.82
## q[1577]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.76
## q[1578]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.82
## q[1579]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1580]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1581]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.82
## q[1582]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1583]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.76
## q[1584]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1585]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.82
## q[1586]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.82
## q[1587]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.76
## q[1588]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.76
## q[1589]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.76
## q[1590]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.82
## q[1591]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1592]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.82
## q[1593]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.82
## q[1594]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.82
## q[1595]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.82
## q[1596]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.82
## q[1597]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.76
## q[1598]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.82
## q[1599]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.82
## q[1600]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1601]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1602]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.76
## q[1603]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.76
## q[1604]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.82
## q[1605]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.82
## q[1606]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.76
## q[1607]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1608]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.76
## q[1609]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.82
## q[1610]     0.71    0.00 0.03     0.65     0.69     0.71     0.72     0.76
## q[1611]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.82
## q[1612]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.82
## q[1613]     0.78    0.00 0.02     0.74     0.77     0.78     0.79     0.82
## q[1614]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1615]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[1616]     0.59    0.00 0.03     0.54     0.57     0.59     0.61     0.65
## q[1617]     0.59    0.00 0.03     0.54     0.57     0.59     0.61     0.65
## q[1618]     0.50    0.00 0.03     0.44     0.48     0.50     0.52     0.56
## q[1619]     0.59    0.00 0.03     0.54     0.57     0.59     0.61     0.65
## q[1620]     0.59    0.00 0.03     0.54     0.57     0.59     0.61     0.65
## q[1621]     0.47    0.00 0.04     0.40     0.44     0.47     0.49     0.54
## q[1622]     0.50    0.00 0.03     0.44     0.48     0.50     0.52     0.56
## q[1623]     0.59    0.00 0.03     0.54     0.57     0.59     0.61     0.65
## q[1624]     0.50    0.00 0.03     0.44     0.48     0.50     0.52     0.56
## q[1625]     0.59    0.00 0.03     0.54     0.57     0.59     0.61     0.65
## q[1626]     0.59    0.00 0.03     0.54     0.57     0.59     0.61     0.65
## q[1627]     0.59    0.00 0.03     0.54     0.57     0.59     0.61     0.65
## q[1628]     0.65    0.00 0.02     0.60     0.63     0.65     0.67     0.69
## q[1629]     0.65    0.00 0.02     0.60     0.63     0.65     0.67     0.69
## q[1630]     0.62    0.00 0.03     0.57     0.60     0.62     0.64     0.67
## q[1631]     0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.76
## q[1632]     0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.76
## q[1633]     0.62    0.00 0.03     0.57     0.60     0.62     0.64     0.67
## q[1634]     0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.76
## q[1635]     0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.76
## q[1636]     0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.76
## q[1637]     0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.76
## q[1638]     0.65    0.00 0.02     0.60     0.63     0.65     0.67     0.69
## q[1639]     0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.76
## q[1640]     0.65    0.00 0.02     0.60     0.63     0.65     0.67     0.69
## q[1641]     0.62    0.00 0.03     0.57     0.60     0.62     0.64     0.67
## q[1642]     0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.76
## q[1643]     0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.76
## q[1644]     0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.76
## q[1645]     0.65    0.00 0.02     0.60     0.63     0.65     0.67     0.69
## q[1646]     0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.76
## q[1647]     0.62    0.00 0.03     0.57     0.60     0.62     0.64     0.67
## q[1648]     0.73    0.00 0.02     0.70     0.72     0.73     0.74     0.76
## q[1649]     0.65    0.00 0.02     0.60     0.63     0.65     0.67     0.69
## q[1650]     0.65    0.00 0.02     0.60     0.63     0.65     0.67     0.69
## q[1651]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1652]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1653]     0.59    0.00 0.03     0.53     0.57     0.59     0.61     0.64
## q[1654]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1655]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1656]     0.59    0.00 0.03     0.53     0.57     0.59     0.61     0.64
## q[1657]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1658]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1659]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1660]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1661]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1662]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1663]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1664]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1665]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1666]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1667]     0.59    0.00 0.03     0.53     0.57     0.59     0.61     0.64
## q[1668]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1669]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1670]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1671]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1672]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1673]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1674]     0.59    0.00 0.03     0.53     0.57     0.59     0.61     0.64
## q[1675]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1676]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1677]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1678]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1679]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1680]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1681]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1682]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1683]     0.59    0.00 0.03     0.53     0.57     0.59     0.61     0.64
## q[1684]     0.62    0.00 0.02     0.57     0.60     0.62     0.63     0.66
## q[1685]     0.59    0.00 0.03     0.53     0.57     0.59     0.61     0.64
## q[1686]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1687]     0.70    0.00 0.02     0.67     0.69     0.70     0.71     0.73
## q[1688]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1689]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1690]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1691]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1692]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1693]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1694]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1695]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1696]     0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.78
## q[1697]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1698]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1699]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1700]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1701]     0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.78
## q[1702]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1703]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1704]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1705]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1706]     0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.78
## q[1707]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1708]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1709]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1710]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1711]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1712]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1713]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1714]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1715]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1716]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1717]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1718]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1719]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1720]     0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.78
## q[1721]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1722]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1723]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1724]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1725]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1726]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1727]     0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.78
## q[1728]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1729]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1730]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1731]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1732]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1733]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1734]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1735]     0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.78
## q[1736]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1737]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1738]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1739]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1740]     0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.78
## q[1741]     0.75    0.00 0.02     0.72     0.74     0.75     0.77     0.79
## q[1742]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1743]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1744]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1745]     0.73    0.00 0.02     0.69     0.72     0.73     0.75     0.78
## q[1746]     0.82    0.00 0.01     0.79     0.81     0.82     0.83     0.84
## q[1747]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1748]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1749]     0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.78
## q[1750]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1751]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1752]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1753]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1754]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1755]     0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.78
## q[1756]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1757]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1758]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1759]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1760]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1761]     0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.78
## q[1762]     0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.78
## q[1763]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1764]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1765]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1766]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1767]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1768]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1769]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1770]     0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.78
## q[1771]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1772]     0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.78
## q[1773]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1774]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1775]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1776]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1777]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1778]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1779]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1780]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1781]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1782]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1783]     0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.78
## q[1784]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1785]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1786]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1787]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1788]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1789]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1790]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1791]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1792]     0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.78
## q[1793]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1794]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1795]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1796]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1797]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1798]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1799]     0.76    0.00 0.02     0.73     0.75     0.76     0.78     0.80
## q[1800]     0.83    0.00 0.01     0.80     0.82     0.83     0.83     0.85
## q[1801]     0.74    0.00 0.02     0.70     0.73     0.74     0.76     0.78
## q[1802]     0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[1803]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1804]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1805]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1806]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1807]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1808]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1809]     0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[1810]     0.54    0.00 0.03     0.49     0.52     0.54     0.56     0.60
## q[1811]     0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[1812]     0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[1813]     0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[1814]     0.54    0.00 0.03     0.49     0.52     0.54     0.56     0.60
## q[1815]     0.54    0.00 0.03     0.49     0.52     0.54     0.56     0.60
## q[1816]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1817]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1818]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1819]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1820]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1821]     0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[1822]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1823]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1824]     0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[1825]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1826]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1827]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1828]     0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[1829]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1830]     0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[1831]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1832]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1833]     0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[1834]     0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[1835]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1836]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1837]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1838]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1839]     0.54    0.00 0.03     0.49     0.52     0.54     0.56     0.60
## q[1840]     0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[1841]     0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[1842]     0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[1843]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1844]     0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[1845]     0.54    0.00 0.03     0.49     0.52     0.54     0.56     0.60
## q[1846]     0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[1847]     0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[1848]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1849]     0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[1850]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1851]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1852]     0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[1853]     0.54    0.00 0.03     0.49     0.52     0.54     0.56     0.60
## q[1854]     0.54    0.00 0.03     0.49     0.52     0.54     0.56     0.60
## q[1855]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1856]     0.54    0.00 0.03     0.49     0.52     0.54     0.56     0.60
## q[1857]     0.54    0.00 0.03     0.49     0.52     0.54     0.56     0.60
## q[1858]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1859]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1860]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1861]     0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[1862]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1863]     0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[1864]     0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[1865]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1866]     0.54    0.00 0.03     0.49     0.52     0.54     0.56     0.60
## q[1867]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1868]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1869]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1870]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1871]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1872]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1873]     0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[1874]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1875]     0.54    0.00 0.03     0.49     0.52     0.54     0.56     0.60
## q[1876]     0.54    0.00 0.03     0.49     0.52     0.54     0.56     0.60
## q[1877]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1878]     0.54    0.00 0.03     0.49     0.52     0.54     0.56     0.60
## q[1879]     0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[1880]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1881]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1882]     0.57    0.00 0.02     0.53     0.56     0.57     0.59     0.62
## q[1883]     0.54    0.00 0.03     0.49     0.52     0.54     0.56     0.60
## q[1884]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1885]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1886]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1887]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1888]     0.66    0.00 0.02     0.62     0.65     0.66     0.68     0.70
## q[1889]     0.54    0.00 0.03     0.49     0.52     0.54     0.56     0.60
## q[1890]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1891]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1892]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1893]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1894]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1895]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1896]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1897]     0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.75
## q[1898]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1899]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1900]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1901]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1902]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1903]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1904]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1905]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1906]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1907]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1908]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1909]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1910]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1911]     0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.75
## q[1912]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1913]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1914]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1915]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1916]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1917]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1918]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1919]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1920]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1921]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1922]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1923]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1924]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1925]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1926]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1927]     0.72    0.00 0.02     0.69     0.71     0.72     0.74     0.76
## q[1928]     0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.75
## q[1929]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1930]     0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.75
## q[1931]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1932]     0.70    0.00 0.02     0.65     0.68     0.70     0.72     0.75
## q[1933]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1934]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1935]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1936]     0.79    0.00 0.01     0.77     0.78     0.79     0.80     0.82
## q[1937]     0.68    0.00 0.02     0.63     0.67     0.68     0.70     0.73
## q[1938]     0.66    0.00 0.03     0.60     0.64     0.66     0.67     0.71
## q[1939]     0.66    0.00 0.03     0.60     0.64     0.66     0.67     0.71
## q[1940]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1941]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1942]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1943]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1944]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1945]     0.68    0.00 0.02     0.63     0.67     0.68     0.70     0.73
## q[1946]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1947]     0.68    0.00 0.02     0.63     0.67     0.68     0.70     0.73
## q[1948]     0.68    0.00 0.02     0.63     0.67     0.68     0.70     0.73
## q[1949]     0.68    0.00 0.02     0.63     0.67     0.68     0.70     0.73
## q[1950]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1951]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1952]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1953]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1954]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1955]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1956]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1957]     0.66    0.00 0.03     0.60     0.64     0.66     0.67     0.71
## q[1958]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[1959]     0.68    0.00 0.02     0.63     0.66     0.68     0.70     0.72
## q[1960]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1961]     0.65    0.00 0.03     0.60     0.63     0.65     0.67     0.71
## q[1962]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1963]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1964]     0.68    0.00 0.02     0.63     0.66     0.68     0.70     0.72
## q[1965]     0.65    0.00 0.03     0.60     0.63     0.65     0.67     0.71
## q[1966]     0.68    0.00 0.02     0.63     0.66     0.68     0.70     0.72
## q[1967]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1968]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1969]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1970]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1971]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1972]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1973]     0.68    0.00 0.02     0.63     0.66     0.68     0.70     0.72
## q[1974]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1975]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1976]     0.68    0.00 0.02     0.63     0.66     0.68     0.70     0.72
## q[1977]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1978]     0.68    0.00 0.02     0.63     0.66     0.68     0.70     0.72
## q[1979]     0.65    0.00 0.03     0.60     0.63     0.65     0.67     0.71
## q[1980]     0.68    0.00 0.02     0.63     0.66     0.68     0.70     0.72
## q[1981]     0.68    0.00 0.02     0.63     0.66     0.68     0.70     0.72
## q[1982]     0.68    0.00 0.02     0.63     0.66     0.68     0.70     0.72
## q[1983]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1984]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1985]     0.65    0.00 0.03     0.60     0.63     0.65     0.67     0.71
## q[1986]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1987]     0.65    0.00 0.03     0.60     0.63     0.65     0.67     0.71
## q[1988]     0.68    0.00 0.02     0.63     0.66     0.68     0.70     0.72
## q[1989]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1990]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1991]     0.68    0.00 0.02     0.63     0.66     0.68     0.70     0.72
## q[1992]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1993]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1994]     0.65    0.00 0.03     0.60     0.63     0.65     0.67     0.71
## q[1995]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1996]     0.68    0.00 0.02     0.63     0.66     0.68     0.70     0.72
## q[1997]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1998]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[1999]     0.76    0.00 0.02     0.72     0.74     0.76     0.77     0.79
## q[2000]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2001]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2002]     0.74    0.00 0.02     0.69     0.72     0.74     0.75     0.78
## q[2003]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2004]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2005]     0.74    0.00 0.02     0.69     0.72     0.74     0.75     0.78
## q[2006]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2007]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2008]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2009]     0.74    0.00 0.02     0.69     0.72     0.74     0.75     0.78
## q[2010]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2011]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2012]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2013]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2014]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2015]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2016]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2017]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2018]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2019]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2020]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2021]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2022]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2023]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2024]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2025]     0.74    0.00 0.02     0.69     0.72     0.74     0.75     0.78
## q[2026]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2027]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2028]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2029]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2030]     0.74    0.00 0.02     0.69     0.72     0.74     0.75     0.78
## q[2031]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2032]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2033]     0.74    0.00 0.02     0.69     0.72     0.74     0.75     0.78
## q[2034]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2035]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2036]     0.74    0.00 0.02     0.69     0.72     0.74     0.75     0.78
## q[2037]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2038]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2039]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2040]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2041]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2042]     0.74    0.00 0.02     0.69     0.72     0.74     0.75     0.78
## q[2043]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2044]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2045]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2046]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2047]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2048]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2049]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2050]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2051]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2052]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2053]     0.74    0.00 0.02     0.69     0.72     0.74     0.75     0.78
## q[2054]     0.76    0.00 0.02     0.72     0.75     0.76     0.77     0.79
## q[2055]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2056]     0.82    0.00 0.01     0.80     0.81     0.82     0.83     0.85
## q[2057]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2058]     0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[2059]     0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[2060]     0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[2061]     0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[2062]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2063]     0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[2064]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2065]     0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[2066]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2067]     0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[2068]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2069]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2070]     0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[2071]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2072]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2073]     0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[2074]     0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[2075]     0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[2076]     0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[2077]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2078]     0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[2079]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2080]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2081]     0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[2082]     0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[2083]     0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[2084]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2085]     0.77    0.00 0.02     0.73     0.76     0.77     0.78     0.80
## q[2086]     0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[2087]     0.83    0.00 0.01     0.80     0.82     0.83     0.84     0.85
## q[2088]     0.75    0.00 0.02     0.70     0.73     0.75     0.76     0.79
## q[2089]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2090]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2091]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2092]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2093]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2094]     0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.77
## q[2095]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2096]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2097]     0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.77
## q[2098]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2099]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2100]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2101]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2102]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2103]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2104]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2105]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2106]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2107]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2108]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2109]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2110]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2111]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2112]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2113]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2114]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2115]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2116]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2117]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2118]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2119]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2120]     0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.77
## q[2121]     0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.77
## q[2122]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2123]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2124]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2125]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2126]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2127]     0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.77
## q[2128]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2129]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2130]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2131]     0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.77
## q[2132]     0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.77
## q[2133]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2134]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2135]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2136]     0.75    0.00 0.02     0.71     0.74     0.75     0.76     0.78
## q[2137]     0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.77
## q[2138]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2139]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2140]     0.81    0.00 0.01     0.79     0.80     0.81     0.82     0.84
## q[2141]     0.72    0.00 0.02     0.68     0.71     0.72     0.74     0.77
## q[2142]     0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.83
## q[2143]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2144]     0.77    0.00 0.02     0.73     0.76     0.77     0.79     0.81
## q[2145]     0.77    0.00 0.02     0.73     0.76     0.77     0.79     0.81
## q[2146]     0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.83
## q[2147]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2148]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2149]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2150]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2151]     0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.83
## q[2152]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2153]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2154]     0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.83
## q[2155]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2156]     0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.83
## q[2157]     0.77    0.00 0.02     0.73     0.76     0.77     0.79     0.81
## q[2158]     0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.83
## q[2159]     0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.83
## q[2160]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2161]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2162]     0.77    0.00 0.02     0.73     0.76     0.77     0.79     0.81
## q[2163]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2164]     0.77    0.00 0.02     0.73     0.76     0.77     0.79     0.81
## q[2165]     0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.83
## q[2166]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2167]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2168]     0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.83
## q[2169]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2170]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2171]     0.79    0.00 0.02     0.75     0.78     0.79     0.80     0.83
## q[2172]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2173]     0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.91
## q[2174]     0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.91
## q[2175]     0.84    0.00 0.02     0.80     0.83     0.84     0.86     0.88
## q[2176]     0.83    0.00 0.02     0.78     0.81     0.83     0.84     0.87
## q[2177]     0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.91
## q[2178]     0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.91
## q[2179]     0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.91
## q[2180]     0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.91
## q[2181]     0.83    0.00 0.02     0.78     0.81     0.83     0.84     0.87
## q[2182]     0.84    0.00 0.02     0.80     0.83     0.84     0.86     0.88
## q[2183]     0.84    0.00 0.02     0.80     0.83     0.84     0.86     0.88
## q[2184]     0.84    0.00 0.02     0.80     0.83     0.84     0.86     0.88
## q[2185]     0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.91
## q[2186]     0.84    0.00 0.02     0.80     0.83     0.84     0.86     0.88
## q[2187]     0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.91
## q[2188]     0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.91
## q[2189]     0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.91
## q[2190]     0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.91
## q[2191]     0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.91
## q[2192]     0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.91
## q[2193]     0.83    0.00 0.02     0.78     0.81     0.83     0.84     0.87
## q[2194]     0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.91
## q[2195]     0.89    0.00 0.01     0.86     0.88     0.89     0.90     0.91
## q[2196]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2197]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2198]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2199]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2200]     0.71    0.00 0.02     0.67     0.70     0.72     0.73     0.75
## q[2201]     0.69    0.00 0.03     0.64     0.67     0.69     0.71     0.74
## q[2202]     0.71    0.00 0.02     0.67     0.70     0.72     0.73     0.75
## q[2203]     0.69    0.00 0.03     0.64     0.67     0.69     0.71     0.74
## q[2204]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2205]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2206]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2207]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2208]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2209]     0.71    0.00 0.02     0.67     0.70     0.72     0.73     0.75
## q[2210]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2211]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2212]     0.71    0.00 0.02     0.67     0.70     0.72     0.73     0.75
## q[2213]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2214]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2215]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2216]     0.69    0.00 0.03     0.64     0.67     0.69     0.71     0.74
## q[2217]     0.71    0.00 0.02     0.67     0.70     0.72     0.73     0.75
## q[2218]     0.71    0.00 0.02     0.67     0.70     0.72     0.73     0.75
## q[2219]     0.71    0.00 0.02     0.67     0.70     0.72     0.73     0.75
## q[2220]     0.71    0.00 0.02     0.67     0.70     0.72     0.73     0.75
## q[2221]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2222]     0.69    0.00 0.03     0.64     0.67     0.69     0.71     0.74
## q[2223]     0.69    0.00 0.03     0.64     0.67     0.69     0.71     0.74
## q[2224]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2225]     0.69    0.00 0.03     0.64     0.67     0.69     0.71     0.74
## q[2226]     0.69    0.00 0.03     0.64     0.67     0.69     0.71     0.74
## q[2227]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2228]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2229]     0.71    0.00 0.02     0.67     0.70     0.72     0.73     0.75
## q[2230]     0.71    0.00 0.02     0.67     0.70     0.72     0.73     0.75
## q[2231]     0.69    0.00 0.03     0.64     0.67     0.69     0.71     0.74
## q[2232]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2233]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2234]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2235]     0.69    0.00 0.03     0.64     0.67     0.69     0.71     0.74
## q[2236]     0.69    0.00 0.03     0.64     0.67     0.69     0.71     0.74
## q[2237]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2238]     0.69    0.00 0.03     0.64     0.67     0.69     0.71     0.74
## q[2239]     0.71    0.00 0.02     0.67     0.70     0.72     0.73     0.75
## q[2240]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2241]     0.71    0.00 0.02     0.67     0.70     0.72     0.73     0.75
## q[2242]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2243]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2244]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2245]     0.79    0.00 0.01     0.76     0.78     0.79     0.80     0.81
## q[2246]     0.69    0.00 0.03     0.64     0.67     0.69     0.71     0.74
## q[2247]     0.57    0.00 0.04     0.49     0.54     0.57     0.60     0.65
## q[2248]     0.60    0.00 0.04     0.52     0.57     0.60     0.62     0.67
## q[2249]     0.60    0.00 0.04     0.52     0.57     0.60     0.62     0.67
## q[2250]     0.60    0.00 0.04     0.52     0.57     0.60     0.62     0.67
## q[2251]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[2252]     0.60    0.00 0.04     0.52     0.57     0.60     0.62     0.67
## q[2253]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[2254]     0.60    0.00 0.04     0.52     0.57     0.60     0.62     0.67
## q[2255]     0.60    0.00 0.04     0.52     0.57     0.60     0.62     0.67
## q[2256]     0.60    0.00 0.04     0.52     0.57     0.60     0.62     0.67
## q[2257]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[2258]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[2259]     0.60    0.00 0.04     0.52     0.57     0.60     0.62     0.67
## q[2260]     0.60    0.00 0.04     0.52     0.57     0.60     0.62     0.67
## q[2261]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[2262]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[2263]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[2264]     0.60    0.00 0.04     0.52     0.57     0.60     0.62     0.67
## q[2265]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[2266]     0.57    0.00 0.04     0.49     0.54     0.57     0.60     0.65
## q[2267]     0.60    0.00 0.04     0.52     0.57     0.60     0.62     0.67
## q[2268]     0.68    0.00 0.03     0.62     0.66     0.68     0.70     0.74
## q[2269]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2270]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2271]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2272]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2273]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2274]     0.78    0.00 0.02     0.73     0.76     0.78     0.79     0.82
## q[2275]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2276]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2277]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2278]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2279]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2280]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2281]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2282]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2283]     0.78    0.00 0.02     0.73     0.76     0.78     0.79     0.82
## q[2284]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2285]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2286]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2287]     0.78    0.00 0.02     0.73     0.76     0.78     0.79     0.82
## q[2288]     0.78    0.00 0.02     0.73     0.76     0.78     0.79     0.82
## q[2289]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2290]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2291]     0.78    0.00 0.02     0.73     0.76     0.78     0.79     0.82
## q[2292]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2293]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2294]     0.78    0.00 0.02     0.73     0.76     0.78     0.79     0.82
## q[2295]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2296]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2297]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2298]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2299]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2300]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2301]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2302]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2303]     0.80    0.00 0.02     0.76     0.78     0.80     0.81     0.83
## q[2304]     0.78    0.00 0.02     0.73     0.76     0.78     0.79     0.82
## q[2305]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2306]     0.85    0.00 0.01     0.82     0.84     0.85     0.86     0.87
## q[2307]     0.70    0.00 0.02     0.66     0.69     0.70     0.72     0.74
## q[2308]     0.68    0.00 0.03     0.63     0.66     0.68     0.69     0.73
## q[2309]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.81
## q[2310]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.81
## q[2311]     0.68    0.00 0.03     0.63     0.66     0.68     0.69     0.73
## q[2312]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.81
## q[2313]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.81
## q[2314]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.81
## q[2315]     0.70    0.00 0.02     0.66     0.69     0.70     0.72     0.74
## q[2316]     0.68    0.00 0.03     0.63     0.66     0.68     0.69     0.73
## q[2317]     0.70    0.00 0.02     0.66     0.69     0.70     0.72     0.74
## q[2318]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.81
## q[2319]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.81
## q[2320]     0.70    0.00 0.02     0.66     0.69     0.70     0.72     0.74
## q[2321]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.81
## q[2322]     0.68    0.00 0.03     0.63     0.66     0.68     0.69     0.73
## q[2323]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.81
## q[2324]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.81
## q[2325]     0.70    0.00 0.02     0.66     0.69     0.70     0.72     0.74
## q[2326]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.81
## q[2327]     0.68    0.00 0.03     0.63     0.66     0.68     0.69     0.73
## q[2328]     0.70    0.00 0.02     0.66     0.69     0.70     0.72     0.74
## q[2329]     0.78    0.00 0.02     0.74     0.76     0.78     0.79     0.81
## q[2330]     0.56    0.00 0.02     0.51     0.54     0.56     0.58     0.61
## q[2331]     0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[2332]     0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[2333]     0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[2334]     0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[2335]     0.56    0.00 0.02     0.51     0.54     0.56     0.58     0.61
## q[2336]     0.56    0.00 0.02     0.51     0.54     0.56     0.58     0.61
## q[2337]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[2338]     0.56    0.00 0.02     0.51     0.54     0.56     0.58     0.61
## q[2339]     0.56    0.00 0.02     0.51     0.54     0.56     0.58     0.61
## q[2340]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[2341]     0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[2342]     0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[2343]     0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[2344]     0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[2345]     0.56    0.00 0.02     0.51     0.54     0.56     0.58     0.61
## q[2346]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[2347]     0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[2348]     0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[2349]     0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[2350]     0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[2351]     0.56    0.00 0.02     0.51     0.54     0.56     0.58     0.61
## q[2352]     0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[2353]     0.56    0.00 0.02     0.51     0.54     0.56     0.58     0.61
## q[2354]     0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[2355]     0.56    0.00 0.02     0.51     0.54     0.56     0.58     0.61
## q[2356]     0.56    0.00 0.02     0.51     0.54     0.56     0.58     0.61
## q[2357]     0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[2358]     0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[2359]     0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[2360]     0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[2361]     0.56    0.00 0.02     0.51     0.54     0.56     0.58     0.61
## q[2362]     0.56    0.00 0.02     0.51     0.54     0.56     0.58     0.61
## q[2363]     0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[2364]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[2365]     0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[2366]     0.56    0.00 0.02     0.51     0.54     0.56     0.58     0.61
## q[2367]     0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[2368]     0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[2369]     0.56    0.00 0.02     0.51     0.54     0.56     0.58     0.61
## q[2370]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[2371]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[2372]     0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[2373]     0.56    0.00 0.02     0.51     0.54     0.56     0.58     0.61
## q[2374]     0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[2375]     0.56    0.00 0.02     0.51     0.54     0.56     0.58     0.61
## q[2376]     0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[2377]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[2378]     0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[2379]     0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[2380]     0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[2381]     0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[2382]     0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[2383]     0.56    0.00 0.02     0.51     0.54     0.56     0.58     0.61
## q[2384]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[2385]     0.56    0.00 0.02     0.51     0.54     0.56     0.58     0.61
## q[2386]     0.56    0.00 0.02     0.51     0.54     0.56     0.58     0.61
## q[2387]     0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[2388]     0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[2389]     0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[2390]     0.56    0.00 0.02     0.51     0.54     0.56     0.58     0.61
## q[2391]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[2392]     0.56    0.00 0.02     0.51     0.54     0.56     0.58     0.61
## q[2393]     0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[2394]     0.65    0.00 0.02     0.61     0.64     0.65     0.66     0.69
## q[2395]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## q[2396]     0.53    0.00 0.03     0.47     0.51     0.53     0.55     0.59
## lp__    -1379.60    0.04 1.55 -1383.42 -1380.43 -1379.30 -1378.45 -1377.53
##         n_eff Rhat
## b[1]     2152    1
## b[2]     3412    1
## b[3]     2214    1
## b[4]     3497    1
## b[5]     3299    1
## OR1      3412    1
## OR2      2152    1
## OR3      3486    1
## OR4      3254    1
## q[1]     2681    1
## q[2]     2314    1
## q[3]     2703    1
## q[4]     2314    1
## q[5]     2681    1
## q[6]     2681    1
## q[7]     2703    1
## q[8]     2681    1
## q[9]     2314    1
## q[10]    2314    1
## q[11]    2314    1
## q[12]    2703    1
## q[13]    2314    1
## q[14]    2314    1
## q[15]    2314    1
## q[16]    2314    1
## q[17]    2314    1
## q[18]    2681    1
## q[19]    2314    1
## q[20]    2314    1
## q[21]    2681    1
## q[22]    2681    1
## q[23]    2703    1
## q[24]    2703    1
## q[25]    2314    1
## q[26]    2314    1
## q[27]    2681    1
## q[28]    2314    1
## q[29]    2681    1
## q[30]    2314    1
## q[31]    2681    1
## q[32]    2314    1
## q[33]    2681    1
## q[34]    2314    1
## q[35]    2703    1
## q[36]    2314    1
## q[37]    2314    1
## q[38]    2314    1
## q[39]    2681    1
## q[40]    2314    1
## q[41]    2314    1
## q[42]    2681    1
## q[43]    2681    1
## q[44]    3355    1
## q[45]    3355    1
## q[46]    3355    1
## q[47]    3707    1
## q[48]    3355    1
## q[49]    3243    1
## q[50]    3243    1
## q[51]    3243    1
## q[52]    3243    1
## q[53]    3355    1
## q[54]    3355    1
## q[55]    3355    1
## q[56]    3355    1
## q[57]    3355    1
## q[58]    3707    1
## q[59]    3355    1
## q[60]    3707    1
## q[61]    3355    1
## q[62]    3355    1
## q[63]    3355    1
## q[64]    3355    1
## q[65]    3243    1
## q[66]    3355    1
## q[67]    3243    1
## q[68]    3707    1
## q[69]    3243    1
## q[70]    3707    1
## q[71]    3707    1
## q[72]    3355    1
## q[73]    3243    1
## q[74]    3355    1
## q[75]    3243    1
## q[76]    3707    1
## q[77]    3355    1
## q[78]    3355    1
## q[79]    3707    1
## q[80]    3243    1
## q[81]    3243    1
## q[82]    3707    1
## q[83]    3243    1
## q[84]    3355    1
## q[85]    3355    1
## q[86]    3355    1
## q[87]    3355    1
## q[88]    3243    1
## q[89]    3355    1
## q[90]    3355    1
## q[91]    3243    1
## q[92]    3355    1
## q[93]    3355    1
## q[94]    3243    1
## q[95]    3243    1
## q[96]    3243    1
## q[97]    3355    1
## q[98]    3355    1
## q[99]    3707    1
## q[100]   3483    1
## q[101]   3483    1
## q[102]   3692    1
## q[103]   3492    1
## q[104]   3492    1
## q[105]   3483    1
## q[106]   3483    1
## q[107]   3483    1
## q[108]   3692    1
## q[109]   3483    1
## q[110]   3483    1
## q[111]   3483    1
## q[112]   3492    1
## q[113]   3492    1
## q[114]   3692    1
## q[115]   3492    1
## q[116]   3483    1
## q[117]   3692    1
## q[118]   3483    1
## q[119]   3692    1
## q[120]   3492    1
## q[121]   3492    1
## q[122]   3483    1
## q[123]   3483    1
## q[124]   3483    1
## q[125]   3492    1
## q[126]   3483    1
## q[127]   3492    1
## q[128]   3492    1
## q[129]   3483    1
## q[130]   3483    1
## q[131]   3692    1
## q[132]   2922    1
## q[133]   2922    1
## q[134]   2922    1
## q[135]   3355    1
## q[136]   3355    1
## q[137]   3317    1
## q[138]   2922    1
## q[139]   2922    1
## q[140]   3317    1
## q[141]   2922    1
## q[142]   2922    1
## q[143]   2922    1
## q[144]   2922    1
## q[145]   2922    1
## q[146]   3355    1
## q[147]   2922    1
## q[148]   2922    1
## q[149]   3317    1
## q[150]   3355    1
## q[151]   3355    1
## q[152]   3355    1
## q[153]   3317    1
## q[154]   3355    1
## q[155]   3317    1
## q[156]   2922    1
## q[157]   2922    1
## q[158]   3317    1
## q[159]   2922    1
## q[160]   3355    1
## q[161]   2922    1
## q[162]   3317    1
## q[163]   3355    1
## q[164]   2922    1
## q[165]   2922    1
## q[166]   3355    1
## q[167]   2922    1
## q[168]   2922    1
## q[169]   3317    1
## q[170]   2922    1
## q[171]   3317    1
## q[172]   3355    1
## q[173]   2922    1
## q[174]   2922    1
## q[175]   2922    1
## q[176]   2922    1
## q[177]   3397    1
## q[178]   2885    1
## q[179]   2885    1
## q[180]   3397    1
## q[181]   2885    1
## q[182]   3077    1
## q[183]   3077    1
## q[184]   2885    1
## q[185]   3077    1
## q[186]   3077    1
## q[187]   2885    1
## q[188]   2885    1
## q[189]   2885    1
## q[190]   3077    1
## q[191]   2885    1
## q[192]   3077    1
## q[193]   2885    1
## q[194]   2885    1
## q[195]   3077    1
## q[196]   3077    1
## q[197]   2885    1
## q[198]   3077    1
## q[199]   3077    1
## q[200]   3077    1
## q[201]   3077    1
## q[202]   3077    1
## q[203]   2885    1
## q[204]   3077    1
## q[205]   3397    1
## q[206]   3397    1
## q[207]   3077    1
## q[208]   2885    1
## q[209]   3077    1
## q[210]   3529    1
## q[211]   3529    1
## q[212]   3529    1
## q[213]   3725    1
## q[214]   3529    1
## q[215]   3428    1
## q[216]   3428    1
## q[217]   3725    1
## q[218]   3428    1
## q[219]   3428    1
## q[220]   3725    1
## q[221]   3529    1
## q[222]   3529    1
## q[223]   3529    1
## q[224]   3725    1
## q[225]   3529    1
## q[226]   3529    1
## q[227]   3529    1
## q[228]   3529    1
## q[229]   3529    1
## q[230]   3428    1
## q[231]   3529    1
## q[232]   3529    1
## q[233]   3529    1
## q[234]   3428    1
## q[235]   3529    1
## q[236]   3725    1
## q[237]   3428    1
## q[238]   3529    1
## q[239]   3428    1
## q[240]   3428    1
## q[241]   3529    1
## q[242]   3529    1
## q[243]   3428    1
## q[244]   3725    1
## q[245]   3428    1
## q[246]   3725    1
## q[247]   3529    1
## q[248]   3529    1
## q[249]   3428    1
## q[250]   3725    1
## q[251]   3529    1
## q[252]   3529    1
## q[253]   3529    1
## q[254]   3529    1
## q[255]   3529    1
## q[256]   3529    1
## q[257]   3725    1
## q[258]   3725    1
## q[259]   3428    1
## q[260]   3428    1
## q[261]   3529    1
## q[262]   3529    1
## q[263]   3529    1
## q[264]   3428    1
## q[265]   3428    1
## q[266]   3529    1
## q[267]   3428    1
## q[268]   3529    1
## q[269]   3725    1
## q[270]   3725    1
## q[271]   2992    1
## q[272]   3410    1
## q[273]   3410    1
## q[274]   3479    1
## q[275]   2992    1
## q[276]   2992    1
## q[277]   3479    1
## q[278]   2992    1
## q[279]   3410    1
## q[280]   2992    1
## q[281]   2992    1
## q[282]   3410    1
## q[283]   2992    1
## q[284]   3410    1
## q[285]   2992    1
## q[286]   2992    1
## q[287]   3410    1
## q[288]   3479    1
## q[289]   3410    1
## q[290]   2992    1
## q[291]   3410    1
## q[292]   3410    1
## q[293]   3479    1
## q[294]   3410    1
## q[295]   3479    1
## q[296]   2992    1
## q[297]   2992    1
## q[298]   3479    1
## q[299]   2992    1
## q[300]   3410    1
## q[301]   2992    1
## q[302]   3410    1
## q[303]   3479    1
## q[304]   3410    1
## q[305]   2992    1
## q[306]   2992    1
## q[307]   2992    1
## q[308]   2992    1
## q[309]   2992    1
## q[310]   3410    1
## q[311]   2992    1
## q[312]   3410    1
## q[313]   2992    1
## q[314]   3479    1
## q[315]   3410    1
## q[316]   3479    1
## q[317]   2992    1
## q[318]   2992    1
## q[319]   2992    1
## q[320]   3175    1
## q[321]   3309    1
## q[322]   3309    1
## q[323]   3663    1
## q[324]   3309    1
## q[325]   3309    1
## q[326]   3175    1
## q[327]   3175    1
## q[328]   3175    1
## q[329]   3663    1
## q[330]   3175    1
## q[331]   3663    1
## q[332]   3309    1
## q[333]   3309    1
## q[334]   3309    1
## q[335]   3309    1
## q[336]   3663    1
## q[337]   3309    1
## q[338]   3309    1
## q[339]   3309    1
## q[340]   3309    1
## q[341]   3663    1
## q[342]   3309    1
## q[343]   3309    1
## q[344]   3309    1
## q[345]   3309    1
## q[346]   3175    1
## q[347]   3309    1
## q[348]   3309    1
## q[349]   3309    1
## q[350]   3309    1
## q[351]   3175    1
## q[352]   3309    1
## q[353]   3175    1
## q[354]   3663    1
## q[355]   3175    1
## q[356]   3175    1
## q[357]   3175    1
## q[358]   3175    1
## q[359]   3663    1
## q[360]   3309    1
## q[361]   3175    1
## q[362]   3309    1
## q[363]   3663    1
## q[364]   3309    1
## q[365]   3309    1
## q[366]   3663    1
## q[367]   3175    1
## q[368]   3309    1
## q[369]   3309    1
## q[370]   3175    1
## q[371]   3309    1
## q[372]   3309    1
## q[373]   3663    1
## q[374]   3175    1
## q[375]   3309    1
## q[376]   3309    1
## q[377]   3309    1
## q[378]   3175    1
## q[379]   3309    1
## q[380]   3309    1
## q[381]   3309    1
## q[382]   3663    1
## q[383]   3309    1
## q[384]   3663    1
## q[385]   3175    1
## q[386]   3309    1
## q[387]   3175    1
## q[388]   3175    1
## q[389]   3309    1
## q[390]   3175    1
## q[391]   3309    1
## q[392]   3309    1
## q[393]   3309    1
## q[394]   3663    1
## q[395]   3309    1
## q[396]   3209    1
## q[397]   3424    1
## q[398]   3424    1
## q[399]   3536    1
## q[400]   3424    1
## q[401]   3424    1
## q[402]   3424    1
## q[403]   3209    1
## q[404]   3209    1
## q[405]   3209    1
## q[406]   3209    1
## q[407]   3536    1
## q[408]   3209    1
## q[409]   3536    1
## q[410]   3424    1
## q[411]   3424    1
## q[412]   3424    1
## q[413]   3424    1
## q[414]   3424    1
## q[415]   3424    1
## q[416]   3209    1
## q[417]   3536    1
## q[418]   3424    1
## q[419]   3424    1
## q[420]   3424    1
## q[421]   3424    1
## q[422]   3424    1
## q[423]   3209    1
## q[424]   3424    1
## q[425]   3424    1
## q[426]   3209    1
## q[427]   3209    1
## q[428]   3424    1
## q[429]   3424    1
## q[430]   3209    1
## q[431]   3209    1
## q[432]   3209    1
## q[433]   3209    1
## q[434]   3424    1
## q[435]   3536    1
## q[436]   3209    1
## q[437]   3424    1
## q[438]   3209    1
## q[439]   3424    1
## q[440]   3209    1
## q[441]   3424    1
## q[442]   3209    1
## q[443]   3209    1
## q[444]   3424    1
## q[445]   3424    1
## q[446]   3424    1
## q[447]   3209    1
## q[448]   3424    1
## q[449]   3424    1
## q[450]   3424    1
## q[451]   3424    1
## q[452]   3424    1
## q[453]   3424    1
## q[454]   3424    1
## q[455]   3424    1
## q[456]   3424    1
## q[457]   3209    1
## q[458]   3424    1
## q[459]   3536    1
## q[460]   3424    1
## q[461]   3536    1
## q[462]   3209    1
## q[463]   3424    1
## q[464]   3424    1
## q[465]   3536    1
## q[466]   3209    1
## q[467]   3424    1
## q[468]   3209    1
## q[469]   3424    1
## q[470]   3424    1
## q[471]   3424    1
## q[472]   3536    1
## q[473]   3424    1
## q[474]   3229    1
## q[475]   3448    1
## q[476]   3448    1
## q[477]   3448    1
## q[478]   3229    1
## q[479]   3229    1
## q[480]   3552    1
## q[481]   3552    1
## q[482]   3552    1
## q[483]   3448    1
## q[484]   3448    1
## q[485]   3448    1
## q[486]   3448    1
## q[487]   3552    1
## q[488]   3448    1
## q[489]   3448    1
## q[490]   3448    1
## q[491]   3229    1
## q[492]   3448    1
## q[493]   3448    1
## q[494]   3448    1
## q[495]   3448    1
## q[496]   3229    1
## q[497]   3448    1
## q[498]   3229    1
## q[499]   3448    1
## q[500]   3448    1
## q[501]   3448    1
## q[502]   3552    1
## q[503]   3229    1
## q[504]   3229    1
## q[505]   3229    1
## q[506]   3229    1
## q[507]   3448    1
## q[508]   3448    1
## q[509]   3552    1
## q[510]   3552    1
## q[511]   3448    1
## q[512]   3552    1
## q[513]   3552    1
## q[514]   3448    1
## q[515]   3448    1
## q[516]   3448    1
## q[517]   3448    1
## q[518]   3229    1
## q[519]   3229    1
## q[520]   3552    1
## q[521]   3448    1
## q[522]   3448    1
## q[523]   3448    1
## q[524]   3448    1
## q[525]   3552    1
## q[526]   3552    1
## q[527]   3229    1
## q[528]   3229    1
## q[529]   3448    1
## q[530]   3448    1
## q[531]   3229    1
## q[532]   3229    1
## q[533]   3448    1
## q[534]   3448    1
## q[535]   3448    1
## q[536]   3552    1
## q[537]   3552    1
## q[538]   3446    1
## q[539]   3446    1
## q[540]   3446    1
## q[541]   3385    1
## q[542]   3385    1
## q[543]   3446    1
## q[544]   3446    1
## q[545]   3786    1
## q[546]   3446    1
## q[547]   3446    1
## q[548]   3446    1
## q[549]   3446    1
## q[550]   3446    1
## q[551]   3385    1
## q[552]   3786    1
## q[553]   3385    1
## q[554]   3385    1
## q[555]   3786    1
## q[556]   3786    1
## q[557]   3446    1
## q[558]   3446    1
## q[559]   3786    1
## q[560]   3385    1
## q[561]   3786    1
## q[562]   3385    1
## q[563]   3446    1
## q[564]   3385    1
## q[565]   3446    1
## q[566]   3786    1
## q[567]   3446    1
## q[568]   3385    1
## q[569]   3446    1
## q[570]   3446    1
## q[571]   3446    1
## q[572]   3526    1
## q[573]   3526    1
## q[574]   3374    1
## q[575]   3374    1
## q[576]   3687    1
## q[577]   3374    1
## q[578]   3526    1
## q[579]   3526    1
## q[580]   3374    1
## q[581]   3687    1
## q[582]   3526    1
## q[583]   3526    1
## q[584]   3374    1
## q[585]   3526    1
## q[586]   3374    1
## q[587]   3526    1
## q[588]   3374    1
## q[589]   3526    1
## q[590]   3526    1
## q[591]   3374    1
## q[592]   3526    1
## q[593]   3526    1
## q[594]   3687    1
## q[595]   3687    1
## q[596]   3374    1
## q[597]   3374    1
## q[598]   3526    1
## q[599]   3687    1
## q[600]   3526    1
## q[601]   3526    1
## q[602]   3526    1
## q[603]   3374    1
## q[604]   3687    1
## q[605]   3374    1
## q[606]   3374    1
## q[607]   3526    1
## q[608]   3526    1
## q[609]   3374    1
## q[610]   3687    1
## q[611]   3374    1
## q[612]   3526    1
## q[613]   3526    1
## q[614]   3687    1
## q[615]   3687    1
## q[616]   3935    1
## q[617]   3653    1
## q[618]   3653    1
## q[619]   3935    1
## q[620]   3935    1
## q[621]   3888    1
## q[622]   3935    1
## q[623]   3888    1
## q[624]   3653    1
## q[625]   3653    1
## q[626]   3653    1
## q[627]   3653    1
## q[628]   3888    1
## q[629]   3653    1
## q[630]   3888    1
## q[631]   3653    1
## q[632]   3653    1
## q[633]   3653    1
## q[634]   3653    1
## q[635]   3653    1
## q[636]   3653    1
## q[637]   3935    1
## q[638]   3935    1
## q[639]   3653    1
## q[640]   3653    1
## q[641]   3653    1
## q[642]   3935    1
## q[643]   3888    1
## q[644]   3653    1
## q[645]   3653    1
## q[646]   3888    1
## q[647]   3653    1
## q[648]   3888    1
## q[649]   3653    1
## q[650]   3935    1
## q[651]   3653    1
## q[652]   3888    1
## q[653]   3935    1
## q[654]   3888    1
## q[655]   3653    1
## q[656]   3653    1
## q[657]   3935    1
## q[658]   3888    1
## q[659]   3935    1
## q[660]   3935    1
## q[661]   3653    1
## q[662]   3653    1
## q[663]   3935    1
## q[664]   3935    1
## q[665]   3935    1
## q[666]   3653    1
## q[667]   3888    1
## q[668]   3888    1
## q[669]   3410    1
## q[670]   2992    1
## q[671]   3410    1
## q[672]   3479    1
## q[673]   3479    1
## q[674]   3479    1
## q[675]   2992    1
## q[676]   2992    1
## q[677]   2992    1
## q[678]   2992    1
## q[679]   2992    1
## q[680]   2992    1
## q[681]   3410    1
## q[682]   2992    1
## q[683]   3410    1
## q[684]   2992    1
## q[685]   2992    1
## q[686]   2992    1
## q[687]   3410    1
## q[688]   3410    1
## q[689]   3410    1
## q[690]   2992    1
## q[691]   2992    1
## q[692]   3479    1
## q[693]   3479    1
## q[694]   2992    1
## q[695]   2992    1
## q[696]   2992    1
## q[697]   2992    1
## q[698]   3410    1
## q[699]   3479    1
## q[700]   2992    1
## q[701]   2992    1
## q[702]   3479    1
## q[703]   3479    1
## q[704]   3410    1
## q[705]   2992    1
## q[706]   3410    1
## q[707]   2992    1
## q[708]   3479    1
## q[709]   2952    1
## q[710]   2952    1
## q[711]   2952    1
## q[712]   2952    1
## q[713]   3377    1
## q[714]   3377    1
## q[715]   3438    1
## q[716]   3377    1
## q[717]   3377    1
## q[718]   2952    1
## q[719]   2952    1
## q[720]   2952    1
## q[721]   3377    1
## q[722]   3438    1
## q[723]   2952    1
## q[724]   2952    1
## q[725]   3377    1
## q[726]   2952    1
## q[727]   3377    1
## q[728]   2952    1
## q[729]   3377    1
## q[730]   2952    1
## q[731]   2952    1
## q[732]   2952    1
## q[733]   3377    1
## q[734]   2952    1
## q[735]   3438    1
## q[736]   3377    1
## q[737]   2952    1
## q[738]   2952    1
## q[739]   3377    1
## q[740]   3438    1
## q[741]   3438    1
## q[742]   3377    1
## q[743]   3377    1
## q[744]   2952    1
## q[745]   3438    1
## q[746]   2952    1
## q[747]   2952    1
## q[748]   2952    1
## q[749]   2952    1
## q[750]   2952    1
## q[751]   3377    1
## q[752]   3438    1
## q[753]   3377    1
## q[754]   3377    1
## q[755]   2952    1
## q[756]   2952    1
## q[757]   2952    1
## q[758]   3377    1
## q[759]   3438    1
## q[760]   3377    1
## q[761]   2952    1
## q[762]   2952    1
## q[763]   3438    1
## q[764]   3438    1
## q[765]   3686    1
## q[766]   3224    1
## q[767]   3224    1
## q[768]   3224    1
## q[769]   3224    1
## q[770]   3686    1
## q[771]   3686    1
## q[772]   3686    1
## q[773]   3686    1
## q[774]   3616    1
## q[775]   3224    1
## q[776]   3224    1
## q[777]   3224    1
## q[778]   3616    1
## q[779]   3224    1
## q[780]   3224    1
## q[781]   3224    1
## q[782]   3686    1
## q[783]   3224    1
## q[784]   3224    1
## q[785]   3224    1
## q[786]   3686    1
## q[787]   3224    1
## q[788]   3686    1
## q[789]   3686    1
## q[790]   3224    1
## q[791]   3224    1
## q[792]   3686    1
## q[793]   3686    1
## q[794]   3686    1
## q[795]   3224    1
## q[796]   3616    1
## q[797]   3616    1
## q[798]   3224    1
## q[799]   3686    1
## q[800]   3224    1
## q[801]   3686    1
## q[802]   3224    1
## q[803]   3224    1
## q[804]   3686    1
## q[805]   3686    1
## q[806]   3224    1
## q[807]   3616    1
## q[808]   3224    1
## q[809]   3224    1
## q[810]   3224    1
## q[811]   3224    1
## q[812]   3224    1
## q[813]   3224    1
## q[814]   3686    1
## q[815]   3686    1
## q[816]   3616    1
## q[817]   3224    1
## q[818]   3224    1
## q[819]   3686    1
## q[820]   3616    1
## q[821]   3224    1
## q[822]   3224    1
## q[823]   3224    1
## q[824]   3085    1
## q[825]   3085    1
## q[826]   3085    1
## q[827]   3453    1
## q[828]   3453    1
## q[829]   3453    1
## q[830]   3085    1
## q[831]   3085    1
## q[832]   3085    1
## q[833]   3453    1
## q[834]   3538    1
## q[835]   3085    1
## q[836]   3453    1
## q[837]   3085    1
## q[838]   3453    1
## q[839]   3085    1
## q[840]   3453    1
## q[841]   3085    1
## q[842]   3538    1
## q[843]   3453    1
## q[844]   3085    1
## q[845]   3453    1
## q[846]   3453    1
## q[847]   3085    1
## q[848]   3085    1
## q[849]   3085    1
## q[850]   3085    1
## q[851]   3085    1
## q[852]   3085    1
## q[853]   3453    1
## q[854]   3453    1
## q[855]   3085    1
## q[856]   3085    1
## q[857]   3538    1
## q[858]   3453    1
## q[859]   3085    1
## q[860]   3085    1
## q[861]   3538    1
## q[862]   2857    1
## q[863]   2440    1
## q[864]   2440    1
## q[865]   2440    1
## q[866]   2440    1
## q[867]   2440    1
## q[868]   2857    1
## q[869]   2857    1
## q[870]   2857    1
## q[871]   2888    1
## q[872]   2440    1
## q[873]   2440    1
## q[874]   2440    1
## q[875]   2440    1
## q[876]   2440    1
## q[877]   2440    1
## q[878]   2888    1
## q[879]   2440    1
## q[880]   2440    1
## q[881]   2440    1
## q[882]   2857    1
## q[883]   2440    1
## q[884]   2440    1
## q[885]   2857    1
## q[886]   2857    1
## q[887]   2888    1
## q[888]   2440    1
## q[889]   2857    1
## q[890]   2440    1
## q[891]   2857    1
## q[892]   2440    1
## q[893]   2440    1
## q[894]   2440    1
## q[895]   2440    1
## q[896]   2440    1
## q[897]   2888    1
## q[898]   2440    1
## q[899]   2857    1
## q[900]   2440    1
## q[901]   2440    1
## q[902]   2857    1
## q[903]   2440    1
## q[904]   2440    1
## q[905]   2888    1
## q[906]   2440    1
## q[907]   2711    1
## q[908]   2923    1
## q[909]   2629    1
## q[910]   2629    1
## q[911]   2923    1
## q[912]   2711    1
## q[913]   2629    1
## q[914]   2711    1
## q[915]   2711    1
## q[916]   2629    1
## q[917]   2711    1
## q[918]   2629    1
## q[919]   2711    1
## q[920]   2711    1
## q[921]   2629    1
## q[922]   2629    1
## q[923]   2923    1
## q[924]   2629    1
## q[925]   2711    1
## q[926]   2629    1
## q[927]   2629    1
## q[928]   2711    1
## q[929]   2629    1
## q[930]   2923    1
## q[931]   2711    1
## q[932]   2923    1
## q[933]   2629    1
## q[934]   2711    1
## q[935]   2711    1
## q[936]   2629    1
## q[937]   2629    1
## q[938]   2711    1
## q[939]   2711    1
## q[940]   2711    1
## q[941]   2711    1
## q[942]   2711    1
## q[943]   2629    1
## q[944]   2711    1
## q[945]   2711    1
## q[946]   2923    1
## q[947]   2923    1
## q[948]   2711    1
## q[949]   2629    1
## q[950]   2711    1
## q[951]   2711    1
## q[952]   3115    1
## q[953]   3115    1
## q[954]   3115    1
## q[955]   3115    1
## q[956]   3115    1
## q[957]   2927    1
## q[958]   2927    1
## q[959]   3447    1
## q[960]   2927    1
## q[961]   2927    1
## q[962]   3447    1
## q[963]   3115    1
## q[964]   3115    1
## q[965]   3115    1
## q[966]   3115    1
## q[967]   3115    1
## q[968]   3447    1
## q[969]   3115    1
## q[970]   3447    1
## q[971]   3115    1
## q[972]   3115    1
## q[973]   3115    1
## q[974]   3115    1
## q[975]   2927    1
## q[976]   3115    1
## q[977]   3115    1
## q[978]   2927    1
## q[979]   3115    1
## q[980]   3115    1
## q[981]   3115    1
## q[982]   3115    1
## q[983]   3447    1
## q[984]   2927    1
## q[985]   3447    1
## q[986]   2927    1
## q[987]   3447    1
## q[988]   2927    1
## q[989]   2927    1
## q[990]   3115    1
## q[991]   3115    1
## q[992]   2927    1
## q[993]   3447    1
## q[994]   3447    1
## q[995]   3115    1
## q[996]   3115    1
## q[997]   3447    1
## q[998]   3447    1
## q[999]   3115    1
## q[1000]  2927    1
## q[1001]  3115    1
## q[1002]  3447    1
## q[1003]  2927    1
## q[1004]  2927    1
## q[1005]  3447    1
## q[1006]  3115    1
## q[1007]  3115    1
## q[1008]  3115    1
## q[1009]  3115    1
## q[1010]  3115    1
## q[1011]  2927    1
## q[1012]  3115    1
## q[1013]  3115    1
## q[1014]  3447    1
## q[1015]  3447    1
## q[1016]  2927    1
## q[1017]  2927    1
## q[1018]  3115    1
## q[1019]  3115    1
## q[1020]  3115    1
## q[1021]  2927    1
## q[1022]  2927    1
## q[1023]  2927    1
## q[1024]  3115    1
## q[1025]  3115    1
## q[1026]  3115    1
## q[1027]  3447    1
## q[1028]  3447    1
## q[1029]  3355    1
## q[1030]  2922    1
## q[1031]  2922    1
## q[1032]  2922    1
## q[1033]  3355    1
## q[1034]  3355    1
## q[1035]  3355    1
## q[1036]  3317    1
## q[1037]  3317    1
## q[1038]  2922    1
## q[1039]  2922    1
## q[1040]  3317    1
## q[1041]  2922    1
## q[1042]  2922    1
## q[1043]  2922    1
## q[1044]  2922    1
## q[1045]  3355    1
## q[1046]  2922    1
## q[1047]  2922    1
## q[1048]  3355    1
## q[1049]  2922    1
## q[1050]  2922    1
## q[1051]  2922    1
## q[1052]  3355    1
## q[1053]  2922    1
## q[1054]  2922    1
## q[1055]  3355    1
## q[1056]  3355    1
## q[1057]  2922    1
## q[1058]  3317    1
## q[1059]  3355    1
## q[1060]  3355    1
## q[1061]  3355    1
## q[1062]  2922    1
## q[1063]  3355    1
## q[1064]  3355    1
## q[1065]  3317    1
## q[1066]  3355    1
## q[1067]  2922    1
## q[1068]  3317    1
## q[1069]  2922    1
## q[1070]  2922    1
## q[1071]  3317    1
## q[1072]  2922    1
## q[1073]  2922    1
## q[1074]  3355    1
## q[1075]  2922    1
## q[1076]  3355    1
## q[1077]  2922    1
## q[1078]  3317    1
## q[1079]  3355    1
## q[1080]  2922    1
## q[1081]  2922    1
## q[1082]  2922    1
## q[1083]  2922    1
## q[1084]  2922    1
## q[1085]  3355    1
## q[1086]  2922    1
## q[1087]  3355    1
## q[1088]  2922    1
## q[1089]  3317    1
## q[1090]  2922    1
## q[1091]  3317    1
## q[1092]  3355    1
## q[1093]  3317    1
## q[1094]  2922    1
## q[1095]  2922    1
## q[1096]  2922    1
## q[1097]  2922    1
## q[1098]  2922    1
## q[1099]  3460    1
## q[1100]  3460    1
## q[1101]  3460    1
## q[1102]  3460    1
## q[1103]  3460    1
## q[1104]  3879    1
## q[1105]  3791    1
## q[1106]  3879    1
## q[1107]  3879    1
## q[1108]  3879    1
## q[1109]  3879    1
## q[1110]  3791    1
## q[1111]  3460    1
## q[1112]  3460    1
## q[1113]  3460    1
## q[1114]  3791    1
## q[1115]  3460    1
## q[1116]  3460    1
## q[1117]  3879    1
## q[1118]  3460    1
## q[1119]  3460    1
## q[1120]  3460    1
## q[1121]  3879    1
## q[1122]  3460    1
## q[1123]  3879    1
## q[1124]  3460    1
## q[1125]  3460    1
## q[1126]  3879    1
## q[1127]  3460    1
## q[1128]  3460    1
## q[1129]  3460    1
## q[1130]  3879    1
## q[1131]  3879    1
## q[1132]  3460    1
## q[1133]  3791    1
## q[1134]  3879    1
## q[1135]  3791    1
## q[1136]  3879    1
## q[1137]  3879    1
## q[1138]  3460    1
## q[1139]  3460    1
## q[1140]  3879    1
## q[1141]  3791    1
## q[1142]  3460    1
## q[1143]  3791    1
## q[1144]  3460    1
## q[1145]  3879    1
## q[1146]  3460    1
## q[1147]  3879    1
## q[1148]  3791    1
## q[1149]  3879    1
## q[1150]  3460    1
## q[1151]  3791    1
## q[1152]  3460    1
## q[1153]  3460    1
## q[1154]  3460    1
## q[1155]  3460    1
## q[1156]  3460    1
## q[1157]  3460    1
## q[1158]  3879    1
## q[1159]  3460    1
## q[1160]  3879    1
## q[1161]  3791    1
## q[1162]  3791    1
## q[1163]  3460    1
## q[1164]  3791    1
## q[1165]  3879    1
## q[1166]  3460    1
## q[1167]  3460    1
## q[1168]  3879    1
## q[1169]  3879    1
## q[1170]  3791    1
## q[1171]  3460    1
## q[1172]  3460    1
## q[1173]  3460    1
## q[1174]  3460    1
## q[1175]  3791    1
## q[1176]  3823    1
## q[1177]  3409    1
## q[1178]  3409    1
## q[1179]  3764    1
## q[1180]  3823    1
## q[1181]  3764    1
## q[1182]  3823    1
## q[1183]  3764    1
## q[1184]  3823    1
## q[1185]  3409    1
## q[1186]  3409    1
## q[1187]  3823    1
## q[1188]  3409    1
## q[1189]  3409    1
## q[1190]  3409    1
## q[1191]  3823    1
## q[1192]  3409    1
## q[1193]  3409    1
## q[1194]  3823    1
## q[1195]  3409    1
## q[1196]  3823    1
## q[1197]  3409    1
## q[1198]  3823    1
## q[1199]  3823    1
## q[1200]  3409    1
## q[1201]  3409    1
## q[1202]  3823    1
## q[1203]  3409    1
## q[1204]  3764    1
## q[1205]  3823    1
## q[1206]  3823    1
## q[1207]  3409    1
## q[1208]  3823    1
## q[1209]  3409    1
## q[1210]  3409    1
## q[1211]  3409    1
## q[1212]  3764    1
## q[1213]  3764    1
## q[1214]  3409    1
## q[1215]  3764    1
## q[1216]  3823    1
## q[1217]  3764    1
## q[1218]  3409    1
## q[1219]  3823    1
## q[1220]  3409    1
## q[1221]  3823    1
## q[1222]  3823    1
## q[1223]  3409    1
## q[1224]  3764    1
## q[1225]  3409    1
## q[1226]  3409    1
## q[1227]  3823    1
## q[1228]  3409    1
## q[1229]  3409    1
## q[1230]  3764    1
## q[1231]  3823    1
## q[1232]  3409    1
## q[1233]  3823    1
## q[1234]  3764    1
## q[1235]  3823    1
## q[1236]  3409    1
## q[1237]  3409    1
## q[1238]  3764    1
## q[1239]  3401    1
## q[1240]  3401    1
## q[1241]  3401    1
## q[1242]  3749    1
## q[1243]  3401    1
## q[1244]  3401    1
## q[1245]  3401    1
## q[1246]  3313    1
## q[1247]  3313    1
## q[1248]  3313    1
## q[1249]  3313    1
## q[1250]  3313    1
## q[1251]  3401    1
## q[1252]  3401    1
## q[1253]  3401    1
## q[1254]  3401    1
## q[1255]  3401    1
## q[1256]  3401    1
## q[1257]  3313    1
## q[1258]  3749    1
## q[1259]  3401    1
## q[1260]  3401    1
## q[1261]  3401    1
## q[1262]  3313    1
## q[1263]  3313    1
## q[1264]  3401    1
## q[1265]  3313    1
## q[1266]  3401    1
## q[1267]  3313    1
## q[1268]  3749    1
## q[1269]  3313    1
## q[1270]  3313    1
## q[1271]  3401    1
## q[1272]  3313    1
## q[1273]  3749    1
## q[1274]  3401    1
## q[1275]  3313    1
## q[1276]  3401    1
## q[1277]  3313    1
## q[1278]  3749    1
## q[1279]  3401    1
## q[1280]  3749    1
## q[1281]  3313    1
## q[1282]  3313    1
## q[1283]  3313    1
## q[1284]  3401    1
## q[1285]  3401    1
## q[1286]  3401    1
## q[1287]  3401    1
## q[1288]  3401    1
## q[1289]  3401    1
## q[1290]  3313    1
## q[1291]  3401    1
## q[1292]  3401    1
## q[1293]  3749    1
## q[1294]  3401    1
## q[1295]  3313    1
## q[1296]  3401    1
## q[1297]  3401    1
## q[1298]  3749    1
## q[1299]  3313    1
## q[1300]  3313    1
## q[1301]  3401    1
## q[1302]  3401    1
## q[1303]  3401    1
## q[1304]  3401    1
## q[1305]  3749    1
## q[1306]  3401    1
## q[1307]  3640    1
## q[1308]  3640    1
## q[1309]  3640    1
## q[1310]  3943    1
## q[1311]  3943    1
## q[1312]  3943    1
## q[1313]  3943    1
## q[1314]  3640    1
## q[1315]  3640    1
## q[1316]  3640    1
## q[1317]  3640    1
## q[1318]  3640    1
## q[1319]  3881    1
## q[1320]  3943    1
## q[1321]  3881    1
## q[1322]  3640    1
## q[1323]  3640    1
## q[1324]  3943    1
## q[1325]  3640    1
## q[1326]  3943    1
## q[1327]  3640    1
## q[1328]  3943    1
## q[1329]  3640    1
## q[1330]  3881    1
## q[1331]  3881    1
## q[1332]  3943    1
## q[1333]  3640    1
## q[1334]  3943    1
## q[1335]  3640    1
## q[1336]  3943    1
## q[1337]  3943    1
## q[1338]  3881    1
## q[1339]  3640    1
## q[1340]  3640    1
## q[1341]  3640    1
## q[1342]  3640    1
## q[1343]  3640    1
## q[1344]  3640    1
## q[1345]  3640    1
## q[1346]  3943    1
## q[1347]  3943    1
## q[1348]  3943    1
## q[1349]  3640    1
## q[1350]  3640    1
## q[1351]  3943    1
## q[1352]  3881    1
## q[1353]  3943    1
## q[1354]  3640    1
## q[1355]  3640    1
## q[1356]  3881    1
## q[1357]  3657    1
## q[1358]  3657    1
## q[1359]  3657    1
## q[1360]  3657    1
## q[1361]  3657    1
## q[1362]  3782    1
## q[1363]  3899    1
## q[1364]  3782    1
## q[1365]  3782    1
## q[1366]  3899    1
## q[1367]  3657    1
## q[1368]  3657    1
## q[1369]  3657    1
## q[1370]  3657    1
## q[1371]  3657    1
## q[1372]  3657    1
## q[1373]  3782    1
## q[1374]  3657    1
## q[1375]  3657    1
## q[1376]  3657    1
## q[1377]  3657    1
## q[1378]  3782    1
## q[1379]  3899    1
## q[1380]  3782    1
## q[1381]  3782    1
## q[1382]  3782    1
## q[1383]  3657    1
## q[1384]  3657    1
## q[1385]  3782    1
## q[1386]  3899    1
## q[1387]  3899    1
## q[1388]  3657    1
## q[1389]  3657    1
## q[1390]  3782    1
## q[1391]  3899    1
## q[1392]  3657    1
## q[1393]  3657    1
## q[1394]  3657    1
## q[1395]  3657    1
## q[1396]  3899    1
## q[1397]  3899    1
## q[1398]  3657    1
## q[1399]  3899    1
## q[1400]  3782    1
## q[1401]  3657    1
## q[1402]  3657    1
## q[1403]  3782    1
## q[1404]  3657    1
## q[1405]  3657    1
## q[1406]  3657    1
## q[1407]  3899    1
## q[1408]  3622    1
## q[1409]  3297    1
## q[1410]  3297    1
## q[1411]  3503    1
## q[1412]  3503    1
## q[1413]  3622    1
## q[1414]  3297    1
## q[1415]  3297    1
## q[1416]  3297    1
## q[1417]  3297    1
## q[1418]  3297    1
## q[1419]  3503    1
## q[1420]  3503    1
## q[1421]  3503    1
## q[1422]  3297    1
## q[1423]  3503    1
## q[1424]  3503    1
## q[1425]  3297    1
## q[1426]  3297    1
## q[1427]  3297    1
## q[1428]  3297    1
## q[1429]  3297    1
## q[1430]  3622    1
## q[1431]  3297    1
## q[1432]  3622    1
## q[1433]  3297    1
## q[1434]  3503    1
## q[1435]  3297    1
## q[1436]  3297    1
## q[1437]  3297    1
## q[1438]  3503    1
## q[1439]  3622    1
## q[1440]  3297    1
## q[1441]  3503    1
## q[1442]  3297    1
## q[1443]  3503    1
## q[1444]  3297    1
## q[1445]  3503    1
## q[1446]  3297    1
## q[1447]  3297    1
## q[1448]  3503    1
## q[1449]  3622    1
## q[1450]  3503    1
## q[1451]  3297    1
## q[1452]  3503    1
## q[1453]  3503    1
## q[1454]  3503    1
## q[1455]  3622    1
## q[1456]  3297    1
## q[1457]  3622    1
## q[1458]  3297    1
## q[1459]  3297    1
## q[1460]  3503    1
## q[1461]  3503    1
## q[1462]  3297    1
## q[1463]  3297    1
## q[1464]  3297    1
## q[1465]  3297    1
## q[1466]  3297    1
## q[1467]  3503    1
## q[1468]  3297    1
## q[1469]  3622    1
## q[1470]  3622    1
## q[1471]  3297    1
## q[1472]  3297    1
## q[1473]  3297    1
## q[1474]  3032    1
## q[1475]  2912    1
## q[1476]  3275    1
## q[1477]  3032    1
## q[1478]  3032    1
## q[1479]  2912    1
## q[1480]  3032    1
## q[1481]  3032    1
## q[1482]  3032    1
## q[1483]  3275    1
## q[1484]  3275    1
## q[1485]  2912    1
## q[1486]  3275    1
## q[1487]  3275    1
## q[1488]  2912    1
## q[1489]  3032    1
## q[1490]  2912    1
## q[1491]  3275    1
## q[1492]  3440    1
## q[1493]  3041    1
## q[1494]  3041    1
## q[1495]  3440    1
## q[1496]  3440    1
## q[1497]  3519    1
## q[1498]  3041    1
## q[1499]  3041    1
## q[1500]  3041    1
## q[1501]  3041    1
## q[1502]  3440    1
## q[1503]  3519    1
## q[1504]  3041    1
## q[1505]  3041    1
## q[1506]  3440    1
## q[1507]  3041    1
## q[1508]  3041    1
## q[1509]  3440    1
## q[1510]  3041    1
## q[1511]  3440    1
## q[1512]  3440    1
## q[1513]  3041    1
## q[1514]  3519    1
## q[1515]  3440    1
## q[1516]  3440    1
## q[1517]  3041    1
## q[1518]  3440    1
## q[1519]  3041    1
## q[1520]  3519    1
## q[1521]  3041    1
## q[1522]  3519    1
## q[1523]  3041    1
## q[1524]  3440    1
## q[1525]  3041    1
## q[1526]  3440    1
## q[1527]  3041    1
## q[1528]  3041    1
## q[1529]  3041    1
## q[1530]  3041    1
## q[1531]  3440    1
## q[1532]  3041    1
## q[1533]  3440    1
## q[1534]  3041    1
## q[1535]  3519    1
## q[1536]  3440    1
## q[1537]  3041    1
## q[1538]  3041    1
## q[1539]  3041    1
## q[1540]  3519    1
## q[1541]  2955    1
## q[1542]  2955    1
## q[1543]  2955    1
## q[1544]  3233    1
## q[1545]  2755    1
## q[1546]  2755    1
## q[1547]  3233    1
## q[1548]  2755    1
## q[1549]  2755    1
## q[1550]  3233    1
## q[1551]  2955    1
## q[1552]  2955    1
## q[1553]  2955    1
## q[1554]  2755    1
## q[1555]  3233    1
## q[1556]  2955    1
## q[1557]  2955    1
## q[1558]  2955    1
## q[1559]  2955    1
## q[1560]  2755    1
## q[1561]  2955    1
## q[1562]  2755    1
## q[1563]  2955    1
## q[1564]  2755    1
## q[1565]  2955    1
## q[1566]  2955    1
## q[1567]  2755    1
## q[1568]  2955    1
## q[1569]  2755    1
## q[1570]  2955    1
## q[1571]  3233    1
## q[1572]  2755    1
## q[1573]  2955    1
## q[1574]  2755    1
## q[1575]  2755    1
## q[1576]  2955    1
## q[1577]  2755    1
## q[1578]  2955    1
## q[1579]  3233    1
## q[1580]  3233    1
## q[1581]  2955    1
## q[1582]  3233    1
## q[1583]  2755    1
## q[1584]  3233    1
## q[1585]  2955    1
## q[1586]  2955    1
## q[1587]  2755    1
## q[1588]  2755    1
## q[1589]  2755    1
## q[1590]  2955    1
## q[1591]  3233    1
## q[1592]  2955    1
## q[1593]  2955    1
## q[1594]  2955    1
## q[1595]  2955    1
## q[1596]  2955    1
## q[1597]  2755    1
## q[1598]  2955    1
## q[1599]  2955    1
## q[1600]  3233    1
## q[1601]  3233    1
## q[1602]  2755    1
## q[1603]  2755    1
## q[1604]  2955    1
## q[1605]  2955    1
## q[1606]  2755    1
## q[1607]  3233    1
## q[1608]  2755    1
## q[1609]  2955    1
## q[1610]  2755    1
## q[1611]  2955    1
## q[1612]  2955    1
## q[1613]  2955    1
## q[1614]  3233    1
## q[1615]  3233    1
## q[1616]  2706    1
## q[1617]  2706    1
## q[1618]  3080    1
## q[1619]  2706    1
## q[1620]  2706    1
## q[1621]  3083    1
## q[1622]  3080    1
## q[1623]  2706    1
## q[1624]  3080    1
## q[1625]  2706    1
## q[1626]  2706    1
## q[1627]  2706    1
## q[1628]  3495    1
## q[1629]  3495    1
## q[1630]  3822    1
## q[1631]  3512    1
## q[1632]  3512    1
## q[1633]  3822    1
## q[1634]  3512    1
## q[1635]  3512    1
## q[1636]  3512    1
## q[1637]  3512    1
## q[1638]  3495    1
## q[1639]  3512    1
## q[1640]  3495    1
## q[1641]  3822    1
## q[1642]  3512    1
## q[1643]  3512    1
## q[1644]  3512    1
## q[1645]  3495    1
## q[1646]  3512    1
## q[1647]  3822    1
## q[1648]  3512    1
## q[1649]  3495    1
## q[1650]  3495    1
## q[1651]  3857    1
## q[1652]  3857    1
## q[1653]  3906    1
## q[1654]  3674    1
## q[1655]  3674    1
## q[1656]  3906    1
## q[1657]  3857    1
## q[1658]  3674    1
## q[1659]  3674    1
## q[1660]  3857    1
## q[1661]  3674    1
## q[1662]  3674    1
## q[1663]  3857    1
## q[1664]  3857    1
## q[1665]  3674    1
## q[1666]  3857    1
## q[1667]  3906    1
## q[1668]  3857    1
## q[1669]  3674    1
## q[1670]  3674    1
## q[1671]  3857    1
## q[1672]  3674    1
## q[1673]  3857    1
## q[1674]  3906    1
## q[1675]  3674    1
## q[1676]  3674    1
## q[1677]  3674    1
## q[1678]  3674    1
## q[1679]  3674    1
## q[1680]  3857    1
## q[1681]  3674    1
## q[1682]  3857    1
## q[1683]  3906    1
## q[1684]  3857    1
## q[1685]  3906    1
## q[1686]  3674    1
## q[1687]  3674    1
## q[1688]  3486    1
## q[1689]  3493    1
## q[1690]  3493    1
## q[1691]  3493    1
## q[1692]  3486    1
## q[1693]  3486    1
## q[1694]  3486    1
## q[1695]  3486    1
## q[1696]  3700    1
## q[1697]  3493    1
## q[1698]  3493    1
## q[1699]  3493    1
## q[1700]  3493    1
## q[1701]  3700    1
## q[1702]  3493    1
## q[1703]  3493    1
## q[1704]  3493    1
## q[1705]  3486    1
## q[1706]  3700    1
## q[1707]  3493    1
## q[1708]  3493    1
## q[1709]  3493    1
## q[1710]  3493    1
## q[1711]  3486    1
## q[1712]  3493    1
## q[1713]  3486    1
## q[1714]  3486    1
## q[1715]  3493    1
## q[1716]  3486    1
## q[1717]  3486    1
## q[1718]  3486    1
## q[1719]  3493    1
## q[1720]  3700    1
## q[1721]  3493    1
## q[1722]  3493    1
## q[1723]  3493    1
## q[1724]  3486    1
## q[1725]  3486    1
## q[1726]  3493    1
## q[1727]  3700    1
## q[1728]  3493    1
## q[1729]  3493    1
## q[1730]  3493    1
## q[1731]  3493    1
## q[1732]  3493    1
## q[1733]  3486    1
## q[1734]  3486    1
## q[1735]  3700    1
## q[1736]  3493    1
## q[1737]  3486    1
## q[1738]  3493    1
## q[1739]  3486    1
## q[1740]  3700    1
## q[1741]  3486    1
## q[1742]  3493    1
## q[1743]  3493    1
## q[1744]  3493    1
## q[1745]  3700    1
## q[1746]  3493    1
## q[1747]  3524    1
## q[1748]  3524    1
## q[1749]  3724    1
## q[1750]  3524    1
## q[1751]  3448    1
## q[1752]  3448    1
## q[1753]  3448    1
## q[1754]  3448    1
## q[1755]  3724    1
## q[1756]  3524    1
## q[1757]  3524    1
## q[1758]  3524    1
## q[1759]  3524    1
## q[1760]  3524    1
## q[1761]  3724    1
## q[1762]  3724    1
## q[1763]  3524    1
## q[1764]  3524    1
## q[1765]  3524    1
## q[1766]  3524    1
## q[1767]  3524    1
## q[1768]  3524    1
## q[1769]  3448    1
## q[1770]  3724    1
## q[1771]  3448    1
## q[1772]  3724    1
## q[1773]  3524    1
## q[1774]  3448    1
## q[1775]  3448    1
## q[1776]  3524    1
## q[1777]  3448    1
## q[1778]  3524    1
## q[1779]  3448    1
## q[1780]  3524    1
## q[1781]  3448    1
## q[1782]  3524    1
## q[1783]  3724    1
## q[1784]  3524    1
## q[1785]  3524    1
## q[1786]  3524    1
## q[1787]  3524    1
## q[1788]  3524    1
## q[1789]  3448    1
## q[1790]  3524    1
## q[1791]  3524    1
## q[1792]  3724    1
## q[1793]  3448    1
## q[1794]  3524    1
## q[1795]  3524    1
## q[1796]  3448    1
## q[1797]  3448    1
## q[1798]  3524    1
## q[1799]  3448    1
## q[1800]  3524    1
## q[1801]  3724    1
## q[1802]  3808    1
## q[1803]  3383    1
## q[1804]  3383    1
## q[1805]  3383    1
## q[1806]  3383    1
## q[1807]  3383    1
## q[1808]  3383    1
## q[1809]  3808    1
## q[1810]  3749    1
## q[1811]  3808    1
## q[1812]  3808    1
## q[1813]  3808    1
## q[1814]  3749    1
## q[1815]  3749    1
## q[1816]  3383    1
## q[1817]  3383    1
## q[1818]  3383    1
## q[1819]  3383    1
## q[1820]  3383    1
## q[1821]  3808    1
## q[1822]  3383    1
## q[1823]  3383    1
## q[1824]  3808    1
## q[1825]  3383    1
## q[1826]  3383    1
## q[1827]  3383    1
## q[1828]  3808    1
## q[1829]  3383    1
## q[1830]  3808    1
## q[1831]  3383    1
## q[1832]  3383    1
## q[1833]  3808    1
## q[1834]  3808    1
## q[1835]  3383    1
## q[1836]  3383    1
## q[1837]  3383    1
## q[1838]  3383    1
## q[1839]  3749    1
## q[1840]  3808    1
## q[1841]  3808    1
## q[1842]  3808    1
## q[1843]  3383    1
## q[1844]  3808    1
## q[1845]  3749    1
## q[1846]  3808    1
## q[1847]  3808    1
## q[1848]  3383    1
## q[1849]  3808    1
## q[1850]  3383    1
## q[1851]  3383    1
## q[1852]  3808    1
## q[1853]  3749    1
## q[1854]  3749    1
## q[1855]  3383    1
## q[1856]  3749    1
## q[1857]  3749    1
## q[1858]  3383    1
## q[1859]  3383    1
## q[1860]  3383    1
## q[1861]  3808    1
## q[1862]  3383    1
## q[1863]  3808    1
## q[1864]  3808    1
## q[1865]  3383    1
## q[1866]  3749    1
## q[1867]  3383    1
## q[1868]  3383    1
## q[1869]  3383    1
## q[1870]  3383    1
## q[1871]  3383    1
## q[1872]  3383    1
## q[1873]  3808    1
## q[1874]  3383    1
## q[1875]  3749    1
## q[1876]  3749    1
## q[1877]  3383    1
## q[1878]  3749    1
## q[1879]  3808    1
## q[1880]  3383    1
## q[1881]  3383    1
## q[1882]  3808    1
## q[1883]  3749    1
## q[1884]  3383    1
## q[1885]  3383    1
## q[1886]  3383    1
## q[1887]  3383    1
## q[1888]  3383    1
## q[1889]  3749    1
## q[1890]  2972    1
## q[1891]  2972    1
## q[1892]  2972    1
## q[1893]  2972    1
## q[1894]  3394    1
## q[1895]  3394    1
## q[1896]  3394    1
## q[1897]  3459    1
## q[1898]  2972    1
## q[1899]  2972    1
## q[1900]  2972    1
## q[1901]  3394    1
## q[1902]  2972    1
## q[1903]  3394    1
## q[1904]  2972    1
## q[1905]  2972    1
## q[1906]  3394    1
## q[1907]  2972    1
## q[1908]  3394    1
## q[1909]  3394    1
## q[1910]  2972    1
## q[1911]  3459    1
## q[1912]  3394    1
## q[1913]  3394    1
## q[1914]  3394    1
## q[1915]  2972    1
## q[1916]  3394    1
## q[1917]  2972    1
## q[1918]  2972    1
## q[1919]  3394    1
## q[1920]  2972    1
## q[1921]  2972    1
## q[1922]  2972    1
## q[1923]  2972    1
## q[1924]  2972    1
## q[1925]  2972    1
## q[1926]  2972    1
## q[1927]  3394    1
## q[1928]  3459    1
## q[1929]  2972    1
## q[1930]  3459    1
## q[1931]  2972    1
## q[1932]  3459    1
## q[1933]  2972    1
## q[1934]  2972    1
## q[1935]  2972    1
## q[1936]  2972    1
## q[1937]  2979    1
## q[1938]  3013    1
## q[1939]  3013    1
## q[1940]  2543    1
## q[1941]  2543    1
## q[1942]  2543    1
## q[1943]  2543    1
## q[1944]  2543    1
## q[1945]  2979    1
## q[1946]  2543    1
## q[1947]  2979    1
## q[1948]  2979    1
## q[1949]  2979    1
## q[1950]  2543    1
## q[1951]  2543    1
## q[1952]  2543    1
## q[1953]  2543    1
## q[1954]  2543    1
## q[1955]  2543    1
## q[1956]  2543    1
## q[1957]  3013    1
## q[1958]  2543    1
## q[1959]  2960    1
## q[1960]  2526    1
## q[1961]  2994    1
## q[1962]  2526    1
## q[1963]  2526    1
## q[1964]  2960    1
## q[1965]  2994    1
## q[1966]  2960    1
## q[1967]  2526    1
## q[1968]  2526    1
## q[1969]  2526    1
## q[1970]  2526    1
## q[1971]  2526    1
## q[1972]  2526    1
## q[1973]  2960    1
## q[1974]  2526    1
## q[1975]  2526    1
## q[1976]  2960    1
## q[1977]  2526    1
## q[1978]  2960    1
## q[1979]  2994    1
## q[1980]  2960    1
## q[1981]  2960    1
## q[1982]  2960    1
## q[1983]  2526    1
## q[1984]  2526    1
## q[1985]  2994    1
## q[1986]  2526    1
## q[1987]  2994    1
## q[1988]  2960    1
## q[1989]  2526    1
## q[1990]  2526    1
## q[1991]  2960    1
## q[1992]  2526    1
## q[1993]  2526    1
## q[1994]  2994    1
## q[1995]  2526    1
## q[1996]  2960    1
## q[1997]  2526    1
## q[1998]  2526    1
## q[1999]  2526    1
## q[2000]  3462    1
## q[2001]  3515    1
## q[2002]  3718    1
## q[2003]  3462    1
## q[2004]  3462    1
## q[2005]  3718    1
## q[2006]  3462    1
## q[2007]  3515    1
## q[2008]  3515    1
## q[2009]  3718    1
## q[2010]  3515    1
## q[2011]  3515    1
## q[2012]  3462    1
## q[2013]  3515    1
## q[2014]  3515    1
## q[2015]  3515    1
## q[2016]  3462    1
## q[2017]  3515    1
## q[2018]  3515    1
## q[2019]  3462    1
## q[2020]  3515    1
## q[2021]  3462    1
## q[2022]  3462    1
## q[2023]  3515    1
## q[2024]  3462    1
## q[2025]  3718    1
## q[2026]  3462    1
## q[2027]  3462    1
## q[2028]  3515    1
## q[2029]  3462    1
## q[2030]  3718    1
## q[2031]  3515    1
## q[2032]  3515    1
## q[2033]  3718    1
## q[2034]  3515    1
## q[2035]  3515    1
## q[2036]  3718    1
## q[2037]  3462    1
## q[2038]  3515    1
## q[2039]  3462    1
## q[2040]  3462    1
## q[2041]  3515    1
## q[2042]  3718    1
## q[2043]  3462    1
## q[2044]  3515    1
## q[2045]  3515    1
## q[2046]  3515    1
## q[2047]  3515    1
## q[2048]  3462    1
## q[2049]  3462    1
## q[2050]  3515    1
## q[2051]  3515    1
## q[2052]  3462    1
## q[2053]  3718    1
## q[2054]  3462    1
## q[2055]  3515    1
## q[2056]  3515    1
## q[2057]  3529    1
## q[2058]  3428    1
## q[2059]  3725    1
## q[2060]  3428    1
## q[2061]  3428    1
## q[2062]  3529    1
## q[2063]  3428    1
## q[2064]  3529    1
## q[2065]  3428    1
## q[2066]  3529    1
## q[2067]  3428    1
## q[2068]  3529    1
## q[2069]  3529    1
## q[2070]  3428    1
## q[2071]  3529    1
## q[2072]  3529    1
## q[2073]  3725    1
## q[2074]  3725    1
## q[2075]  3428    1
## q[2076]  3428    1
## q[2077]  3529    1
## q[2078]  3725    1
## q[2079]  3529    1
## q[2080]  3529    1
## q[2081]  3428    1
## q[2082]  3725    1
## q[2083]  3428    1
## q[2084]  3529    1
## q[2085]  3428    1
## q[2086]  3725    1
## q[2087]  3529    1
## q[2088]  3725    1
## q[2089]  3504    1
## q[2090]  3408    1
## q[2091]  3408    1
## q[2092]  3408    1
## q[2093]  3504    1
## q[2094]  3662    1
## q[2095]  3504    1
## q[2096]  3504    1
## q[2097]  3662    1
## q[2098]  3408    1
## q[2099]  3408    1
## q[2100]  3408    1
## q[2101]  3504    1
## q[2102]  3408    1
## q[2103]  3408    1
## q[2104]  3408    1
## q[2105]  3504    1
## q[2106]  3408    1
## q[2107]  3504    1
## q[2108]  3408    1
## q[2109]  3504    1
## q[2110]  3504    1
## q[2111]  3408    1
## q[2112]  3408    1
## q[2113]  3408    1
## q[2114]  3504    1
## q[2115]  3504    1
## q[2116]  3504    1
## q[2117]  3408    1
## q[2118]  3408    1
## q[2119]  3408    1
## q[2120]  3662    1
## q[2121]  3662    1
## q[2122]  3408    1
## q[2123]  3504    1
## q[2124]  3408    1
## q[2125]  3504    1
## q[2126]  3408    1
## q[2127]  3662    1
## q[2128]  3408    1
## q[2129]  3408    1
## q[2130]  3504    1
## q[2131]  3662    1
## q[2132]  3662    1
## q[2133]  3408    1
## q[2134]  3504    1
## q[2135]  3408    1
## q[2136]  3504    1
## q[2137]  3662    1
## q[2138]  3408    1
## q[2139]  3408    1
## q[2140]  3408    1
## q[2141]  3662    1
## q[2142]  3168    1
## q[2143]  3374    1
## q[2144]  3502    1
## q[2145]  3502    1
## q[2146]  3168    1
## q[2147]  3374    1
## q[2148]  3374    1
## q[2149]  3374    1
## q[2150]  3374    1
## q[2151]  3168    1
## q[2152]  3374    1
## q[2153]  3374    1
## q[2154]  3168    1
## q[2155]  3374    1
## q[2156]  3168    1
## q[2157]  3502    1
## q[2158]  3168    1
## q[2159]  3168    1
## q[2160]  3374    1
## q[2161]  3374    1
## q[2162]  3502    1
## q[2163]  3374    1
## q[2164]  3502    1
## q[2165]  3168    1
## q[2166]  3374    1
## q[2167]  3374    1
## q[2168]  3168    1
## q[2169]  3374    1
## q[2170]  3374    1
## q[2171]  3168    1
## q[2172]  3374    1
## q[2173]  2739    1
## q[2174]  2739    1
## q[2175]  2656    1
## q[2176]  2959    1
## q[2177]  2739    1
## q[2178]  2739    1
## q[2179]  2739    1
## q[2180]  2739    1
## q[2181]  2959    1
## q[2182]  2656    1
## q[2183]  2656    1
## q[2184]  2656    1
## q[2185]  2739    1
## q[2186]  2656    1
## q[2187]  2739    1
## q[2188]  2739    1
## q[2189]  2739    1
## q[2190]  2739    1
## q[2191]  2739    1
## q[2192]  2739    1
## q[2193]  2959    1
## q[2194]  2739    1
## q[2195]  2739    1
## q[2196]  2871    1
## q[2197]  2871    1
## q[2198]  2871    1
## q[2199]  2871    1
## q[2200]  3302    1
## q[2201]  3352    1
## q[2202]  3302    1
## q[2203]  3352    1
## q[2204]  2871    1
## q[2205]  2871    1
## q[2206]  2871    1
## q[2207]  2871    1
## q[2208]  2871    1
## q[2209]  3302    1
## q[2210]  2871    1
## q[2211]  2871    1
## q[2212]  3302    1
## q[2213]  2871    1
## q[2214]  2871    1
## q[2215]  2871    1
## q[2216]  3352    1
## q[2217]  3302    1
## q[2218]  3302    1
## q[2219]  3302    1
## q[2220]  3302    1
## q[2221]  2871    1
## q[2222]  3352    1
## q[2223]  3352    1
## q[2224]  2871    1
## q[2225]  3352    1
## q[2226]  3352    1
## q[2227]  2871    1
## q[2228]  2871    1
## q[2229]  3302    1
## q[2230]  3302    1
## q[2231]  3352    1
## q[2232]  2871    1
## q[2233]  2871    1
## q[2234]  2871    1
## q[2235]  3352    1
## q[2236]  3352    1
## q[2237]  2871    1
## q[2238]  3352    1
## q[2239]  3302    1
## q[2240]  2871    1
## q[2241]  3302    1
## q[2242]  2871    1
## q[2243]  2871    1
## q[2244]  2871    1
## q[2245]  2871    1
## q[2246]  3352    1
## q[2247]  2535    1
## q[2248]  2528    1
## q[2249]  2528    1
## q[2250]  2528    1
## q[2251]  2223    1
## q[2252]  2528    1
## q[2253]  2223    1
## q[2254]  2528    1
## q[2255]  2528    1
## q[2256]  2528    1
## q[2257]  2223    1
## q[2258]  2223    1
## q[2259]  2528    1
## q[2260]  2528    1
## q[2261]  2223    1
## q[2262]  2223    1
## q[2263]  2223    1
## q[2264]  2528    1
## q[2265]  2223    1
## q[2266]  2535    1
## q[2267]  2528    1
## q[2268]  2223    1
## q[2269]  3321    1
## q[2270]  3127    1
## q[2271]  3127    1
## q[2272]  3321    1
## q[2273]  3321    1
## q[2274]  3468    1
## q[2275]  3321    1
## q[2276]  3127    1
## q[2277]  3321    1
## q[2278]  3321    1
## q[2279]  3127    1
## q[2280]  3127    1
## q[2281]  3321    1
## q[2282]  3127    1
## q[2283]  3468    1
## q[2284]  3127    1
## q[2285]  3321    1
## q[2286]  3127    1
## q[2287]  3468    1
## q[2288]  3468    1
## q[2289]  3321    1
## q[2290]  3321    1
## q[2291]  3468    1
## q[2292]  3127    1
## q[2293]  3127    1
## q[2294]  3468    1
## q[2295]  3127    1
## q[2296]  3321    1
## q[2297]  3321    1
## q[2298]  3321    1
## q[2299]  3321    1
## q[2300]  3127    1
## q[2301]  3127    1
## q[2302]  3321    1
## q[2303]  3127    1
## q[2304]  3468    1
## q[2305]  3321    1
## q[2306]  3321    1
## q[2307]  3179    1
## q[2308]  3220    1
## q[2309]  2744    1
## q[2310]  2744    1
## q[2311]  3220    1
## q[2312]  2744    1
## q[2313]  2744    1
## q[2314]  2744    1
## q[2315]  3179    1
## q[2316]  3220    1
## q[2317]  3179    1
## q[2318]  2744    1
## q[2319]  2744    1
## q[2320]  3179    1
## q[2321]  2744    1
## q[2322]  3220    1
## q[2323]  2744    1
## q[2324]  2744    1
## q[2325]  3179    1
## q[2326]  2744    1
## q[2327]  3220    1
## q[2328]  3179    1
## q[2329]  2744    1
## q[2330]  3686    1
## q[2331]  3224    1
## q[2332]  3224    1
## q[2333]  3224    1
## q[2334]  3224    1
## q[2335]  3686    1
## q[2336]  3686    1
## q[2337]  3616    1
## q[2338]  3686    1
## q[2339]  3686    1
## q[2340]  3616    1
## q[2341]  3224    1
## q[2342]  3224    1
## q[2343]  3224    1
## q[2344]  3224    1
## q[2345]  3686    1
## q[2346]  3616    1
## q[2347]  3224    1
## q[2348]  3224    1
## q[2349]  3224    1
## q[2350]  3224    1
## q[2351]  3686    1
## q[2352]  3224    1
## q[2353]  3686    1
## q[2354]  3224    1
## q[2355]  3686    1
## q[2356]  3686    1
## q[2357]  3224    1
## q[2358]  3224    1
## q[2359]  3224    1
## q[2360]  3224    1
## q[2361]  3686    1
## q[2362]  3686    1
## q[2363]  3224    1
## q[2364]  3616    1
## q[2365]  3224    1
## q[2366]  3686    1
## q[2367]  3224    1
## q[2368]  3224    1
## q[2369]  3686    1
## q[2370]  3616    1
## q[2371]  3616    1
## q[2372]  3224    1
## q[2373]  3686    1
## q[2374]  3224    1
## q[2375]  3686    1
## q[2376]  3224    1
## q[2377]  3616    1
## q[2378]  3224    1
## q[2379]  3224    1
## q[2380]  3224    1
## q[2381]  3224    1
## q[2382]  3224    1
## q[2383]  3686    1
## q[2384]  3616    1
## q[2385]  3686    1
## q[2386]  3686    1
## q[2387]  3224    1
## q[2388]  3224    1
## q[2389]  3224    1
## q[2390]  3686    1
## q[2391]  3616    1
## q[2392]  3686    1
## q[2393]  3224    1
## q[2394]  3224    1
## q[2395]  3616    1
## q[2396]  3616    1
## lp__     1722    1
## 
## Samples were drawn using NUTS(diag_e) at Thu Jun  6 14:26:34 2019.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&#34;section level1&#34;&gt;
&lt;h1&gt;檢查模型參數的收斂情況&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(bayesplot)

color_scheme_set(&amp;quot;mix-brightblue-gray&amp;quot;)

posterior5.5 &amp;lt;- rstan::extract(fit1, inc_warmup = TRUE, permuted = FALSE)

p &amp;lt;- mcmc_trace(posterior5.5, n_warmup = 0, pars = c(&amp;quot;b[1]&amp;quot;, &amp;quot;b[2]&amp;quot;, &amp;quot;b[3]&amp;quot;,   &amp;quot;lp__&amp;quot;), facet_args = list(nrow = 2, labeller = label_parsed))

p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:chapter5-5&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://winterwang.github.io/post/2019-01-27-logistic2-rstan_files/figure-html/chapter5-5-1.png&#34; alt=&#34;用 bayesplot包數繪製的模型5-5的MCMC鏈式軌跡圖 (trace plot)。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: 用 bayesplot包數繪製的模型5-5的MCMC鏈式軌跡圖 (trace plot)。
&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- mcmc_acf_bar(posterior5.5, pars = c(&amp;quot;b[1]&amp;quot;, &amp;quot;b[2]&amp;quot;, &amp;quot;b[3]&amp;quot;,   &amp;quot;lp__&amp;quot;))
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:chapter5-5-acf&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://winterwang.github.io/post/2019-01-27-logistic2-rstan_files/figure-html/chapter5-5-acf-1.png&#34; alt=&#34;用 bayesplot包數繪製的事後樣本自相關圖(autocorrelation)。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: 用 bayesplot包數繪製的事後樣本自相關圖(autocorrelation)。
&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- mcmc_dens_overlay(posterior5.5, pars = c(&amp;quot;b[1]&amp;quot;, &amp;quot;b[2]&amp;quot;, &amp;quot;b[3]&amp;quot;, &amp;quot;OR1&amp;quot;,  &amp;quot;OR2&amp;quot;, &amp;quot;lp__&amp;quot;), color_chains = T)
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step5-5-density&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://winterwang.github.io/post/2019-01-27-logistic2-rstan_files/figure-html/step5-5-density-1.png&#34; alt=&#34;用 bayesplot包數繪製的事後樣本密度分佈圖。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: 用 bayesplot包數繪製的事後樣本密度分佈圖。
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;section level1&#34;&gt;
&lt;h1&gt;檢查模型的擬合情況&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ms &amp;lt;- rstan::extract(fit1)
set.seed(123)
logistic &amp;lt;- function(x) 1/(1+exp(-x))
X &amp;lt;- 30:200
q_qua &amp;lt;- logistic(t(sapply(1:length(X), function(i) {
  q_mcmc &amp;lt;- ms$b[,1] + ms$b[,3]*X[i]/200
  quantile(q_mcmc, probs=c(0.1, 0.5, 0.9))
})))
d_est &amp;lt;- data.frame(X, q_qua)
colnames(d_est) &amp;lt;- c(&amp;#39;X&amp;#39;, &amp;#39;p10&amp;#39;, &amp;#39;p50&amp;#39;, &amp;#39;p90&amp;#39;)
d$A &amp;lt;- as.factor(d$A)

p &amp;lt;- ggplot(d_est, aes(x=X, y=p50))
p &amp;lt;- p + theme_bw(base_size=18)
p &amp;lt;- p + geom_ribbon(aes(ymin=p10, ymax=p90), fill=&amp;#39;black&amp;#39;, alpha=2/6)
p &amp;lt;- p + geom_line(size=1)
p &amp;lt;- p + geom_point(data=subset(d, A==0 &amp;amp; Weather==&amp;#39;A&amp;#39;), aes(x=Score, y=Y, color=A),
  position=position_jitter(w=0, h=0.1), size=1)
p &amp;lt;- p + labs(x=&amp;#39;Score&amp;#39;, y=&amp;#39;q&amp;#39;)
p &amp;lt;- p + scale_color_manual(values=c(&amp;#39;black&amp;#39;))
p &amp;lt;- p + scale_y_continuous(breaks=seq(0, 1, 0.2))
p &amp;lt;- p + xlim(30, 200)
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:validity-of-model&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://winterwang.github.io/post/2019-01-27-logistic2-rstan_files/figure-html/validity-of-model-1.png&#34; alt=&#34;不喜歡打工，且天氣晴天的情況下，分數在 30-200 點之間的學生的出勤概率 q 和它的 80% 可信區間範圍。圖中黑色實線是預測概率的事後分佈的中央值，灰色帶是可信區間範圍。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4: 不喜歡打工，且天氣晴天的情況下，分數在 30-200 點之間的學生的出勤概率 q 和它的 80% 可信區間範圍。圖中黑色實線是預測概率的事後分佈的中央值，灰色帶是可信區間範圍。
&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# ggsave(file=&amp;#39;output/fig5-9.png&amp;#39;, plot=p, dpi=300, w=4.5, h=3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;圖&lt;a href=&#34;#fig:validity-of-model&#34;&gt;4&lt;/a&gt;試圖把分數範圍在 30-200 之間的學生中，通過模型計算獲得的，在天氣晴朗，且不愛打工的孩子們的事後出勤概率的預測值(黑色實線)，和它的事後概率80%可信區間，以及對應的實際觀測值的結果(黑點)。但是，當預測變量越來越多，模型結果的可視化變得越來越困難。下面我們介紹兩種常見的評價邏輯回歸擬合結果的可視化圖。&lt;/p&gt;
&lt;p&gt;首先是圖 &lt;a href=&#34;#fig:validity-of-model1&#34;&gt;5&lt;/a&gt; 顯示的事後出勤概率，和實際觀察出勤結果之間的關係圖。在這個圖中，橫軸是 &lt;span class=&#34;math inline&#34;&gt;\(q[i]\)&lt;/span&gt; 的事後分佈的中央值(每名學生都有自己的事後出勤概率預測，它的中央值)，縱軸是該名學生實際是否在該次課上出勤的觀察結果。如果模型擬合的理想的話，那麼在 &lt;span class=&#34;math inline&#34;&gt;\(Y=0\)&lt;/span&gt;，也就是圖中的下半部分，大多數的預測點應該靠近概率較低的部分(也就是靠近左側)，同時，&lt;span class=&#34;math inline&#34;&gt;\(y = 1\)&lt;/span&gt; 的部分預測概率應該大多數在靠近左側的部分。此圖其實提示我們該模型的擬合效果不理想。不能明顯地將出勤與不出勤較爲準確地區分開來。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)
ms &amp;lt;- rstan::extract(fit1)
d_qua &amp;lt;- t(apply(ms$q, 2, quantile, prob=c(0.1, 0.5, 0.9)))
colnames(d_qua) &amp;lt;- c(&amp;#39;p10&amp;#39;, &amp;#39;p50&amp;#39;, &amp;#39;p90&amp;#39;)
d_qua &amp;lt;- data.frame(d, d_qua)
d_qua$Y &amp;lt;- as.factor(d_qua$Y)
d_qua$A &amp;lt;- as.factor(d_qua$A)

p &amp;lt;- ggplot(data=d_qua, aes(x=Y, y=p50))
p &amp;lt;- p + theme_bw(base_size=18)
p &amp;lt;- p + coord_flip()
p &amp;lt;- p + geom_violin(trim=FALSE, size=1.5, color=&amp;#39;grey80&amp;#39;)
p &amp;lt;- p + geom_point(aes(color=A), position=position_jitter(w=0.4, h=0), size=1)
p &amp;lt;- p + scale_color_manual(values=c(&amp;#39;grey5&amp;#39;, &amp;#39;grey50&amp;#39;))
p &amp;lt;- p + labs(x=&amp;#39;Y&amp;#39;, y=&amp;#39;q&amp;#39;)
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:validity-of-model1&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://winterwang.github.io/post/2019-01-27-logistic2-rstan_files/figure-html/validity-of-model1-1.png&#34; alt=&#34;把計算獲得的事後概率的中央值作爲每名學生是否出勤的概率預測(x 軸)，和實際觀察的出勤結果(y 軸)，繪製的散點圖。其中，灰色的小提琴圖其實是根據獲得的事後概率的中央值的概率密度曲線，繪製的上下對稱的圖形，形似小提琴。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 5: 把計算獲得的事後概率的中央值作爲每名學生是否出勤的概率預測(x 軸)，和實際觀察的出勤結果(y 軸)，繪製的散點圖。其中，灰色的小提琴圖其實是根據獲得的事後概率的中央值的概率密度曲線，繪製的上下對稱的圖形，形似小提琴。
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Rstan Wonderful R-(4)</title>
      <link>https://winterwang.github.io/post/logistic-rstan/</link>
      <pubDate>Sun, 27 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/logistic-rstan/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#-rstan-&#34;&gt;邏輯回歸模型的 Rstan 貝葉斯實現&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a&gt;確定分析目的&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a&gt;確認數據分佈&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a&gt;寫下數學模型表達式&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a&gt;確認收斂效果&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;-rstan-&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;邏輯回歸模型的 Rstan 貝葉斯實現&lt;/h1&gt;
&lt;p&gt;本小節使用的&lt;a href=&#34;https://raw.githubusercontent.com/MatsuuraKentaro/RStanBook/master/chap05/input/data-attendance-2.txt&#34;&gt;數據&lt;/a&gt;，和前一節的出勤率數據很類似:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d &amp;lt;- read.table(&amp;quot;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap05/input/data-attendance-2.txt&amp;quot;, 
                     sep = &amp;quot;,&amp;quot;, header = T)
head(d)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   PersonID A Score  M  Y
## 1        1 0    69 43 38
## 2        2 1   145 56 40
## 3        3 0   125 32 24
## 4        4 1    86 45 33
## 5        5 1   158 33 23
## 6        6 0   133 61 60&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;PersonID&lt;/code&gt;: 是學生的編號；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;A&lt;/code&gt;, &lt;code&gt;Score&lt;/code&gt;: 和之前一樣用來預測出勤率的兩個預測變量，分別是表示是否喜歡打工的 &lt;code&gt;A&lt;/code&gt;，和表示對學習本身是否喜歡的評分 (滿分200)；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;M&lt;/code&gt;: 過去三個月內，該名學生一共需要上課的總課時數；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Y&lt;/code&gt;: 過去三個月內，該名學生實際上出勤的課時數。&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;section level1&#34;&gt;
&lt;h1&gt;確定分析目的&lt;/h1&gt;
&lt;p&gt;需要回答的問題依然是，&lt;span class=&#34;math inline&#34;&gt;\(A, Score\)&lt;/span&gt; 分別在多大程度上預測學生的出勤率？另外，我們希望知道的是，當需要修的課時數固定的事後，這兩個預測變量能準確提供 &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; 的多少信息？&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level1&#34;&gt;
&lt;h1&gt;確認數據分佈&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(GGally)

set.seed(1)
d &amp;lt;- d[, -1]
# d &amp;lt;- read.csv(file=&amp;#39;input/data-attendance-2.txt&amp;#39;)[,-1]
d$A &amp;lt;- as.factor(d$A)
d &amp;lt;- transform(d, ratio=Y/M)
N_col &amp;lt;- ncol(d)
ggp &amp;lt;- ggpairs(d, upper=&amp;#39;blank&amp;#39;, diag=&amp;#39;blank&amp;#39;, lower=&amp;#39;blank&amp;#39;)

for(i in 1:N_col) {
  x &amp;lt;- d[,i]
  p &amp;lt;- ggplot(data.frame(x, A=d$A), aes(x))
  p &amp;lt;- p + theme_bw(base_size=14)
  p &amp;lt;- p + theme(axis.text.x=element_text(angle=40, vjust=1, hjust=1))
  if (class(x) == &amp;#39;factor&amp;#39;) {
    p &amp;lt;- p + geom_bar(aes(fill=A), color=&amp;#39;grey20&amp;#39;)
  } else {
    bw &amp;lt;- (max(x)-min(x))/10
    p &amp;lt;- p + geom_histogram(aes(fill=A), color=&amp;#39;grey20&amp;#39;, binwidth=bw)
    p &amp;lt;- p + geom_line(eval(bquote(aes(y=..count..*.(bw)))), stat=&amp;#39;density&amp;#39;)
  }
  p &amp;lt;- p + geom_label(data=data.frame(x=-Inf, y=Inf, label=colnames(d)[i]), aes(x=x, y=y, label=label), hjust=0, vjust=1)
  p &amp;lt;- p + scale_fill_manual(values=alpha(c(&amp;#39;white&amp;#39;, &amp;#39;grey40&amp;#39;), 0.5))
  ggp &amp;lt;- putPlot(ggp, p, i, i)
}

zcolat &amp;lt;- seq(-1, 1, length=81)
zcolre &amp;lt;- c(zcolat[1:40]+1, rev(zcolat[41:81]))

for(i in 1:(N_col-1)) {
  for(j in (i+1):N_col) {
    x &amp;lt;- as.numeric(d[,i])
    y &amp;lt;- as.numeric(d[,j])
    r &amp;lt;- cor(x, y, method=&amp;#39;spearman&amp;#39;, use=&amp;#39;pairwise.complete.obs&amp;#39;)
    zcol &amp;lt;- lattice::level.colors(r, at=zcolat, col.regions=grey(zcolre))
    textcol &amp;lt;- ifelse(abs(r) &amp;lt; 0.4, &amp;#39;grey20&amp;#39;, &amp;#39;white&amp;#39;)
    ell &amp;lt;- ellipse::ellipse(r, level=0.95, type=&amp;#39;l&amp;#39;, npoints=50, scale=c(.2, .2), centre=c(.5, .5))
    p &amp;lt;- ggplot(data.frame(ell), aes(x=x, y=y))
    p &amp;lt;- p + theme_bw() + theme(
      plot.background=element_blank(),
      panel.grid.major=element_blank(), panel.grid.minor=element_blank(),
      panel.border=element_blank(), axis.ticks=element_blank()
    )
    p &amp;lt;- p + geom_polygon(fill=zcol, color=zcol)
    p &amp;lt;- p + geom_text(data=NULL, x=.5, y=.5, label=100*round(r, 2), size=6, col=textcol)
    ggp &amp;lt;- putPlot(ggp, p, i, j)
  }
}

for(j in 1:(N_col-1)) {
  for(i in (j+1):N_col) {
    x &amp;lt;- d[,j]
    y &amp;lt;- d[,i]
    p &amp;lt;- ggplot(data.frame(x, y, gr=d$A), aes(x=x, y=y, fill=gr, shape=gr))
    p &amp;lt;- p + theme_bw(base_size=14)
    p &amp;lt;- p + theme(axis.text.x=element_text(angle=40, vjust=1, hjust=1))
    if (class(x) == &amp;#39;factor&amp;#39;) {
      p &amp;lt;- p + geom_boxplot(aes(group=x), alpha=3/6, outlier.size=0, fill=&amp;#39;white&amp;#39;)
      p &amp;lt;- p + geom_point(position=position_jitter(w=0.4, h=0), size=2)
    } else {
      p &amp;lt;- p + geom_point(size=2)
    }
    p &amp;lt;- p + scale_shape_manual(values=c(21, 24))
    p &amp;lt;- p + scale_fill_manual(values=alpha(c(&amp;#39;white&amp;#39;, &amp;#39;grey40&amp;#39;), 0.5))
    ggp &amp;lt;- putPlot(ggp, p, i, j)
  }
}

ggp&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step1&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://winterwang.github.io/post/2019-01-27-logistic-rstan_files/figure-html/step1-1.png&#34; alt=&#34;三個變量的分佈觀察圖，相比之前增加了 $ratio = Y/M$ 列。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: 三個變量的分佈觀察圖，相比之前增加了 &lt;span class=&#34;math inline&#34;&gt;\(ratio = Y/M\)&lt;/span&gt; 列。
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;從圖 &lt;a href=&#34;#fig:step1&#34;&gt;1&lt;/a&gt; 還可以看出，由於總課時數越多，學生實際出勤的課時數也會越多所以 &lt;span class=&#34;math inline&#34;&gt;\(M, Y\)&lt;/span&gt; 兩者之間理應有很強的正相關。另外可能可以推測的是 &lt;span class=&#34;math inline&#34;&gt;\(Ratio\)&lt;/span&gt; 和是否愛學習的分數之間大概有可能有正相關，和是否喜歡打工之間大概可能有負相關。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level1&#34;&gt;
&lt;h1&gt;寫下數學模型表達式&lt;/h1&gt;
&lt;p&gt;在 Stan 的語法中，使用的是反邏輯函數 (inverse logit): &lt;code&gt;inv_logit&lt;/code&gt; 來描述下面的邏輯回歸模型 5-4。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{array}{l}
q[n] = \text{inv_logit}(b_1 + b_2 A[n] + b_3Score[n]) &amp;amp; n = 1, 2, \dots, N \\
Y[n] \sim \text{Binomial}(M[n], q[n])                 &amp;amp; n = 1, 2, \dots, N \\
\end{array}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;上面的數學模型，可以被翻譯成下面的 Stan 語言:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data {
  int N; 
  int&amp;lt;lower=0, upper=1&amp;gt; A[N]; 
  real&amp;lt;lower=0, upper=1&amp;gt; Score[N]; 
  int&amp;lt;lower=0&amp;gt; M[N];
  int&amp;lt;lower=0&amp;gt; Y[N];
}

parameters {
  real b1; 
  real b2; 
  real b3;
}

transformed parameters {
  real q[N];
  for (n in 1:N) {
    q[n] = inv_logit(b1 + b2*A[n] + b3*Score[n]);
  }
}

model {
  for (n in 1:N) {
    Y[n] ~ binomial(M[n], q[n]); 
  }
}

generated quantities {
  real y_pred[N]; 
  for (n in 1:N) {
    y_pred[n] = binomial_rng(M[n], q[n]);
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;下面的 R 代碼用來實現對上面 Stan 模型的擬合:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rstan)
d &amp;lt;- read.csv(file=&amp;#39;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap05/input/data-attendance-2.txt&amp;#39;, header = T)
data &amp;lt;- list(N=nrow(d), A=d$A, Score=d$Score/200, M=d$M, Y=d$Y)
fit &amp;lt;- stan(file=&amp;#39;stanfiles/model5-4.stan&amp;#39;, data=data, seed=1234)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &amp;#39;model5-4&amp;#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 2.6e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.26 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.418554 seconds (Warm-up)
## Chain 1:                0.393323 seconds (Sampling)
## Chain 1:                0.811877 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &amp;#39;model5-4&amp;#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 2.1e-05 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.21 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.435721 seconds (Warm-up)
## Chain 2:                0.431322 seconds (Sampling)
## Chain 2:                0.867043 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &amp;#39;model5-4&amp;#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 2.3e-05 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.23 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.385254 seconds (Warm-up)
## Chain 3:                0.366883 seconds (Sampling)
## Chain 3:                0.752137 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &amp;#39;model5-4&amp;#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 1.5e-05 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.415028 seconds (Warm-up)
## Chain 4:                0.409585 seconds (Sampling)
## Chain 4:                0.824613 seconds (Total)
## Chain 4:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Stan model: model5-4.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##                mean se_mean   sd     2.5%      25%      50%      75%
## b1             0.09    0.01 0.22    -0.35    -0.06     0.09     0.24
## b2            -0.62    0.00 0.10    -0.81    -0.68    -0.62    -0.55
## b3             1.90    0.01 0.36     1.17     1.66     1.91     2.15
## q[1]           0.68    0.00 0.02     0.63     0.66     0.68     0.70
## q[2]           0.70    0.00 0.02     0.67     0.69     0.70     0.71
## q[3]           0.78    0.00 0.01     0.76     0.78     0.78     0.79
## q[4]           0.57    0.00 0.02     0.53     0.56     0.57     0.59
## q[5]           0.73    0.00 0.02     0.69     0.71     0.73     0.74
## q[6]           0.80    0.00 0.01     0.77     0.79     0.80     0.80
## q[7]           0.76    0.00 0.01     0.73     0.75     0.76     0.77
## q[8]           0.70    0.00 0.02     0.67     0.69     0.70     0.72
## q[9]           0.81    0.00 0.01     0.79     0.81     0.81     0.82
## q[10]          0.81    0.00 0.01     0.79     0.80     0.81     0.82
## q[11]          0.69    0.00 0.02     0.66     0.68     0.69     0.70
## q[12]          0.80    0.00 0.01     0.78     0.79     0.80     0.81
## q[13]          0.64    0.00 0.02     0.61     0.63     0.64     0.65
## q[14]          0.76    0.00 0.01     0.73     0.75     0.76     0.77
## q[15]          0.76    0.00 0.01     0.73     0.75     0.76     0.76
## q[16]          0.60    0.00 0.02     0.57     0.59     0.60     0.61
## q[17]          0.76    0.00 0.01     0.74     0.76     0.76     0.77
## q[18]          0.70    0.00 0.02     0.67     0.69     0.71     0.72
## q[19]          0.86    0.00 0.02     0.83     0.85     0.86     0.88
## q[20]          0.72    0.00 0.02     0.69     0.71     0.72     0.73
## q[21]          0.57    0.00 0.02     0.53     0.56     0.57     0.59
## q[22]          0.62    0.00 0.02     0.59     0.61     0.62     0.63
## q[23]          0.62    0.00 0.02     0.58     0.61     0.62     0.63
## q[24]          0.70    0.00 0.02     0.67     0.69     0.70     0.71
## q[25]          0.64    0.00 0.02     0.61     0.63     0.64     0.65
## q[26]          0.67    0.00 0.01     0.64     0.66     0.67     0.68
## q[27]          0.77    0.00 0.01     0.75     0.76     0.77     0.78
## q[28]          0.77    0.00 0.01     0.75     0.76     0.77     0.78
## q[29]          0.83    0.00 0.01     0.81     0.83     0.84     0.84
## q[30]          0.76    0.00 0.01     0.74     0.75     0.76     0.77
## q[31]          0.74    0.00 0.02     0.70     0.73     0.74     0.75
## q[32]          0.54    0.00 0.03     0.49     0.52     0.54     0.56
## q[33]          0.69    0.00 0.02     0.66     0.68     0.69     0.70
## q[34]          0.66    0.00 0.01     0.63     0.65     0.66     0.67
## q[35]          0.78    0.00 0.01     0.76     0.78     0.78     0.79
## q[36]          0.79    0.00 0.01     0.77     0.78     0.79     0.80
## q[37]          0.62    0.00 0.02     0.58     0.60     0.62     0.63
## q[38]          0.76    0.00 0.01     0.73     0.75     0.76     0.77
## q[39]          0.72    0.00 0.02     0.68     0.71     0.72     0.73
## q[40]          0.72    0.00 0.02     0.68     0.71     0.72     0.73
## q[41]          0.79    0.00 0.01     0.77     0.78     0.79     0.80
## q[42]          0.80    0.00 0.01     0.77     0.79     0.80     0.80
## q[43]          0.78    0.00 0.01     0.75     0.77     0.78     0.79
## q[44]          0.82    0.00 0.01     0.79     0.81     0.82     0.83
## q[45]          0.86    0.00 0.02     0.83     0.85     0.86     0.87
## q[46]          0.75    0.00 0.01     0.72     0.74     0.75     0.76
## q[47]          0.64    0.00 0.03     0.57     0.62     0.64     0.66
## q[48]          0.82    0.00 0.01     0.79     0.81     0.82     0.83
## q[49]          0.74    0.00 0.01     0.71     0.73     0.74     0.75
## q[50]          0.60    0.00 0.02     0.57     0.59     0.60     0.61
## y_pred[1]     29.25    0.06 3.29    23.00    27.00    29.00    32.00
## y_pred[2]     39.25    0.06 3.59    32.00    37.00    39.00    42.00
## y_pred[3]     25.04    0.04 2.39    20.00    24.00    25.00    27.00
## y_pred[4]     25.73    0.06 3.47    19.00    23.00    26.00    28.00
## y_pred[5]     23.93    0.04 2.66    18.00    22.00    24.00    26.00
## y_pred[6]     48.53    0.05 3.21    42.00    46.00    49.00    51.00
## y_pred[7]     37.30    0.05 3.03    31.00    35.00    37.00    39.00
## y_pred[8]     53.56    0.07 4.21    45.00    51.00    54.00    56.00
## y_pred[9]     63.56    0.06 3.58    56.00    61.00    64.00    66.00
## y_pred[10]    52.05    0.05 3.25    45.00    50.00    52.00    54.00
## y_pred[11]    23.49    0.04 2.75    18.00    22.00    24.00    25.00
## y_pred[12]    35.20    0.05 2.79    29.00    33.00    35.00    37.00
## y_pred[13]    34.28    0.06 3.60    27.00    32.00    34.00    37.00
## y_pred[14]    30.38    0.04 2.73    25.00    29.00    30.00    32.00
## y_pred[15]    42.31    0.05 3.30    35.00    40.00    42.00    45.00
## y_pred[16]    35.41    0.06 3.85    28.00    33.00    35.00    38.00
## y_pred[17]    29.07    0.04 2.60    24.00    27.00    29.00    31.00
## y_pred[18]    31.68    0.05 3.18    25.00    30.00    32.00    34.00
## y_pred[19]    38.89    0.04 2.40    34.00    37.00    39.00    41.00
## y_pred[20]    55.55    0.07 4.10    47.00    53.00    56.00    58.00
## y_pred[21]    40.02    0.08 4.45    31.00    37.00    40.00    43.00
## y_pred[22]    47.90    0.07 4.46    39.00    45.00    48.00    51.00
## y_pred[23]    38.81    0.07 4.06    31.00    36.00    39.00    42.00
## y_pred[24]    47.39    0.06 3.96    39.00    45.00    47.00    50.00
## y_pred[25]    32.03    0.06 3.46    25.00    30.00    32.00    34.00
## y_pred[26]    34.06    0.06 3.45    27.00    32.00    34.00    36.00
## y_pred[27]    22.40    0.04 2.32    17.00    21.00    23.00    24.00
## y_pred[28]    28.63    0.04 2.53    23.00    27.00    29.00    30.00
## y_pred[29]    15.02    0.03 1.57    12.00    14.00    15.00    16.00
## y_pred[30]    37.35    0.05 3.11    31.00    35.00    37.00    40.00
## y_pred[31]    55.46    0.07 4.06    47.00    53.00    56.00    58.00
## y_pred[32]     6.48    0.03 1.73     3.00     5.00     6.00     8.00
## y_pred[33]    15.78    0.04 2.21    11.00    14.00    16.00    17.00
## y_pred[34]    24.29    0.05 2.90    19.00    22.00    24.00    26.00
## y_pred[35]    46.33    0.05 3.26    40.00    44.00    46.00    49.00
## y_pred[36]    43.56    0.05 3.03    37.00    42.00    44.00    46.00
## y_pred[37]    54.14    0.08 4.77    45.00    51.00    54.00    57.00
## y_pred[38]    35.61    0.05 3.03    30.00    34.00    36.00    38.00
## y_pred[39]    15.82    0.03 2.14    11.00    14.00    16.00    17.00
## y_pred[40]    29.32    0.05 2.98    23.00    27.00    29.00    31.00
## y_pred[41]    44.99    0.05 3.20    38.00    43.00    45.00    47.00
## y_pred[42]    25.45    0.04 2.32    21.00    24.00    26.00    27.00
## y_pred[43]    41.19    0.05 3.14    35.00    39.00    41.00    43.00
## y_pred[44]    25.35    0.03 2.20    21.00    24.00    25.00    27.00
## y_pred[45]    19.77    0.03 1.72    16.00    19.00    20.00    21.00
## y_pred[46]    38.21    0.05 3.19    32.00    36.00    38.00    40.00
## y_pred[47]    14.09    0.04 2.31     9.00    13.00    14.00    16.00
## y_pred[48]    31.14    0.04 2.40    26.00    30.00    31.00    33.00
## y_pred[49]    16.97    0.04 2.21    12.00    16.00    17.00    19.00
## y_pred[50]    40.35    0.07 4.13    32.00    38.00    40.00    43.00
## lp__       -1389.38    0.03 1.22 -1392.66 -1389.96 -1389.06 -1388.47
##               97.5% n_eff Rhat
## b1             0.54  1340    1
## b2            -0.44  1593    1
## b3             2.60  1251    1
## q[1]           0.72  1502    1
## q[2]           0.73  2134    1
## q[3]           0.80  2200    1
## q[4]           0.62  1656    1
## q[5]           0.76  1835    1
## q[6]           0.82  2090    1
## q[7]           0.78  2122    1
## q[8]           0.74  2077    1
## q[9]           0.84  1849    1
## q[10]          0.84  1868    1
## q[11]          0.72  2255    1
## q[12]          0.82  2019    1
## q[13]          0.67  2548    1
## q[14]          0.78  2122    1
## q[15]          0.78  2078    1
## q[16]          0.64  1964    1
## q[17]          0.79  2178    1
## q[18]          0.74  1612    1
## q[19]          0.89  1457    1
## q[20]          0.76  1871    1
## q[21]          0.62  1656    1
## q[22]          0.65  2287    1
## q[23]          0.65  2209    1
## q[24]          0.73  2193    1
## q[25]          0.67  2528    1
## q[26]          0.69  2586    1
## q[27]          0.80  2218    1
## q[28]          0.80  2218    1
## q[29]          0.86  1627    1
## q[30]          0.79  2158    1
## q[31]          0.78  1731    1
## q[32]          0.60  1487    1
## q[33]          0.72  2351    1
## q[34]          0.69  2607    1
## q[35]          0.81  2190    1
## q[36]          0.81  2123    1
## q[37]          0.65  2164    1
## q[38]          0.78  2101    1
## q[39]          0.75  1702    1
## q[40]          0.75  1687    1
## q[41]          0.81  2153    1
## q[42]          0.82  2090    1
## q[43]          0.80  2218    1
## q[44]          0.84  1812    1
## q[45]          0.89  1470    1
## q[46]          0.77  1996    1
## q[47]          0.70  1423    1
## q[48]          0.84  1778    1
## q[49]          0.77  1871    1
## q[50]          0.64  1964    1
## y_pred[1]     35.00  3492    1
## y_pred[2]     46.00  3681    1
## y_pred[3]     29.00  3860    1
## y_pred[4]     32.00  3179    1
## y_pred[5]     29.00  3841    1
## y_pred[6]     54.00  3965    1
## y_pred[7]     43.00  3935    1
## y_pred[8]     62.00  3527    1
## y_pred[9]     70.00  3465    1
## y_pred[10]    58.00  3590    1
## y_pred[11]    29.00  3806    1
## y_pred[12]    40.00  3503    1
## y_pred[13]    41.00  3954    1
## y_pred[14]    36.00  3874    1
## y_pred[15]    48.00  3874    1
## y_pred[16]    43.00  3583    1
## y_pred[17]    34.00  3850    1
## y_pred[18]    38.00  3684    1
## y_pred[19]    43.00  3086    1
## y_pred[20]    63.00  3161    1
## y_pred[21]    49.00  3455    1
## y_pred[22]    56.00  3998    1
## y_pred[23]    46.00  3761    1
## y_pred[24]    55.00  3822    1
## y_pred[25]    39.00  3767    1
## y_pred[26]    41.00  3890    1
## y_pred[27]    27.00  4104    1
## y_pred[28]    33.00  3965    1
## y_pred[29]    18.00  3819    1
## y_pred[30]    43.00  3948    1
## y_pred[31]    63.00  3164    1
## y_pred[32]    10.00  3446    1
## y_pred[33]    20.00  3789    1
## y_pred[34]    30.00  3379    1
## y_pred[35]    52.00  3549    1
## y_pred[36]    49.00  4180    1
## y_pred[37]    64.00  3795    1
## y_pred[38]    41.00  3934    1
## y_pred[39]    20.00  3770    1
## y_pred[40]    35.00  3784    1
## y_pred[41]    51.00  3895    1
## y_pred[42]    30.00  4015    1
## y_pred[43]    47.00  3828    1
## y_pred[44]    29.00  4082    1
## y_pred[45]    23.00  3556    1
## y_pred[46]    44.00  3796    1
## y_pred[47]    18.00  3669    1
## y_pred[48]    35.00  3792    1
## y_pred[49]    21.00  3580    1
## y_pred[50]    48.00  3800    1
## lp__       -1387.98  1351    1
## 
## Samples were drawn using NUTS(diag_e) at Sat Feb  2 23:13:35 2019.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;把獲得的參數事後樣本的均值代入上面的數學模型中可得:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{array}{l}
q[n] = \text{inv_logit}(0.09 - 0.62 A[n] + 1.90Score[n]) &amp;amp; n = 1, 2, \dots, N \\
Y[n] \sim \text{Binomial}(M[n], q[n])                 &amp;amp; n = 1, 2, \dots, N \\
\end{array}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level1&#34;&gt;
&lt;h1&gt;確認收斂效果&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(bayesplot)

color_scheme_set(&amp;quot;mix-brightblue-gray&amp;quot;)

posterior2 &amp;lt;- rstan::extract(fit, inc_warmup = TRUE, permuted = FALSE)

p &amp;lt;- mcmc_trace(posterior2, n_warmup = 0, pars = c(&amp;quot;b1&amp;quot;, &amp;quot;b2&amp;quot;, &amp;quot;b3&amp;quot;, &amp;quot;lp__&amp;quot;),
                facet_args = list(nrow = 2, labeller = label_parsed))
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step53&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://winterwang.github.io/post/2019-01-27-logistic-rstan_files/figure-html/step53-1.png&#34; alt=&#34;用 bayesplot包數繪製的模型5-3的MCMC鏈式軌跡圖 (trace plot)。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: 用 bayesplot包數繪製的模型5-3的MCMC鏈式軌跡圖 (trace plot)。
&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ms &amp;lt;- rstan::extract(fit)

d_qua &amp;lt;- t(apply(ms$y_pred, 2, quantile, prob=c(0.1, 0.5, 0.9)))
colnames(d_qua) &amp;lt;- c(&amp;#39;p10&amp;#39;, &amp;#39;p50&amp;#39;, &amp;#39;p90&amp;#39;)
d_qua &amp;lt;- data.frame(d, d_qua)
d_qua$A &amp;lt;- as.factor(d_qua$A)

p &amp;lt;- ggplot(data=d_qua, aes(x=Y, y=p50, ymin=p10, ymax=p90, shape=A, fill=A))
p &amp;lt;- p + theme_bw(base_size=18) + theme(legend.key.height=grid::unit(2.5,&amp;#39;line&amp;#39;))
p &amp;lt;- p + coord_fixed(ratio=1, xlim=c(5, 70), ylim=c(5, 70))
p &amp;lt;- p + geom_pointrange(size=0.8, color=&amp;#39;grey5&amp;#39;)
p &amp;lt;- p + geom_abline(aes(slope=1, intercept=0), color=&amp;#39;black&amp;#39;, alpha=3/5, linetype=&amp;#39;31&amp;#39;)
p &amp;lt;- p + scale_shape_manual(values=c(21, 24))
p &amp;lt;- p + scale_fill_manual(values=c(&amp;#39;white&amp;#39;, &amp;#39;grey70&amp;#39;))
p &amp;lt;- p + labs(x=&amp;#39;Observed&amp;#39;, y=&amp;#39;Predicted&amp;#39;)
p &amp;lt;- p + scale_x_continuous(breaks=seq(from=0, to=70, by=20))
p &amp;lt;- p + scale_y_continuous(breaks=seq(from=0, to=70, by=20))
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:fig58&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://winterwang.github.io/post/2019-01-27-logistic-rstan_files/figure-html/fig58-1.png&#34; alt=&#34;觀測值(x)，和預測值(y)的散點圖，以及預測值的80%預測區間。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: 觀測值(x)，和預測值(y)的散點圖，以及預測值的80%預測區間。
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Rstan Wonderful R-(3)</title>
      <link>https://winterwang.github.io/post/rstan-wonderful-r3/</link>
      <pubDate>Tue, 22 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/rstan-wonderful-r3/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#-multiple-regression&#34;&gt;多重回歸 multiple regression&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#step-1.-&#34;&gt;Step 1. 確認數據分佈&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-2.-&#34;&gt;Step 2. 寫下數學模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-3.-&#34;&gt;Step 3. 看圖確認模型擬合狀況&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-4.-mcmc-&#34;&gt;Step 4. MCMC 樣本的散點圖矩陣&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;-multiple-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;多重回歸 multiple regression&lt;/h1&gt;
&lt;p&gt;本章使用的數據，大學生出勤記錄也是&lt;a href=&#34;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap05/input/data-attendance-1.txt&#34;&gt;架空的數據&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;有大學記錄了50名大學生的出勤狀況：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;A,Score,Y
0,69,0.286
1,145,0.196
0,125,0.261
1,86,0.109
1,158,0.23
0,133,0.35
0,111,0.33
1,147,0.194
0,146,0.413
0,145,0.36
1,141,0.225
0,137,0.423
1,118,0.186
0,111,0.287
...
0,99,0.268
1,99,0.234&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;: 是學生大學二年級時進行的問卷調查時回答是否喜歡打零工的結果（0:不喜歡打工；1:喜歡打工）&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Score\)&lt;/span&gt;: 是大學二年級時進行的問卷調查時計算的該學生對學習是否感興趣的數值評分(200分滿分，分數越高，該學生越熱愛學習)&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;: 是該學生一年內的出勤率&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在本次分析範例中，把&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;出勤率當作是連續型結果變量，我們來用Stan實施多重回歸分析，回答學生喜歡打零工與否，和學生對學習的熱情程度兩個變量能解釋多少出勤率。&lt;/p&gt;
&lt;div id=&#34;step-1.-&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 1. 確認數據分佈&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# The following figure codes come from the authors website: 
# https://github.com/MatsuuraKentaro/RStanBook/blob/master/chap05/fig5-1.R
library(ggplot2)
library(GGally)

set.seed(123)
d &amp;lt;- read.csv(file=&amp;#39;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap05/input/data-attendance-1.txt&amp;#39;, header = T)
d$A &amp;lt;- as.factor(d$A)

N_col &amp;lt;- ncol(d)
ggp &amp;lt;- ggpairs(d, upper=&amp;#39;blank&amp;#39;, diag=&amp;#39;blank&amp;#39;, lower=&amp;#39;blank&amp;#39;)

for(i in 1:N_col) {
  x &amp;lt;- d[,i]
  p &amp;lt;- ggplot(data.frame(x, A=d$A), aes(x))
  p &amp;lt;- p + theme_bw(base_size=14)
  p &amp;lt;- p + theme(axis.text.x=element_text(angle=40, vjust=1, hjust=1))
  if (class(x) == &amp;#39;factor&amp;#39;) {
    p &amp;lt;- p + geom_bar(aes(fill=A), color=&amp;#39;grey5&amp;#39;)
  } else {
    bw &amp;lt;- (max(x)-min(x))/10
    p &amp;lt;- p + geom_histogram(binwidth=bw, aes(fill=A), color=&amp;#39;grey5&amp;#39;) #繪製柱狀圖
    p &amp;lt;- p + geom_line(eval(bquote(aes(y=..count..*.(bw)))), stat=&amp;#39;density&amp;#39;) #添加概率密度曲線
  }
  p &amp;lt;- p + geom_label(data=data.frame(x=-Inf, y=Inf, label=colnames(d)[i]), aes(x=x, y=y, label=label), hjust=0, vjust=1)
  p &amp;lt;- p + scale_fill_manual(values=alpha(c(&amp;#39;white&amp;#39;, &amp;#39;grey40&amp;#39;), 0.5))
  ggp &amp;lt;- putPlot(ggp, p, i, i)
}

zcolat &amp;lt;- seq(-1, 1, length=81)
zcolre &amp;lt;- c(zcolat[1:40]+1, rev(zcolat[41:81]))

for(i in 1:(N_col-1)) {
  for(j in (i+1):N_col) {
    x &amp;lt;- as.numeric(d[,i])
    y &amp;lt;- as.numeric(d[,j])
    r &amp;lt;- cor(x, y, method=&amp;#39;spearman&amp;#39;, use=&amp;#39;pairwise.complete.obs&amp;#39;)
    zcol &amp;lt;- lattice::level.colors(r, at=zcolat, col.regions=grey(zcolre))
    textcol &amp;lt;- ifelse(abs(r) &amp;lt; 0.4, &amp;#39;grey20&amp;#39;, &amp;#39;white&amp;#39;)
    ell &amp;lt;- ellipse::ellipse(r, level=0.95, type=&amp;#39;l&amp;#39;, npoints=50, scale=c(.2, .2), centre=c(.5, .5))
    p &amp;lt;- ggplot(data.frame(ell), aes(x=x, y=y))
    p &amp;lt;- p + theme_bw() + theme(
      plot.background=element_blank(),
      panel.grid.major=element_blank(), panel.grid.minor=element_blank(),
      panel.border=element_blank(), axis.ticks=element_blank()
    )
    p &amp;lt;- p + geom_polygon(fill=zcol, color=zcol)
    p &amp;lt;- p + geom_text(data=NULL, x=.5, y=.5, label=100*round(r, 2), size=6, col=textcol)
    ggp &amp;lt;- putPlot(ggp, p, i, j)
  }
}

for(j in 1:(N_col-1)) {
  for(i in (j+1):N_col) {
    x &amp;lt;- d[,j]
    y &amp;lt;- d[,i]
    p &amp;lt;- ggplot(data.frame(x, y, gr=d$A), aes(x=x, y=y, fill=gr, shape=gr))
    p &amp;lt;- p + theme_bw(base_size=14)
    p &amp;lt;- p + theme(axis.text.x=element_text(angle=40, vjust=1, hjust=1))
    if (class(x) == &amp;#39;factor&amp;#39;) {
      p &amp;lt;- p + geom_boxplot(aes(group=x), alpha=3/6, outlier.size=0, fill=&amp;#39;white&amp;#39;)
      p &amp;lt;- p + geom_point(position=position_jitter(w=0.4, h=0), size=2)
    } else {
      p &amp;lt;- p + geom_point(size=2)
    }
    p &amp;lt;- p + scale_shape_manual(values=c(21, 24))
    p &amp;lt;- p + scale_fill_manual(values=alpha(c(&amp;#39;white&amp;#39;, &amp;#39;grey40&amp;#39;), 0.5))
    ggp &amp;lt;- putPlot(ggp, p, i, j)
  }
}

ggp&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step1&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://winterwang.github.io/post/2019-01-22-rstan-wonderful-r-multi-lm_files/figure-html/step1-1.png&#34; alt=&#34;三個變量的分佈觀察圖，對角線上是三個變量各自的柱狀圖 (histogram) 和計算獲得的概率密度函數曲線；左下角三個圖是三個變量的箱式圖和散點圖；右上角三個圖是三個變量兩兩計算獲得的 Spearman 秩相關乘以100之後的數值。對角線上及左下角三個圖中數據點和形狀的不同分別表示學生喜歡(三角形)和不喜歡(圓形)打工。右上角表示秩相關的數值越接近0，顏色越白圖形越接近圓形，相關係數的絕對值越接近1，則顏色越深，橢圓越細長。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: 三個變量的分佈觀察圖，對角線上是三個變量各自的柱狀圖 (histogram) 和計算獲得的概率密度函數曲線；左下角三個圖是三個變量的箱式圖和散點圖；右上角三個圖是三個變量兩兩計算獲得的 Spearman 秩相關乘以100之後的數值。對角線上及左下角三個圖中數據點和形狀的不同分別表示學生喜歡(三角形)和不喜歡(圓形)打工。右上角表示秩相關的數值越接近0，顏色越白圖形越接近圓形，相關係數的絕對值越接近1，則顏色越深，橢圓越細長。
&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# png(file=&amp;#39;output/fig5-1.png&amp;#39;, w=1600, h=1600, res=300)
# print(ggp, left=0.3, bottom=0.3)
# dev.off()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;step-2.-&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 2. 寫下數學模型&lt;/h2&gt;
&lt;p&gt;Model can be written as (Model5-1):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{array}{l}
Y[n]        = b_1 + b_2A[n] + b_3Sore[n] + \varepsilon [n]&amp;amp;  n = 1,2,\dots,N \\
\varepsilon[n] \sim \text{Normal}(0, \sigma) &amp;amp; n = 1,2,\dots,N \\ 
\end{array}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; 表示學生的人數，&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;則是學生編號的下標；&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(b_1\)&lt;/span&gt; 是回歸直線的截距；&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(b_2\)&lt;/span&gt; 是&lt;span class=&#34;math inline&#34;&gt;\(Score\)&lt;/span&gt;保持不變時，&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;從&lt;span class=&#34;math inline&#34;&gt;\(0\rightarrow 1\)&lt;/span&gt;時出勤率的變化(增加，或者減少)；&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(b_3\)&lt;/span&gt; 是&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;保持不變時，&lt;span class=&#34;math inline&#34;&gt;\(Score\)&lt;/span&gt;增加一個單位時出勤率的變化(增加，或者減少)。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Model can also be written as (Model5-2):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{array}{l}
Y[n]       \sim \text{Normal}(b_1 + b_2A[n] + b_3Score[n], \sigma) &amp;amp;  n = 1,2,\dots,N \\
\end{array}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;如果認爲&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(Score\)&lt;/span&gt;所能預測的出勤率有一個基礎的均值 &lt;span class=&#34;math inline&#34;&gt;\(\mu[n]\)&lt;/span&gt;，剩下的每名學生的出勤率服從這個均值和標準差爲 &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; 的正態分佈，那麼模型又可以繼續改寫成爲下面的 Model 5-3:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{array}{l}
\mu[n]        = b_1 + b_2A[n] + b_3Sore[n] &amp;amp;  n = 1,2,\dots,N \\
Y[n] \sim \text{Normal}(\mu[n], \sigma) &amp;amp; n = 1,2,\dots,N \\ 
\end{array}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;下面的 Stan 模型是按照 Model 5-3 寫的，它的模型參數有四個，&lt;span class=&#34;math inline&#34;&gt;\(b_1, b_2, b_3, \sigma\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(\mu[n]\)&lt;/span&gt;通過 &lt;code&gt;transformed parameter&lt;/code&gt; 計算獲得:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data {
  int N; 
  int&amp;lt;lower=0, upper=1&amp;gt; A[N];
  real&amp;lt;lower=0, upper=1&amp;gt; Score[N];
  real&amp;lt;lower=0, upper=1&amp;gt; Y[N];
}

parameters {
  real b1; 
  real b2;
  real b3;
  real&amp;lt;lower=0&amp;gt; sigma;
}

transformed parameters {
  real mu[N];
  for (n in 1:N) {
    mu[n] = b1 + b2*A[n] + b3*Score[n];
  }
}

model {
  for (n in 1:N) {
    Y[n] ~ normal(mu[n], sigma);
  }
}

generated quantities {
  real y_pred[N];
  for (n in 1:N) {
    y_pred[n] = normal_rng(mu[n], sigma);
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;下面的 R 代碼用來實現對上面 Stan 模型的擬合:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rstan)
d &amp;lt;- read.csv(file=&amp;#39;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap05/input/data-attendance-1.txt&amp;#39;, header = T)
data &amp;lt;- list(N=nrow(d), A=d$A, Score=d$Score/200, Y=d$Y)
fit &amp;lt;- stan(file=&amp;#39;stanfiles/model5-3.stan&amp;#39;, data=data, seed=1234)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &amp;#39;model5-3&amp;#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 3.7e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.37 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.134662 seconds (Warm-up)
## Chain 1:                0.131457 seconds (Sampling)
## Chain 1:                0.266119 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &amp;#39;model5-3&amp;#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 7e-06 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.140699 seconds (Warm-up)
## Chain 2:                0.155449 seconds (Sampling)
## Chain 2:                0.296148 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &amp;#39;model5-3&amp;#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 7e-06 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.133523 seconds (Warm-up)
## Chain 3:                0.131937 seconds (Sampling)
## Chain 3:                0.26546 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &amp;#39;model5-3&amp;#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 7e-06 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.127764 seconds (Warm-up)
## Chain 4:                0.118133 seconds (Sampling)
## Chain 4:                0.245897 seconds (Total)
## Chain 4:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Stan model: model5-3.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##              mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff
## b1           0.12    0.00 0.03   0.06   0.10   0.12   0.15   0.19  1844
## b2          -0.14    0.00 0.01  -0.17  -0.15  -0.14  -0.13  -0.11  2973
## b3           0.32    0.00 0.05   0.22   0.29   0.32   0.36   0.43  1880
## sigma        0.05    0.00 0.01   0.04   0.05   0.05   0.05   0.06  2478
## mu[1]        0.24    0.00 0.02   0.20   0.22   0.24   0.25   0.27  1996
## mu[2]        0.22    0.00 0.01   0.19   0.21   0.22   0.22   0.24  2809
## mu[3]        0.33    0.00 0.01   0.31   0.32   0.33   0.33   0.34  3531
## mu[4]        0.12    0.00 0.02   0.09   0.11   0.12   0.13   0.15  2784
## mu[5]        0.24    0.00 0.01   0.21   0.23   0.24   0.25   0.27  2468
## mu[6]        0.34    0.00 0.01   0.32   0.33   0.34   0.35   0.36  3463
## mu[7]        0.30    0.00 0.01   0.29   0.30   0.30   0.31   0.32  3039
## mu[8]        0.22    0.00 0.01   0.19   0.21   0.22   0.23   0.24  2747
## mu[9]        0.36    0.00 0.01   0.34   0.35   0.36   0.37   0.38  3046
## mu[10]       0.36    0.00 0.01   0.34   0.35   0.36   0.37   0.38  3080
## mu[11]       0.21    0.00 0.01   0.18   0.20   0.21   0.22   0.23  2942
## mu[12]       0.35    0.00 0.01   0.33   0.34   0.35   0.35   0.37  3351
## mu[13]       0.17    0.00 0.01   0.15   0.16   0.17   0.18   0.19  3511
## mu[14]       0.30    0.00 0.01   0.29   0.30   0.30   0.31   0.32  3039
## mu[15]       0.30    0.00 0.01   0.28   0.29   0.30   0.31   0.32  2943
## mu[16]       0.14    0.00 0.01   0.11   0.13   0.14   0.15   0.17  3138
## mu[17]       0.31    0.00 0.01   0.29   0.30   0.31   0.31   0.33  3183
## mu[18]       0.26    0.00 0.01   0.23   0.25   0.26   0.27   0.28  2136
## mu[19]       0.42    0.00 0.02   0.39   0.41   0.42   0.44   0.46  2308
## mu[20]       0.23    0.00 0.01   0.20   0.22   0.23   0.24   0.26  2511
## mu[21]       0.12    0.00 0.02   0.09   0.11   0.12   0.13   0.15  2784
## mu[22]       0.16    0.00 0.01   0.13   0.15   0.15   0.16   0.18  3387
## mu[23]       0.15    0.00 0.01   0.13   0.14   0.15   0.16   0.18  3337
## mu[24]       0.21    0.00 0.01   0.19   0.20   0.21   0.22   0.24  2874
## mu[25]       0.17    0.00 0.01   0.15   0.16   0.17   0.18   0.19  3508
## mu[26]       0.19    0.00 0.01   0.16   0.18   0.19   0.20   0.21  3373
## mu[27]       0.32    0.00 0.01   0.30   0.31   0.32   0.32   0.33  3393
## mu[28]       0.32    0.00 0.01   0.30   0.31   0.32   0.32   0.33  3393
## mu[29]       0.38    0.00 0.01   0.36   0.38   0.38   0.39   0.41  2639
## mu[30]       0.31    0.00 0.01   0.29   0.30   0.31   0.31   0.33  3135
## mu[31]       0.25    0.00 0.02   0.22   0.24   0.25   0.26   0.28  2340
## mu[32]       0.10    0.00 0.02   0.06   0.09   0.10   0.11   0.13  2546
## mu[33]       0.20    0.00 0.01   0.18   0.20   0.20   0.21   0.23  3046
## mu[34]       0.18    0.00 0.01   0.16   0.17   0.18   0.19   0.20  3463
## mu[35]       0.33    0.00 0.01   0.31   0.32   0.33   0.33   0.35  3538
## mu[36]       0.34    0.00 0.01   0.32   0.33   0.34   0.34   0.35  3504
## mu[37]       0.15    0.00 0.01   0.13   0.14   0.15   0.16   0.17  3310
## mu[38]       0.30    0.00 0.01   0.28   0.30   0.30   0.31   0.32  2991
## mu[39]       0.27    0.00 0.01   0.24   0.26   0.27   0.28   0.29  2259
## mu[40]       0.27    0.00 0.01   0.24   0.26   0.27   0.27   0.29  2239
## mu[41]       0.33    0.00 0.01   0.31   0.33   0.33   0.34   0.35  3530
## mu[42]       0.34    0.00 0.01   0.32   0.33   0.34   0.35   0.36  3463
## mu[43]       0.32    0.00 0.01   0.30   0.31   0.32   0.33   0.34  3482
## mu[44]       0.36    0.00 0.01   0.34   0.36   0.36   0.37   0.39  2981
## mu[45]       0.42    0.00 0.02   0.38   0.41   0.42   0.43   0.46  2336
## mu[46]       0.29    0.00 0.01   0.27   0.29   0.29   0.30   0.31  2760
## mu[47]       0.21    0.00 0.02   0.17   0.19   0.21   0.22   0.24  1907
## mu[48]       0.37    0.00 0.01   0.34   0.36   0.37   0.38   0.39  2919
## mu[49]       0.28    0.00 0.01   0.26   0.28   0.28   0.29   0.30  2527
## mu[50]       0.14    0.00 0.01   0.11   0.13   0.14   0.15   0.17  3138
## y_pred[1]    0.24    0.00 0.06   0.13   0.20   0.24   0.27   0.34  3503
## y_pred[2]    0.21    0.00 0.05   0.11   0.18   0.21   0.25   0.32  3779
## y_pred[3]    0.33    0.00 0.05   0.22   0.29   0.33   0.36   0.43  3944
## y_pred[4]    0.12    0.00 0.05   0.01   0.08   0.12   0.16   0.23  3762
## y_pred[5]    0.24    0.00 0.05   0.13   0.20   0.24   0.27   0.34  4038
## y_pred[6]    0.34    0.00 0.05   0.24   0.30   0.34   0.37   0.44  3457
## y_pred[7]    0.30    0.00 0.05   0.20   0.27   0.30   0.34   0.41  4030
## y_pred[8]    0.22    0.00 0.05   0.11   0.18   0.22   0.26   0.33  3217
## y_pred[9]    0.36    0.00 0.05   0.26   0.33   0.36   0.40   0.47  3869
## y_pred[10]   0.36    0.00 0.05   0.26   0.32   0.36   0.39   0.46  3884
## y_pred[11]   0.21    0.00 0.05   0.11   0.17   0.21   0.25   0.31  3871
## y_pred[12]   0.34    0.00 0.05   0.24   0.31   0.34   0.38   0.45  3866
## y_pred[13]   0.17    0.00 0.05   0.07   0.13   0.17   0.21   0.27  4064
## y_pred[14]   0.30    0.00 0.05   0.20   0.27   0.30   0.34   0.41  3869
## y_pred[15]   0.30    0.00 0.05   0.20   0.26   0.30   0.33   0.40  3929
## y_pred[16]   0.14    0.00 0.05   0.03   0.11   0.14   0.18   0.25  3900
## y_pred[17]   0.31    0.00 0.05   0.21   0.27   0.31   0.35   0.41  4048
## y_pred[18]   0.26    0.00 0.05   0.15   0.22   0.26   0.29   0.36  3705
## y_pred[19]   0.42    0.00 0.05   0.32   0.39   0.42   0.46   0.53  3474
## y_pred[20]   0.23    0.00 0.05   0.13   0.20   0.23   0.27   0.34  3792
## y_pred[21]   0.12    0.00 0.05   0.01   0.08   0.12   0.15   0.23  3581
## y_pred[22]   0.16    0.00 0.05   0.05   0.12   0.16   0.19   0.26  4075
## y_pred[23]   0.15    0.00 0.05   0.04   0.12   0.15   0.19   0.26  3975
## y_pred[24]   0.21    0.00 0.05   0.11   0.18   0.21   0.25   0.32  3489
## y_pred[25]   0.17    0.00 0.05   0.06   0.14   0.17   0.20   0.27  3938
## y_pred[26]   0.19    0.00 0.05   0.08   0.15   0.19   0.22   0.29  3955
## y_pred[27]   0.32    0.00 0.05   0.21   0.28   0.32   0.35   0.42  4075
## y_pred[28]   0.32    0.00 0.05   0.21   0.28   0.32   0.35   0.42  4110
## y_pred[29]   0.38    0.00 0.05   0.28   0.35   0.38   0.42   0.49  3631
## y_pred[30]   0.31    0.00 0.05   0.20   0.27   0.31   0.34   0.41  3865
## y_pred[31]   0.25    0.00 0.05   0.14   0.21   0.25   0.28   0.35  3818
## y_pred[32]   0.10    0.00 0.05  -0.01   0.06   0.10   0.14   0.20  3857
## y_pred[33]   0.20    0.00 0.05   0.10   0.17   0.20   0.24   0.31  3304
## y_pred[34]   0.18    0.00 0.05   0.08   0.14   0.18   0.22   0.28  3946
## y_pred[35]   0.33    0.00 0.05   0.22   0.29   0.33   0.36   0.43  4182
## y_pred[36]   0.34    0.00 0.05   0.23   0.30   0.34   0.37   0.44  4022
## y_pred[37]   0.15    0.00 0.05   0.05   0.12   0.15   0.19   0.25  3719
## y_pred[38]   0.30    0.00 0.05   0.20   0.27   0.30   0.34   0.41  3655
## y_pred[39]   0.27    0.00 0.05   0.16   0.23   0.27   0.30   0.37  3522
## y_pred[40]   0.27    0.00 0.05   0.16   0.23   0.27   0.30   0.37  3742
## y_pred[41]   0.33    0.00 0.05   0.23   0.30   0.33   0.37   0.44  3979
## y_pred[42]   0.34    0.00 0.05   0.24   0.30   0.34   0.37   0.44  3914
## y_pred[43]   0.32    0.00 0.05   0.22   0.29   0.32   0.36   0.43  4005
## y_pred[44]   0.36    0.00 0.05   0.26   0.33   0.36   0.40   0.47  3818
## y_pred[45]   0.42    0.00 0.05   0.31   0.38   0.42   0.45   0.53  3443
## y_pred[46]   0.29    0.00 0.05   0.19   0.26   0.29   0.33   0.40  3839
## y_pred[47]   0.21    0.00 0.06   0.09   0.17   0.21   0.24   0.31  3611
## y_pred[48]   0.37    0.00 0.05   0.26   0.33   0.37   0.40   0.47  3784
## y_pred[49]   0.28    0.00 0.05   0.18   0.25   0.28   0.32   0.39  3745
## y_pred[50]   0.14    0.00 0.05   0.03   0.10   0.14   0.17   0.24  3701
## lp__       120.94    0.04 1.39 117.48 120.23 121.24 121.96 122.70  1407
##            Rhat
## b1            1
## b2            1
## b3            1
## sigma         1
## mu[1]         1
## mu[2]         1
## mu[3]         1
## mu[4]         1
## mu[5]         1
## mu[6]         1
## mu[7]         1
## mu[8]         1
## mu[9]         1
## mu[10]        1
## mu[11]        1
## mu[12]        1
## mu[13]        1
## mu[14]        1
## mu[15]        1
## mu[16]        1
## mu[17]        1
## mu[18]        1
## mu[19]        1
## mu[20]        1
## mu[21]        1
## mu[22]        1
## mu[23]        1
## mu[24]        1
## mu[25]        1
## mu[26]        1
## mu[27]        1
## mu[28]        1
## mu[29]        1
## mu[30]        1
## mu[31]        1
## mu[32]        1
## mu[33]        1
## mu[34]        1
## mu[35]        1
## mu[36]        1
## mu[37]        1
## mu[38]        1
## mu[39]        1
## mu[40]        1
## mu[41]        1
## mu[42]        1
## mu[43]        1
## mu[44]        1
## mu[45]        1
## mu[46]        1
## mu[47]        1
## mu[48]        1
## mu[49]        1
## mu[50]        1
## y_pred[1]     1
## y_pred[2]     1
## y_pred[3]     1
## y_pred[4]     1
## y_pred[5]     1
## y_pred[6]     1
## y_pred[7]     1
## y_pred[8]     1
## y_pred[9]     1
## y_pred[10]    1
## y_pred[11]    1
## y_pred[12]    1
## y_pred[13]    1
## y_pred[14]    1
## y_pred[15]    1
## y_pred[16]    1
## y_pred[17]    1
## y_pred[18]    1
## y_pred[19]    1
## y_pred[20]    1
## y_pred[21]    1
## y_pred[22]    1
## y_pred[23]    1
## y_pred[24]    1
## y_pred[25]    1
## y_pred[26]    1
## y_pred[27]    1
## y_pred[28]    1
## y_pred[29]    1
## y_pred[30]    1
## y_pred[31]    1
## y_pred[32]    1
## y_pred[33]    1
## y_pred[34]    1
## y_pred[35]    1
## y_pred[36]    1
## y_pred[37]    1
## y_pred[38]    1
## y_pred[39]    1
## y_pred[40]    1
## y_pred[41]    1
## y_pred[42]    1
## y_pred[43]    1
## y_pred[44]    1
## y_pred[45]    1
## y_pred[46]    1
## y_pred[47]    1
## y_pred[48]    1
## y_pred[49]    1
## y_pred[50]    1
## lp__          1
## 
## Samples were drawn using NUTS(diag_e) at Thu Jan 24 00:02:26 2019.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;上述代碼中值得注意的是我們對 &lt;span class=&#34;math inline&#34;&gt;\(Score\)&lt;/span&gt; 進行了全部除以 &lt;span class=&#34;math inline&#34;&gt;\(200\)&lt;/span&gt; 的數據縮放調整 (scaling)。這樣有助於我們的模型在進行 MCMC 計算時加速其達到收斂時所需要的時間。&lt;/p&gt;
&lt;p&gt;把計算獲得的事後模型參數平均值代入模型 Model 5-3:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{array}{l}
\mu[n]        = 0.12 - 0.14A[n] + 0.32Sore[n] &amp;amp;  n = 1,2,\dots,N \\
Y[n] \sim \text{Normal}(\mu[n], 0.05) &amp;amp; n = 1,2,\dots,N \\ 
\end{array}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;從輸出的結果報告來看，所有的 &lt;code&gt;Rhat&lt;/code&gt; 都小於1.1，可以認爲採樣已經達到收斂效果，再來確認一下軌跡圖：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(bayesplot)

color_scheme_set(&amp;quot;mix-brightblue-gray&amp;quot;)

posterior2 &amp;lt;- rstan::extract(fit, inc_warmup = TRUE, permuted = FALSE)

p &amp;lt;- mcmc_trace(posterior2, n_warmup = 0, pars = c(&amp;quot;b1&amp;quot;, &amp;quot;b2&amp;quot;, &amp;quot;b3&amp;quot;, &amp;quot;sigma&amp;quot;, &amp;quot;lp__&amp;quot;),
                facet_args = list(nrow = 2, labeller = label_parsed))
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step53&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://winterwang.github.io/post/2019-01-22-rstan-wonderful-r-multi-lm_files/figure-html/step53-1.png&#34; alt=&#34;用 bayesplot包數繪製的模型5-3的MCMC鏈式軌跡圖 (trace plot)。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: 用 bayesplot包數繪製的模型5-3的MCMC鏈式軌跡圖 (trace plot)。
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;收斂效果很不錯，下面來解釋回歸係數的事後均值的涵義：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;b3&lt;/code&gt;的事後均值是&lt;span class=&#34;math inline&#34;&gt;\(0.32\)&lt;/span&gt;，所以，&lt;span class=&#34;math inline&#34;&gt;\(Score=150\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(Score=50\)&lt;/span&gt;的兩名學生，當他們同時都是喜歡或者同時都不喜歡打工時，&lt;span class=&#34;math inline&#34;&gt;\(Score = 150\)&lt;/span&gt;的學生的出勤率平均比 &lt;span class=&#34;math inline&#34;&gt;\(Score = 50\)&lt;/span&gt; 的學生的出勤率高 &lt;span class=&#34;math inline&#34;&gt;\(0.32 \times (150-50)/200 = 0.16\)&lt;/span&gt;。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;b2&lt;/code&gt;的事後均值是&lt;span class=&#34;math inline&#34;&gt;\(-0.14\)&lt;/span&gt;，所以，同樣地，&lt;span class=&#34;math inline&#34;&gt;\(Score\)&lt;/span&gt;相同的兩名學生，喜歡打工的學生比不喜歡打工的學生出勤率平均要低 &lt;span class=&#34;math inline&#34;&gt;\(0.14\)&lt;/span&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;step-3.-&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 3. 看圖確認模型擬合狀況&lt;/h2&gt;
&lt;p&gt;下圖繪製了上面貝葉斯多重線性回歸模型計算獲得的事後貝葉斯預測區間，和觀測值&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;出勤率之間的直觀關係：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;source(&amp;quot;commonRstan.R&amp;quot;)

ms &amp;lt;- rstan::extract(fit)

Score_new &amp;lt;- 50:200
N_X &amp;lt;- length(Score_new)
N_mcmc &amp;lt;- length(ms$lp__)

set.seed(1234)
y_base_mcmc &amp;lt;- as.data.frame(matrix(nrow=N_mcmc, ncol=N_X))
y_base_a0_mcmc &amp;lt;- as.data.frame(matrix(nrow = N_mcmc, ncol = N_X))
y_mcmc &amp;lt;- as.data.frame(matrix(nrow=N_mcmc, ncol=N_X))
y_a0_mcmc &amp;lt;- as.data.frame(matrix(nrow=N_mcmc, ncol=N_X))

for (i in 1:N_X) {
  y_base_mcmc[,i] &amp;lt;- ms$b1 + ms$b2 + ms$b3 * Score_new[i]/200
  y_base_a0_mcmc[] &amp;lt;- ms$b1 + ms$b2*0 + ms$b3 * Score_new[i]/200
  y_mcmc[,i] &amp;lt;- rnorm(n=N_mcmc, mean=y_base_mcmc[,i], sd=ms$sigma)
  y_a0_mcmc[,i] &amp;lt;- rnorm(n=N_mcmc, mean=y_base_a0_mcmc[,i], sd=ms$sigma)
}

customize.ggplot.axis &amp;lt;- function(p) {
  p &amp;lt;- p + labs(x=&amp;#39;Score&amp;#39;, y=&amp;#39;Y&amp;#39;)
  p &amp;lt;- p + scale_y_continuous(breaks=seq(from=-0.2, to=0.8, by=0.2))
  p &amp;lt;- p + coord_cartesian(xlim=c(50, 200), ylim=c(-0.2, 0.6))
  return(p)
}

d_est &amp;lt;- data.frame.quantile.mcmc(x=Score_new, y_mcmc=y_mcmc)
d_esta0 &amp;lt;- data.frame.quantile.mcmc(x=Score_new, y_mcmc=y_a0_mcmc)
# p &amp;lt;- ggplot.5quantile(data=d_est)
# p2 &amp;lt;- ggplot.5quantile(data = d_esta0)
# p &amp;lt;- p + geom_point(data=d[d$A==1, ], aes(x=Score, y=Y), shape=24, size=5)
# p2 &amp;lt;- p2 + geom_point(data=d[d$A==0, ], aes(x=Score, y=Y), shape=1, size=5)
# p &amp;lt;- customize.ggplot.axis(p)
# p2 &amp;lt;- customize.ggplot.axis(p2)

visuals = rbind(d_est,d_esta0)
visuals$A=c(rep(1,151),rep(0,151)) # 151 points of each flavour

qn &amp;lt;- colnames(visuals)[-1]
p &amp;lt;- ggplot(data=visuals, aes(x=X, y=p50, group = A))
p &amp;lt;- p + my_theme()
p &amp;lt;- p + geom_ribbon(aes_string(ymin=qn[1], ymax=qn[5]), fill=&amp;#39;black&amp;#39;, alpha=1/6)
p &amp;lt;- p + geom_ribbon(aes_string(ymin=qn[2], ymax=qn[4]), fill=&amp;#39;black&amp;#39;, alpha=2/6)
p &amp;lt;- p + geom_line(size=1)
p &amp;lt;- p + geom_point(data=d[d$A==1, ], aes(x=Score, y=Y), shape=24, size=5)
p &amp;lt;- p + geom_point(data=d[d$A==0, ], aes(x=Score, y=Y), shape=20, size=5)
p &amp;lt;- customize.ggplot.axis(p)
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:fig52&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://winterwang.github.io/post/2019-01-22-rstan-wonderful-r-multi-lm_files/figure-html/fig52-1.png&#34; alt=&#34;黑色原點(不喜歡打工)，和無色三角形(喜歡打工)的學生的出勤率，和模型計算獲得的貝葉斯事後預測區間。黑色線是中位數，灰色帶是50%預測區間和95%預測區間。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: 黑色原點(不喜歡打工)，和無色三角形(喜歡打工)的學生的出勤率，和模型計算獲得的貝葉斯事後預測區間。黑色線是中位數，灰色帶是50%預測區間和95%預測區間。
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;上述觀察預測值區間和實際觀測之間的關係的視覺化圖形，在多重線性回歸模型只有兩個預測變量的事後還較爲容易獲得，當模型中有三個或以上的預測變量時，可視化變得困難重重。&lt;/p&gt;
&lt;p&gt;此時我們推薦繪製“實際觀測值和預測值”，以及模型給出的每個預測值的隨機誤差&lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt;分佈範圍，相結合的圖形來判斷模型擬合程度。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d_qua &amp;lt;- t(apply(ms$y_pred, 2, quantile, prob=c(0.1, 0.5, 0.9)))
colnames(d_qua) &amp;lt;- c(&amp;#39;p10&amp;#39;, &amp;#39;p50&amp;#39;, &amp;#39;p90&amp;#39;)
d_qua &amp;lt;- data.frame(d, d_qua)
d_qua$A &amp;lt;- as.factor(d_qua$A)

p &amp;lt;- ggplot(data=d_qua, aes(x=Y, y=p50, ymin=p10, ymax=p90, shape=A, fill=A))
p &amp;lt;- p + theme_bw(base_size=18) + theme(legend.key.height=grid::unit(2.5,&amp;#39;line&amp;#39;))
p &amp;lt;- p + coord_fixed(ratio=1, xlim=c(0, 0.5), ylim=c(0, 0.5))
p &amp;lt;- p + geom_pointrange(size=0.8, color=&amp;#39;grey5&amp;#39;)
p &amp;lt;- p + geom_abline(aes(slope=1, intercept=0), color=&amp;#39;black&amp;#39;, alpha=3/5, linetype=&amp;#39;31&amp;#39;)
p &amp;lt;- p + scale_shape_manual(values=c(21, 24))
p &amp;lt;- p + scale_fill_manual(values=c(&amp;#39;white&amp;#39;, &amp;#39;grey70&amp;#39;))
p &amp;lt;- p + labs(x=&amp;#39;Observed&amp;#39;, y=&amp;#39;Predicted&amp;#39;)
p &amp;lt;- p + scale_x_continuous(breaks=seq(from=0, to=0.5, by=0.1))
p &amp;lt;- p + scale_y_continuous(breaks=seq(from=0, to=0.5, by=0.1))
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:fig53&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://winterwang.github.io/post/2019-01-22-rstan-wonderful-r-multi-lm_files/figure-html/fig53-1.png&#34; alt=&#34;觀測值(x)，和預測值(y)的散點圖，以及預測值的80%預測區間。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4: 觀測值(x)，和預測值(y)的散點圖，以及預測值的80%預測區間。
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;從上圖中可以看出，大多數的觀測點和預測點以及預測的80%區間基本都在 &lt;span class=&#34;math inline&#34;&gt;\(y = x\)&lt;/span&gt; 這條對角線上。大致可以認爲本次貝葉斯多重線性回歸擬合效果尚且能夠接受。&lt;/p&gt;
&lt;p&gt;隨機誤差 &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon[n]\)&lt;/span&gt; 被認爲服從 &lt;span class=&#34;math inline&#34;&gt;\(\text{Normal}(0, \sigma)\)&lt;/span&gt; 的正態分佈。從模型中可以計算獲得每個學生出勤率的預測值和實際觀測值之間的差，這就是隨機誤差。貝葉斯框架之下，我們實際獲得的會是每名學生隨機誤差的分佈：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;N_mcmc &amp;lt;- length(ms$lp__)

d_noise &amp;lt;- data.frame(t(-t(ms$mu) + d$Y))
colnames(d_noise) &amp;lt;- paste0(&amp;#39;noise&amp;#39;, 1:nrow(d))
d_est &amp;lt;- data.frame(mcmc=1:N_mcmc, d_noise)
d_melt &amp;lt;- reshape2::melt(d_est, id=c(&amp;#39;mcmc&amp;#39;), variable.name=&amp;#39;X&amp;#39;)

d_mode &amp;lt;- data.frame(t(apply(d_noise, 2, function(x) {
  dens &amp;lt;- density(x)
  mode_i &amp;lt;- which.max(dens$y)
  mode_x &amp;lt;- dens$x[mode_i]
  mode_y &amp;lt;- dens$y[mode_i]
  c(mode_x, mode_y)
})))
colnames(d_mode) &amp;lt;- c(&amp;#39;X&amp;#39;, &amp;#39;Y&amp;#39;)

p &amp;lt;- ggplot()
p &amp;lt;- p + theme_bw(base_size=18)
p &amp;lt;- p + geom_line(data=d_melt, aes(x=value, group=X), stat=&amp;#39;density&amp;#39;, color=&amp;#39;black&amp;#39;, alpha=0.4)
p &amp;lt;- p + geom_segment(data=d_mode, aes(x=X, xend=X, y=Y, yend=0), color=&amp;#39;black&amp;#39;, linetype=&amp;#39;dashed&amp;#39;, alpha=0.4)
p &amp;lt;- p + geom_rug(data=d_mode, aes(x=X), sides=&amp;#39;b&amp;#39;)
p &amp;lt;- p + labs(x=&amp;#39;value&amp;#39;, y=&amp;#39;density&amp;#39;)
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:fig54left&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://winterwang.github.io/post/2019-01-22-rstan-wonderful-r-multi-lm_files/figure-html/fig54left-1.png&#34; alt=&#34;每名學生的出勤率隨機誤差的分佈&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 5: 每名學生的出勤率隨機誤差的分佈
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;實際上我們只需要選取每名學生模型計算獲得的事後隨機誤差的代表值，比如可以是平均值，中央值，或者是MAP值（事後確率最大推定値，maximum a posteriori estimate），來觀察就可以了：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s_dens &amp;lt;- density(ms$s)
s_MAP &amp;lt;- s_dens$x[which.max(s_dens$y)]
bw &amp;lt;- 0.01
p &amp;lt;- ggplot(data=d_mode, aes(x=X))
p &amp;lt;- p + theme_bw(base_size=18)
p &amp;lt;- p + geom_histogram(binwidth=bw, color=&amp;#39;black&amp;#39;, fill=&amp;#39;white&amp;#39;)
p &amp;lt;- p + geom_density(eval(bquote(aes(y=..count..*.(bw)))), alpha=0.5, color=&amp;#39;black&amp;#39;, fill=&amp;#39;gray20&amp;#39;)
p &amp;lt;- p + stat_function(fun=function(x) nrow(d)*bw*dnorm(x, mean=0, sd=s_MAP), linetype=&amp;#39;dashed&amp;#39;)
p &amp;lt;- p + labs(x=&amp;#39;value&amp;#39;, y=&amp;#39;count&amp;#39;)
p &amp;lt;- p + xlim(range(density(d_mode$X)$x))
p&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 2 rows containing missing values (geom_bar).&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:fig54right&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://winterwang.github.io/post/2019-01-22-rstan-wonderful-r-multi-lm_files/figure-html/fig54right-1.png&#34; alt=&#34;每名學生事後出勤率隨機誤差的MAP值的柱狀圖，和相應的概率密度函數（灰色鐘罩），點狀虛線是均值爲0，標準差是模型計算的事後隨機誤差標準差的 MAP 值的正態分佈的形狀。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 6: 每名學生事後出勤率隨機誤差的MAP值的柱狀圖，和相應的概率密度函數（灰色鐘罩），點狀虛線是均值爲0，標準差是模型計算的事後隨機誤差標準差的 MAP 值的正態分佈的形狀。
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;step-4.-mcmc-&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 4. MCMC 樣本的散點圖矩陣&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(GGally)
library(hexbin)


d &amp;lt;- data.frame(b1=ms$b1, b2=ms$b2, b3=ms$b3, sigma=ms$sigma, mu1=ms$mu[,1], mu50=ms$mu[,50], lp__=ms$lp__)
N_col &amp;lt;- ncol(d)
ggp &amp;lt;- ggpairs(d, upper=&amp;#39;blank&amp;#39;, diag=&amp;#39;blank&amp;#39;, lower=&amp;#39;blank&amp;#39;)

label_list &amp;lt;- list(b1=&amp;#39;b1&amp;#39;, b2=&amp;#39;b2&amp;#39;, b3=&amp;#39;b3&amp;#39;, sigma=&amp;#39;sigma&amp;#39;, mu1=&amp;#39;mu[1]&amp;#39;, mu50=&amp;#39;mu[50]&amp;#39;, lp__=&amp;#39;lp__&amp;#39;)
for(i in 1:N_col) {
  x &amp;lt;- d[,i]
  bw &amp;lt;- (max(x)-min(x))/10
  p &amp;lt;- ggplot(data.frame(x), aes(x))
  p &amp;lt;- p + theme_bw(base_size=14)
  p &amp;lt;- p + theme(axis.text.x=element_text(angle=60, vjust=1, hjust=1))
  p &amp;lt;- p + geom_histogram(binwidth=bw, fill=&amp;#39;white&amp;#39;, color=&amp;#39;grey5&amp;#39;)
  p &amp;lt;- p + geom_line(eval(bquote(aes(y=..count..*.(bw)))), stat=&amp;#39;density&amp;#39;)
  p &amp;lt;- p + geom_label(data=data.frame(x=-Inf, y=Inf, label=label_list[[colnames(d)[i]]]), aes(x=x, y=y, label=label), hjust=0, vjust=1)
  ggp &amp;lt;- putPlot(ggp, p, i, i)
}

zcolat &amp;lt;- seq(-1, 1, length=81)
zcolre &amp;lt;- c(zcolat[1:40]+1, rev(zcolat[41:81]))

for(i in 1:(N_col-1)) {
  for(j in (i+1):N_col) {
    x &amp;lt;- as.numeric(d[,i])
    y &amp;lt;- as.numeric(d[,j])
    r &amp;lt;- cor(x, y, method=&amp;#39;spearman&amp;#39;, use=&amp;#39;pairwise.complete.obs&amp;#39;)
    zcol &amp;lt;- lattice::level.colors(r, at=zcolat, col.regions=grey(zcolre))
    textcol &amp;lt;- ifelse(abs(r) &amp;lt; 0.4, &amp;#39;grey20&amp;#39;, &amp;#39;white&amp;#39;)
    ell &amp;lt;- ellipse::ellipse(r, level=0.95, type=&amp;#39;l&amp;#39;, npoints=50, scale=c(.2, .2), centre=c(.5, .5))
    p &amp;lt;- ggplot(data.frame(ell), aes(x=x, y=y))
    p &amp;lt;- p + theme_bw() + theme(
      plot.background=element_blank(),
      panel.grid.major=element_blank(), panel.grid.minor=element_blank(),
      panel.border=element_blank(), axis.ticks=element_blank()
    )
    p &amp;lt;- p + geom_polygon(fill=zcol, color=zcol)
    p &amp;lt;- p + geom_text(data=NULL, x=.5, y=.5, label=100*round(r, 2), size=6, col=textcol)
    ggp &amp;lt;- putPlot(ggp, p, i, j)
  }
}

for(j in 1:(N_col-1)) {
  for(i in (j+1):N_col) {
    x &amp;lt;- d[,j]
    y &amp;lt;- d[,i]
    p &amp;lt;- ggplot(data.frame(x, y), aes(x=x, y=y))
    p &amp;lt;- p + theme_bw(base_size=14)
    p &amp;lt;- p + theme(axis.text.x=element_text(angle=60, vjust=1, hjust=1))
    p &amp;lt;- p + geom_hex()
    p &amp;lt;- p + scale_fill_gradientn(colours=gray.colors(7, start=0.1, end=0.9))
    ggp &amp;lt;- putPlot(ggp, p, i, j)
  }
}
ggp&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:fig55&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://winterwang.github.io/post/2019-01-22-rstan-wonderful-r-multi-lm_files/figure-html/fig55-1.png&#34; alt=&#34;MCMC樣本的事後矩陣。對角線上是各個參數事後樣本的柱狀圖和相應的概率密度曲線。右上角是各個參數事後樣本之間的 Spearman 秩相關係數。左下角是兩兩的散點圖。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 7: MCMC樣本的事後矩陣。對角線上是各個參數事後樣本的柱狀圖和相應的概率密度曲線。右上角是各個參數事後樣本之間的 Spearman 秩相關係數。左下角是兩兩的散點圖。
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Simple linear regression using Rstan--Rstan Wonderful R-(2)</title>
      <link>https://winterwang.github.io/post/simple-linear-regression-using-rstan/</link>
      <pubDate>Tue, 15 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/simple-linear-regression-using-rstan/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#step-1-&#34;&gt;Step 1, 確認數據分佈&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-2-&#34;&gt;Step 2, 描述線性模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-3-stan&#34;&gt;Step 3, 寫下Stan模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-4-stan&#34;&gt;Step 4, 診斷Stan貝葉斯模型的收斂程度&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-5mcmc&#34;&gt;Step 5，修改MCMC條件設定&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-6-&#34;&gt;Step 6, 並行（平行）計算的設定&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-7-&#34;&gt;Step 7, 計算貝葉斯可信區間和貝葉斯預測區間&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a&gt;練習題&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap04/input/data-salary.txt&#34;&gt;數據 data-salary.txt&lt;/a&gt;是架空的。&lt;/p&gt;
&lt;p&gt;某公司社員的年齡 &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;（歲），和年收入 &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;（萬日元）的數據如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;X,Y
24,472
24,403
26,454
32,575
33,546
35,781
38,750
40,601
40,814
43,792
43,745
44,837
48,868
52,988
56,1092
56,1007
57,1233
58,1202
59,1123
59,1314
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;年收入 &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; 被認爲是由基本年收 &lt;span class=&#34;math inline&#34;&gt;\(y_{base}\)&lt;/span&gt; 和其他影響因素 &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt; 構成。由於該公司是典型的年功序列式的日本傳統企業，所以基本年收本身和社員年齡成正比例。 &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt; 則被認爲是由該員工當年的業績等隨機誤差造成的，但是所有員工的 &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt; 的均值被認爲是零。&lt;/p&gt;
&lt;p&gt;g分析目的：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;借用這個數據來分析並回答如下的問題：在該公司如果採用了一名50歲的員工，他/她的年收入的預期值會是多少。&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;step-1-&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 1, 確認數據分佈&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Salary &amp;lt;- read.table(&amp;quot;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap04/input/data-salary.txt&amp;quot;, 
                     sep = &amp;quot;,&amp;quot;, header = T)
library(ggplot2)

ggplot(Salary, aes(x = X, y = Y)) + 
  geom_point(shape = 1, size = 4)  + theme(plot.subtitle = element_text(vjust = 1), 
    plot.caption = element_text(vjust = 1), 
    axis.line = element_line(colour = &amp;quot;bisque4&amp;quot;, 
        size = 0.2, linetype = &amp;quot;solid&amp;quot;), 
    axis.ticks = element_line(size = 0.7), 
    axis.title = element_text(size = 16), 
    axis.text = element_text(size = 16, colour = &amp;quot;gray0&amp;quot;), 
    panel.background = element_rect(fill = &amp;quot;gray98&amp;quot;)) +
  scale_y_continuous(limits = c(200, 1400), breaks = c(200, 600, 1000, 1400))&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step1&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://winterwang.github.io/post/2019-01-15-simple-linear-regression-using-rstan_files/figure-html/step1-1.png&#34; alt=&#34;橫軸爲 $X$，縱軸爲 $Y$ 的散點圖&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: 橫軸爲 &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;，縱軸爲 &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; 的散點圖
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;從這個散點圖的特徵可以看出年收入確實似乎和年齡呈線性正相關。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-2-&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 2, 描述線性模型&lt;/h2&gt;
&lt;p&gt;這個簡單線性回歸模型的數學表達式可以描述如下：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{array}{l}
Y[n]        = y_{base}[n] + \varepsilon [n]&amp;amp;  n = 1,2,\dots,N \\
y_{base}[n] = a + bX[n]                    &amp;amp;  n = 1,2,\dots,N \\
\varepsilon[n] \sim \text{Normal}(0, \sigma) &amp;amp; n = 1,2,\dots,N \\ 
\end{array}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;同樣的模型你可以簡化描述成爲：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y[n] \sim \text{Normal}(a + bX[n], \sigma)\;\; n = 1,2,\dots,N
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;那麼如果一個統計師只有經過傳統概率論觀點的訓練，他/她會在R裏面這樣來分析這個數據：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res_lm &amp;lt;- lm(Y ~ X, data = Salary)
summary(res_lm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Y ~ X, data = Salary)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -155.471  -51.523   -6.663   52.822  141.349 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) -119.697     68.148  -1.756    0.096 .  
## X             21.904      1.518  14.428 2.47e-11 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 79.1 on 18 degrees of freedom
## Multiple R-squared:  0.9204, Adjusted R-squared:  0.916 
## F-statistic: 208.2 on 1 and 18 DF,  p-value: 2.466e-11&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 用這個線性回歸模型來對上面模型中的參數作出預測：

X_new &amp;lt;- data.frame(X=23:60)
conf_95 &amp;lt;- predict(res_lm, X_new, interval = &amp;quot;confidence&amp;quot;, level = 0.95)
pred_95 &amp;lt;- predict(res_lm, X_new, interval = &amp;quot;prediction&amp;quot;, level = 0.95)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;temp_var &amp;lt;- predict(res_lm, interval=&amp;quot;prediction&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in predict.lm(res_lm, interval = &amp;quot;prediction&amp;quot;): predictions on current data refer to _future_ responses&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_df &amp;lt;- cbind(Salary, temp_var)

ggplot(new_df, aes(x = X, y = Y)) + 
  geom_point(shape = 1, size = 4)  + theme(plot.subtitle = element_text(vjust = 1), 
    plot.caption = element_text(vjust = 1), 
    axis.line = element_line(colour = &amp;quot;bisque4&amp;quot;, 
        size = 0.2, linetype = &amp;quot;solid&amp;quot;), 
    axis.ticks = element_line(size = 0.7), 
    axis.title = element_text(size = 16), 
    axis.text = element_text(size = 16, colour = &amp;quot;gray0&amp;quot;), 
    panel.background = element_rect(fill = &amp;quot;gray98&amp;quot;)) + 
  geom_smooth(method = lm, se=TRUE, size = 0.3)+
  scale_y_continuous(limits = c(200, 1400), breaks = c(200, 600, 1000, 1400)) +
   geom_line(aes(y=lwr), color = &amp;quot;red&amp;quot;, linetype = &amp;quot;dashed&amp;quot;)+
    geom_line(aes(y=upr), color = &amp;quot;red&amp;quot;, linetype = &amp;quot;dashed&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step2&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://winterwang.github.io/post/2019-01-15-simple-linear-regression-using-rstan_files/figure-html/step2-1.png&#34; alt=&#34;用簡單線性回歸模型計算的基本年收的信賴區間(灰色陰影)和預測區間(紅色點線)。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: 用簡單線性回歸模型計算的基本年收的信賴區間(灰色陰影)和預測區間(紅色點線)。
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;step-3-stan&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 3, 寫下Stan模型&lt;/h2&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;data {
    int N; 
    real X[N]; 
    real Y[N];
}

parameters {
    real a;
    real b;
    real&amp;lt;lower=0&amp;gt; sigma;
}

model {
    for(n in 1:N) {
        Y[n] ~ normal(a + b*X[n], sigma);
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;參數部分 &lt;code&gt;real&amp;lt;lower=0&amp;gt; sigma&lt;/code&gt; 的代碼表示標準差不可採集負數作爲樣本。&lt;/p&gt;
&lt;p&gt;實際運行上面的Stan代碼：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rstan)
data &amp;lt;- list(N=nrow(Salary), X=Salary$X, Y = Salary$Y)
fit &amp;lt;- sampling(model4_5, data, seed = 1234) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &amp;#39;5b73686886069c0bad70513d4ea4141a&amp;#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 8e-06 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.072158 seconds (Warm-up)
## Chain 1:                0.04332 seconds (Sampling)
## Chain 1:                0.115478 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &amp;#39;5b73686886069c0bad70513d4ea4141a&amp;#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 3e-06 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.065082 seconds (Warm-up)
## Chain 2:                0.043787 seconds (Sampling)
## Chain 2:                0.108869 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &amp;#39;5b73686886069c0bad70513d4ea4141a&amp;#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 3e-06 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.058978 seconds (Warm-up)
## Chain 3:                0.039954 seconds (Sampling)
## Chain 3:                0.098932 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &amp;#39;5b73686886069c0bad70513d4ea4141a&amp;#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 3e-06 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.061095 seconds (Warm-up)
## Chain 4:                0.045731 seconds (Sampling)
## Chain 4:                0.106826 seconds (Total)
## Chain 4:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Stan model: 5b73686886069c0bad70513d4ea4141a.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##          mean se_mean    sd    2.5%     25%     50%    75%  97.5% n_eff
## a     -121.53    2.05 75.97 -270.45 -167.02 -120.34 -73.00  26.46  1379
## b       21.96    0.05  1.69   18.71   20.84   21.93  23.00  25.30  1350
## sigma   85.09    0.37 15.38   61.62   73.63   83.07  94.33 121.28  1697
## lp__   -93.63    0.04  1.31  -96.87  -94.24  -93.29 -92.66 -92.13  1045
##       Rhat
## a        1
## b        1
## sigma    1
## lp__     1
## 
## Samples were drawn using NUTS(diag_e) at Thu Jan 17 15:50:00 2019.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;輸出結果的前三行，是該次MCMC的設定條件，其中模型名稱是Rmarkdown文件中隨機產生的。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;第二行則說明的是該次MCMC進行了4條鏈的採樣，每條鏈2000次，其中前1000次被當作是 burn-in (或者叫 warmup)。可以看到一共獲得了4000個事後樣本。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;接下來的五行是參數的事後樣本的事後分析總結，一共有11列。
&lt;ul&gt;
&lt;li&gt;第1列是參數名稱，最後一個 &lt;code&gt;lp__&lt;/code&gt;是Stan特有的算法得到的產物，具體解釋爲對數事後概率 (log posterior)，當然它也需要得到收斂才行。&lt;/li&gt;
&lt;li&gt;第2列是獲得的4000個參數的事後樣本的事後平均值(posterior mean)。例如&lt;code&gt;b&lt;/code&gt;（回歸直線的斜率）的事後平均值是21.96，也就是說年齡每增加一歲，基本年收入平均增加21.96萬日元。你可以和之前的概率論算法相比較(&lt;code&gt;b = 21.904&lt;/code&gt;)。&lt;/li&gt;
&lt;li&gt;第3列&lt;code&gt;se_mean&lt;/code&gt;是事後平均值的標準誤(standard error of posterior mean)。說白了是MCMC事後樣本的方差除以第10列的有效樣本量&lt;code&gt;n_eff&lt;/code&gt;之後取根號獲得的值。&lt;/li&gt;
&lt;li&gt;第4列&lt;code&gt;sd&lt;/code&gt;是MCMC事後樣本的標準差(standard deviation of posterior MCMC sample)。&lt;/li&gt;
&lt;li&gt;第5-9列是MCMC事後樣本的四分位點。也就是貝葉斯統計算法獲得的事後可信區間。&lt;/li&gt;
&lt;li&gt;第10列&lt;code&gt;n_eff&lt;/code&gt;是Stan在基於事後樣本自相關程度來判斷的有效事後樣本量大小。爲了有效地計算和繪製事後分佈的統計量，這個有效樣本量需要至少有100個以上吧（作者觀點）。如果報告給出的事後有效樣本量過小的話也是模型收斂不佳的表現之一。&lt;/li&gt;
&lt;li&gt;第11列&lt;code&gt;Rhat&lt;/code&gt;&lt;span class=&#34;math inline&#34;&gt;\((\hat R)\)&lt;/span&gt;是主要用於判斷模型是否達到收斂的重要指標，每個參數都會被計算一個&lt;code&gt;Rhat&lt;/code&gt;值。當MCMC鏈條數在3以上，且同時所有的模型參數的 &lt;code&gt;Rhat &amp;lt; 1.1&lt;/code&gt;的話，可以認爲模型達到了良好的收斂。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;step-4-stan&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 4, 診斷Stan貝葉斯模型的收斂程度&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggmcmc)

ggmcmc(ggs(fit, inc_warmup = TRUE, stan_include_auxiliar = TRUE), plot = &amp;quot;traceplot&amp;quot;, dev_type_html = &amp;quot;png&amp;quot;, 
       file = &amp;quot;trace.html&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;上面的代碼，會自動生成四個模型參數的軌跡MCMC鏈式圖報告。&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step4&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://winterwang.github.io/img/traceplot-model4-5.png&#34; alt=&#34;用ggmcmc函數製作而成的MCMC鏈式圖報告。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: 用ggmcmc函數製作而成的MCMC鏈式圖報告。
&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(bayesplot)

color_scheme_set(&amp;quot;mix-brightblue-gray&amp;quot;)

posterior2 &amp;lt;- rstan::extract(fit, inc_warmup = TRUE, permuted = FALSE)

p &amp;lt;- mcmc_trace(posterior2, n_warmup = 0,
                facet_args = list(nrow = 2, labeller = label_parsed))
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step41&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://winterwang.github.io/post/2019-01-15-simple-linear-regression-using-rstan_files/figure-html/step41-1.png&#34; alt=&#34;用 bayesplot包數繪製的MCMC鏈式圖。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4: 用 bayesplot包數繪製的MCMC鏈式圖。
&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- mcmc_acf_bar(posterior2)
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step42&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://winterwang.github.io/post/2019-01-15-simple-linear-regression-using-rstan_files/figure-html/step42-1.png&#34; alt=&#34;用 bayesplot包數繪製的事後樣本自相關圖(autocorrelation)。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 5: 用 bayesplot包數繪製的事後樣本自相關圖(autocorrelation)。
&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- mcmc_dens_overlay(posterior2, color_chains = T)
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step43&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://winterwang.github.io/post/2019-01-15-simple-linear-regression-using-rstan_files/figure-html/step43-1.png&#34; alt=&#34;用 bayesplot包數繪製的事後樣本密度分佈圖。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 6: 用 bayesplot包數繪製的事後樣本密度分佈圖。
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;step-5mcmc&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 5，修改MCMC條件設定&lt;/h2&gt;
&lt;p&gt;進行貝葉斯模型擬合的過程中，常常需要不停地修改模型的條件，例如縮短warm-up等。下面的Rstan代碼可以實現簡便地頻繁修改MCMC條件設定：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# library(rstan) uncomment if run for the first time
data &amp;lt;- list(N=nrow(Salary), X=Salary$X, Y = Salary$Y)
fit2 &amp;lt;- sampling(
    model4_5, 
    data = data, 
    pars = c(&amp;quot;b&amp;quot;, &amp;quot;sigma&amp;quot;), 
    init = function(){
      list(a = runif(1, -10, 10), b = runif(1, 0, 10), sigma = 10)
    },
    seed = 123,
    chains = 3, iter = 1000, warmup = 200, thin = 2
) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &amp;#39;5b73686886069c0bad70513d4ea4141a&amp;#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 5e-06 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 1: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 1: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 1: Iteration: 201 / 1000 [ 20%]  (Sampling)
## Chain 1: Iteration: 300 / 1000 [ 30%]  (Sampling)
## Chain 1: Iteration: 400 / 1000 [ 40%]  (Sampling)
## Chain 1: Iteration: 500 / 1000 [ 50%]  (Sampling)
## Chain 1: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 1: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 1: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 1: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 1: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.032778 seconds (Warm-up)
## Chain 1:                0.027433 seconds (Sampling)
## Chain 1:                0.060211 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &amp;#39;5b73686886069c0bad70513d4ea4141a&amp;#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 4e-06 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 2: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 2: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 2: Iteration: 201 / 1000 [ 20%]  (Sampling)
## Chain 2: Iteration: 300 / 1000 [ 30%]  (Sampling)
## Chain 2: Iteration: 400 / 1000 [ 40%]  (Sampling)
## Chain 2: Iteration: 500 / 1000 [ 50%]  (Sampling)
## Chain 2: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 2: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 2: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 2: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 2: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.037233 seconds (Warm-up)
## Chain 2:                0.027168 seconds (Sampling)
## Chain 2:                0.064401 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &amp;#39;5b73686886069c0bad70513d4ea4141a&amp;#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 3e-06 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 3: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 3: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 3: Iteration: 201 / 1000 [ 20%]  (Sampling)
## Chain 3: Iteration: 300 / 1000 [ 30%]  (Sampling)
## Chain 3: Iteration: 400 / 1000 [ 40%]  (Sampling)
## Chain 3: Iteration: 500 / 1000 [ 50%]  (Sampling)
## Chain 3: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 3: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 3: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 3: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 3: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.043306 seconds (Warm-up)
## Chain 3:                0.029513 seconds (Sampling)
## Chain 3:                0.072819 seconds (Total)
## Chain 3:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Stan model: 5b73686886069c0bad70513d4ea4141a.
## 3 chains, each with iter=1000; warmup=200; thin=2; 
## post-warmup draws per chain=400, total post-warmup draws=1200.
## 
##         mean se_mean    sd   2.5%    25%    50%    75%  97.5% n_eff Rhat
## b      21.99    0.07  1.68  18.79  20.94  21.96  23.00  25.47   622 1.01
## sigma  85.70    0.55 16.23  61.47  74.32  83.42  93.80 126.71   874 1.01
## lp__  -93.72    0.06  1.40 -97.45 -94.40 -93.38 -92.64 -92.13   642 1.00
## 
## Samples were drawn using NUTS(diag_e) at Thu Jan 17 15:50:04 2019.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中&lt;code&gt;fit&lt;/code&gt;的最後一行是修改各種條件的示例：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;chains&lt;/code&gt;至少要三條；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;iter&lt;/code&gt;一開始可以設定在500~1000左右，確定模型可以收斂以後，再加大這個數值以獲得穩定的事後統計量，多多益善；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;warmup&lt;/code&gt;，也就MCMC採樣開始後多少樣本可以丟棄。這個數值需要參考trace plot；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;thin&lt;/code&gt;，通常只需要保持默認值 1。和WinBUGS, JAGS相比Stan算法採集的事後樣本自相關比較低。&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;step-6-&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 6, 並行（平行）計算的設定&lt;/h2&gt;
&lt;p&gt;如果你寫出來的貝葉斯模型需要很長時間的計算和收斂，可以充分利用你的計算機的多核計算，把每條MCMC鏈單獨進行計算加速這個過程：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;parallel::detectCores() #我的桌上型電腦有8個核可以用於平行計算&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;但是平行計算時如果計算中出錯則由於每條鏈都是相互獨立地進行，報錯就減少了。所以如果要使用多核同時計算的話，建議先減少採樣數，確認不會報錯以後再用多核平行計算增加採樣量。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;step-7-&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 7, 計算貝葉斯可信區間和貝葉斯預測區間&lt;/h2&gt;
&lt;p&gt;這一步就又回到一開始提出的研究問題上來，我們來計算基本年收的貝葉斯可信區間和貝葉斯預測區間。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ms &amp;lt;- rstan::extract(fit)

quantile(ms$b, probs = c(0.025, 0.975))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     2.5%    97.5% 
## 18.71095 25.29837&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d_mcmc &amp;lt;- data.frame(a = ms$a, b = ms$b, sigma = ms$sigma)

head(d_mcmc)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            a        b    sigma
## 1  -35.53766 20.67385 82.67516
## 2 -163.53803 22.48624 62.91622
## 3  -60.86149 20.44636 70.66507
## 4 -134.79928 22.74275 63.11801
## 5 -159.15523 22.39544 63.89505
## 6 -196.94649 24.17285 72.84033&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1 &amp;lt;- ggplot(d_mcmc, aes(x = a, y = b)) + 
 geom_point(shape = 1, size = 4)

ggExtra::ggMarginal(
  p = p1,
  type = &amp;#39;density&amp;#39;,
  margins = &amp;#39;both&amp;#39;,
  size = 4,
  colour = &amp;#39;black&amp;#39;,
  fill = &amp;#39;#2D077A&amp;#39;
)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step71&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://winterwang.github.io/post/2019-01-15-simple-linear-regression-using-rstan_files/figure-html/step71-1.png&#34; alt=&#34;MCMC樣本的兩個模型參數的事後散點圖，及它們之間的邊緣分佈密度圖。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 7: MCMC樣本的兩個模型參數的事後散點圖，及它們之間的邊緣分佈密度圖。
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;從圖&lt;a href=&#34;#fig:step71&#34;&gt;7&lt;/a&gt;中可觀察到該貝葉斯線性模型獲得的事後模型參數樣本中，截距&lt;code&gt;a&lt;/code&gt;，和斜率&lt;code&gt;b&lt;/code&gt;之間呈極強的負相關關係。也就是說，截距是工資的起點（年齡爲0歲時），這個起點的理論值越低，斜率越大（歲年齡增加工資上升的速度越大）。&lt;/p&gt;
&lt;p&gt;根據上面分析的結果，下面的R代碼可以計算一名50歲的人被這家公司採用的時候，她/他的預期基本年收入的分佈（中獲得的MCMC樣本），和她/他的預期總年收的預測分佈（中獲得的MCMC樣本）。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;N_mcmc &amp;lt;- length(ms$lp__)
y50_base &amp;lt;- ms$a + ms$b*50
y50 &amp;lt;- rnorm(n = N_mcmc, mean = y50_base, sd = ms$sigma)
d_mcmc &amp;lt;- data.frame(a = ms$a, b = ms$b, sigma = ms$sigma, y50_base, y50)
head(d_mcmc)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            a        b    sigma  y50_base       y50
## 1  -35.53766 20.67385 82.67516  998.1549  985.4279
## 2 -163.53803 22.48624 62.91622  960.7742 1033.5019
## 3  -60.86149 20.44636 70.66507  961.4563  981.6609
## 4 -134.79928 22.74275 63.11801 1002.3380 1003.7707
## 5 -159.15523 22.39544 63.89505  960.6167  999.0094
## 6 -196.94649 24.17285 72.84033 1011.6960 1110.6892&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# the following codes are also available from the author&amp;#39;s page:
# https://github.com/MatsuuraKentaro/RStanBook/blob/master/chap04/fig4-8.R
# library(ggplot2)
source(&amp;#39;commonRstan.R&amp;#39;)

# load(&amp;#39;output/result-model4-5.RData&amp;#39;)
ms &amp;lt;- rstan::extract(fit)

X_new &amp;lt;- 23:60
N_X &amp;lt;- length(X_new)
N_mcmc &amp;lt;- length(ms$lp__)

set.seed(1234)
y_base_mcmc &amp;lt;- as.data.frame(matrix(nrow=N_mcmc, ncol=N_X))
y_mcmc &amp;lt;- as.data.frame(matrix(nrow=N_mcmc, ncol=N_X))
for (i in 1:N_X) {
  y_base_mcmc[,i] &amp;lt;- ms$a + ms$b * X_new[i]
  y_mcmc[,i] &amp;lt;- rnorm(n=N_mcmc, mean=y_base_mcmc[,i], sd=ms$sigma)
}

customize.ggplot.axis &amp;lt;- function(p) {
  p &amp;lt;- p + labs(x=&amp;#39;X&amp;#39;, y=&amp;#39;Y&amp;#39;)
  p &amp;lt;- p + scale_y_continuous(breaks=seq(from=200, to=1400, by=400))
  p &amp;lt;- p + coord_cartesian(xlim=c(22, 61), ylim=c(200, 1400))
  return(p)
}

d_est &amp;lt;- data.frame.quantile.mcmc(x=X_new, y_mcmc=y_base_mcmc)
p &amp;lt;- ggplot.5quantile(data=d_est)
p &amp;lt;- p + geom_point(data=Salary, aes(x=X, y=Y), shape=1, size=3)
p &amp;lt;- customize.ggplot.axis(p)
# ggsave(file=&amp;#39;output/fig4-8-left.png&amp;#39;, plot=p, dpi=300, w=4, h=3)
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step72&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://winterwang.github.io/post/2019-01-15-simple-linear-regression-using-rstan_files/figure-html/step72-1.png&#34; alt=&#34;MCMC樣本計算獲得的基本年收的貝葉斯可信區間。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 8: MCMC樣本計算獲得的基本年收的貝葉斯可信區間。
&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d_est &amp;lt;- data.frame.quantile.mcmc(x=X_new, y_mcmc=y_mcmc)
p &amp;lt;- ggplot.5quantile(data=d_est)
p &amp;lt;- p + geom_point(data=Salary, aes(x=X, y=Y), shape=1, size=3)
p &amp;lt;- customize.ggplot.axis(p)
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:step73&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://winterwang.github.io/post/2019-01-15-simple-linear-regression-using-rstan_files/figure-html/step73-1.png&#34; alt=&#34;MCMC樣本計算獲得的預期總年收的貝葉斯預測區間。（顏色較深的是50%預測區間帶，黑線是事後樣本的中央值）&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 9: MCMC樣本計算獲得的預期總年收的貝葉斯預測區間。（顏色較深的是50%預測區間帶，黑線是事後樣本的中央值）
&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# ggsave(file=&amp;#39;output/fig4-8-right.png&amp;#39;, plot=p, dpi=300, w=4, h=3)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;練習題&lt;/h2&gt;
&lt;p&gt;用模擬數據來嘗試進行貝葉斯t檢驗&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)
N1 &amp;lt;- 30
N2 &amp;lt;- 20
Y1 &amp;lt;- rnorm(n=N1, mean=0, sd=5)
Y2 &amp;lt;- rnorm(n=N2, mean=1, sd=4)&lt;/code&gt;&lt;/pre&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;請繪製上面代碼生成的兩組數據的示意圖&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d1 &amp;lt;- data.frame(group=1, Y=Y1)
d2 &amp;lt;- data.frame(group=2, Y=Y2)
d &amp;lt;- rbind(d1, d2)
d$group &amp;lt;- as.factor(d$group)

p &amp;lt;- ggplot(data=d, aes(x=group, y=Y, group=group, col=group))
p &amp;lt;- p + geom_boxplot(outlier.size=0)
p &amp;lt;- p + geom_point(position=position_jitter(w=0.4, h=0), size=2)
p&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:exe11&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://winterwang.github.io/post/2019-01-15-simple-linear-regression-using-rstan_files/figure-html/exe11-1.png&#34; alt=&#34;隨機生成的兩組數據的散點圖和箱式圖。&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 10: 隨機生成的兩組數據的散點圖和箱式圖。
&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#ggsave(file=&amp;#39;fig-ex1.png&amp;#39;, plot=p, dpi=300, w=4, h=3)&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;寫下相當於t檢驗的數學式，表示各組之間方差或者標準差如果相等時，均值比較的檢驗模型。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;hypotheses:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;observations in each group follow a normal distribution&lt;/li&gt;
&lt;li&gt;all observations are independent&lt;/li&gt;
&lt;li&gt;The two population variance/standard deviations are known (and can be considered equal)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{H}_0: \mu_2 - \mu_1 = 0 \\
\text{H}_1: \mu_2 - \mu_1 \neq 0 \\ 
\text{If H}_0 \text{ is true, then:} \\
Z=\frac{\bar{Y_2} - \bar{Y_1}}{\sqrt{(\sigma_2^2/n_2) + (\sigma_1^2/n_1)}} \\
\text{follows a standard normal distribution with zero mean} \\
\Rightarrow \text{ if two variances are considered the same}\\ 
Y_1[n] \sim N(\mu_1, \sigma) \;\; n = 1,2,\dots,N \\
Y_2[n] \sim N(\mu_2, \sigma) \;\; n = 1,2,\dots,N
\]&lt;/span&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;寫下上一步模型的Stan代碼，並嘗試在R裏運行&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Stan代碼如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data {
  int N1;
  int N2;
  real Y1[N1];
  real Y2[N2];
}

parameters {
  real mu1;
  real mu2;
  real&amp;lt;lower=0&amp;gt; sigma;
}

model {
  for (n in 1:N1)
    Y1[n] ~ normal(mu1, sigma);
  for (n in 1:N2)
    Y2[n] ~ normal(mu2, sigma);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;R代碼如下：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rstan)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggplot2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: StanHeaders&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## rstan (Version 2.18.2, GitRev: 2e1f913d3ca3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## For execution on a local, multicore CPU with excess RAM we recommend calling
## options(mc.cores = parallel::detectCores()).
## To avoid recompilation of unchanged Stan programs, we recommend calling
## rstan_options(auto_write = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data &amp;lt;- list(N1=N1, N2=N2, Y1=Y1, Y2=Y2)
exe13 &amp;lt;- stan_model(file = &amp;quot;stanfiles/ex3.stan&amp;quot;)
fit &amp;lt;- sampling(exe13, data=data, seed=1234)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &amp;#39;ex3&amp;#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 1e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.030568 seconds (Warm-up)
## Chain 1:                0.023971 seconds (Sampling)
## Chain 1:                0.054539 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &amp;#39;ex3&amp;#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 5e-06 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.030368 seconds (Warm-up)
## Chain 2:                0.079919 seconds (Sampling)
## Chain 2:                0.110287 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &amp;#39;ex3&amp;#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 8e-06 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.04719 seconds (Warm-up)
## Chain 3:                0.02995 seconds (Sampling)
## Chain 3:                0.07714 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &amp;#39;ex3&amp;#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 4e-06 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.02759 seconds (Warm-up)
## Chain 4:                0.024686 seconds (Sampling)
## Chain 4:                0.052276 seconds (Total)
## Chain 4:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Stan model: ex3.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##         mean se_mean   sd    2.5%    25%    50%    75%  97.5% n_eff Rhat
## mu1    -0.24    0.01 0.83   -1.90  -0.79  -0.23   0.32   1.36  3550    1
## mu2     1.62    0.02 1.00   -0.29   0.93   1.62   2.28   3.62  3606    1
## sigma   4.49    0.01 0.46    3.69   4.17   4.44   4.77   5.52  3499    1
## lp__  -97.74    0.03 1.27 -100.95 -98.33 -97.40 -96.83 -96.33  1896    1
## 
## Samples were drawn using NUTS(diag_e) at Thu Jun  6 14:23:50 2019.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;從獲取到的事後參數的MCMC樣本計算 &lt;span class=&#34;math inline&#34;&gt;\(\text{Prob}[\mu_1 &amp;lt; \mu_2]\)&lt;/span&gt;：&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ms &amp;lt;- extract(fit)
prob &amp;lt;- mean(ms$mu1 &amp;lt; ms$mu2)  #=&amp;gt; 0.932
prob&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.932&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;所以可以認爲地一組均值，小於第二組均值的事後概率是93.2%&lt;/p&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;如果不能認爲兩組的方差相等的話，模型又該改成什麼樣子？&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y_1[n] \sim N(\mu_1, \sigma_1) \;\; n = 1,2,\dots,N \\
Y_2[n] \sim N(\mu_2, \sigma_2) \;\; n = 1,2,\dots,N
\]&lt;/span&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data {
  int N1;
  int N2;
  real Y1[N1];
  real Y2[N2];
}

parameters {
  real mu1;
  real mu2;
  real&amp;lt;lower=0&amp;gt; sigma1;
  real&amp;lt;lower=0&amp;gt; sigma2;
}

model {
  for (n in 1:N1)
    Y1[n] ~ normal(mu1, sigma1);
  for (n in 1:N2)
    Y2[n] ~ normal(mu2, sigma2);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;下面的代碼相當於實施Welch的t檢驗：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rstan)
data &amp;lt;- list(N1=N1, N2=N2, Y1=Y1, Y2=Y2)
exe15 &amp;lt;- stan_model(file = &amp;quot;stanfiles/ex5.stan&amp;quot;)

fit &amp;lt;- sampling(exe15, data=data, seed=1234)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &amp;#39;ex5&amp;#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 7e-06 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.028188 seconds (Warm-up)
## Chain 1:                0.025709 seconds (Sampling)
## Chain 1:                0.053897 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &amp;#39;ex5&amp;#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 4e-06 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.029274 seconds (Warm-up)
## Chain 2:                0.05258 seconds (Sampling)
## Chain 2:                0.081854 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &amp;#39;ex5&amp;#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 8e-06 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.037706 seconds (Warm-up)
## Chain 3:                0.02522 seconds (Sampling)
## Chain 3:                0.062926 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &amp;#39;ex5&amp;#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 4e-06 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.029103 seconds (Warm-up)
## Chain 4:                0.023529 seconds (Sampling)
## Chain 4:                0.052632 seconds (Total)
## Chain 4:&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Stan model: ex5.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##          mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat
## mu1     -0.24    0.02 0.93  -2.07  -0.87  -0.23   0.39   1.62  3671    1
## mu2      1.64    0.01 0.85  -0.06   1.09   1.63   2.19   3.33  3657    1
## sigma1   5.12    0.01 0.69   3.98   4.63   5.04   5.53   6.68  3808    1
## sigma2   3.63    0.01 0.65   2.63   3.16   3.54   3.99   5.15  3226    1
## lp__   -95.37    0.03 1.44 -98.86 -96.13 -95.04 -94.28 -93.52  1732    1
## 
## Samples were drawn using NUTS(diag_e) at Thu Jun  6 14:24:28 2019.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ms &amp;lt;- extract(fit)
prob &amp;lt;- mean(ms$mu1 &amp;lt; ms$mu2)  #=&amp;gt; 0.93725
prob&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.93725&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>2018-12 todo</title>
      <link>https://winterwang.github.io/post/2018-12-todo/</link>
      <pubDate>Mon, 03 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/2018-12-todo/</guid>
      <description>&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;2018-12-03

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=16Rd46SE-20&amp;amp;list=PL7F907999BA1994A1&#34; target=&#34;_blank&#34;&gt;Learn Emacs;&lt;/a&gt; day 1 done&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Return the email from LP;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;learn &lt;a href=&#34;https://github.com/noamross/redoc&#34; target=&#34;_blank&#34;&gt;reversible R markdown /MS word documents package&lt;/a&gt;;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Pay the you-know-what ticket;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Read &lt;a href=&#34;https://www.amazon.com/Evidence-Evolution-Logic-Behind-Science-ebook/dp/B00KILLNIO/ref=mt_kindle?_encoding=UTF8&amp;amp;me=&amp;amp;qid=1543812059&#34; target=&#34;_blank&#34;&gt;Evidence and Evolution&lt;/a&gt; 1%&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2018-12-04

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Prepare the manuscript for AJCN 10% left; FINALLY!!!!!&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=16Rd46SE-20&amp;amp;list=PL7F907999BA1994A1&#34; target=&#34;_blank&#34;&gt;Learn Emacs;&lt;/a&gt; day 2&amp;amp;3 done;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2018-12-05

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Read the victim book, and keep memo to P151;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=16Rd46SE-20&amp;amp;list=PL7F907999BA1994A1&#34; target=&#34;_blank&#34;&gt;Learn Emacs;&lt;/a&gt; day 4&amp;amp;5 done;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2018-12-06

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Move all todo to &lt;a href=&#34;https://wangcc.me/Emacsnotes/TODO.html&#34; target=&#34;_blank&#34;&gt;Org-mode page&lt;/a&gt;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Rstan Wonderful R-(1)</title>
      <link>https://winterwang.github.io/post/rstan-wonderful-r/</link>
      <pubDate>Thu, 22 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/rstan-wonderful-r/</guid>
      <description>


&lt;p&gt;P16&lt;/p&gt;
&lt;p&gt;事後分布 &lt;span class=&#34;math inline&#34;&gt;\(p(\theta | Y)\)&lt;/span&gt;の値が最大になる点&lt;span class=&#34;math inline&#34;&gt;\(\theta^*\)&lt;/span&gt;を事後確率最大推定値 (maximum a posteriori estimate)と呼ぶ．略してMAP推定値 (MAP estimate)．&lt;/p&gt;
&lt;p&gt;我們把能夠將事後概率分布取極大值的參數點 &lt;span class=&#34;math inline&#34;&gt;\(\theta^*\)&lt;/span&gt; 稱爲事後概率的最大似然估計值 (maximum a posteriori estimate)，簡稱 MAP估計值 (MAP estimate)。&lt;/p&gt;
&lt;p&gt;P19&lt;/p&gt;
&lt;p&gt;統計建模的一般順序&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;確定分析目的&lt;/li&gt;
&lt;li&gt;確定數據分布&lt;/li&gt;
&lt;li&gt;想象數據產生本身的機制：思考數據與數據之間可能的關系&lt;/li&gt;
&lt;li&gt;寫下你所認爲的數據模型的數學表達式&lt;/li&gt;
&lt;li&gt;用 R 模擬(simulation)並確認前一步寫下的數學模型的性質，特點&lt;/li&gt;
&lt;li&gt;用 Stan 實際進行模型參數的推斷&lt;/li&gt;
&lt;li&gt;獲得推斷結果，解釋其事後概率分布的意義，繪制易於理解的模型示意圖&lt;/li&gt;
&lt;li&gt;繪制成功之後的模型示意圖和最先使用的模型之間進行比對，重新查缺補漏&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;P23&lt;/p&gt;
&lt;p&gt;ただいたずらにモデルを複雑化させるのは解釈のしにくさを招く．&lt;/p&gt;
&lt;p&gt;P30&lt;/p&gt;
&lt;p&gt;最初にmodel ブロックの尤度の部分（と事前分布の部分）を書く．その尤度の部分に登場した変数のうち，データの変数をdataブロックに，残りの変数をparametersブロックに書いていく．&lt;/p&gt;
&lt;p&gt;Stan的基本文法構成&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data {
 數據描述
}

parameters {
 想要進行MCMC事後樣本採集的參數描述
}

model {
 p(Y|theta) 似然的描述
 先驗概率分布 p(theta) 的描述
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;把下面的模型&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
Y[n] &amp;amp; \sim \text{Normal}(\mu, 1) \;\; n = 1, \dots, N \\
\mu  &amp;amp; \sim \text{Normal}(0, 100)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;翻譯成爲 Stan 模型語言是：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data {
  int N;
  real Y[N];
}

parameters {
  real mu;
}

model {
  for (n in 1:N) {
    Y[n] ~ normal(mu, 1);
  }
  mu ~ normal(0, 100);
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中我們按照實際模型書寫的順序 model -&amp;gt; data -&amp;gt; parameter 來逐個解釋：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;model&lt;/code&gt; 模塊中 &lt;code&gt;for (n in 1:N)&lt;/code&gt; 開始的循環部分（三行）對應數學模型的 $Y[n] (, 1) n = 1, , N $　部分。&lt;/li&gt;
&lt;li&gt;Stan 語言中，每一行描述的結尾需要用分號 &lt;code&gt;;&lt;/code&gt; 來結束。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mu ~ normal(0,100)&lt;/code&gt; 則對應數學模型中寫的先驗概率 &lt;span class=&#34;math inline&#34;&gt;\(\mu \sim \text{Normal}(0, 100)\)&lt;/span&gt; 部分。這裏給均值的先驗概率分佈是一個方差很大的無信息先驗概率分佈 (noninformative prior)。事實上在 Stan 軟件語言中，如果不特別指出先驗概率分佈，系統會默認給參數以無信息的先驗概率分佈，這樣即使沒有這一行，模型也是可以跑的。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;data&lt;/code&gt; 模塊中寫明的是 &lt;code&gt;model&lt;/code&gt; 模塊中描述的模型將要使用的數據。它包括宣示數據的個數（樣本量 &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;），以及數據本身。其中 &lt;code&gt;int N&lt;/code&gt; 意爲樣本量的數量是整數個 (integer)，&lt;code&gt;real Y[N]&lt;/code&gt; 則宣示實數有 N 個作爲數據。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;parameter&lt;/code&gt; 模塊是告訴軟件需要採樣且關注的未知參數 (parameter) 是 &lt;code&gt;mu&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;在 Stan 語言中，還可以和其他語言一樣爲模型加註解釋的文字，只需要在想要做註釋的文字最開始的部分增加 &lt;code&gt;//&lt;/code&gt;，如果註釋的文字超過一行，那麼在註釋的模塊前後加上 &lt;code&gt;/*&lt;/code&gt; 和 &lt;code&gt;*/&lt;/code&gt; 即可。&lt;/li&gt;
&lt;li&gt;另外，目前爲止主流的貝葉斯模型軟件中使用精確度 (precision) ，也就是方差的倒數來描述正態分佈 &lt;code&gt;normal(mean, 1/variance)&lt;/code&gt; ，但是在Stan的語法中使用的是 &lt;code&gt;normal(mean, sd)&lt;/code&gt;，也就是用標準差來描述正態分佈。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;寫Stan（或者說寫大多數的代碼）時，請遵守以下的原則：&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;適當縮進，以便於閱讀；&lt;/li&gt;
&lt;li&gt;表示數據的部分用大寫字母，表示參數的部分，用小寫字母；&lt;/li&gt;
&lt;li&gt;每個部分之間至少使用一個空行加以區分；&lt;/li&gt;
&lt;li&gt;請不要用&lt;code&gt;camelCase&lt;/code&gt;這樣的方式（單詞之間用大寫隔開），請在單詞之間用下劃線 &lt;code&gt;camel_case&lt;/code&gt; 的標記方法；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;~&lt;/code&gt;或者&lt;code&gt;=&lt;/code&gt;前後用一個字符大小的空格來隔開。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;最低限度的話，也請依照1,2兩個標準來書寫你的Stan代碼。不爲他人，也爲自己將來再讀代碼時能快速理解其涵義。往Stan的官方論壇投稿時，也必須遵守它們在手冊裏提供的 “Stan Program Style Guide” 代碼書寫規則，也是對其他寫，讀代碼的人的尊重。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Notes from reading</title>
      <link>https://winterwang.github.io/post/notes/</link>
      <pubDate>Wed, 07 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/notes/</guid>
      <description>&lt;p&gt;P18:
所謂“無法顯示”，完全是謊言。這個網站一直存在並且運作良好。當然，說謊的並不是電腦。&lt;/p&gt;

&lt;p&gt;盡管不是出乎意料，我仍然感到震驚。受難者們已經死亡三十多年。當年他們死亡的時候，大多數人的骨灰都沒有保留，更談不上安葬。三十年後，在電腦網絡的虛擬空間裏，都不容許有他們的安息之地，是爲了什麼？是誰，做了決定禁止受難者的名字在網上？&lt;/p&gt;

&lt;p&gt;P21:
流落人間者，泰山一毫芒&lt;/p&gt;

&lt;p&gt;P23:
中國是一個最講尊師重道的古老文明古國，而且尊師的傳統從未斷絕過。&lt;/p&gt;

&lt;p&gt;P26:
對於一個患了嚴重失憶症的民族，王友琴博士這部文革受難者真是一劑及時良藥。&lt;/p&gt;

&lt;p&gt;P28:
毛首先罷黜了那些試圖約束青年的同僚，以此掃清了道路，致使很多地方陷入霍布斯式的自然狀態（即相互爭鬥，人人自危的野蠻狀態），中學生和大學生實施暴力和恐怖整整兩年，最先鬥老師，然後鬥黨內幹部，最後自己互相鬥。。。。　但是真正的研究可能形成對整個一代人的指控－－那些參與者和觀看者。他們正在掌握國家的領導權。&lt;/p&gt;

&lt;p&gt;P29:
她是受難者的一個活資料庫。她一個人抗拒着數億人的遺忘。&lt;/p&gt;

&lt;p&gt;P40:
其實，看看事實，就知道這不但不是什麼向權勢者“造反”，而且從開始就是極權勢力的一次直接擴張。&lt;/p&gt;

&lt;p&gt;P46:
作爲一個中學校長，她從來沒有也幾乎不可能在上級指示之外做什麼標新立異之事，也沒有違抗過他們的命令。高層領導人的孩子，都在她主管的學校上學。然而，當革命需要打擊目標的時候，上級們就可以翻臉不認人，把一個個活人當做靶子拋出來，批判鬥爭，處分懲罰。他們根本不把他們的下級當人來看待，而只是一些數字和百分比，一些可以服務於革命目標的工具甚至靶子。冷酷是文革的一個重要特徵。文革不但嚴厲打擊反對革命的人，而且嚴厲打擊未曾反對革命的人。&lt;/p&gt;

&lt;p&gt;P48:
我們永遠無法知道，在卞仲耘死前的幾個小時裡，當她遭到這樣殘酷的毆打和折磨的時候，她想了些什麼。雖然她一直被人群包圍，&lt;strong&gt;她死在絕對的孤獨之中&lt;/strong&gt;。當她被打的時候，沒有一個人出來制止暴行。當她快要死去的時候，沒有一個人在身邊表示同情。&lt;strong&gt;她從來沒有與這些打死她的人為敵，但是這些人不但打死了她，而且，在打她的時候毫不猶豫，在她被打死後也沒有覺得任何後悔或者羞愧&lt;/strong&gt;。她孤立無援地死在紅衛兵學生的亂棒之下，甚至沒有可能作一點但反抗來保護自己。從一個活人的世界上，她被無情無義地背叛了，被拋棄了被犧牲了。&lt;/p&gt;

&lt;p&gt;P51:
1966年的夏天，全中國的學校變成了刑訊室，監獄，甚至殺人場。大批老師被迫害致死。&lt;/p&gt;

&lt;p&gt;P52:
1966年10月召開的“中共中央工作會議”發放了一個題為《把舊世界打個落花流水》的文章，其中被列為紅衛兵功績之一的，是1968年8月20日到9月底北京有1772人被打死。有理由認為實際死亡數據大於此數。但是，此數已經是極其巨大的數字。卞仲耘的死尚不在此數之中。1966年8月5日發生的卞仲耘之死，是8月殺戮的開端，經過三個星期的發展，在8月底前後達到每日被害人數的最高峰。由最高權力者號召鼓動，用中學生紅衛兵為打手，打死手無寸鐵的教育工作者如卞仲耘，以及大批沒有防衛能力的和平居民，還視為偉大功績，這實在是二十世紀統治者所作的最為殘忍和無恥的行為之一。&lt;/p&gt;

&lt;p&gt;P62:
卞仲耘，一個教育工作者的死，標誌了這個血腥時代的開始。讓我們記住這個名字和這個日期，記住在文明的進程中可能發生什麼樣的逆轉和災難。&lt;/p&gt;

&lt;p&gt;P63:
1993年，筆者到校中攝下一張宿舍樓的照片。卞仲耘被打死在這座宿舍樓門口的台階上。四個住在樓裡的高中三年級的學生問我：“20多年前有人在這兒被打死，這是真的嗎？我們什麼都沒聽說過。”&lt;/p&gt;

&lt;p&gt;P101:
文革前，清華大學有108名教授，曾經被人開玩笑說好像《水滸傳》裏有“108將”。陳祖東就是這“108將”之一。陳祖東的家人聽說，到1978年，這108人只死剩下40多人了。&lt;/p&gt;

&lt;p&gt;P104:
我在文革後考進北京大學中文系讀書，從來沒有聽到人提起程賢策的名字和他在文革中自殺的事情。雖然這個大學剛剛發生過文革這樣的重大歷史事件，文革歷史還未得到記錄和分析，但是，有着著名文科科系的北京大學，卻不教學生去認識和分析這些發生在自己學校的重要歷史事實，這顯然不恰當也相當具有諷刺性。不過，這也是普遍現象。其中主要的原因，是最高權力當局的嚴格禁止。&lt;/p&gt;

&lt;p&gt;P110:
1968年，北京大學建立了一所校園監獄，命名爲&amp;rdquo;黑幫監改大院&amp;rdquo;，把二百多名教職員工關在裏面。那年6月18日，關在&amp;rdquo;監改大院&amp;rdquo;裏的人被拉出來&amp;rdquo;鬥爭&amp;rdquo;，當他們排隊穿過校園的時候，甬道兩側站滿了學生，手持棍棒皮鞭，爭相痛打他們。然後，他們被拉到各系，施以種種酷刑。&lt;strong&gt;那一天，北大校園裏充滿了狂熱的殘忍於惡毒。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;P110:
北京大學建立於1898年的維新運動中。大學本該是中國現代化，科學和文明的代表。但是在文革中，北京大學卻變成最野蠻殘酷的行爲發生的地方。暴力性的&amp;rdquo;鬥爭會&amp;rdquo;，包括毆打侮辱掛黑牌戴高帽子等等，校園&amp;rdquo;勞改隊&amp;rdquo;，校園監獄，都在北京大學領先開始，更不要說道德方面的墮落如誣陷，謊言，讒佞等等普遍發生。北京大學發生的這種巨大變化，是文革真正劇烈改變社會傳統以及行爲規範最&amp;rdquo;成功&amp;rdquo;的例子。這種成功，令人震驚，也令人思考。&lt;/p&gt;

&lt;p&gt;P112:
一批一批的人成爲&amp;rdquo;敵人&amp;rdquo;，一旦被指控，就被無情地清洗出去，既不能自我辯護，也逃脫不了殘酷的處罰。革命的巨爪不但在農村，也在這所中國最早建立的現代大學裏面，把人一把一把抓起來，糟蹋丟棄。&lt;/p&gt;

&lt;p&gt;P112:
在1966年，程賢策從&amp;rdquo;革命者&amp;rdquo;變成爲&amp;rdquo;革命&amp;rdquo;打擊的對象。看起來，文革好像是非邏輯的。但是實際上這一切有其內在的邏輯。檢視往事，現在可以看出，一批人在參與迫害的同時，也鋪就了迫害他們自己的道路。因爲他們參與的迫害，不只是對一些個人的否定，而且是對法治，對程序正義，對一個公民應該具有的公民權利的根本否定。&lt;/p&gt;

&lt;p&gt;P139:
在1991年範長江的名字被用來命名“新聞獎”，但是，他本人爲什麼從1952年就不能再做新聞工作，爲什麼他在1970年悲慘地死於井中，這些卻沒有報道和分析。&lt;/p&gt;

&lt;p&gt;P139:
樊西曼，女，1915年生，鐵道部中共黨校黨委副書記，1966年8月25日被兒子的同學，北京師範大學附屬第二中學紅衛兵綁架到學校，在學校內一個磚砌的乒乓球臺子上被打死。兒子曹濱海從此精神失常。同一天在校中被打死的，還有這個中學的語文老師斳正宇和學校負責人姜培良。&lt;/p&gt;

&lt;p&gt;P142:
從此，鬥打，亂殺事件日益嚴重，由開始打鬥個別“表現不好”的“四類分子”（地主，富農，反革命分子，壞分子），發展到打鬥一般的四類分子；由一個大隊消滅一兩個，兩三個四類分子，發展到亂殺家屬子女和有一般問題的人，最後發展到全家被滅絕。子8月27日至9月1日，該縣的13個公社，48個大隊，先後殺害“四類分子”及其家屬供325人。最大的80歲，最小的僅38天，有22戶被殺絕。&lt;/p&gt;

&lt;p&gt;P147:
傅雷，男，1908年生，上海居民，翻譯家，翻譯大量法語作品，在1957年被劃成“右派分子”，1966年8月下旬被抄家和“鬥爭”，9月3日在寓所中和妻子朱梅馥一起留下遺書自殺身亡。傅雷時年53歲。文革後傅雷得到“平反”。《傅雷家書》出版後，成爲受歡迎的暢銷書。“家書”是他和兒子的通信。他有兩個兒子，一名“聰”，一名“敏”，都出生與1930年代。1966年時，傅敏是北京第一女子中學的英文教員。1966年8月北京的中學教員和校長們遭到紅衛兵學生的野蠻攻擊，傅敏在學校附近投水自殺，幸而未死。他的哥哥是鋼琴家傅聰，1958年在公派波蘭學習畢業的時候，不回中國，去了英國，當時被稱作“叛國分子”。&lt;/p&gt;

&lt;p&gt;P150:
乒乓球和政治和思想觀念沒有直接的關聯。然而，文革不但整死作家，教員和演員，還把這些乒乓球運動員整死，這是怎樣的殘酷和瘋狂？在毛澤東之前和之後，還沒有一個暴君做過這樣的事情。&lt;/p&gt;

&lt;p&gt;P151:
高斌，男，湖北人，1940年代留學英國，曾任北京外國語學院俄國文學教授，調陝西師範大學後，在“反右運動”中被定位“右派分子”，送農場“勞動改造”，“摘帽”後恢復授課，但降薪降級。1966年遭到“批鬥”後自縊身亡。&lt;/p&gt;

&lt;p&gt;P158:
夜裏，龔維泰就躺在“一教”的地板上，靜悄悄地殺死了自己。很難想象，什麼樣的絕望會讓人這樣結束自己的生命！他自殺，沒有抗議，沒有抱怨，甚至在流血中漸漸死去的過程中沒有呻吟，沒有響動，以至躺在他身邊的人們都不知道發生了什麼。
。。。
龔維泰的事情聽起來確實很“奇怪”。龔維泰參加共產黨組織的“民青”反對國民黨政府，他被逮捕以後並沒有治他的罪。沒有證據說明他被捕後爲國民黨政府做過任何傷害共產黨的事情，他卻在20年後的“清理階級隊伍運動”中，爲此被捕事件遭到嚴酷的審查，最後這樣可怕的死去。&lt;strong&gt;實際上，從法律的角度看，除非是殺了人，不管龔維泰在那時候做了什麼，都已經過了法律的追溯期限，不能再作追究。但是“革命”壓倒一切的時候，法律是紙上空文。非常諷刺的是，文革後有人控告文革中的殺人兇手，北京的檢察院卻以“法律追溯時限已過”拒絕。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;P169:
“中華人民共和國憲法”上冠冕堂皇地寫着中國公民享有“言論自由”，甚至在文革中修改過的新憲法中，雖然去掉了“遷徙自由”，卻依然留有“言論自由”。然而，在實際上，言論竟被當作判處死刑的根據。
&amp;hellip;
顧文選的第二條罪狀是逃離中國，在材料上被稱爲“叛國投敵”。在共產黨宣傳中，人民是“國家的主人”，但是在1950，1960，1970年代，普通人民根本不被允許得到護照出國。如果他們想要離開中國，只有祕密離開。祕密離開中國是十分危險的事情。他們可能在中途被打死。如果被抓住，竟然可以成爲判處死刑的根據。&lt;/p&gt;

&lt;p&gt;在歷史上，從來沒有過這樣殘酷的執法。在德國有過“柏林牆”。在1989年柏林牆被推倒以前，試圖偷越“柏林牆”的人，有一百多人被哨兵打死，這是非常兇殘的事情，因此在柏林牆被推倒之後，下令開槍射擊的東德領導人被法庭起訴。&lt;/p&gt;

&lt;p&gt;P170:
在文革後很多年，仍然有很人認爲文革是一個“大民主”。這種看法的“根據”是，普通人可以對各級領導幹部“造反”。且不說當時的“鬥爭會”等形式是多麼野蠻和違法的手段，也不說可以“造反”的內容僅僅是那些人“反對毛澤東思想”，這樣的看法無視顧文選這樣的人被殘酷殺害的事實，無視聞佳這樣的人被判重型的事實，創造了一個遠離事實的文革神話。&lt;/p&gt;

&lt;p&gt;P172:
至今一些人還在肯定文革“反對官僚制度”的正面貢獻，因爲毛澤東說了要“精兵簡政”以及“打破重疊的行政機構”（1968年3月）。然而文革中實際上工作了的，是取消了公安局，檢察院，法院三家分開並且獨立這樣的社會組織形式，&lt;strong&gt;取消了定罪和審判的法律程序，在製造對人的迫害方面大大提高了效率&lt;/strong&gt;。這是一種多麼可怕的提高效率啊。
&amp;hellip;&lt;/p&gt;

&lt;p&gt;文革中掌管北京的“公檢法”長達十年的，是原爲南京軍區某部軍委副政委的劉傳新。1976年毛澤東死去以及“四人幫”隨之被逮捕。在文革中被劉傳新關押迫害過的一批老幹部重新回到權力位置上。1977年1月27日，劉傳新被免去北京市公安局長的職務。他被“隔離”在東交民巷他居住的院落裏受“審查”。1977年5月18日，他接到了北京市公安局第二天要開“聲討劉傳新大會”的通知。半夜，他在一棵樹上上吊自殺。&lt;/p&gt;

&lt;p&gt;劉傳新自殺，當然是因爲他的失勢，也可能是由於懼怕他用於別人身上的殘酷做法會被用到他自己身上。但是他懼怕的人中間，不會有顧文選。這不但因爲顧文選已經被殺死，也因爲顧文選本來也只是一個沒有權力的普通人。&lt;/p&gt;

&lt;p&gt;P186:
把社會中的一個很大的人羣，劃出來進行“審查”，隔離審訊，再從中劃出一部分作爲受到永久性處罰的“敵人”，這樣的做法幾十年來不斷實行，以致有的中國人已經把這樣的做法視爲像颳風下雨一樣的常態，從不從根本上去質疑和反對。&lt;/p&gt;

&lt;p&gt;P187:
也使人吃驚的是，被裝載籮筐裏遭到“鬥爭”的人，對於他自己被安的罪名不承認，但是對他自己曾經發動的對幾百萬人進行過的於此類似的“鬥爭”，至死也並未覺得不安。在1990年代出版的他的女兒寫的關於他的書裏（《紅色家族檔案：羅瑞卿的女兒的點點回憶》，羅點點，南海出版公司，海口1999年），細膩深情地寫到他在文革中的遭遇多麼不公平，&lt;strong&gt;卻一字也沒有提到他曾多麼殘酷地對待千千萬萬別的人。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;P187:
在歷史上，也還從來沒有一個時代和一個政權，可以把普通人民控制到這樣嚴密的程度。他們不告訴人民這些人的“反革命”活動到底是些什麼，卻要求每一個人都“表態”來支持殺死這些人。他們不但殺人，還要造成一個“衆口一詞”的形勢。他們用恐怖來塑造輿論，這輿論反過來又來支持恐怖。
。。。
一位被訪者說：你能想象那時候北京人有多壞嗎？他們根本不把別人的命當回事兒。他們喊過“槍斃槍斃”，就趕快回家吃飯去了。&lt;/p&gt;

&lt;p&gt;魯迅寫的阿Q，在他自己被殺之前，曾經很興奮地去看殺別人的頭。文革年代，普通人也活得像魯迅筆下的阿Q，會興高采烈地參加“公判大會”，把別人被槍斃當作好戲看。&lt;/p&gt;

&lt;p&gt;P189：
以意識形態的名義和革命的名義，把人類的一部分宣佈爲必須消滅的“敵人”，通過一系列預先設計的所謂“政治運動”，一個政權把社會中的一個羣體，不是一個人兩個人，也不是幾十個或幾百個人，而是一個及其巨大的人數&amp;ndash;人羣中的一個百分比，有計劃有組織有系統地予以打擊和消滅，這就是毛澤東對顧文選和億萬中國人所作的。&lt;/p&gt;

&lt;p&gt;P192:
筆者曾經問清華附中的受訪者，你們是怎麼知道紅衛兵不準醫院搶救郭蘭蕙的。兩位認識郭蘭蕙的學生說，紅衛兵曾經在學校當衆宣佈，由於郭蘭蕙是自殺的，&lt;strong&gt;醫院打電話到清華附中詢問她是什麼人，是否有“問題”，清華附中紅衛兵接了電話告訴醫院，郭蘭蕙是“右派學生”，於是醫院不給搶救，讓郭蘭蕙在醫院的地板上死去。&lt;/strong&gt;紅衛兵不但對郭蘭蕙自殺毫無憐憫之心，而且用得意洋洋的口氣在學校裏告訴其他學生這些情況，顯示他們主宰生死的權力和威風。&lt;/p&gt;

&lt;p&gt;郭蘭蕙死時只有19歲。她曾經病休一年，所以1966年她上高二，而不是高三。&lt;/p&gt;

&lt;p&gt;P192：
在郭蘭蕙死亡兩個星期以前，該校高一（二）班學生楊愛倫也因相同的原因試圖自殺。楊愛倫的父親在1949年以前的政府海關做事，於是被認定爲“壞家庭出身”。她在1966年7月底就開始在班裏被紅衛兵“鬥爭”。清華附中紅衛兵的領導人之一曾經到她的班上詳細指示如何整她。她被緊閉在一間小屋裏，被強迫寫“檢查交代”。1966年8月8日，楊愛倫到“清華園”火車站附近臥軌自殺。火車頭把她鏟出了軌道。她沒有死，但是臉部和身體受到重傷，並且失去了三個手指，成爲永久性傷殘。&lt;/p&gt;

&lt;p&gt;P197-199:
韓光第，男，牙醫，家住四川省漢源縣富林鎮第二居民段。在1968年夏天因說毛澤東送給“首都工農毛澤東思想宣傳隊”的芒果“像一條紅薯沒什麼看頭”，被逮捕，長期關押之後，1970年被以“現行反革命”罪判處死刑，在富林鎮郊被槍斃。
。。。&lt;/p&gt;

&lt;p&gt;實際上，送到四川漢源鎮上的這個芒果，也根本不是真的芒果。真的芒果要保存那麼長的時間，早已經腐爛了。在漢源鎮上展示的，只可能是個蠟製的複製件。當時在全國各地作了無數這類的複製品或者芒果照片，強制八億人崇拜。1968年，是文革中害死人最多的“清理階級隊伍運動”進行的時候，也是對毛澤東的個人崇拜最嚴重的時候之一。
。。。&lt;/p&gt;

&lt;p&gt;漢源鎮上的人，包括很多小學生，看到了韓光第被槍殺的場面。這樣的場面，無疑對他們的一生都影響深遠。它們從此再不會敢對任何和毛澤東有關的東西，說任何自己想作的評論。它們變得非常謹慎小心。甚至在35年之後，它們還得顧慮是否要把這樣的事實說出來。
。。。&lt;/p&gt;

&lt;p&gt;文革時期曾經秩序相當混亂，當時的“專政機關”審批死刑的手續也被簡化（文革前死刑要經過最高法院批准，文革中這個權力被下放了），但是&lt;strong&gt;文革並不是一個失控的時期&lt;/strong&gt;，被判處死刑的人，都是當時上面下令開始的政治運動高潮的結果，有一套逮捕人和處分人的規定。&lt;/p&gt;

&lt;p&gt;P201: 南京第13中學位於南京市“西家大塘”，離開市中心不願。韓康是13中學的圖書館員。韓康的家就在學校附近。因1949年以前曾參加過國民黨，紅衛兵抄了他的家，把他家的東西雜碎毀壞。&lt;/p&gt;

&lt;p&gt;1966年9月5日早上韓康被十三中的紅衛兵揪到學校操場上“批鬥”。另有十三中體育老師夏忠謀，也因歷史上參加過三青團等“問題”，一起被揪到操場批鬥。批鬥中，紅衛兵對二人拳打腳踢，用皮帶抽，用磚頭砸。由於紅衛兵說韓康的態度不好，對他打得更是厲害。直至下午三四點中，韓康兩三次昏死過去，但都被紅衛兵用冷水潑醒後再接着批鬥，到傍晚，韓康終於被活活打死。&lt;/p&gt;

&lt;p&gt;夏忠謀因一直低頭任由紅衛兵批鬥，所以僥倖沒有被當場打死，晚上他被關押到學校實驗室，外面有紅衛兵看守。夏忠謀看到韓康被打死，自己被關押第二天要接着被批鬥，肯定是難逃一死，因此晚上用衣服撕成布繩上吊自殺身亡。兩人都死於1966年9月5日一天。&lt;/p&gt;

&lt;p&gt;P202:
韓志穎，西安市第五中學校長，男，中國民主同盟盟員。1966年8月，第五中學的紅衛兵把凳子壘成高臺，讓韓志穎站上去接受“批鬥”。他不吸菸，紅衛兵把數隻香菸點燃後分別插入韓的耳朵，鼻孔和嘴中，用煙熏韓。韓志穎在1966年被“批鬥”至死。&lt;/p&gt;

&lt;p&gt;P206:
這種“主席大手筆”的說法正是文革思維的產物。實際上，在文明設會裏，儘管人們之間有貧富之分，有地位高低的區別，但是，沒有一個人有權力在法庭之外剝奪另一個人的生命，是普遍的基本原則。文革以“革命”的名義，打破了這一條原則。而且，不但這樣做了，而且，樹立了這樣的“大手筆”觀念，即認爲是可以做的，特別是對“偉大”人物來說。&lt;/p&gt;

&lt;p&gt;在建設“紀念園”的時候，不止一人向筆者提過這樣的問題：寫這種普通人受難者的故事有什麼意義？這種質疑和這種“主席大手筆”說法有實質相連。如果說這種話的是有權勢的人，那是權勢使它們眼裏沒有普通人的位置。如果說這種話的是普通人，那麼是長期精神奴役的結果。&lt;/p&gt;

&lt;p&gt;P224:
文革後進入北京航空學院讀書的一位被訪者說，那時候學校裏依然流傳着文革時代留下的一句話：該校主樓的每一扇窗戶，差不多都曾經有人跳下來自殺。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>2018-11 todo</title>
      <link>https://winterwang.github.io/post/2018-11-todo/</link>
      <pubDate>Mon, 05 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/2018-11-todo/</guid>
      <description>&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;2018-11-05

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Paper comments to Lin;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Cancel the tsumitate;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Ask about the insurance;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Submit nenmatsu;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Modify the poster (adding logo), etc;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2018-11-06

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;&lt;a href=&#34;https://wangcc.me/LSHTMlearningnote/section-85.html#practical-bayesian-statistics-06&#34; target=&#34;_blank&#34;&gt;Bayesian statistics practical 6&lt;/a&gt; done;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Prepare AJCN template (Rmd) file;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Prepare AJCN draft (.docx) file;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;MLCA analyses done stratified by men and women;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2018-11-07

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;fix the racket before Wed.&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;&lt;a href=&#34;https://wangcc.me/LSHTMlearningnote/section-85.html&#34; target=&#34;_blank&#34;&gt;Bayesian statistics Chapter 7&lt;/a&gt; done 20%;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Check the problem of the gh-page on github.&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2018-11-08~11

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Search for guideline of eradication among adolescents of H. &lt;em&gt;pylori&lt;/em&gt; in the world;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Protocol comments to TMS;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Read the victim book, and keep memo;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Prepare the manuscript for AJCN 10%;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Modify the Presentation Slides&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2018-11-12

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;&lt;a href=&#34;https://wangcc.me/LSHTMlearningnote/section-85.html&#34; target=&#34;_blank&#34;&gt;Bayesian statistics Chapter 7&lt;/a&gt; done 80%;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Read the victim book, and keep memo;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Comment the Presentation Slides from LP;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Labeling the learning note bookdown book.&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2018-11-13

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;&lt;a href=&#34;https://wangcc.me/LSHTMlearningnote/section-85.html&#34; target=&#34;_blank&#34;&gt;Bayesian statistics Chapter 7&lt;/a&gt; done 100%;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Prepare the manuscript for AJCN 25%;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Comment the manuscript from Lin;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Domestic travel paper work&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2018-11-14

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Read the victim book, and keep memo;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Prepare the manuscript for AJCN 40%;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2018-11-15~16

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;&lt;a href=&#34;https://wangcc.me/LSHTMlearningnote/section-85.html#practical-bayesian-statistics-07&#34; target=&#34;_blank&#34;&gt;Bayesian statistics practical 7&lt;/a&gt; done 100%;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Prepare the manuscript for AJCN the tables, figures done;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2018-11-19

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Prepare the manuscript for AJCN 80%;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Confirm the feedback of the project;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2018-11-20

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Comment KS application form (2018-11-21).&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Perform the analyses using sex as an interaction, combine three tables. done 20%&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Read the victim book, and keep memo;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;&lt;a href=&#34;https://wangcc.me/LSHTMlearningnote/section-86.html&#34; target=&#34;_blank&#34;&gt;Bayesian statistics Chapter 8&lt;/a&gt; 20% done;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2018-11-21

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Learn Stan for Bayesian stats 1%;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Perform the analyses using sex as an interaction, combine three tables. done;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;&lt;a href=&#34;https://wangcc.me/LSHTMlearningnote/section-86.html&#34; target=&#34;_blank&#34;&gt;Bayesian statistics Chapter 8&lt;/a&gt; 30% done;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2018-11-22

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Learn Stan for Bayesian stats 5%;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;&lt;a href=&#34;https://wangcc.me/LSHTMlearningnote/section-86.html&#34; target=&#34;_blank&#34;&gt;Bayesian statistics Chapter 8&lt;/a&gt; 35% done;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Manuscript to AMU about the overseas study&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2018-11-24

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Learn Stan for Bayesian stats 6%;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Review the JAT meta-ana paper (deadline: 2018-11-28);&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2018-11-25~12-02 (annual leave)&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>The day-time patterns of carbohydrate intake in the UK adults - results from the NDNS RP (2008-16)</title>
      <link>https://winterwang.github.io/talk/ndns-talk-london/</link>
      <pubDate>Sun, 04 Nov 2018 00:00:00 +0900</pubDate>
      
      <guid>https://winterwang.github.io/talk/ndns-talk-london/</guid>
      <description>&lt;p&gt;Recent evidence suggests that there are three types of eaters (grazers, early eaters, and late eaters) according to the timing of energy consumption[1,2]. This study aims at finding both timing and quantity eating patterns specifically for carbohydrate (CH) intake in UK adults.&lt;/p&gt;

&lt;p&gt;Data from the National Diet and Nutrition Survey (NDNS) Rolling Programme (&lt;sup&gt;2008&lt;/sup&gt;&amp;frasl;&lt;sub&gt;09&lt;/sub&gt;-&lt;sup&gt;15&lt;/sup&gt;&amp;frasl;&lt;sub&gt;16&lt;/sub&gt;) included 6155 adults aged 19 or older in the UK. Time of the day was categorized into 7 slots: 6-9 am, 9-12 noon, 12-2 pm, 2-5 pm, 5-8 pm, 8-10 pm and 10 pm-6 am. Responses for CH intake within each time slot were categorised into:1) no energy intake, 2) CH contributed  50% or 3) CH contributed  50% of total energy. Multilevel latent class analysis (MLCA)[3] models were applied to explore latent classes of CH consumption, accounting for the repeated measurement of intake on 3-4 days nested within individuals. Survey-designed multivariable regression models were used to assess the associations of CH eating patterns with hypertension and obesity.&lt;/p&gt;

&lt;p&gt;Three CH eating day patterns (low/high percentage, and regular meal days) emerged from 24483 observation days, based on which three types of CH eaters were defined which could be broadly labelled as low (28.1%), moderate (28.8%), and high (43.1%) CH eaters. On average, low-CH eaters (Fig.A) consumed the highest amount of total energy intake (7985.8 kJ, p-value &amp;lt; 0.001), and they had higher percentages of energy contributed by fat and alcohol, especially after 8 pm. Moderate-CH eaters (Fig.B) consumed the lowest amount of total energy (7341.8 kJ) while they had the tendency of eating CH later in the day. High-CH eaters (Fig.C) consumed most of their carbohydrates and energy within time slots of 6-9 am, 12-2 pm and 5-8 pm.&lt;/p&gt;

&lt;p&gt;The high-CH eaters profile seemed to be the healthiest. Low-CH eating which was crudely associated with higher prevalence of hypertension and obesity (p-values respectively of &amp;lt; 0.001, and 0.024) may have resulted from health/weight concerns, leading to fat or alcohol as replacements for CH. To ascertain the direction of causality in the association of CH patterns with blood pressure and obesity, prospective longitudinal studies are warranted.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://winterwang.github.io/img/compo.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Mansukhani R, Palla L. Proc Nutr Soc. 2018 77(OCE1).&lt;/li&gt;
&lt;li&gt;Leech RM, Worsley A, Timperio A, McNaughton SA.  Int J Behav Nutr Phys Act. 2017 14(1):3.&lt;/li&gt;
&lt;li&gt;Finch H, Bolin J, Bolin J. Multilevel Modeling Using Mplus. Chapman and Hall/CRC; 2017&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>2018_10 todo</title>
      <link>https://winterwang.github.io/post/2018-10-todo/</link>
      <pubDate>Tue, 02 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://winterwang.github.io/post/2018-10-todo/</guid>
      <description>&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;2018-10-02 ~ 2018-10-17

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Prepare the proposal to JSPS.&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2018-10-02

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Background, key questions&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;objective, importance&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2018-10-09

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;&lt;a href=&#34;https://wangcc.me/LSHTMlearningnote/section-80.html&#34; target=&#34;_blank&#34;&gt;Bayesian statistics Chapter 1&lt;/a&gt; done;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2018-10-10

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;&lt;a href=&#34;https://wangcc.me/LSHTMlearningnote/MC-estimation.html&#34; target=&#34;_blank&#34;&gt;Bayesian statistics Chapter 2&lt;/a&gt; done;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2018-10-11~12

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;&lt;a href=&#34;https://wangcc.me/LSHTMlearningnote/-conjugate-priors.html&#34; target=&#34;_blank&#34;&gt;Bayesian statistics Chapter 3&lt;/a&gt; done;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2018-10-13

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Review of the paper about HIV;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2018-10-15

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Wait for the comments from LP, SA;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2018-10-16

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Modification of the proposal;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Submission of the proposal;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Learn how to apply for the data usage of Japan National Nutrional Survey;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2018-10-17~19

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;&lt;a href=&#34;https://wangcc.me/LSHTMlearningnote/mcmcbugs.html&#34; target=&#34;_blank&#34;&gt;Bayesian statistics Chapter 4&lt;/a&gt; done;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2018-10-23

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;&lt;a href=&#34;https://wangcc.me/LSHTMlearningnote/section-84.html&#34; target=&#34;_blank&#34;&gt;Bayesian statistics Chapter 5&lt;/a&gt; done;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2018-10-24

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Learn how to use Stan (just a beginning);&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2018-10-25

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;&lt;a href=&#34;https://github.com/winterwang/PAC_PAM&#34; target=&#34;_blank&#34;&gt;Some preliminary analysis&lt;/a&gt; using vague prior to do &lt;a href=&#34;http://www.sciencedirect.com/science/article/pii/S1341321X18300783&#34; target=&#34;_blank&#34;&gt;MabeRCT&lt;/a&gt;;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;&lt;a href=&#34;https://wangcc.me/LSHTMlearningnote/section-85.html&#34; target=&#34;_blank&#34;&gt;Bayesian statistics Chapter 6&lt;/a&gt; done;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Talk with LP AS, the next steps of writing the carb paper;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2018-10-26~31

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;Prepare poster for &lt;a href=&#34;https://www.bna.org.uk/mediacentre/events/chrononutrition-from-epidemiology-to-molecular-mechanism-symposium-london/&#34; target=&#34;_blank&#34;&gt;conference&lt;/a&gt;.&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;LCA analyses done stratified by men and women;&lt;/del&gt;&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
