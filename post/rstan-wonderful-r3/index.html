<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.55.6" />
  <meta name="author" content="王 超辰 (Chaochen Wang)">
  <meta name="description" content="Assistant Professor">

  
  <link rel="alternate" hreflang="en-us" href="https://winterwang.github.io/post/rstan-wonderful-r3/">

  
  


  

  
  
  
  
    
  
  
    
    
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
    
  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.1/css/academicons.min.css" integrity="sha512-NThgw3XKQ1absAahW6to7Ey42uycrVvfNfyjqcFNgCmOCQ5AR4AO0SiXrN+8ZtYeappp56lk1WtvjVmEa+VR6A==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.css" integrity="sha512-M2wvCLH6DSRazYeZRIm1JnYyh22purTM+FDB5CsyxtQJYeKq83arPe5wgbNmcFXGqiSH2XR8dT/fJISVA1r/zQ==" crossorigin="anonymous">
  
  
  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Playfair&#43;Display:400,700%7cFauna&#43;One">
  
  <link rel="stylesheet" href="/styles.css">
  

  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-21867861-1', 'auto');
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  

  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://winterwang.github.io/post/rstan-wonderful-r3/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="Be ambitious">
  <meta property="og:url" content="https://winterwang.github.io/post/rstan-wonderful-r3/">
  <meta property="og:title" content="Rstan Wonderful R-(3) | Be ambitious">
  <meta property="og:description" content=""><meta property="og:image" content="https://winterwang.github.io/img/052816_bayesian-opener_free.jpg">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2019-01-22T00:00:00&#43;00:00">
  
  <meta property="article:modified_time" content="2019-01-22T00:00:00&#43;00:00">
  

  

  <title>Rstan Wonderful R-(3) | Be ambitious</title>

</head>
<body id="top" data-spy="scroll" data-target="#toc" data-offset="71" >

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">Be ambitious</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      
      <ul class="nav navbar-nav navbar-right">
        

        

        
          
        

        <li class="nav-item">
          <a href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        

        
          
        

        <li class="nav-item">
          <a href="/#publications_selected">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        

        
          
        

        <li class="nav-item">
          <a href="/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        

        
          
        

        <li class="nav-item">
          <a href="/#projects">
            
            <span>Projects</span>
            
          </a>
        </li>

        
        

        

        
          
        

        <li class="nav-item">
          <a href="/#teaching">
            
            <span>Teaching</span>
            
          </a>
        </li>

        
        

        

        
          
        

        <li class="nav-item">
          <a href="/#slides">
            
            <span>Presentations/slides</span>
            
          </a>
        </li>

        
        

        

        
          
        

        <li class="nav-item">
          <a href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  
<div class="article-header">
  <img src="/img/052816_bayesian-opener_free.jpg" class="article-banner" itemprop="image">
  
</div>



  <div class="article-container">
    <div class="article-inner">
      <h1 itemprop="name">Rstan Wonderful R-(3)</h1>

      

<div class="article-metadata">

  <span class="article-date">
    
    <time datetime="2019-01-22 00:00:00 &#43;0000 UTC" itemprop="datePublished">
      Tue, Jan 22, 2019
    </time>
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    18 min read
  </span>
  

  
  
  <span class="middot-divider"></span>
  <a href="https://winterwang.github.io/post/rstan-wonderful-r3/#disqus_thread"></a>
  

  
  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fa fa-folder"></i>
    
    <a href="/categories/bayesian">Bayesian</a
    >, 
    
    <a href="/categories/r-techniques">R techniques</a
    >, 
    
    <a href="/categories/statistics">statistics</a
    >
    
  </span>
  
  

  
  
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=Rstan%20Wonderful%20R-%283%29&amp;url=https%3a%2f%2fwinterwang.github.io%2fpost%2frstan-wonderful-r3%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fwinterwang.github.io%2fpost%2frstan-wonderful-r3%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-facebook"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fwinterwang.github.io%2fpost%2frstan-wonderful-r3%2f&amp;title=Rstan%20Wonderful%20R-%283%29"
         target="_blank" rel="noopener">
        <i class="fa fa-linkedin"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=https%3a%2f%2fwinterwang.github.io%2fpost%2frstan-wonderful-r3%2f&amp;title=Rstan%20Wonderful%20R-%283%29"
         target="_blank" rel="noopener">
        <i class="fa fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=Rstan%20Wonderful%20R-%283%29&amp;body=https%3a%2f%2fwinterwang.github.io%2fpost%2frstan-wonderful-r3%2f">
        <i class="fa fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>


      <div class="article-style" itemprop="articleBody">
        

<div id="TOC">
<ul>
<li><a href="#-multiple-regression">多重回歸 multiple regression</a><ul>
<li><a href="#step-1.-">Step 1. 確認數據分佈</a></li>
<li><a href="#step-2.-">Step 2. 寫下數學模型</a></li>
<li><a href="#step-3.-">Step 3. 看圖確認模型擬合狀況</a></li>
<li><a href="#step-4.-mcmc-">Step 4. MCMC 樣本的散點圖矩陣</a></li>
</ul></li>
</ul>
</div>

<div id="-multiple-regression" class="section level1">
<h1>多重回歸 multiple regression</h1>
<p>本章使用的數據，大學生出勤記錄也是<a href="https://raw.githubusercontent.com/winterwang/RStanBook/master/chap05/input/data-attendance-1.txt">架空的數據</a>。</p>
<p>有大學記錄了50名大學生的出勤狀況：</p>
<pre><code>A,Score,Y
0,69,0.286
1,145,0.196
0,125,0.261
1,86,0.109
1,158,0.23
0,133,0.35
0,111,0.33
1,147,0.194
0,146,0.413
0,145,0.36
1,141,0.225
0,137,0.423
1,118,0.186
0,111,0.287
...
0,99,0.268
1,99,0.234</code></pre>
<p>其中，</p>
<ul>
<li><span class="math inline">\(A\)</span>: 是學生大學二年級時進行的問卷調查時回答是否喜歡打零工的結果（0:不喜歡打工；1:喜歡打工）</li>
<li><span class="math inline">\(Score\)</span>: 是大學二年級時進行的問卷調查時計算的該學生對學習是否感興趣的數值評分(200分滿分，分數越高，該學生越熱愛學習)</li>
<li><span class="math inline">\(Y\)</span>: 是該學生一年內的出勤率</li>
</ul>
<p>在本次分析範例中，把<span class="math inline">\(Y\)</span>出勤率當作是連續型結果變量，我們來用Stan實施多重回歸分析，回答學生喜歡打零工與否，和學生對學習的熱情程度兩個變量能解釋多少出勤率。</p>
<div id="step-1.-" class="section level2">
<h2>Step 1. 確認數據分佈</h2>
<pre class="r"><code># The following figure codes come from the authors website: 
# https://github.com/MatsuuraKentaro/RStanBook/blob/master/chap05/fig5-1.R
library(ggplot2)
library(GGally)

set.seed(123)
d &lt;- read.csv(file=&#39;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap05/input/data-attendance-1.txt&#39;, header = T)
d$A &lt;- as.factor(d$A)

N_col &lt;- ncol(d)
ggp &lt;- ggpairs(d, upper=&#39;blank&#39;, diag=&#39;blank&#39;, lower=&#39;blank&#39;)

for(i in 1:N_col) {
  x &lt;- d[,i]
  p &lt;- ggplot(data.frame(x, A=d$A), aes(x))
  p &lt;- p + theme_bw(base_size=14)
  p &lt;- p + theme(axis.text.x=element_text(angle=40, vjust=1, hjust=1))
  if (class(x) == &#39;factor&#39;) {
    p &lt;- p + geom_bar(aes(fill=A), color=&#39;grey5&#39;)
  } else {
    bw &lt;- (max(x)-min(x))/10
    p &lt;- p + geom_histogram(binwidth=bw, aes(fill=A), color=&#39;grey5&#39;) #繪製柱狀圖
    p &lt;- p + geom_line(eval(bquote(aes(y=..count..*.(bw)))), stat=&#39;density&#39;) #添加概率密度曲線
  }
  p &lt;- p + geom_label(data=data.frame(x=-Inf, y=Inf, label=colnames(d)[i]), aes(x=x, y=y, label=label), hjust=0, vjust=1)
  p &lt;- p + scale_fill_manual(values=alpha(c(&#39;white&#39;, &#39;grey40&#39;), 0.5))
  ggp &lt;- putPlot(ggp, p, i, i)
}

zcolat &lt;- seq(-1, 1, length=81)
zcolre &lt;- c(zcolat[1:40]+1, rev(zcolat[41:81]))

for(i in 1:(N_col-1)) {
  for(j in (i+1):N_col) {
    x &lt;- as.numeric(d[,i])
    y &lt;- as.numeric(d[,j])
    r &lt;- cor(x, y, method=&#39;spearman&#39;, use=&#39;pairwise.complete.obs&#39;)
    zcol &lt;- lattice::level.colors(r, at=zcolat, col.regions=grey(zcolre))
    textcol &lt;- ifelse(abs(r) &lt; 0.4, &#39;grey20&#39;, &#39;white&#39;)
    ell &lt;- ellipse::ellipse(r, level=0.95, type=&#39;l&#39;, npoints=50, scale=c(.2, .2), centre=c(.5, .5))
    p &lt;- ggplot(data.frame(ell), aes(x=x, y=y))
    p &lt;- p + theme_bw() + theme(
      plot.background=element_blank(),
      panel.grid.major=element_blank(), panel.grid.minor=element_blank(),
      panel.border=element_blank(), axis.ticks=element_blank()
    )
    p &lt;- p + geom_polygon(fill=zcol, color=zcol)
    p &lt;- p + geom_text(data=NULL, x=.5, y=.5, label=100*round(r, 2), size=6, col=textcol)
    ggp &lt;- putPlot(ggp, p, i, j)
  }
}

for(j in 1:(N_col-1)) {
  for(i in (j+1):N_col) {
    x &lt;- d[,j]
    y &lt;- d[,i]
    p &lt;- ggplot(data.frame(x, y, gr=d$A), aes(x=x, y=y, fill=gr, shape=gr))
    p &lt;- p + theme_bw(base_size=14)
    p &lt;- p + theme(axis.text.x=element_text(angle=40, vjust=1, hjust=1))
    if (class(x) == &#39;factor&#39;) {
      p &lt;- p + geom_boxplot(aes(group=x), alpha=3/6, outlier.size=0, fill=&#39;white&#39;)
      p &lt;- p + geom_point(position=position_jitter(w=0.4, h=0), size=2)
    } else {
      p &lt;- p + geom_point(size=2)
    }
    p &lt;- p + scale_shape_manual(values=c(21, 24))
    p &lt;- p + scale_fill_manual(values=alpha(c(&#39;white&#39;, &#39;grey40&#39;), 0.5))
    ggp &lt;- putPlot(ggp, p, i, j)
  }
}

ggp</code></pre>
<div class="figure" style="text-align: center"><span id="fig:step1"></span>
<img src="/post/2019-01-22-rstan-wonderful-r-multi-lm_files/figure-html/step1-1.png" alt="三個變量的分佈觀察圖，對角線上是三個變量各自的柱狀圖 (histogram) 和計算獲得的概率密度函數曲線；左下角三個圖是三個變量的箱式圖和散點圖；右上角三個圖是三個變量兩兩計算獲得的 Spearman 秩相關乘以100之後的數值。對角線上及左下角三個圖中數據點和形狀的不同分別表示學生喜歡(三角形)和不喜歡(圓形)打工。右上角表示秩相關的數值越接近0，顏色越白圖形越接近圓形，相關係數的絕對值越接近1，則顏色越深，橢圓越細長。" width="80%" />
<p class="caption">
Figure 1: 三個變量的分佈觀察圖，對角線上是三個變量各自的柱狀圖 (histogram) 和計算獲得的概率密度函數曲線；左下角三個圖是三個變量的箱式圖和散點圖；右上角三個圖是三個變量兩兩計算獲得的 Spearman 秩相關乘以100之後的數值。對角線上及左下角三個圖中數據點和形狀的不同分別表示學生喜歡(三角形)和不喜歡(圓形)打工。右上角表示秩相關的數值越接近0，顏色越白圖形越接近圓形，相關係數的絕對值越接近1，則顏色越深，橢圓越細長。
</p>
</div>
<pre class="r"><code># png(file=&#39;output/fig5-1.png&#39;, w=1600, h=1600, res=300)
# print(ggp, left=0.3, bottom=0.3)
# dev.off()</code></pre>
</div>
<div id="step-2.-" class="section level2">
<h2>Step 2. 寫下數學模型</h2>
<p>Model can be written as (Model5-1):</p>
<p><span class="math display">\[
\begin{array}{l}
Y[n]        = b_1 + b_2A[n] + b_3Sore[n] + \varepsilon [n]&amp;  n = 1,2,\dots,N \\
\varepsilon[n] \sim \text{Normal}(0, \sigma) &amp; n = 1,2,\dots,N \\ 
\end{array}
\]</span></p>
<p>其中，</p>
<ul>
<li><span class="math inline">\(N\)</span> 表示學生的人數，<span class="math inline">\(n\)</span>則是學生編號的下標；</li>
<li><span class="math inline">\(b_1\)</span> 是回歸直線的截距；</li>
<li><span class="math inline">\(b_2\)</span> 是<span class="math inline">\(Score\)</span>保持不變時，<span class="math inline">\(A\)</span>從<span class="math inline">\(0\rightarrow 1\)</span>時出勤率的變化(增加，或者減少)；</li>
<li><span class="math inline">\(b_3\)</span> 是<span class="math inline">\(A\)</span>保持不變時，<span class="math inline">\(Score\)</span>增加一個單位時出勤率的變化(增加，或者減少)。</li>
</ul>
<p>Model can also be written as (Model5-2):</p>
<p><span class="math display">\[
\begin{array}{l}
Y[n]       \sim \text{Normal}(b_1 + b_2A[n] + b_3Score[n], \sigma) &amp;  n = 1,2,\dots,N \\
\end{array}
\]</span></p>
<p>如果認爲<span class="math inline">\(A\)</span>和<span class="math inline">\(Score\)</span>所能預測的出勤率有一個基礎的均值 <span class="math inline">\(\mu[n]\)</span>，剩下的每名學生的出勤率服從這個均值和標準差爲 <span class="math inline">\(\sigma\)</span> 的正態分佈，那麼模型又可以繼續改寫成爲下面的 Model 5-3:</p>
<p><span class="math display">\[
\begin{array}{l}
\mu[n]        = b_1 + b_2A[n] + b_3Sore[n] &amp;  n = 1,2,\dots,N \\
Y[n] \sim \text{Normal}(\mu[n], \sigma) &amp; n = 1,2,\dots,N \\ 
\end{array}
\]</span></p>
<p>下面的 Stan 模型是按照 Model 5-3 寫的，它的模型參數有四個，<span class="math inline">\(b_1, b_2, b_3, \sigma\)</span>，<span class="math inline">\(\mu[n]\)</span>通過 <code>transformed parameter</code> 計算獲得:</p>
<pre><code>data {
  int N; 
  int&lt;lower=0, upper=1&gt; A[N];
  real&lt;lower=0, upper=1&gt; Score[N];
  real&lt;lower=0, upper=1&gt; Y[N];
}

parameters {
  real b1; 
  real b2;
  real b3;
  real&lt;lower=0&gt; sigma;
}

transformed parameters {
  real mu[N];
  for (n in 1:N) {
    mu[n] = b1 + b2*A[n] + b3*Score[n];
  }
}

model {
  for (n in 1:N) {
    Y[n] ~ normal(mu[n], sigma);
  }
}

generated quantities {
  real y_pred[N];
  for (n in 1:N) {
    y_pred[n] = normal_rng(mu[n], sigma);
  }
}
</code></pre>
<p>下面的 R 代碼用來實現對上面 Stan 模型的擬合:</p>
<pre class="r"><code>library(rstan)
d &lt;- read.csv(file=&#39;https://raw.githubusercontent.com/winterwang/RStanBook/master/chap05/input/data-attendance-1.txt&#39;, header = T)
data &lt;- list(N=nrow(d), A=d$A, Score=d$Score/200, Y=d$Y)
fit &lt;- stan(file=&#39;stanfiles/model5-3.stan&#39;, data=data, seed=1234)</code></pre>
<pre><code>## 
## SAMPLING FOR MODEL &#39;model5-3&#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 3.7e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.37 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.134662 seconds (Warm-up)
## Chain 1:                0.131457 seconds (Sampling)
## Chain 1:                0.266119 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &#39;model5-3&#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 7e-06 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.140699 seconds (Warm-up)
## Chain 2:                0.155449 seconds (Sampling)
## Chain 2:                0.296148 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &#39;model5-3&#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 7e-06 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.133523 seconds (Warm-up)
## Chain 3:                0.131937 seconds (Sampling)
## Chain 3:                0.26546 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &#39;model5-3&#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 7e-06 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.127764 seconds (Warm-up)
## Chain 4:                0.118133 seconds (Sampling)
## Chain 4:                0.245897 seconds (Total)
## Chain 4:</code></pre>
<pre class="r"><code>fit</code></pre>
<pre><code>## Inference for Stan model: model5-3.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##              mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff
## b1           0.12    0.00 0.03   0.06   0.10   0.12   0.15   0.19  1844
## b2          -0.14    0.00 0.01  -0.17  -0.15  -0.14  -0.13  -0.11  2973
## b3           0.32    0.00 0.05   0.22   0.29   0.32   0.36   0.43  1880
## sigma        0.05    0.00 0.01   0.04   0.05   0.05   0.05   0.06  2478
## mu[1]        0.24    0.00 0.02   0.20   0.22   0.24   0.25   0.27  1996
## mu[2]        0.22    0.00 0.01   0.19   0.21   0.22   0.22   0.24  2809
## mu[3]        0.33    0.00 0.01   0.31   0.32   0.33   0.33   0.34  3531
## mu[4]        0.12    0.00 0.02   0.09   0.11   0.12   0.13   0.15  2784
## mu[5]        0.24    0.00 0.01   0.21   0.23   0.24   0.25   0.27  2468
## mu[6]        0.34    0.00 0.01   0.32   0.33   0.34   0.35   0.36  3463
## mu[7]        0.30    0.00 0.01   0.29   0.30   0.30   0.31   0.32  3039
## mu[8]        0.22    0.00 0.01   0.19   0.21   0.22   0.23   0.24  2747
## mu[9]        0.36    0.00 0.01   0.34   0.35   0.36   0.37   0.38  3046
## mu[10]       0.36    0.00 0.01   0.34   0.35   0.36   0.37   0.38  3080
## mu[11]       0.21    0.00 0.01   0.18   0.20   0.21   0.22   0.23  2942
## mu[12]       0.35    0.00 0.01   0.33   0.34   0.35   0.35   0.37  3351
## mu[13]       0.17    0.00 0.01   0.15   0.16   0.17   0.18   0.19  3511
## mu[14]       0.30    0.00 0.01   0.29   0.30   0.30   0.31   0.32  3039
## mu[15]       0.30    0.00 0.01   0.28   0.29   0.30   0.31   0.32  2943
## mu[16]       0.14    0.00 0.01   0.11   0.13   0.14   0.15   0.17  3138
## mu[17]       0.31    0.00 0.01   0.29   0.30   0.31   0.31   0.33  3183
## mu[18]       0.26    0.00 0.01   0.23   0.25   0.26   0.27   0.28  2136
## mu[19]       0.42    0.00 0.02   0.39   0.41   0.42   0.44   0.46  2308
## mu[20]       0.23    0.00 0.01   0.20   0.22   0.23   0.24   0.26  2511
## mu[21]       0.12    0.00 0.02   0.09   0.11   0.12   0.13   0.15  2784
## mu[22]       0.16    0.00 0.01   0.13   0.15   0.15   0.16   0.18  3387
## mu[23]       0.15    0.00 0.01   0.13   0.14   0.15   0.16   0.18  3337
## mu[24]       0.21    0.00 0.01   0.19   0.20   0.21   0.22   0.24  2874
## mu[25]       0.17    0.00 0.01   0.15   0.16   0.17   0.18   0.19  3508
## mu[26]       0.19    0.00 0.01   0.16   0.18   0.19   0.20   0.21  3373
## mu[27]       0.32    0.00 0.01   0.30   0.31   0.32   0.32   0.33  3393
## mu[28]       0.32    0.00 0.01   0.30   0.31   0.32   0.32   0.33  3393
## mu[29]       0.38    0.00 0.01   0.36   0.38   0.38   0.39   0.41  2639
## mu[30]       0.31    0.00 0.01   0.29   0.30   0.31   0.31   0.33  3135
## mu[31]       0.25    0.00 0.02   0.22   0.24   0.25   0.26   0.28  2340
## mu[32]       0.10    0.00 0.02   0.06   0.09   0.10   0.11   0.13  2546
## mu[33]       0.20    0.00 0.01   0.18   0.20   0.20   0.21   0.23  3046
## mu[34]       0.18    0.00 0.01   0.16   0.17   0.18   0.19   0.20  3463
## mu[35]       0.33    0.00 0.01   0.31   0.32   0.33   0.33   0.35  3538
## mu[36]       0.34    0.00 0.01   0.32   0.33   0.34   0.34   0.35  3504
## mu[37]       0.15    0.00 0.01   0.13   0.14   0.15   0.16   0.17  3310
## mu[38]       0.30    0.00 0.01   0.28   0.30   0.30   0.31   0.32  2991
## mu[39]       0.27    0.00 0.01   0.24   0.26   0.27   0.28   0.29  2259
## mu[40]       0.27    0.00 0.01   0.24   0.26   0.27   0.27   0.29  2239
## mu[41]       0.33    0.00 0.01   0.31   0.33   0.33   0.34   0.35  3530
## mu[42]       0.34    0.00 0.01   0.32   0.33   0.34   0.35   0.36  3463
## mu[43]       0.32    0.00 0.01   0.30   0.31   0.32   0.33   0.34  3482
## mu[44]       0.36    0.00 0.01   0.34   0.36   0.36   0.37   0.39  2981
## mu[45]       0.42    0.00 0.02   0.38   0.41   0.42   0.43   0.46  2336
## mu[46]       0.29    0.00 0.01   0.27   0.29   0.29   0.30   0.31  2760
## mu[47]       0.21    0.00 0.02   0.17   0.19   0.21   0.22   0.24  1907
## mu[48]       0.37    0.00 0.01   0.34   0.36   0.37   0.38   0.39  2919
## mu[49]       0.28    0.00 0.01   0.26   0.28   0.28   0.29   0.30  2527
## mu[50]       0.14    0.00 0.01   0.11   0.13   0.14   0.15   0.17  3138
## y_pred[1]    0.24    0.00 0.06   0.13   0.20   0.24   0.27   0.34  3503
## y_pred[2]    0.21    0.00 0.05   0.11   0.18   0.21   0.25   0.32  3779
## y_pred[3]    0.33    0.00 0.05   0.22   0.29   0.33   0.36   0.43  3944
## y_pred[4]    0.12    0.00 0.05   0.01   0.08   0.12   0.16   0.23  3762
## y_pred[5]    0.24    0.00 0.05   0.13   0.20   0.24   0.27   0.34  4038
## y_pred[6]    0.34    0.00 0.05   0.24   0.30   0.34   0.37   0.44  3457
## y_pred[7]    0.30    0.00 0.05   0.20   0.27   0.30   0.34   0.41  4030
## y_pred[8]    0.22    0.00 0.05   0.11   0.18   0.22   0.26   0.33  3217
## y_pred[9]    0.36    0.00 0.05   0.26   0.33   0.36   0.40   0.47  3869
## y_pred[10]   0.36    0.00 0.05   0.26   0.32   0.36   0.39   0.46  3884
## y_pred[11]   0.21    0.00 0.05   0.11   0.17   0.21   0.25   0.31  3871
## y_pred[12]   0.34    0.00 0.05   0.24   0.31   0.34   0.38   0.45  3866
## y_pred[13]   0.17    0.00 0.05   0.07   0.13   0.17   0.21   0.27  4064
## y_pred[14]   0.30    0.00 0.05   0.20   0.27   0.30   0.34   0.41  3869
## y_pred[15]   0.30    0.00 0.05   0.20   0.26   0.30   0.33   0.40  3929
## y_pred[16]   0.14    0.00 0.05   0.03   0.11   0.14   0.18   0.25  3900
## y_pred[17]   0.31    0.00 0.05   0.21   0.27   0.31   0.35   0.41  4048
## y_pred[18]   0.26    0.00 0.05   0.15   0.22   0.26   0.29   0.36  3705
## y_pred[19]   0.42    0.00 0.05   0.32   0.39   0.42   0.46   0.53  3474
## y_pred[20]   0.23    0.00 0.05   0.13   0.20   0.23   0.27   0.34  3792
## y_pred[21]   0.12    0.00 0.05   0.01   0.08   0.12   0.15   0.23  3581
## y_pred[22]   0.16    0.00 0.05   0.05   0.12   0.16   0.19   0.26  4075
## y_pred[23]   0.15    0.00 0.05   0.04   0.12   0.15   0.19   0.26  3975
## y_pred[24]   0.21    0.00 0.05   0.11   0.18   0.21   0.25   0.32  3489
## y_pred[25]   0.17    0.00 0.05   0.06   0.14   0.17   0.20   0.27  3938
## y_pred[26]   0.19    0.00 0.05   0.08   0.15   0.19   0.22   0.29  3955
## y_pred[27]   0.32    0.00 0.05   0.21   0.28   0.32   0.35   0.42  4075
## y_pred[28]   0.32    0.00 0.05   0.21   0.28   0.32   0.35   0.42  4110
## y_pred[29]   0.38    0.00 0.05   0.28   0.35   0.38   0.42   0.49  3631
## y_pred[30]   0.31    0.00 0.05   0.20   0.27   0.31   0.34   0.41  3865
## y_pred[31]   0.25    0.00 0.05   0.14   0.21   0.25   0.28   0.35  3818
## y_pred[32]   0.10    0.00 0.05  -0.01   0.06   0.10   0.14   0.20  3857
## y_pred[33]   0.20    0.00 0.05   0.10   0.17   0.20   0.24   0.31  3304
## y_pred[34]   0.18    0.00 0.05   0.08   0.14   0.18   0.22   0.28  3946
## y_pred[35]   0.33    0.00 0.05   0.22   0.29   0.33   0.36   0.43  4182
## y_pred[36]   0.34    0.00 0.05   0.23   0.30   0.34   0.37   0.44  4022
## y_pred[37]   0.15    0.00 0.05   0.05   0.12   0.15   0.19   0.25  3719
## y_pred[38]   0.30    0.00 0.05   0.20   0.27   0.30   0.34   0.41  3655
## y_pred[39]   0.27    0.00 0.05   0.16   0.23   0.27   0.30   0.37  3522
## y_pred[40]   0.27    0.00 0.05   0.16   0.23   0.27   0.30   0.37  3742
## y_pred[41]   0.33    0.00 0.05   0.23   0.30   0.33   0.37   0.44  3979
## y_pred[42]   0.34    0.00 0.05   0.24   0.30   0.34   0.37   0.44  3914
## y_pred[43]   0.32    0.00 0.05   0.22   0.29   0.32   0.36   0.43  4005
## y_pred[44]   0.36    0.00 0.05   0.26   0.33   0.36   0.40   0.47  3818
## y_pred[45]   0.42    0.00 0.05   0.31   0.38   0.42   0.45   0.53  3443
## y_pred[46]   0.29    0.00 0.05   0.19   0.26   0.29   0.33   0.40  3839
## y_pred[47]   0.21    0.00 0.06   0.09   0.17   0.21   0.24   0.31  3611
## y_pred[48]   0.37    0.00 0.05   0.26   0.33   0.37   0.40   0.47  3784
## y_pred[49]   0.28    0.00 0.05   0.18   0.25   0.28   0.32   0.39  3745
## y_pred[50]   0.14    0.00 0.05   0.03   0.10   0.14   0.17   0.24  3701
## lp__       120.94    0.04 1.39 117.48 120.23 121.24 121.96 122.70  1407
##            Rhat
## b1            1
## b2            1
## b3            1
## sigma         1
## mu[1]         1
## mu[2]         1
## mu[3]         1
## mu[4]         1
## mu[5]         1
## mu[6]         1
## mu[7]         1
## mu[8]         1
## mu[9]         1
## mu[10]        1
## mu[11]        1
## mu[12]        1
## mu[13]        1
## mu[14]        1
## mu[15]        1
## mu[16]        1
## mu[17]        1
## mu[18]        1
## mu[19]        1
## mu[20]        1
## mu[21]        1
## mu[22]        1
## mu[23]        1
## mu[24]        1
## mu[25]        1
## mu[26]        1
## mu[27]        1
## mu[28]        1
## mu[29]        1
## mu[30]        1
## mu[31]        1
## mu[32]        1
## mu[33]        1
## mu[34]        1
## mu[35]        1
## mu[36]        1
## mu[37]        1
## mu[38]        1
## mu[39]        1
## mu[40]        1
## mu[41]        1
## mu[42]        1
## mu[43]        1
## mu[44]        1
## mu[45]        1
## mu[46]        1
## mu[47]        1
## mu[48]        1
## mu[49]        1
## mu[50]        1
## y_pred[1]     1
## y_pred[2]     1
## y_pred[3]     1
## y_pred[4]     1
## y_pred[5]     1
## y_pred[6]     1
## y_pred[7]     1
## y_pred[8]     1
## y_pred[9]     1
## y_pred[10]    1
## y_pred[11]    1
## y_pred[12]    1
## y_pred[13]    1
## y_pred[14]    1
## y_pred[15]    1
## y_pred[16]    1
## y_pred[17]    1
## y_pred[18]    1
## y_pred[19]    1
## y_pred[20]    1
## y_pred[21]    1
## y_pred[22]    1
## y_pred[23]    1
## y_pred[24]    1
## y_pred[25]    1
## y_pred[26]    1
## y_pred[27]    1
## y_pred[28]    1
## y_pred[29]    1
## y_pred[30]    1
## y_pred[31]    1
## y_pred[32]    1
## y_pred[33]    1
## y_pred[34]    1
## y_pred[35]    1
## y_pred[36]    1
## y_pred[37]    1
## y_pred[38]    1
## y_pred[39]    1
## y_pred[40]    1
## y_pred[41]    1
## y_pred[42]    1
## y_pred[43]    1
## y_pred[44]    1
## y_pred[45]    1
## y_pred[46]    1
## y_pred[47]    1
## y_pred[48]    1
## y_pred[49]    1
## y_pred[50]    1
## lp__          1
## 
## Samples were drawn using NUTS(diag_e) at Thu Jan 24 00:02:26 2019.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).</code></pre>
<p>上述代碼中值得注意的是我們對 <span class="math inline">\(Score\)</span> 進行了全部除以 <span class="math inline">\(200\)</span> 的數據縮放調整 (scaling)。這樣有助於我們的模型在進行 MCMC 計算時加速其達到收斂時所需要的時間。</p>
<p>把計算獲得的事後模型參數平均值代入模型 Model 5-3:</p>
<p><span class="math display">\[
\begin{array}{l}
\mu[n]        = 0.12 - 0.14A[n] + 0.32Sore[n] &amp;  n = 1,2,\dots,N \\
Y[n] \sim \text{Normal}(\mu[n], 0.05) &amp; n = 1,2,\dots,N \\ 
\end{array}
\]</span></p>
<p>從輸出的結果報告來看，所有的 <code>Rhat</code> 都小於1.1，可以認爲採樣已經達到收斂效果，再來確認一下軌跡圖：</p>
<pre class="r"><code>library(bayesplot)

color_scheme_set(&quot;mix-brightblue-gray&quot;)

posterior2 &lt;- rstan::extract(fit, inc_warmup = TRUE, permuted = FALSE)

p &lt;- mcmc_trace(posterior2, n_warmup = 0, pars = c(&quot;b1&quot;, &quot;b2&quot;, &quot;b3&quot;, &quot;sigma&quot;, &quot;lp__&quot;),
                facet_args = list(nrow = 2, labeller = label_parsed))
p</code></pre>
<div class="figure" style="text-align: center"><span id="fig:step53"></span>
<img src="/post/2019-01-22-rstan-wonderful-r-multi-lm_files/figure-html/step53-1.png" alt="用 bayesplot包數繪製的模型5-3的MCMC鏈式軌跡圖 (trace plot)。" width="80%" />
<p class="caption">
Figure 2: 用 bayesplot包數繪製的模型5-3的MCMC鏈式軌跡圖 (trace plot)。
</p>
</div>
<p>收斂效果很不錯，下面來解釋回歸係數的事後均值的涵義：</p>
<ul>
<li><code>b3</code>的事後均值是<span class="math inline">\(0.32\)</span>，所以，<span class="math inline">\(Score=150\)</span>和<span class="math inline">\(Score=50\)</span>的兩名學生，當他們同時都是喜歡或者同時都不喜歡打工時，<span class="math inline">\(Score = 150\)</span>的學生的出勤率平均比 <span class="math inline">\(Score = 50\)</span> 的學生的出勤率高 <span class="math inline">\(0.32 \times (150-50)/200 = 0.16\)</span>。</li>
<li><code>b2</code>的事後均值是<span class="math inline">\(-0.14\)</span>，所以，同樣地，<span class="math inline">\(Score\)</span>相同的兩名學生，喜歡打工的學生比不喜歡打工的學生出勤率平均要低 <span class="math inline">\(0.14\)</span>。</li>
</ul>
</div>
<div id="step-3.-" class="section level2">
<h2>Step 3. 看圖確認模型擬合狀況</h2>
<p>下圖繪製了上面貝葉斯多重線性回歸模型計算獲得的事後貝葉斯預測區間，和觀測值<span class="math inline">\(Y\)</span>出勤率之間的直觀關係：</p>
<pre class="r"><code>source(&quot;commonRstan.R&quot;)

ms &lt;- rstan::extract(fit)

Score_new &lt;- 50:200
N_X &lt;- length(Score_new)
N_mcmc &lt;- length(ms$lp__)

set.seed(1234)
y_base_mcmc &lt;- as.data.frame(matrix(nrow=N_mcmc, ncol=N_X))
y_base_a0_mcmc &lt;- as.data.frame(matrix(nrow = N_mcmc, ncol = N_X))
y_mcmc &lt;- as.data.frame(matrix(nrow=N_mcmc, ncol=N_X))
y_a0_mcmc &lt;- as.data.frame(matrix(nrow=N_mcmc, ncol=N_X))

for (i in 1:N_X) {
  y_base_mcmc[,i] &lt;- ms$b1 + ms$b2 + ms$b3 * Score_new[i]/200
  y_base_a0_mcmc[] &lt;- ms$b1 + ms$b2*0 + ms$b3 * Score_new[i]/200
  y_mcmc[,i] &lt;- rnorm(n=N_mcmc, mean=y_base_mcmc[,i], sd=ms$sigma)
  y_a0_mcmc[,i] &lt;- rnorm(n=N_mcmc, mean=y_base_a0_mcmc[,i], sd=ms$sigma)
}

customize.ggplot.axis &lt;- function(p) {
  p &lt;- p + labs(x=&#39;Score&#39;, y=&#39;Y&#39;)
  p &lt;- p + scale_y_continuous(breaks=seq(from=-0.2, to=0.8, by=0.2))
  p &lt;- p + coord_cartesian(xlim=c(50, 200), ylim=c(-0.2, 0.6))
  return(p)
}

d_est &lt;- data.frame.quantile.mcmc(x=Score_new, y_mcmc=y_mcmc)
d_esta0 &lt;- data.frame.quantile.mcmc(x=Score_new, y_mcmc=y_a0_mcmc)
# p &lt;- ggplot.5quantile(data=d_est)
# p2 &lt;- ggplot.5quantile(data = d_esta0)
# p &lt;- p + geom_point(data=d[d$A==1, ], aes(x=Score, y=Y), shape=24, size=5)
# p2 &lt;- p2 + geom_point(data=d[d$A==0, ], aes(x=Score, y=Y), shape=1, size=5)
# p &lt;- customize.ggplot.axis(p)
# p2 &lt;- customize.ggplot.axis(p2)

visuals = rbind(d_est,d_esta0)
visuals$A=c(rep(1,151),rep(0,151)) # 151 points of each flavour

qn &lt;- colnames(visuals)[-1]
p &lt;- ggplot(data=visuals, aes(x=X, y=p50, group = A))
p &lt;- p + my_theme()
p &lt;- p + geom_ribbon(aes_string(ymin=qn[1], ymax=qn[5]), fill=&#39;black&#39;, alpha=1/6)
p &lt;- p + geom_ribbon(aes_string(ymin=qn[2], ymax=qn[4]), fill=&#39;black&#39;, alpha=2/6)
p &lt;- p + geom_line(size=1)
p &lt;- p + geom_point(data=d[d$A==1, ], aes(x=Score, y=Y), shape=24, size=5)
p &lt;- p + geom_point(data=d[d$A==0, ], aes(x=Score, y=Y), shape=20, size=5)
p &lt;- customize.ggplot.axis(p)
p</code></pre>
<div class="figure" style="text-align: center"><span id="fig:fig52"></span>
<img src="/post/2019-01-22-rstan-wonderful-r-multi-lm_files/figure-html/fig52-1.png" alt="黑色原點(不喜歡打工)，和無色三角形(喜歡打工)的學生的出勤率，和模型計算獲得的貝葉斯事後預測區間。黑色線是中位數，灰色帶是50%預測區間和95%預測區間。" width="80%" />
<p class="caption">
Figure 3: 黑色原點(不喜歡打工)，和無色三角形(喜歡打工)的學生的出勤率，和模型計算獲得的貝葉斯事後預測區間。黑色線是中位數，灰色帶是50%預測區間和95%預測區間。
</p>
</div>
<p>上述觀察預測值區間和實際觀測之間的關係的視覺化圖形，在多重線性回歸模型只有兩個預測變量的事後還較爲容易獲得，當模型中有三個或以上的預測變量時，可視化變得困難重重。</p>
<p>此時我們推薦繪製“實際觀測值和預測值”，以及模型給出的每個預測值的隨機誤差<span class="math inline">\(\varepsilon\)</span>分佈範圍，相結合的圖形來判斷模型擬合程度。</p>
<pre class="r"><code>d_qua &lt;- t(apply(ms$y_pred, 2, quantile, prob=c(0.1, 0.5, 0.9)))
colnames(d_qua) &lt;- c(&#39;p10&#39;, &#39;p50&#39;, &#39;p90&#39;)
d_qua &lt;- data.frame(d, d_qua)
d_qua$A &lt;- as.factor(d_qua$A)

p &lt;- ggplot(data=d_qua, aes(x=Y, y=p50, ymin=p10, ymax=p90, shape=A, fill=A))
p &lt;- p + theme_bw(base_size=18) + theme(legend.key.height=grid::unit(2.5,&#39;line&#39;))
p &lt;- p + coord_fixed(ratio=1, xlim=c(0, 0.5), ylim=c(0, 0.5))
p &lt;- p + geom_pointrange(size=0.8, color=&#39;grey5&#39;)
p &lt;- p + geom_abline(aes(slope=1, intercept=0), color=&#39;black&#39;, alpha=3/5, linetype=&#39;31&#39;)
p &lt;- p + scale_shape_manual(values=c(21, 24))
p &lt;- p + scale_fill_manual(values=c(&#39;white&#39;, &#39;grey70&#39;))
p &lt;- p + labs(x=&#39;Observed&#39;, y=&#39;Predicted&#39;)
p &lt;- p + scale_x_continuous(breaks=seq(from=0, to=0.5, by=0.1))
p &lt;- p + scale_y_continuous(breaks=seq(from=0, to=0.5, by=0.1))
p</code></pre>
<div class="figure" style="text-align: center"><span id="fig:fig53"></span>
<img src="/post/2019-01-22-rstan-wonderful-r-multi-lm_files/figure-html/fig53-1.png" alt="觀測值(x)，和預測值(y)的散點圖，以及預測值的80%預測區間。" width="80%" />
<p class="caption">
Figure 4: 觀測值(x)，和預測值(y)的散點圖，以及預測值的80%預測區間。
</p>
</div>
<p>從上圖中可以看出，大多數的觀測點和預測點以及預測的80%區間基本都在 <span class="math inline">\(y = x\)</span> 這條對角線上。大致可以認爲本次貝葉斯多重線性回歸擬合效果尚且能夠接受。</p>
<p>隨機誤差 <span class="math inline">\(\varepsilon[n]\)</span> 被認爲服從 <span class="math inline">\(\text{Normal}(0, \sigma)\)</span> 的正態分佈。從模型中可以計算獲得每個學生出勤率的預測值和實際觀測值之間的差，這就是隨機誤差。貝葉斯框架之下，我們實際獲得的會是每名學生隨機誤差的分佈：</p>
<pre class="r"><code>N_mcmc &lt;- length(ms$lp__)

d_noise &lt;- data.frame(t(-t(ms$mu) + d$Y))
colnames(d_noise) &lt;- paste0(&#39;noise&#39;, 1:nrow(d))
d_est &lt;- data.frame(mcmc=1:N_mcmc, d_noise)
d_melt &lt;- reshape2::melt(d_est, id=c(&#39;mcmc&#39;), variable.name=&#39;X&#39;)

d_mode &lt;- data.frame(t(apply(d_noise, 2, function(x) {
  dens &lt;- density(x)
  mode_i &lt;- which.max(dens$y)
  mode_x &lt;- dens$x[mode_i]
  mode_y &lt;- dens$y[mode_i]
  c(mode_x, mode_y)
})))
colnames(d_mode) &lt;- c(&#39;X&#39;, &#39;Y&#39;)

p &lt;- ggplot()
p &lt;- p + theme_bw(base_size=18)
p &lt;- p + geom_line(data=d_melt, aes(x=value, group=X), stat=&#39;density&#39;, color=&#39;black&#39;, alpha=0.4)
p &lt;- p + geom_segment(data=d_mode, aes(x=X, xend=X, y=Y, yend=0), color=&#39;black&#39;, linetype=&#39;dashed&#39;, alpha=0.4)
p &lt;- p + geom_rug(data=d_mode, aes(x=X), sides=&#39;b&#39;)
p &lt;- p + labs(x=&#39;value&#39;, y=&#39;density&#39;)
p</code></pre>
<div class="figure" style="text-align: center"><span id="fig:fig54left"></span>
<img src="/post/2019-01-22-rstan-wonderful-r-multi-lm_files/figure-html/fig54left-1.png" alt="每名學生的出勤率隨機誤差的分佈" width="80%" />
<p class="caption">
Figure 5: 每名學生的出勤率隨機誤差的分佈
</p>
</div>
<p>實際上我們只需要選取每名學生模型計算獲得的事後隨機誤差的代表值，比如可以是平均值，中央值，或者是MAP值（事後確率最大推定値，maximum a posteriori estimate），來觀察就可以了：</p>
<pre class="r"><code>s_dens &lt;- density(ms$s)
s_MAP &lt;- s_dens$x[which.max(s_dens$y)]
bw &lt;- 0.01
p &lt;- ggplot(data=d_mode, aes(x=X))
p &lt;- p + theme_bw(base_size=18)
p &lt;- p + geom_histogram(binwidth=bw, color=&#39;black&#39;, fill=&#39;white&#39;)
p &lt;- p + geom_density(eval(bquote(aes(y=..count..*.(bw)))), alpha=0.5, color=&#39;black&#39;, fill=&#39;gray20&#39;)
p &lt;- p + stat_function(fun=function(x) nrow(d)*bw*dnorm(x, mean=0, sd=s_MAP), linetype=&#39;dashed&#39;)
p &lt;- p + labs(x=&#39;value&#39;, y=&#39;count&#39;)
p &lt;- p + xlim(range(density(d_mode$X)$x))
p</code></pre>
<pre><code>## Warning: Removed 2 rows containing missing values (geom_bar).</code></pre>
<div class="figure" style="text-align: center"><span id="fig:fig54right"></span>
<img src="/post/2019-01-22-rstan-wonderful-r-multi-lm_files/figure-html/fig54right-1.png" alt="每名學生事後出勤率隨機誤差的MAP值的柱狀圖，和相應的概率密度函數（灰色鐘罩），點狀虛線是均值爲0，標準差是模型計算的事後隨機誤差標準差的 MAP 值的正態分佈的形狀。" width="80%" />
<p class="caption">
Figure 6: 每名學生事後出勤率隨機誤差的MAP值的柱狀圖，和相應的概率密度函數（灰色鐘罩），點狀虛線是均值爲0，標準差是模型計算的事後隨機誤差標準差的 MAP 值的正態分佈的形狀。
</p>
</div>
</div>
<div id="step-4.-mcmc-" class="section level2">
<h2>Step 4. MCMC 樣本的散點圖矩陣</h2>
<pre class="r"><code>library(ggplot2)
library(GGally)
library(hexbin)


d &lt;- data.frame(b1=ms$b1, b2=ms$b2, b3=ms$b3, sigma=ms$sigma, mu1=ms$mu[,1], mu50=ms$mu[,50], lp__=ms$lp__)
N_col &lt;- ncol(d)
ggp &lt;- ggpairs(d, upper=&#39;blank&#39;, diag=&#39;blank&#39;, lower=&#39;blank&#39;)

label_list &lt;- list(b1=&#39;b1&#39;, b2=&#39;b2&#39;, b3=&#39;b3&#39;, sigma=&#39;sigma&#39;, mu1=&#39;mu[1]&#39;, mu50=&#39;mu[50]&#39;, lp__=&#39;lp__&#39;)
for(i in 1:N_col) {
  x &lt;- d[,i]
  bw &lt;- (max(x)-min(x))/10
  p &lt;- ggplot(data.frame(x), aes(x))
  p &lt;- p + theme_bw(base_size=14)
  p &lt;- p + theme(axis.text.x=element_text(angle=60, vjust=1, hjust=1))
  p &lt;- p + geom_histogram(binwidth=bw, fill=&#39;white&#39;, color=&#39;grey5&#39;)
  p &lt;- p + geom_line(eval(bquote(aes(y=..count..*.(bw)))), stat=&#39;density&#39;)
  p &lt;- p + geom_label(data=data.frame(x=-Inf, y=Inf, label=label_list[[colnames(d)[i]]]), aes(x=x, y=y, label=label), hjust=0, vjust=1)
  ggp &lt;- putPlot(ggp, p, i, i)
}

zcolat &lt;- seq(-1, 1, length=81)
zcolre &lt;- c(zcolat[1:40]+1, rev(zcolat[41:81]))

for(i in 1:(N_col-1)) {
  for(j in (i+1):N_col) {
    x &lt;- as.numeric(d[,i])
    y &lt;- as.numeric(d[,j])
    r &lt;- cor(x, y, method=&#39;spearman&#39;, use=&#39;pairwise.complete.obs&#39;)
    zcol &lt;- lattice::level.colors(r, at=zcolat, col.regions=grey(zcolre))
    textcol &lt;- ifelse(abs(r) &lt; 0.4, &#39;grey20&#39;, &#39;white&#39;)
    ell &lt;- ellipse::ellipse(r, level=0.95, type=&#39;l&#39;, npoints=50, scale=c(.2, .2), centre=c(.5, .5))
    p &lt;- ggplot(data.frame(ell), aes(x=x, y=y))
    p &lt;- p + theme_bw() + theme(
      plot.background=element_blank(),
      panel.grid.major=element_blank(), panel.grid.minor=element_blank(),
      panel.border=element_blank(), axis.ticks=element_blank()
    )
    p &lt;- p + geom_polygon(fill=zcol, color=zcol)
    p &lt;- p + geom_text(data=NULL, x=.5, y=.5, label=100*round(r, 2), size=6, col=textcol)
    ggp &lt;- putPlot(ggp, p, i, j)
  }
}

for(j in 1:(N_col-1)) {
  for(i in (j+1):N_col) {
    x &lt;- d[,j]
    y &lt;- d[,i]
    p &lt;- ggplot(data.frame(x, y), aes(x=x, y=y))
    p &lt;- p + theme_bw(base_size=14)
    p &lt;- p + theme(axis.text.x=element_text(angle=60, vjust=1, hjust=1))
    p &lt;- p + geom_hex()
    p &lt;- p + scale_fill_gradientn(colours=gray.colors(7, start=0.1, end=0.9))
    ggp &lt;- putPlot(ggp, p, i, j)
  }
}
ggp</code></pre>
<div class="figure" style="text-align: center"><span id="fig:fig55"></span>
<img src="/post/2019-01-22-rstan-wonderful-r-multi-lm_files/figure-html/fig55-1.png" alt="MCMC樣本的事後矩陣。對角線上是各個參數事後樣本的柱狀圖和相應的概率密度曲線。右上角是各個參數事後樣本之間的 Spearman 秩相關係數。左下角是兩兩的散點圖。" width="80%" />
<p class="caption">
Figure 7: MCMC樣本的事後矩陣。對角線上是各個參數事後樣本的柱狀圖和相應的概率密度曲線。右上角是各個參數事後樣本之間的 Spearman 秩相關係數。左下角是兩兩的散點圖。
</p>
</div>
</div>
</div>

      </div>

      


<div class="article-tags">
  
  <a class="btn btn-primary btn-outline" href="/tags/bayesian">Bayesian</a>
  
  <a class="btn btn-primary btn-outline" href="/tags/medical-statistics">Medical Statistics</a>
  
</div>



    </div>
  </div>

</article>



<div class="article-container article-widget">
  <div class="hr-light"></div>
  <h3>Related</h3>
  <ul>
    
    <li><a href="/post/simple-linear-regression-using-rstan/">Simple linear regression using Rstan--Rstan Wonderful R-(2)</a></li>
    
    <li><a href="/post/rstan-wonderful-r/">Rstan Wonderful R-(1)</a></li>
    
    <li><a href="/post/words-notes-and-sentences-that-may-be-useful/">Words, notes, and sentences that may be useful </a></li>
    
    <li><a href="/post/summer-project-schedule/">Summer Project Schedule</a></li>
    
    <li><a href="/post/construction-of-a-hypothesis-test/">徒手打造一個假設檢驗</a></li>
    
  </ul>
</div>



<div class="container article-widget">
  <nav>
  <ul class="pager">
    
    <li class="previous"><a href="https://winterwang.github.io/post/simple-linear-regression-using-rstan/"><span
      aria-hidden="true">&larr;</span> Simple linear regression using Rstan--Rstan Wonderful R-(2)</a></li>
    

    
    <li class="next"><a href="https://winterwang.github.io/post/logistic-rstan/">Rstan Wonderful R-(4) <span
      aria-hidden="true">&rarr;</span></a></li>
    
  </ul>
</nav>

</div>


<div class="article-container">
  
<section id="comments">
  <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "ccwang" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>


</div>

<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; 2017-2019 Chaochen Wang | 王超辰 &middot; 

      Powered by the
      <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
      <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    
    <script id="dsq-count-scr" src="//ccwang.disqus.com/count.js" async></script>
    

    

    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.js" integrity="sha512-lInM/apFSqyy1o6s89K4iQUKg6ppXEgsVxT35HbzUupEVRh2Eu9Wdl4tHj7dZO0s1uvplcYGmt3498TtHq+log==" crossorigin="anonymous"></script>
    
    
    <script src="/js/hugo-academic.js"></script>
    

    
    
      
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
      

      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
    </script>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML" integrity="sha512-tOav5w1OjvsSJzePRtt2uQPFwBoHt1VZcUq8l8nm5284LEKE9FSJBQryzMBzHxY5P0zRdNqEcpLIRVYFNgu1jw==" crossorigin="anonymous"></script>
    
    

  </body>
</html>

