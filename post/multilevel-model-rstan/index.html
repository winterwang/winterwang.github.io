<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.55.6" />
  <meta name="author" content="王 超辰 (Chaochen Wang)">
  <meta name="description" content="Assistant Professor">

  
  <link rel="alternate" hreflang="en-us" href="https://winterwang.github.io/post/multilevel-model-rstan/">

  
  


  

  
  
  
  
    
  
  
    
    
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
    
  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.1/css/academicons.min.css" integrity="sha512-NThgw3XKQ1absAahW6to7Ey42uycrVvfNfyjqcFNgCmOCQ5AR4AO0SiXrN+8ZtYeappp56lk1WtvjVmEa+VR6A==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.css" integrity="sha512-M2wvCLH6DSRazYeZRIm1JnYyh22purTM+FDB5CsyxtQJYeKq83arPe5wgbNmcFXGqiSH2XR8dT/fJISVA1r/zQ==" crossorigin="anonymous">
  
  
  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Playfair&#43;Display:400,700%7cFauna&#43;One">
  
  <link rel="stylesheet" href="/styles.css">
  

  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-21867861-1', 'auto');
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  

  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://winterwang.github.io/post/multilevel-model-rstan/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="Be ambitious">
  <meta property="og:url" content="https://winterwang.github.io/post/multilevel-model-rstan/">
  <meta property="og:title" content="等級線性回歸模型的 Rstan 貝葉斯實現 | Be ambitious">
  <meta property="og:description" content=""><meta property="og:image" content="https://winterwang.github.io/img/052816_bayesian-opener_free.jpg">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2019-08-16T00:00:00&#43;00:00">
  
  <meta property="article:modified_time" content="2019-08-16T00:00:00&#43;00:00">
  

  

  <title>等級線性回歸模型的 Rstan 貝葉斯實現 | Be ambitious</title>

</head>
<body id="top" data-spy="scroll" data-target="#toc" data-offset="71" >

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">Be ambitious</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      
      <ul class="nav navbar-nav navbar-right">
        

        

        
          
        

        <li class="nav-item">
          <a href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        

        
          
        

        <li class="nav-item">
          <a href="/#publications_selected">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        

        
          
        

        <li class="nav-item">
          <a href="/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        

        
          
        

        <li class="nav-item">
          <a href="/#projects">
            
            <span>Projects</span>
            
          </a>
        </li>

        
        

        

        
          
        

        <li class="nav-item">
          <a href="/#teaching">
            
            <span>Teaching</span>
            
          </a>
        </li>

        
        

        

        
          
        

        <li class="nav-item">
          <a href="/#slides">
            
            <span>Presentations/slides</span>
            
          </a>
        </li>

        
        

        

        
          
        

        <li class="nav-item">
          <a href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  
<div class="article-header">
  <img src="/img/052816_bayesian-opener_free.jpg" class="article-banner" itemprop="image">
  <span class="article-header-caption">Thomas Bayes</span>
</div>



  <div class="article-container">
    <div class="article-inner">
      <h1 itemprop="name">等級線性回歸模型的 Rstan 貝葉斯實現</h1>

      

<div class="article-metadata">

  <span class="article-date">
    
    <time datetime="2019-08-16 00:00:00 &#43;0000 UTC" itemprop="datePublished">
      Fri, Aug 16, 2019
    </time>
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    19 min read
  </span>
  

  
  
  <span class="middot-divider"></span>
  <a href="https://winterwang.github.io/post/multilevel-model-rstan/#disqus_thread"></a>
  

  
  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fa fa-folder"></i>
    
    <a href="/categories/bayesian">Bayesian</a
    >, 
    
    <a href="/categories/r-techniques">R techniques</a
    >, 
    
    <a href="/categories/statistics">statistics</a
    >
    
  </span>
  
  

  
  
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=%e7%ad%89%e7%b4%9a%e7%b7%9a%e6%80%a7%e5%9b%9e%e6%ad%b8%e6%a8%a1%e5%9e%8b%e7%9a%84%20Rstan%20%e8%b2%9d%e8%91%89%e6%96%af%e5%af%a6%e7%8f%be&amp;url=https%3a%2f%2fwinterwang.github.io%2fpost%2fmultilevel-model-rstan%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fwinterwang.github.io%2fpost%2fmultilevel-model-rstan%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-facebook"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fwinterwang.github.io%2fpost%2fmultilevel-model-rstan%2f&amp;title=%e7%ad%89%e7%b4%9a%e7%b7%9a%e6%80%a7%e5%9b%9e%e6%ad%b8%e6%a8%a1%e5%9e%8b%e7%9a%84%20Rstan%20%e8%b2%9d%e8%91%89%e6%96%af%e5%af%a6%e7%8f%be"
         target="_blank" rel="noopener">
        <i class="fa fa-linkedin"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=https%3a%2f%2fwinterwang.github.io%2fpost%2fmultilevel-model-rstan%2f&amp;title=%e7%ad%89%e7%b4%9a%e7%b7%9a%e6%80%a7%e5%9b%9e%e6%ad%b8%e6%a8%a1%e5%9e%8b%e7%9a%84%20Rstan%20%e8%b2%9d%e8%91%89%e6%96%af%e5%af%a6%e7%8f%be"
         target="_blank" rel="noopener">
        <i class="fa fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=%e7%ad%89%e7%b4%9a%e7%b7%9a%e6%80%a7%e5%9b%9e%e6%ad%b8%e6%a8%a1%e5%9e%8b%e7%9a%84%20Rstan%20%e8%b2%9d%e8%91%89%e6%96%af%e5%af%a6%e7%8f%be&amp;body=https%3a%2f%2fwinterwang.github.io%2fpost%2fmultilevel-model-rstan%2f">
        <i class="fa fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>


      <div class="article-style" itemprop="articleBody">
        

<div id="TOC">
<ul>
<li><a href="#-multilevelmixed-effect-regression-model">多層(等級)線性回歸模型/混合效應模型 multilevel/mixed effect regression model</a><ul>
<li><a>適用於等級線性回歸模型的數據</a></li>
<li><a>確認數據分佈</a></li>
<li><a>如果不考慮組間(公司間)差異</a></li>
<li><a>如果要考慮組間差異</a></li>
</ul></li>
<li><a>等級線性回歸的貝葉斯實現</a><ul>
<li><a href="#-mechanism">模型機制 mechanism</a></li>
</ul></li>
</ul>
</div>

<div id="-multilevelmixed-effect-regression-model" class="section level2">
<h2>多層(等級)線性回歸模型/混合效應模型 multilevel/mixed effect regression model</h2>
<p>關於等級線性回歸的基本知識和概念，請參考<a href="https://wangcc.me/LSHTMlearningnote/Hierarchical.html">讀書筆記58-60章節</a>。簡單來說，等級線性回歸通過給數據內部可能存在或者已知存在的結構或者層級增加隨機截距或者隨機斜率的方式來輔助解釋組間差異和組內的差異。</p>
<div class="section level3">
<h3>適用於等級線性回歸模型的數據</h3>
<p>本章節使用的數據是四家大公司40名社員的年齡和年收入數據：</p>
<pre class="r"><code>d &lt;- read.csv(file=&#39;../../static/files/data-salary-2.txt&#39;)
d</code></pre>
<pre><code>##     X   Y KID
## 1   7 457   1
## 2  10 482   1
## 3  16 518   1
## 4  25 535   1
## 5   5 427   1
## 6  25 603   1
## 7  26 610   1
## 8  18 484   1
## 9  17 508   1
## 10  1 380   1
## 11  5 453   1
## 12  4 391   1
## 13 19 559   1
## 14 10 453   1
## 15 21 517   1
## 16 12 553   2
## 17 17 653   2
## 18 22 763   2
## 19  9 538   2
## 20 18 708   2
## 21 21 740   2
## 22  6 437   2
## 23 15 646   2
## 24  4 422   2
## 25  7 444   2
## 26 10 504   2
## 27  2 376   2
## 28 15 522   3
## 29 27 623   3
## 30 14 515   3
## 31 18 542   3
## 32 20 529   3
## 33 18 540   3
## 34 11 411   3
## 35 26 666   3
## 36 22 641   3
## 37 25 592   3
## 38 28 722   4
## 39 24 726   4
## 40 22 728   4</code></pre>
<ul>
<li><code>X</code>: 社員年齡減去23獲得的數據（23歲是大部分人大學畢業入職時的年齡）</li>
<li><code>Y</code>: 年收入（萬日元）</li>
<li><code>KID</code>: 公司編號</li>
</ul>
<p>我們認爲，年收入 <code>Y</code>，是基本平均年收入和隨機誤差（服從均值爲零，方差是 <span class="math inline">\(\sigma^2\)</span> 的正態分佈）之和。且基本平均年收入和年齡成正比（年功序列型企業）。但是呢，因爲不同的公司入職時的基本收入可能不同，且可能隨着年齡增加而增長薪水的速度可能也不一樣。那麼由於不同公司所造成的差異，可以被認爲是組間差異。</p>
</div>
<div class="section level3">
<h3>確認數據分佈</h3>
<p>這次分析的目的是要瞭解「每個公司<code>KID</code>內隨着年齡的增加而增長的薪水幅度是多少」，那麼我們要在結果報告中體現的就是每家公司的基本年收入，新入職時的年收入，以及隨着年齡增長而上升的薪水的事後分佈。</p>
<p>我們先來看把四家公司職員放在一起時的整體圖形：</p>
<pre class="r"><code>library(ggplot2)

d$KID &lt;- as.factor(d$KID)
res_lm &lt;- lm(Y ~ X, data=d)
coef &lt;- as.numeric(res_lm$coefficients)

p &lt;- ggplot(d, aes(X, Y, shape=KID))
p &lt;- p + theme_bw(base_size=18)
p &lt;- p + geom_abline(intercept=coef[1], slope=coef[2], size=2, alpha=0.3)
p &lt;- p + geom_point(size=2)
p &lt;- p + scale_shape_manual(values=c(16, 2, 4, 9))
p &lt;- p + labs(x=&#39;X (age-23)&#39;, y=&#39;Y (10,000 Yen/year)&#39;)
p</code></pre>
<div class="figure" style="text-align: center"><span id="fig:fig8-1"></span>
<img src="/post/2019-08-16-multilevel-model-rstan_files/figure-html/fig8-1-1.png" alt="年齡和年收入的散點圖，不同點的形狀代表不同的公司編號。" width="80%" />
<p class="caption">
Figure 1: 年齡和年收入的散點圖，不同點的形狀代表不同的公司編號。
</p>
</div>
<p>從總體的散點圖 <a href="#fig:fig8-1">1</a> 來看，似乎年收入確實是隨着年齡增長而呈現直線增加的趨勢。但是公司編號 <code>KID = 4</code> 的三名社員薪水似乎是在同一水平的並無明顯變化。這一點可以把四家公司社員的數據分開來看更加清晰:</p>
<pre class="r"><code>p &lt;- ggplot(d, aes(X, Y, shape=KID))
p &lt;- p + theme_bw(base_size=20)
p &lt;- p + geom_abline(intercept=coef[1], slope=coef[2], size=2, alpha=0.3)
p &lt;- p + facet_wrap(~KID)
p &lt;- p + geom_line(stat=&#39;smooth&#39;, method=&#39;lm&#39;, se=FALSE, size=1, color=&#39;black&#39;, linetype=&#39;31&#39;, alpha=0.8)
p &lt;- p + geom_point(size=3)
p &lt;- p + scale_shape_manual(values=c(16, 2, 4, 9))
p &lt;- p + labs(x=&#39;X (age-23)&#39;, y=&#39;Y (10,000 Yen/year)&#39;)
p</code></pre>
<div class="figure" style="text-align: center"><span id="fig:fig8-2"></span>
<img src="/post/2019-08-16-multilevel-model-rstan_files/figure-html/fig8-2-1.png" alt="年齡和年收入的散點圖，不同的公司在四個平面中展示,黑色點線是每家公司數據單獨使用線性回歸時獲得的直線。" width="80%" />
<p class="caption">
Figure 2: 年齡和年收入的散點圖，不同的公司在四個平面中展示,黑色點線是每家公司數據單獨使用線性回歸時獲得的直線。
</p>
</div>
</div>
<div class="section level3">
<h3>如果不考慮組間(公司間)差異</h3>
<ul>
<li>模型的數學描述</li>
</ul>
<p><span class="math display">\[
\begin{aligned}
              Y[n] &amp; = y_{\text{base}}[n] + \varepsilon[n] &amp; n = 1, \dots, N \\
y_{\text{base}}[n] &amp; = a + bX[n]                           &amp; n = 1, \dots, N \\
    \varepsilon[n] &amp; \sim \text{Normal}(0, \sigma_Y^2)     &amp; n = 1, \dots, N \\
\end{aligned}
\]</span></p>
<p>當然，如果你想，模型可以直接簡化成：</p>
<p><span class="math display">\[
Y[n] \sim \text{Normal}(a + bX[n], \sigma^2_Y) \;\;\;\;\;\; n = 1, \dots, N \\
\]</span></p>
<p>上述簡化版的模型，翻譯成Stan語言如下：</p>
<pre><code>data {
  int N;
  real X[N];
  real Y[N];
}

parameters{
  real a;
  real b;
  real&lt;lower = 0&gt; s_Y;
}

model {
  for (n in 1 : N)
  Y[n] = normal(a + b * X[n], s_Y);
}
</code></pre>
<p>下面的 R 代碼用來實現對上面 Stan 模型的擬合:</p>
<pre class="r"><code>library(rstan)
d &lt;- read.csv(file=&#39;../../static/files/data-salary-2.txt&#39;)
d$KID &lt;- as.factor(d$KID)

data &lt;- list(N=nrow(d), X=d$X, Y=d$Y)
fit &lt;- stan(file=&#39;stanfiles/model8-1.stan&#39;, data=data, seed=1234)</code></pre>
<pre><code>## 
## SAMPLING FOR MODEL &#39;model8-1&#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 1.3e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.061556 seconds (Warm-up)
## Chain 1:                0.04322 seconds (Sampling)
## Chain 1:                0.104776 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &#39;model8-1&#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 3e-06 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.068632 seconds (Warm-up)
## Chain 2:                0.04646 seconds (Sampling)
## Chain 2:                0.115092 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &#39;model8-1&#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 4e-06 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.074084 seconds (Warm-up)
## Chain 3:                0.042106 seconds (Sampling)
## Chain 3:                0.11619 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &#39;model8-1&#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 4e-06 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.091418 seconds (Warm-up)
## Chain 4:                0.037644 seconds (Sampling)
## Chain 4:                0.129062 seconds (Total)
## Chain 4:</code></pre>
<pre class="r"><code>fit</code></pre>
<pre><code>## Inference for Stan model: model8-1.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##         mean se_mean    sd    2.5%     25%     50%     75%   97.5% n_eff
## a     375.30    0.56 23.05  329.60  359.95  375.48  390.43  420.51  1692
## b      11.10    0.03  1.35    8.42   10.20   11.11   11.97   13.75  1680
## s_Y    68.26    0.19  7.88   54.73   62.72   67.76   73.07   85.78  1747
## lp__ -184.02    0.03  1.22 -187.36 -184.54 -183.71 -183.14 -182.66  1488
##      Rhat
## a       1
## b       1
## s_Y     1
## lp__    1
## 
## Samples were drawn using NUTS(diag_e) at Sat Aug 17 09:44:25 2019.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).</code></pre>
<p>現在有更加方便的 <code>rstanarm</code> 包可以幫助我們省去寫 Stan 模型的過程：</p>
<pre class="r"><code>library(rstanarm)

rstanarm_results = stan_glm(Y ~ X, data=d, iter=2000, warmup=1000, cores=4)
summary(rstanarm_results, probs=c(.025, .975), digits=3)</code></pre>
<pre><code>## 
## Model Info:
## 
##  function:     stan_glm
##  family:       gaussian [identity]
##  formula:      Y ~ X
##  algorithm:    sampling
##  priors:       see help(&#39;prior_summary&#39;)
##  sample:       4000 (posterior sample size)
##  observations: 40
##  predictors:   2
## 
## Estimates:
##                 mean     sd       2.5%     97.5% 
## (Intercept)    375.532   24.482  326.341  424.144
## X               11.066    1.401    8.381   13.822
## sigma           67.969    8.108   54.265   86.110
## mean_PPD       547.524   15.362  517.413  577.476
## log-posterior -235.159    1.259 -238.296 -233.724
## 
## Diagnostics:
##               mcse  Rhat  n_eff
## (Intercept)   0.400 1.000 3755 
## X             0.023 1.000 3621 
## sigma         0.130 1.000 3893 
## mean_PPD      0.250 1.000 3769 
## log-posterior 0.031 1.002 1689 
## 
## For each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).</code></pre>
<p>可以看到強制不同公司社員的年收入來自同一個正態分佈時，方差顯得非常的大。</p>
</div>
<div class="section level3">
<h3>如果要考慮組間差異</h3>
<p>我們認爲每家公司社員新入職時的起點薪水不同(截距不同-隨機截距)，進入公司之後隨年齡增加的薪水幅度也不同(斜率不同-隨機斜率)。因此，用 <span class="math inline">\(a[1]\sim a[K], K = 1, 2, 3, 4\)</span> 表示每家公司的截距，用 <span class="math inline">\(b[1] \sim b[K], K = 1, 2, 3, 4\)</span> 表示每家公司薪水上升的斜率。那麼每家公司的薪水年齡線性回歸模型可以寫作是 <span class="math inline">\(a[K] + b[K] X, K = 1, 2, 3, 4\)</span></p>
<ul>
<li>模型數學描述</li>
</ul>
<p><span class="math display">\[
Y[n] \sim \text{Normal}(a[\text{KID[n]}] + b[\text{KID}[n]] X[n], \sigma^2_Y) \\ n = 1, \dots, N
\]</span></p>
<p>上述模型的 Stan 譯文如下：</p>
<pre><code>data {
  int N;
  int K;
  real X[N];
  real Y[N];
  int&lt;lower = 1, upper = K&gt; KID[N];
}

parameters {
  real a[K];
  real b[K];
  real&lt;lower = 0&gt; s_Y; 
}

model {
  for (n in 1:N)
  Y[n] ~ normal(a[KID[n]] + b[KID[n]] * X[n], s_Y);
}
</code></pre>
<p>下面的 R 代碼用來實現上面貝葉斯多組不同截距不同斜率線性回歸模型的擬合:</p>
<pre class="r"><code>library(rstan)
d &lt;- read.csv(file=&#39;../../static/files/data-salary-2.txt&#39;)

data &lt;- list(N=nrow(d), X=d$X, Y=d$Y, KID = d$KID, K = 4)
fit &lt;- stan(file=&#39;stanfiles/model8-2.stan&#39;, data=data, seed=1234)</code></pre>
<pre><code>## 
## SAMPLING FOR MODEL &#39;model8-2&#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 1.8e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.405729 seconds (Warm-up)
## Chain 1:                0.233254 seconds (Sampling)
## Chain 1:                0.638983 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &#39;model8-2&#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 6e-06 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.397525 seconds (Warm-up)
## Chain 2:                0.239473 seconds (Sampling)
## Chain 2:                0.636998 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &#39;model8-2&#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 7e-06 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.417931 seconds (Warm-up)
## Chain 3:                0.239022 seconds (Sampling)
## Chain 3:                0.656953 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &#39;model8-2&#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 4e-06 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.380292 seconds (Warm-up)
## Chain 4:                0.252886 seconds (Sampling)
## Chain 4:                0.633178 seconds (Total)
## Chain 4:</code></pre>
<pre class="r"><code>fit</code></pre>
<pre><code>## Inference for Stan model: model8-2.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##         mean se_mean     sd    2.5%     25%     50%     75%   97.5% n_eff
## a[1]  386.61    0.28  14.45  357.34  377.00  386.89  396.27  415.10  2717
## a[2]  329.71    0.34  16.69  296.71  318.64  329.41  340.45  363.05  2398
## a[3]  314.71    0.62  33.71  248.80  291.88  314.63  337.27  381.26  2910
## a[4]  746.96    3.39 163.77  420.66  639.47  744.75  852.60 1079.82  2339
## b[1]    7.54    0.02   0.89    5.79    6.95    7.52    8.14    9.26  2712
## b[2]   19.78    0.02   1.23   17.38   18.95   19.81   20.58   22.18  2501
## b[3]   12.42    0.03   1.68    9.04   11.29   12.45   13.54   15.73  2873
## b[4]   -0.87    0.14   6.61  -14.20   -5.09   -0.88    3.47   12.33  2316
## s_Y    27.33    0.07   3.68   21.17   24.75   26.95   29.48   35.81  2909
## lp__ -148.17    0.06   2.41 -153.91 -149.51 -147.78 -146.40 -144.58  1519
##      Rhat
## a[1]    1
## a[2]    1
## a[3]    1
## a[4]    1
## b[1]    1
## b[2]    1
## b[3]    1
## b[4]    1
## s_Y     1
## lp__    1
## 
## Samples were drawn using NUTS(diag_e) at Sat Aug 17 13:06:52 2019.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).</code></pre>
</div>
</div>
<div class="section level2">
<h2>等級線性回歸的貝葉斯實現</h2>
<div id="-mechanism" class="section level3">
<h3>模型機制 mechanism</h3>
<p>如果我們認爲每家公司的起點薪水 <span class="math inline">\(a[k]\)</span> 服從正態分佈，且該正態分佈的平均值是全體公司的起點薪水的均值 <span class="math inline">\(a_\mu\)</span>，方差是 <span class="math inline">\(\sigma^2_a\)</span>。類似地，假設每家公司內隨着年齡增長而增加薪水的幅度 <span class="math inline">\(b[k]\)</span> 也服從某個正態分佈，均值和方差分別是 <span class="math inline">\(b_\mu, \sigma^2_b\)</span>。這樣我們就不僅僅是允許了各家公司的薪水年齡回歸直線擁有不同的斜率和截距，還對這些隨機斜率和截距的前概率分佈進行了設定。</p>
<p>此時，隨機效應模型的數學表達式就可以寫成下面這樣:</p>
<p><span class="math display">\[
\begin{aligned}
Y[n] &amp;\sim \text{Normal}(a[\text{KID[n]}] + b[\text{KID}[n]] X[n], \sigma^2_Y) &amp; n = 1, \dots, N \\
a[k] &amp;= a_\mu + a_\varepsilon[k]   &amp; k = 1, \dots, K \\
a_\varepsilon[k] &amp; \sim \text{Normal}(0, \sigma^2_a) &amp; k = 1, \dots, K \\
b[k] &amp; = b_\mu + b_\varepsilon[k]  &amp; k = 1, \dots, K \\
b_\varepsilon[k] &amp;\sim \text{Normal}(0, \sigma^2_b) &amp; k = 1, \dots, K
\end{aligned}
\]</span></p>
<p>使用 <code>rstanarm</code> 包可以使用下面的代碼實現</p>
<pre class="r"><code>library(rstanarm)
M1_stanlmer &lt;- stan_lmer(formula = Y ~ X  + (X | KID), 
                            data = d,
                            seed = 1234)</code></pre>
<pre><code>## 
## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0.000134 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.34 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 7.7395 seconds (Warm-up)
## Chain 1:                2.46336 seconds (Sampling)
## Chain 1:                10.2029 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 2.3e-05 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.23 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 7.53278 seconds (Warm-up)
## Chain 2:                2.84168 seconds (Sampling)
## Chain 2:                10.3745 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 4.1e-05 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.41 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 7.02717 seconds (Warm-up)
## Chain 3:                3.764 seconds (Sampling)
## Chain 3:                10.7912 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 4.5e-05 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.45 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 11.8397 seconds (Warm-up)
## Chain 4:                2.97051 seconds (Sampling)
## Chain 4:                14.8102 seconds (Total)
## Chain 4:</code></pre>
<pre class="r"><code>print(M1_stanlmer, digits = 2)</code></pre>
<pre><code>## stan_lmer
##  family:       gaussian [identity]
##  formula:      Y ~ X + (X | KID)
##  observations: 40
## ------
##             Median MAD_SD
## (Intercept) 359.53  14.79
## X            12.77   2.94
## 
## Auxiliary parameter(s):
##       Median MAD_SD
## sigma 29.78   3.76 
## 
## Error terms:
##  Groups   Name        Std.Dev. Corr 
##  KID      (Intercept) 22.69         
##           X            8.38    -0.22
##  Residual             30.21         
## Num. levels: KID 4 
## 
## Sample avg. posterior predictive distribution of y:
##          Median MAD_SD
## mean_PPD 548.15   6.71
## 
## ------
## * For help interpreting the printed output see ?print.stanreg
## * For info on the priors used see ?prior_summary.stanreg</code></pre>
<pre class="r"><code>summary(M1_stanlmer, 
        pars = c(&quot;(Intercept)&quot;, &quot;X&quot;,&quot;sigma&quot;, 
                 &quot;Sigma[KID:(Intercept),(Intercept)]&quot;,
                 &quot;Sigma[KID:X,(Intercept)]&quot;, &quot;Sigma[KID:X,X]&quot;),
        probs = c(0.025, 0.975),
        digits = 2)</code></pre>
<pre><code>## 
## Model Info:
## 
##  function:     stan_lmer
##  family:       gaussian [identity]
##  formula:      Y ~ X + (X | KID)
##  algorithm:    sampling
##  priors:       see help(&#39;prior_summary&#39;)
##  sample:       4000 (posterior sample size)
##  observations: 40
##  groups:       KID (4)
## 
## Estimates:
##                                      mean    sd      2.5%    97.5%
## (Intercept)                         359.28   16.91  325.12  391.37
## X                                    12.69    3.85    3.91   20.47
## sigma                                30.21    3.81   23.84   38.49
## Sigma[KID:(Intercept),(Intercept)]  514.96 1198.00    1.59 3292.97
## Sigma[KID:X,(Intercept)]            -42.04  142.28 -351.27  123.35
## Sigma[KID:X,X]                       70.28  105.02    6.79  331.93
## 
## Diagnostics:
##                                    mcse  Rhat  n_eff
## (Intercept)                         0.30  1.00 3206 
## X                                   0.11  1.00 1239 
## sigma                               0.08  1.00 2251 
## Sigma[KID:(Intercept),(Intercept)] 31.88  1.00 1412 
## Sigma[KID:X,(Intercept)]            3.15  1.00 2045 
## Sigma[KID:X,X]                      2.81  1.01 1398 
## 
## For each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).</code></pre>
<p>和非貝葉斯版本的概率論隨機效應線性回歸模型的結果相對比一下：</p>
<pre class="r"><code>library(lme4)
M1 &lt;- lmer(formula = Y ~ X  + (X | KID), 
           data = d, 
           REML = TRUE)
summary(M1)</code></pre>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: Y ~ X + (X | KID)
##    Data: d
## 
## REML criterion at convergence: 387.2
## 
## Scaled residuals: 
##      Min       1Q   Median       3Q      Max 
## -2.36969 -0.51837 -0.03545  0.76358  1.87881 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev. Corr 
##  KID      (Intercept) 503.78   22.445        
##           X            28.53    5.341   -1.00
##  Residual             833.95   28.878        
## Number of obs: 40, groups:  KID, 4
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)  358.207     15.383  23.286
## X             13.067      2.741   4.767
## 
## Correlation of Fixed Effects:
##   (Intr)
## X -0.848
## convergence code: 0
## boundary (singular) fit: see ?isSingular</code></pre>
</div>
</div>

      </div>

      


<div class="article-tags">
  
  <a class="btn btn-primary btn-outline" href="/tags/bayesian">Bayesian</a>
  
  <a class="btn btn-primary btn-outline" href="/tags/medical-statistics">Medical Statistics</a>
  
</div>



    </div>
  </div>

</article>



<div class="article-container article-widget">
  <div class="hr-light"></div>
  <h3>Related</h3>
  <ul>
    
    <li><a href="/post/poisson-stan/">泊松回歸模型的貝葉斯Stan實現</a></li>
    
    <li><a href="/post/logistic-rstan2/">Rstan Wonderful R-(5)</a></li>
    
    <li><a href="/post/logistic-rstan/">Rstan Wonderful R-(4)</a></li>
    
    <li><a href="/post/rstan-wonderful-r3/">Rstan Wonderful R-(3)</a></li>
    
    <li><a href="/post/simple-linear-regression-using-rstan/">Simple linear regression using Rstan--Rstan Wonderful R-(2)</a></li>
    
  </ul>
</div>



<div class="container article-widget">
  <nav>
  <ul class="pager">
    
    <li class="previous"><a href="https://winterwang.github.io/post/entrance-done/"><span
      aria-hidden="true">&larr;</span> 房子還是只有地基</a></li>
    

    
  </ul>
</nav>

</div>


<div class="article-container">
  
<section id="comments">
  <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "ccwang" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>


</div>

<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; 2017-2019 Chaochen Wang | 王超辰 &middot; 

      Powered by the
      <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
      <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    
    <script id="dsq-count-scr" src="//ccwang.disqus.com/count.js" async></script>
    

    

    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.js" integrity="sha512-lInM/apFSqyy1o6s89K4iQUKg6ppXEgsVxT35HbzUupEVRh2Eu9Wdl4tHj7dZO0s1uvplcYGmt3498TtHq+log==" crossorigin="anonymous"></script>
    
    
    <script src="/js/hugo-academic.js"></script>
    

    
    
      
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
      

      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
    </script>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML" integrity="sha512-tOav5w1OjvsSJzePRtt2uQPFwBoHt1VZcUq8l8nm5284LEKE9FSJBQryzMBzHxY5P0zRdNqEcpLIRVYFNgu1jw==" crossorigin="anonymous"></script>
    
    

  </body>
</html>

